query,ids,prompt,rag_response
"Given the GPU memory constraints across different engines, what specific technical limitation forces a particular deployment pattern for multi-engine model inference?","[(16389, 'fc9440e2-8dcc-42ad-96db-380f1fa02d4c'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (11298, 'e265f736-1fce-44dc-83cd-0e039d2d5f03'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (9256, '7f549d96-c927-4084-9071-7979bb44293e'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (17456, '3d0b462d-4f6f-4e29-bdf7-d84ce48e8560'), (14392, 'a2810719-2d62-4548-b722-d214c70b2bb9'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (19526, '790c4385-8da2-4412-bf50-64e4a5ffadd3'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (21076, 'f7b2eb77-c2fc-4c18-926b-38ac4c905bda'), (23642, '683e3a08-8ea3-4d72-a020-1876a44a65ed'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (5724, 'ae2f389b-62aa-4c28-8801-d9c732341e58'), (5723, '111bcbbf-0f74-4848-9a80-b24f02299fe0'), (23648, '3b33fc73-d107-47f5-b569-17fb196f4213'), (5729, '8325f704-8f9e-470a-9d07-777ab1401244'), (30308, '3c03744c-6269-49dd-8fef-5a61c47d6282'), (5733, 'd7b94280-d5e4-4cbe-8307-82acc7c174e7'), (30310, '831de66f-4e5c-47ad-b209-de1901609d89'), (30311, 'e0a52fe8-a4c9-4de7-8857-c3c0299d9c2e'), (16486, '44b89854-89ae-4b45-9b98-62b3d9edc5a7'), (30313, '2dcddbd0-be46-4804-b5cb-0503231ad849'), (5738, '1b3f6207-1eaf-4d30-b7f5-15ce5479c98a'), (5739, 'f5d36f0c-9b14-42d7-ae2e-93a729880b58'), (30319, 'a04b6e09-b517-497d-bf98-2432285b7d43'), (19568, '6b7c6d4d-3fa2-445d-89f4-171fb9e18a91'), (16498, 'a0be24ac-e9ed-4da3-b810-dcceddc0929b'), (30323, '549f9335-f914-4c3d-b063-c1eeb94fb9ea'), (26229, '81993a76-b643-4458-bf4c-17b0dbd5017a'), (30327, 'a62abcc1-500d-4f5e-a758-64fcef6ee3e7'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (4217, '1a8e3437-ea12-4e7b-b9d9-6929a9a940d2'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (10369, 'db79384c-eaa9-489a-b9d0-97a5b2723de0'), (18062, '6bf329e4-a764-400f-8a3e-d39885c58f97'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (10388, '57cd8299-58e1-486b-868d-3c23e03aaf11'), (17045, 'e20ede76-eeae-4f93-b5d6-e6926918562c'), (18073, '156a236c-723e-4010-9498-eaff060870d7'), (10393, '43e5459d-ae41-4d26-bc74-84e225d947f9'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (24230, 'bd10f584-b63e-4b9f-89e3-194c6bdb0f54'), (6335, '7e912b49-1ada-42b1-93aa-cb5905d48dfa'), (26855, 'daaa5d9b-d90e-4dcb-b7be-640944232958'), (28908, '777723f1-2176-4d81-92fd-310f4af75f5d'), (25841, '944d2878-310e-4406-a580-e27546617a5b'), (25842, '5b8c1753-2875-46c1-bc03-2f045ea3f2d1'), (24826, '9ef2464e-432b-4b80-9f22-265e35fcc810'), (25852, 'e68a74a5-1c65-4315-9cac-e0006e78744c'), (21255, '39264857-6445-4484-8671-b1636338871c'), (21257, '4e2019bc-0e34-4fb0-8ac0-d159bad34fad'), (21258, '779c7ef6-7888-4895-abef-b218722f5819'), (21260, '6c4d3255-fad8-4e92-a376-84c4058c5cd7'), (21266, '92adf75c-feec-4fca-a3d6-d7ec3ab3c977'), (16660, '1dfbc513-b29b-4f26-ac16-6bccdb48a3d4'), (2836, 'dad2f3b4-337f-49e3-ae8a-2714f6bb79e0'), (21270, '350cad2f-bffd-4158-957e-c42776bfcb21'), (16661, 'f6db86f2-9435-45f8-814c-5423a566c4e4'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (21274, '6dc1af30-a439-4f39-baba-fb652d70f065'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (16682, '720a39dd-fc05-469b-bf80-d43c9b1c1cc8'), (16685, '7c6e8e31-1bcd-4645-a528-f25eb43d88d2'), (21805, 'b66d8d5b-534a-49b7-9bdc-0473323226b3'), (16686, 'c8c886fb-9d61-452a-bd92-dbffcff7db90'), (1328, '83e4417a-430e-4f49-bd41-8194f2f9f1de'), (1329, '2f47f7b5-e760-450f-9b8d-6d09dab7133a'), (21819, '7efc7f5e-7c99-47a3-aba5-8758a1889d6b'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (31556, '4e8903fe-1f83-478a-8981-808e95896798'), (14663, '4814da72-44c5-416d-84cf-7d56aa314256'), (31559, '90654692-0b80-4f6c-a648-22c9c20254cb'), (31573, 'f7c3e7a7-e05e-474d-8b5b-cfeafb26a2b9'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (26974, '4b9a17a1-5440-4d6e-a28c-a8955b88fae6'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (4977, '61eb50ab-f72b-4e58-82d7-c7b9c056c9fb'), (14706, '332b6a8d-ec91-483f-a989-0ec3c2d81a1c'), (1398, '509f9c15-2115-49e4-8ec4-e9e5176b6456'), (23931, '8f9f128a-7da6-4f3e-ac03-82ff4dad7ee6'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (24457, 'b32d2172-8d5c-4609-a629-ac9c292249dd'), (5513, 'b3a8369f-1492-44a4-b5d2-a4218a4c58c8'), (23438, '123931a8-6608-4336-86da-61b0be5ad3d2'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (7064, '284ad4d1-ec71-4b4f-b163-bbf70b5378f9'), (411, '96253f57-9a1a-4672-829b-a8e8241df3cf'), (24476, '7416abe4-8708-4b38-a154-27508d996c92'), (7592, '68ddeb8e-b285-47f9-8754-09b4fd273d73'), (7595, 'e9a0cc5c-98d6-4e8d-8b64-1c28f8f15d45'), (7596, '61cfe7c8-b21a-4aa4-b4c3-9612db7eabb3'), (7605, '6a31a75f-b73e-4756-8d17-1be0db7c661c'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (10696, 'd7e8042d-bfce-457e-a6e7-9a1041ca49c7'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (10719, 'edba7a55-76e0-4cbd-ae70-be42abfec02a'), (26080, '9b91b646-fa9f-4bb6-af8a-938897f96a99'), (28642, '06226230-6d80-432a-8d7d-5d796e9b8d05'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (14844, '4bd36fd8-e2e4-49a1-b5ca-f9683414fc32')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. Respond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

For me, what’s lacking is a [custom terraform provider](https://www.hashicorp.com/blog/writing-custom-terraform-providers). It’s all well and good deploying an inference endpoint from a [GitHub action](https://github.com/features/actions) using hugie, as we do, but it would be better if we could use the awesome state machine that is terraform to keep track of these. I’m pretty sure that someone (if not Hugging Face) will write one soon enough — if not, we will.

### Hosting multiple models on a single endpoint

Philipp Schmid posted a really nice blog about how to write a custom [Endpoint Handler](https://www.philschmid.de/multi-model-inference-endpoints) class to allow you to host multiple models on a single endpoint, potentially saving you quite a bit of money. His blog was about GPU inference, and the only real limitation is how many models you can fit into the GPU memory. I assume this will also work for CPU instances, though I’ve not tried yet.

## To conclude…
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
> As a conclusion, it is important to remember that model quantization trades improved memory efficiency against accuracy and in some cases inference time.

If GPU memory is not a constraint for your use case, there is often no need to look into quantization. However many GPUs simply can't run LLMs without quantization methods and in this case, 4-bit and 8-bit quantization schemes are extremely useful tools.

For more in-detail usage information, we strongly recommend taking a look at the [Transformers Quantization Docs](https://huggingface.co/docs/transformers/main_classes/quantization#general-usage).
Next, let's look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture.

# 2. Flash Attention: A Leap Forward

Today's top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers.
> As a conclusion, it is important to remember that model quantization trades improved memory efficiency against accuracy and in some cases inference time.

If GPU memory is not a constraint for your use case, there is often no need to look into quantization. However many GPUs simply can't run LLMs without quantization methods and in this case, 4-bit and 8-bit quantization schemes are extremely useful tools.

For more in-detail usage information, we strongly recommend taking a look at the [Transformers Quantization Docs](https://huggingface.co/docs/transformers/main_classes/quantization#general-usage).
Next, let's look into how we can improve computational and memory efficiency by using better algorithms and an improved model architecture.

## 2. Flash Attention

Today's top-performing LLMs share more or less the same fundamental architecture that consists of feed-forward layers, activation layers, layer normalization layers, and most crucially, self-attention layers.
Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the way for their deployment on inference endpoints
serving multiple requests in parallel.

There is still plenty of room for improvement though:
- in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory.
Alternative options such as pipelining are currently integrated,
- the static sequence length limits the model ability to encode long contexts. It would be interesting to see if attention sinks might be a valid option to address this.
Accelerated inference on NVIDIA GPUs

By default, ONNX Runtime runs inference on CPU devices. However, it is possible to place supported operations on an NVIDIA GPU, while leaving any unsupported ones on CPU. In most cases, this allows costly operations to be placed on GPU and significantly accelerate inference.

This guide will show you how to run inference on two execution providers that ONNX Runtime supports for NVIDIA GPUs:

* `CUDAExecutionProvider`: Generic acceleration on NVIDIA CUDA-enabled GPUs.
* `TensorrtExecutionProvider`: Uses NVIDIA’s [TensorRT](https://developer.nvidia.com/tensorrt) inference engine and generally provides the best runtime performance.

<Tip warning={true}>

Due to a limitation of ONNX Runtime, it is not possible to run quantized models on `CUDAExecutionProvider` and only models with static quantization can be run on `TensorrtExecutionProvider`.

</Tip>

## CUDAExecutionProvider

### CUDA installation
### Inference speed and memory footprint

While recent years have seen increasingly large ML models achieve high performance on a large variety of tasks and benchmarks, deploying these multi-billion parameter models in practice can be a challenge in itself, and many organizations lack the resources for this. This is why considering the **inference speed** and **memory footprint** of models is important, especially when doing online model evaluation.

Inference speed refers to the time that it takes for a model to make a prediction -- this will vary depending on the hardware used and the way in which models are queried, e.g. in real time via an API or in batch jobs that run once a day.

Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# GPU inference

GPUs are the standard choice of hardware for machine learning, unlike CPUs, because they are optimized for memory bandwidth and parallelism. To keep up with the larger sizes of modern models or to run these large models on existing and older hardware, there are several optimizations you can use to speed up GPU inference. In this guide, you'll learn how to use FlashAttention-2 (a more memory-efficient attention mechanism), BetterTransformer (a PyTorch native fastpath execution), and bitsandbytes to quantize your model to a lower precision. Finally, learn how to use 🤗 Optimum to accelerate inference with ONNX Runtime on Nvidia and AMD GPUs.

<Tip>

The majority of the optimizations described here also apply to multi-GPU setups!

</Tip>

## FlashAttention-2

<Tip>
### Training at scale

Training at scale can be challenging. The first challenge is fitting the model and its optimizer states on the available GPU devices. The amount of GPU memory a single parameter takes depends on its “precision” (or more specifically `dtype`). The most common `dtype` being `float32` (32-bit), `float16`, and `bfloat16` (16-bit). More recently “exotic” precisions are supported out-of-the-box for training and inference (with certain conditions and constraints) such as `int8` (8-bit). In a nutshell, to load a model on a GPU device each billion parameters costs 4GB in float32 precision, 2GB in float16, and 1GB in int8. If you would like to learn more about this topic, have a look at this blogpost which dives deeper: [https://huggingface.co/blog/hf-bitsandbytes-integration](https://huggingface.co/blog/hf-bitsandbytes-integration).
```

To load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you want to allocate to each GPU. For example, to distribute 1GB of memory to the first GPU and 2GB of memory to the second GPU:

```py
max_memory_mapping = {0: ""1GB"", 1: ""2GB""}
model_name = ""bigscience/bloom-3b""
model_8bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=""auto"", load_in_8bit=True, max_memory=max_memory_mapping
)
```

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/66_optimum_inference/results.png"" alt=""Latency & F1 results""/>
</figure>

We managed to accelerate our model latency from `117.61ms` to `64.94ms` or roughly 2x while keeping `99.61%` of the accuracy. Something we should keep in mind is that we used a mid-performant CPU instance with 2 physical cores. By switching to GPU or a more performant CPU instance, e.g. [ice-lake powered you can decrease the latency number down to a few milliseconds.](https://huggingface.co/blog/bert-cpu-scaling-part-2#more-efficient-ai-processing-on-latest-intel-ice-lake-cpus)

## 4. Current Limitations

We just started supporting inference in [https://github.com/huggingface/optimum](https://github.com/huggingface/optimum) so we would like to share current limitations as well. All of those limitations are on the roadmap and will be resolved in the near future.
```

- The model should fit on 16GB GPU for inference. For training/fine-tuning it would take much more GPU RAM. Adam
  optimizer for example makes four copies of the model: model, gradients, average and squared average of the gradients.
  So it would need at least 4x model size GPU memory, even with mixed precision as gradient updates are in fp32. This
  is not including the activations and data batches, which would again require some more GPU RAM. So one should explore
  solutions such as DeepSpeed, to train/fine-tune the model. Another option is to use the original codebase to
  train/fine-tune the model on TPU and then convert the model to Transformers format for inference. Instructions for
  that could be found [here](https://github.com/kingoflolz/mesh-transformer-jax/blob/master/howto_finetune.md)
In the above calculations, we didn't consider memory required for intermediate activation checkpointing which is considerably huge. We leverage Flash Attention V2 and Gradient Checkpointing to overcome this issue. 

1. For QLoRA along with flash attention V2 and gradient checkpointing, the total memory occupied by the model on a single A100 40GB GPU is **26 GB** with a **batch size of 4**.
2. For full fine-tuning using FSDP along with Flash Attention V2 and Gradient Checkpointing, the memory occupied per GPU ranges between **70 GB to 77.6 GB** with a **per_gpu_batch_size of 1**.

Please refer to the [model-memory-usage](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) to easily calculate how much vRAM is needed to train and perform big model inference on a model hosted on the 🤗 Hugging Face Hub.

## Full Finetuning
```

By using `device_map=""auto""` the attention layers would be equally distributed over all available GPUs.

In this guide, we will use [bigcode/octocoder](https://huggingface.co/bigcode/octocoder) as it can be run on a single 40 GB A100 GPU device chip. Note that all memory and speed optimizations that we will apply going forward, are equally applicable to models that require model or tensor parallelism.

Since the model is loaded in bfloat16 precision, using our rule of thumb above, we would expect the memory requirement to run inference with `bigcode/octocoder` to be around 31 GB VRAM. Let's give it a try.

We first load the model and tokenizer and then pass both to Transformers' [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) object.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
```

Since for inference there is no need for additional large memory used by the optimizer states and the gradients you
should be able to fit much larger batches and/or sequence length onto the same hardware.

Additionally DeepSpeed is currently developing a related product called Deepspeed-Inference which has no relationship
to the ZeRO technology, but instead uses tensor parallelism to scale models that can't fit onto a single GPU. This is a
work in progress and we will provide the integration once that product is complete.


### Memory Requirements

Since Deepspeed ZeRO can offload memory to CPU (and NVMe) the framework provides utils that allow one to tell how much CPU and GPU memory will be needed depending on the number of GPUs being used.

Let's estimate how much memory is needed to finetune ""bigscience/T0_3B"" on a single GPU:
```

By using `device_map=""auto""` the attention layers would be equally distributed over all available GPUs.

In this notebook, we will use [bigcode/octocoder](https://huggingface.co/bigcode/octocoder) as it can be run on a single 40 GB A100 GPU device chip. Note that all memory and speed optimizations that we will apply going forward, are equally applicable to models that require model or tensor parallelism.

Since the model is loaded in bfloat16 precision, using our rule of thumb above, we would expect the memory requirement to run inference with `bigcode/octocoder` to be around 31 GB VRAM. Let's give it a try.

We first load the model and tokenizer and then pass both to Transformers' [pipeline](https://huggingface.co/docs/transformers/main_classes/pipelines) object.

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
```

Finally, if you have multiple devices available to you, you can distribute the workload using [Tensor Parallelism](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many#tensor-parallelism) and obtain lower latency. With Tensor Parallelism, you split the memory bandwidth burden across multiple devices, but you now have to consider inter-device communication bottlenecks in addition to the monetary cost of running multiple devices. The benefits depend largely on the model size: models that easily fit on a single consumer device see very limited benefits. Taking the results from this [DeepSpeed blog post](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/), you see that you can spread a 17B parameter model across 4 GPUs to reduce the latency by 1.5x (Figure 7).
```

### 4-bit

To load a model in 4-bit for inference, use the `load_in_4bit` parameter. The `device_map` parameter is optional, but we recommend setting it to `""auto""` to allow 🤗 Accelerate to automatically and efficiently allocate the model given the available resources in the environment.

```py
from transformers import AutoModelForCausalLM

model_name = ""bigscience/bloom-2b5""
model_4bit = AutoModelForCausalLM.from_pretrained(model_name, device_map=""auto"", load_in_4bit=True)
```

To load a model in 4-bit for inference with multiple GPUs, you can control how much GPU RAM you want to allocate to each GPU. For example, to distribute 600MB of memory to the first GPU and 1GB of memory to the second GPU:

```py
max_memory_mapping = {0: ""600MB"", 1: ""1GB""}
model_name = ""bigscience/bloom-3b""
model_4bit = AutoModelForCausalLM.from_pretrained(
    model_name, device_map=""auto"", load_in_4bit=True, max_memory=max_memory_mapping
)
2. DeepSpeed-Inference also uses custom CUDA kernels to avoid allocating too much memory and doing tensor copying to and from GPUs. The effect of this is lesser memory requirements and fewer kernel starts which improves the throughput and allows for bigger batch sizes leading to higher overall throughput.

If you are interested in more examples you can take a look at [Accelerate GPT-J inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/gptj-deepspeed-inference) or [Accelerate BERT inference with DeepSpeed-Inference on GPUs](https://www.philschmid.de/bert-deepspeed-inference).

## Deepspeed ZeRO-Inference


[Deepspeed ZeRO](https://www.deepspeed.ai/tutorials/zero/) uses a magical sharding approach which can take almost any model and scale it across a few or hundreds of GPUs and the do training or inference on it.

### Setup
```

We see that already a relatively small batch size almost fills up our GPU's entire memory. However, a larger batch size 
can often result in faster model convergence or better end performance. So ideally we want to tune the batch size to our
model's needs and not to the GPU limitations. What's interesting is that we use much more memory than the size of the model. 
To understand a bit better why this is the case let's have a look at a model's operations and memory needs.

## Anatomy of Model's Operations

Transformers architecture includes 3 main groups of operations grouped below by compute-intensity.

1. **Tensor Contractions**

    Linear layers and components of Multi-Head Attention all do batched **matrix-matrix multiplications**. These operations are the most compute-intensive part of training a transformer.

2. **Statistical Normalizations**
Here is the question: Given the GPU memory constraints across different engines, what specific technical limitation forces a particular deployment pattern for multi-engine model inference?
Here is answer 1:The requirement for Unity's PhysX system to maintain consistent layer interactions requires splitting model stages across GPUs with 15GB VRAM each
Here is answer 2: Due to Godot's TCP server architecture requiring dedicated GPU memory, inference must be distributed across multiple GPUs
Here is answer3: When Blender's admin-level GPU access combines with Unity's engine-wide layer constraints, Stage 1 must be allocated to a primary GPU while Stages 2/3 share a secondary GPU with exactly 15GB VRAM
Here is answer 4: The combination of Unity's layer constraints, Godot's TCP requirements, and Blender's real-time updates necessitates a single GPU with at least 40GB VRAM",Answer 4
What technical requirement emerges from implementing cross-engine documentation while maintaining proper GPU resource allocation?,"[(30208, '3574f580-c52d-4005-876f-ffae04aab5fe'), (16389, 'fc9440e2-8dcc-42ad-96db-380f1fa02d4c'), (25615, '01944eb1-009d-4629-9882-c62631a911ec'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (14875, '863f500b-8805-487f-a9d0-1a5268b9eb36'), (11292, '8a991fa4-93a3-42c9-a43b-ed7420db4d07'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (24608, '1f19bd41-f5e6-4343-bde3-8d14025739cc'), (1058, '6cec5090-3461-44a3-a5b6-bdc99a04fd47'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (10276, 'debee365-bf66-467b-9618-eea6f12bffa9'), (10277, 'ca5bca8f-2e79-4905-8308-919d5c1a5a51'), (29733, 'ac5d581e-6d2b-4aa2-9577-fed18fbe08f4'), (4654, 'fbbddf5c-d10a-4009-8b85-ad5a1a8b7fd5'), (29744, '6954830f-0ba6-4c15-a60d-3cd7e635d535'), (6197, '36c5cec3-a6ae-4270-8e58-75e974db78d1'), (10300, '4c09aeb7-71b2-4501-8a66-fd42121a6a51'), (10304, '7939d5d1-b5ad-4196-8744-4b704b4f89f4'), (19526, '790c4385-8da2-4412-bf50-64e4a5ffadd3'), (7239, '4cdbdf79-0082-4f64-b036-4fa23b1f13cb'), (19531, 'f5ef36fb-4b52-4e2c-9c4b-7c0f909869b8'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (77, '2f91eea2-7777-4217-9a99-89b6541598d5'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (10328, '5116f8df-31d7-4927-9198-f82e48242ea2'), (21081, 'e0a3e5ea-3ee7-4a1a-bb3a-5ea4032d7685'), (9306, '3dbc94d7-ff5d-4397-8ed8-7d27b3955e2d'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (7774, '5824b2b4-f100-4de8-a61c-bf6e28cb0bf5'), (5729, '8325f704-8f9e-470a-9d07-777ab1401244'), (6260, '0ca41fbf-d89c-4c12-beaa-bb503334fa52'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (14457, '24a91ac0-d333-49c9-a9a6-d6908a59a520'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (10366, 'a14dbb50-f0a6-449b-9861-c78b85c9e4f1'), (4734, '45277eff-6f73-4fa7-ae08-09ab8f76bc8a'), (10373, 'dfdde91b-de27-4d21-80c3-e40d04093ae3'), (9865, '8b6631c1-22b3-4b9d-bd6a-fdb950d87d51'), (10389, 'a8a284e8-5f91-4b84-ae4b-bc995fb2ccc7'), (10391, '5642d227-e91b-4c72-a718-62b6c1cb847c'), (10393, '43e5459d-ae41-4d26-bc74-84e225d947f9'), (10394, 'a838ccff-aa78-4fcb-a870-9502ccb8e9d9'), (10397, 'c9710061-56b6-4b17-bac5-419a6ab68e7d'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (12965, '49181bcd-f818-41f4-be75-bb559aa682d5'), (12966, '9cf957c5-904b-4743-8a3b-c98ffe28eb27'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (28349, '7a08fcf4-4a28-4b14-bd63-ad16725963f9'), (19135, 'd743a82b-46bc-41fd-98b0-40e850c520b4'), (6335, '7e912b49-1ada-42b1-93aa-cb5905d48dfa'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (10960, '9a3bd870-6c14-4877-a99d-81160bce8e55'), (25308, '8b6f2168-8617-45b3-a551-d649b58807c6'), (24297, '23e563a1-038a-46c4-9308-9752e26b3551'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (11004, '4264f9e2-877b-4f6a-9094-a66b999c44a8'), (20228, '80c0b1f0-1831-4c92-887c-c94602e01bd3'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (2840, 'a7a5b6a3-c3fc-42f7-8acd-d505fc631ec4'), (15648, 'f58ba64c-38eb-47ea-9da1-86ef9f6ad5ed'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (21805, 'b66d8d5b-534a-49b7-9bdc-0473323226b3'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (29501, '672f1de2-afa6-4605-b508-293f01e7d666'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (14663, '4814da72-44c5-416d-84cf-7d56aa314256'), (1865, '3dae4aea-d38e-4f26-abc6-846747817fa5'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (8541, 'a6a556dd-c62f-4664-b405-3d29166739c4'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (13158, 'd5de80e9-25fb-4ef5-8868-7bb0688c1611'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (14707, '653a154c-0baf-4e34-98d6-d2362d781d53'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (14712, '1861bc32-ca30-45b0-bdb8-516bd6dd3448'), (5508, '18f70486-80e2-4c2d-a2f1-f9e6174711dd'), (5509, '509fc88e-4d96-444c-8ea2-74d4ce6ee3d0'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (5014, '4c63896c-82a3-4862-9e85-f4ec53318607'), (29598, '78d2e6f1-098e-4154-9814-d3529bde1b1a'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (10661, '01ce8b20-a130-4f43-a3f0-d0b15ddcd48c'), (10665, 'e14680b1-7738-4c0b-a87e-9a53b513187f'), (18857, 'cf1102bb-031b-4d60-9c5b-0c0a10c61c11'), (25002, '22fbe702-faab-4fce-b201-61d82588743f'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (6071, 'c319c8be-67ec-412e-90d4-1616197a8123'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (27080, 'b8868dda-97f7-46ea-836c-1762469db76d'), (12240, 'fbb7afd4-5b55-4581-93cc-d2b01ffb45c2'), (27603, '444e61be-d56f-4a1d-b57c-07660c2ec5b3'), (10710, 'de151053-2ea6-4820-900c-be4504b58b61'), (10720, 'da7ecd0b-ac5c-4985-b64f-10bcbd788583'), (28645, 'e45197bc-16dd-4f3e-a65a-638beda2fd35'), (28646, '01aa3f64-595f-4dfb-ac16-fbaad8a24452'), (12261, '1f488997-08ee-403b-a36f-c98cabddd129'), (28647, 'd02afa5d-00f3-427d-9bb9-077fbdf21f7f'), (19947, '5a6d24f6-3f7e-48fb-a9b8-27c6edd2afec'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (20463, '4cd629c7-7e89-48ab-aadc-dcbd3e778504')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. Respond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
## Considerations for Effective Autoscaling

While autoscaling offers convenient resource management, certain considerations should be kept in mind to ensure its effectiveness:

- **Model Initialization Time**: During the initialization of a new replica, the model is downloaded and loaded into memory. If your replicas have a long initialization time, autoscaling may not be as effective. This is because the average GPU utilization might fall below the threshold during that time, triggering the automatic scaling down of your endpoint.

- **Enterprise Plan Control**: If you have an [enterprise plan](https://huggingface.co/inference-endpoints/enterprise), you have full control over the autoscaling definitions. This allows you to customize the scaling thresholds, behavior and criteria based on your specific requirements.

## Scaling to 0
Overall, this requires less GPUs to serve all distinct models, even though we already had a way to share GPUs between deployments to maximize their compute usage. In a **2min** time frame, there are approximately **10** distinct LoRA weights that are requested. Instead of spawning 10 deployments, and keeping them warm, we simply serve all of them with 1 to 2 GPUs (or more if there is a request burst).


## Implementation

We implemented LoRA mutualization in the Inference API. When a request is performed on a model available in our platform, we first determine whether this is a LoRA or not. We then identify the base model for the LoRA and route the request to a common backend farm, with the ability to serve requests for the said model. Inference requests get served by keeping the base model warm and loading/unloading LoRAs on the fly. This way we can ultimately reuse the same compute resources to serve many distinct models at once.

### LoRA structure
This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM). 
That repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular, it 
contains a hybrid model parallel approach using ""tensor parallel"" and ""pipeline parallel"" techniques.

## Usage tips

We have provided pretrained [GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m) checkpoints
for use to evaluate or finetuning downstream tasks.

To access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)
Registry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).

Alternatively, you can directly download the checkpoints using:
Now that each component fits individually into both CPU and GPU memory,
we need to make sure that components have all the CPU and GPU memory for
themselves when needed.

Diffusers supports modularly loading individual components i.e. we can
load the text encoder without loading the UNet. This modular loading
will ensure that we only load the component we need at a given step in
the pipeline to avoid exhausting the available CPU RAM and GPU VRAM.

Let\'s give it a try 🚀

![t2i_64](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/if/t2i_64.png)

## Available resources

The free-tier Google Colab comes with around 13 GB CPU RAM:

``` python
!grep MemTotal /proc/meminfo
We keep these steps in mind while moving on to actually getting these running which is described in the following sections.

## Training Stable Diffusion with DDPO

### Setup

To get started, when it comes to the hardware side of things and this implementation of DDPO, at the very least access to an A100 NVIDIA GPU is required for successful training. Anything below this GPU type will soon run into Out-of-memory issues.

Use pip to install the `trl` library

```bash
pip install trl[diffusers]
In this blog post, you will learn how to easily deploy `GPT-J` using [Amazon SageMaker](https://aws.amazon.com/de/sagemaker/) and the [Hugging Face Inference Toolkit](https://github.com/aws/sagemaker-huggingface-inference-toolkit) with a few lines of code for scalable, reliable, and secure real-time inference using a regular size GPU instance with NVIDIA T4 (~500$/m). 

But before we get into it, I want to explain why deploying `GPT-J` into production is challenging. 

---

## Background

The weights of the 6 billion parameter model represent a ~24GB memory footprint. To load it in float32, one would need at least 2x model size CPU RAM: 1x for initial weights and another 1x to load the checkpoint. So for `GPT-J` it would require at least 48GB of CPU RAM to just load the model.
```

Instead of loading our pipeline directly to GPU, we instead enable smart CPU offloading which 
can be achieved with the [`enable_model_cpu_offload` function](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet#diffusers.StableDiffusionControlNetPipeline.enable_model_cpu_offload).

Remember that during inference diffusion models, such as Stable Diffusion require not just one but multiple model components that are run sequentially.
In the case of Stable Diffusion with ControlNet, we first use the CLIP text encoder, then the diffusion model unet and control net, then the VAE decoder and finally run a safety checker.
Most components are only run once during the diffusion process and are thus not required to occupy GPU memory all the time. By enabling smart model offloading, we make sure 
that each component is only loaded into GPU when it's needed so that we can significantly save memory consumption without significantly slowing down infenence.
Let's depict the GPU requirements in the following table:


| n gpus | decorator                      |
|--------+--------------------------------|
| `>= 0` | `@require_torch`               |
| `>= 1` | `@require_torch_gpu`           |
| `>= 2` | `@require_torch_multi_gpu`     |
| `< 2`  | `@require_torch_non_multi_gpu` |
| `< 3`  | `@require_torch_up_to_2_gpus`  |


For example, here is a test that must be run only when there are 2 or more GPUs available and pytorch is installed:

```python no-style
@require_torch_multi_gpu
def test_example_with_multi_gpu():
It is very important that before you start the porting process, you can **efficiently** debug code in the original
repository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or
even a pull request in the original repository. The maintainers of this repository are most likely very happy about
someone looking into their code!

At this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original
model. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to
dive into the original repository and also when starting to write the 🤗 Transformers implementation of the model. Only
at the very end, when the model has already been successfully ported to 🤗 Transformers, one should verify that the
model also works as expected on GPU.

In general, there are two possible debugging environments for running the original model
```

**Performance tuning:**

- enabling `offload_optimizer` should reduce GPU RAM usage (it requires `""stage"": 2`)
- `""overlap_comm"": true` trades off increased GPU RAM usage to lower all-reduce latency. `overlap_comm` uses 4.5x
  the `allgather_bucket_size` and `reduce_bucket_size` values. So if they are set to 5e8, this requires a 9GB
  footprint (`5e8 x 2Bytes x 2 x 4.5`). Therefore, if you have a GPU with 8GB or less RAM, to avoid getting
  OOM-errors you will need to reduce those parameters to about `2e8`, which would require 3.6GB. You will want to do
  the same on larger capacity GPU as well, if you're starting to hit OOM.
- when reducing these buffers you're trading communication speed to avail more GPU RAM. The smaller the buffer size is,
  the slower the communication gets, and the more GPU RAM will be available to other tasks. So if a bigger batch size is
  important, getting a slightly slower training time could be a good trade.
[`~StableDiffusionPipeline.enable_sequential_cpu_offload`] is a stateful operation that installs hooks on the models.

</Tip>

## Model offloading

<Tip>

Model offloading requires 🤗 Accelerate version 0.17.0 or higher.

</Tip>

[Sequential CPU offloading](#cpu-offloading) preserves a lot of memory but it makes inference slower because submodules are moved to GPU as needed, and they're immediately returned to the CPU when a new module runs.

Full-model offloading is an alternative that moves whole models to the GPU, instead of handling each model's constituent *submodules*. There is a negligible impact on inference time (compared with moving the pipeline to `cuda`), and it still provides some memory savings.

During model offloading, only one of the main components of the pipeline (typically the text encoder, UNet and VAE)
is placed on the GPU while the others wait on the CPU. Components like the UNet that run for multiple iterations stay on the GPU until they're no longer needed.
It is very important that before you start the porting process, that you
can **efficiently** debug code in the original repository! Also,
remember that you are working with an open-source library, so do not
hesitate to open an issue, or even a pull request in the original
repository. The maintainers of this repository are most likely very
happy about someone looking into their code!

At this point, it is really up to you which debugging environment and
strategy you prefer to use to debug the original model. We strongly
advise against setting up a costly GPU environment, but simply work on a
CPU both when starting to dive into the original repository and also
when starting to write the 🤗 Transformers implementation of the model.
Only at the very end, when the model has already been successfully
ported to 🤗 Transformers, one should verify that the model also works as
expected on GPU.

In general, there are two possible debugging environments for running
the original model
It is very important that before you start the porting process, that you
can **efficiently** debug code in the original repository! Also,
remember that you are working with an open-source library, so do not
hesitate to open an issue, or even a pull request in the original
repository. The maintainers of this repository are most likely very
happy about someone looking into their code!

At this point, it is really up to you which debugging environment and
strategy you prefer to use to debug the original model. We strongly
advise against setting up a costly GPU environment, but simply work on a
CPU both when starting to dive into the original repository and also
when starting to write the 🤗 Transformers implementation of the model.
Only at the very end, when the model has already been successfully
ported to 🤗 Transformers, one should verify that the model also works as
expected on GPU.

In general, there are two possible debugging environments for running
the original model
To make sure your DeepFloyd IF application runs in high performance in production, you may want to allocate and manage your resources wisely. In this respect, BentoML allows you to scale the Runners independently for each Stage. For example, you can use more Pods for your Stage 1 Runners or allocate more powerful GPU servers to them.

## Preparing the environment

[This GitHub repository](https://github.com/bentoml/IF-multi-GPUs-demo) stores all necessary files for this project. To run this project locally, make sure you have the following:

- Python 3.8+
- `pip` installed
- At least 2x16GB VRAM GPU or 1x40 VRAM GPU. For this project, we used a machine of type `n1-standard-16` from Google Cloud plus 64 GB of RAM and 2 NVIDIA T4 GPUs. Note that while it is possible to run IF on a single T4, it is not recommended for production-grade serving

Once the prerequisites are met, clone the project repository to your local machine and navigate to the target directory.
On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

### Supported models

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

## Conclusion and final words

In this blogpost we have presented the integration of the [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ) in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs. 

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.
```

This is handy when you want to run different tasks on different GPUs.

Some tests must be run on CPU-only, others on either CPU or GPU or TPU, yet others on multiple-GPUs. The following skip
decorators are used to set the requirements of tests CPU/GPU/TPU-wise:

- `require_torch` - this test will run only under torch
- `require_torch_gpu` - as `require_torch` plus requires at least 1 GPU
- `require_torch_multi_gpu` - as `require_torch` plus requires at least 2 GPUs
- `require_torch_non_multi_gpu` - as `require_torch` plus requires 0 or 1 GPUs
- `require_torch_up_to_2_gpus` - as `require_torch` plus requires 0 or 1 or 2 GPUs
- `require_torch_tpu` - as `require_torch` plus requires at least 1 TPU

Let's depict the GPU requirements in the following table:
![LLM](assets/96_hf_bitsandbytes_integration/LLM3.png)

Therefore, these models are hard to run on easily accessible devices. For example, just to do inference on BLOOM-176B, you would need to have 8x 80GB A100 GPUs (~$15k each). To fine-tune BLOOM-176B, you'd need 72 of these GPUs! Much larger models, like PaLM would require even more resources.

Because these huge models require so many GPUs to run, we need to find ways to reduce these requirements while preserving the model's performance. Various technologies have been developed that try to shrink the model size, you may have heard of quantization and distillation, and there are many others.
```

A big question that remains is how all the data and models will be distributed across several GPUs. This sounds like a complex task but actually only requires a single line of code with 🤗 Accelerate.

```Python
model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
    model, optimizer, train_dataloader, eval_dataloader)
# Technical Specifications [optional]

**Section Overview:** This section includes details about the model objective and architecture, and the compute infrastructure. It is useful for people interested in model development. Writing this section usually requires the model developer to be directly involved.


## Model Architecture and Objective

`model_specs`

## Compute Infrastructure

`compute_infrastructure`

### Hardware

`hardware_requirements`

_What are the minimum hardware requirements, e.g. processing, storage, and memory requirements?_

### Software

`software`


# Citation [optional]


**Section Overview:** The developers’ preferred citation for this model. This is often a paper.


### BibTeX 

`citation_bibtex`


### APA 

`citation_apa`


# Glossary [optional]


**Section Overview:** This section defines common terms and how metrics are calculated.


`glossary`

_Clearly define terms in order to be accessible across audiences._

# More Information [optional]
Here is the question: What technical requirement emerges from implementing cross-engine documentation while maintaining proper GPU resource allocation?
Here is answer 1:Documentation must be generated using Sphinx with language-specific builds, while GPU resources are managed through Poetry's virtual environments
Here is answer 2: Due to the HF_colliders extension requirements, documentation must include both engine-specific and shared components, with GPU allocation documented per deployment stage
Here is answer3: The combination of Sphinx documentation requirements, YouTube transcript API integration, and BentoML's GPU stage allocation necessitates separate documentation builds for each GPU configuration
Here is answer 4: Documentation must be structured to handle both engine-specific physics constraints and multi-GPU deployment patterns, with Sphinx generating separate builds for each supported language and GPU configuration","Here is the answer: Due to the HF_colliders extension requirements, documentation must include both engine-specific and shared components, with GPU allocation documented per deployment stage"
What specific technical limitation arises when implementing real-time model inference across multiple engines while maintaining proper documentation and subtitle support?,"[(2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (25117, '78f76fe1-619d-48ce-ae7d-0cc91aa67d16'), (19489, '43505cd9-a764-4a3f-8290-b2ce01a4ab4b'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (23137, 'bc2e25b5-8803-4791-b31b-dbe5cb0a633a'), (30308, '3c03744c-6269-49dd-8fef-5a61c47d6282'), (5733, 'd7b94280-d5e4-4cbe-8307-82acc7c174e7'), (21110, '3b4b59de-4da6-4c2a-89c8-597f807f0538'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (6787, 'bf27eed4-2ffe-4237-9812-088c33404198'), (30852, '140bd3bf-c87c-44f1-8231-5017fbe767e0'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (10388, '57cd8299-58e1-486b-868d-3c23e03aaf11'), (18069, '7fefe8d6-3011-423e-bdd6-14fcfd155d01'), (18072, '733709e1-83d6-441c-8a55-8f01ca879e4c'), (10393, '43e5459d-ae41-4d26-bc74-84e225d947f9'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (24230, 'bd10f584-b63e-4b9f-89e3-194c6bdb0f54'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8366, '802fb177-2a03-456c-b39c-e9d1d7f566ed'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (22209, 'd7ca43f5-90c6-448e-8d33-58c7acba90c4'), (20673, '0ad1dd04-3ec3-4c8f-9d67-b44414433153'), (20675, 'a7493216-179e-4052-bc5d-21d6e64fa6a3'), (21188, 'e53bd9c2-9844-411f-810d-5e35fa824c01'), (28356, 'b8b471a3-a61b-4ee5-9154-be89b6b87789'), (8390, '8c49470d-57df-498b-957a-3113863febe4'), (8391, '31b94b1d-9202-40c1-a45e-130821ef4321'), (20674, '546e66a5-8819-4bb6-bea8-35f0cbcdc601'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (21195, 'faa2b5eb-cd0a-4c23-a187-337ef7e890bd'), (10960, '9a3bd870-6c14-4877-a99d-81160bce8e55'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (14041, 'bfe9d047-a549-404a-a384-445fa6966aa2'), (25306, '9d30be4f-6f9f-4452-84ee-5219508fa7be'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (22766, '3ebc4752-059f-4ce0-ac98-1c73047459bb'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (30965, '6adffa38-ab02-41e9-a179-aaca1c2624e8'), (20730, 'b9f51b25-0a14-4b14-a1a6-92b1e0278fc3'), (18685, '52fdc5bf-09c1-40d7-85f6-9c586ebe5ef0'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (20740, '8ea0cbc1-6505-4aba-8eeb-7045546046c8'), (21255, '39264857-6445-4484-8671-b1636338871c'), (17160, '91bdea0b-d26a-4435-9b08-9f133f7cad52'), (17172, '21bca8c4-9aae-42ed-a43d-da87912fd598'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (17186, 'c18dd444-ad12-4a01-9b30-9dc2d5500852'), (17699, 'f896ccff-a53b-4e44-ab27-f7e12cc98e4a'), (5928, '46517678-0861-4d86-b92b-eb731ae44060'), (20777, '6b031a1e-1a0f-416a-865c-e7d8c0560fbf'), (16682, '720a39dd-fc05-469b-bf80-d43c9b1c1cc8'), (29487, '2321da63-59f7-4fbb-b56d-00810837d618'), (4403, 'e8bbbe12-29e1-4b33-9b38-8873d737fec4'), (21818, '73d48f15-6f37-4445-aa1f-d7942fd7ca06'), (30523, 'e801a7e2-9c3d-4d3e-a2a9-95e64422f62c'), (30528, '69aa4012-1b33-4373-af24-1d94ec8dd03d'), (30529, '7f1d7eaf-0acc-4c8d-a42c-32659a276651'), (14663, '4814da72-44c5-416d-84cf-7d56aa314256'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (21342, '43c34d2b-8a1a-4a72-8117-9d57107098c8'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (26978, '97c25830-edc8-4a62-a3ab-e265164d3ac1'), (26979, 'eb432651-3843-474d-a2df-74d722a2c569'), (4455, 'fc4d5145-f3dd-4855-be3f-9ebb4f928754'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (14706, '332b6a8d-ec91-483f-a989-0ec3c2d81a1c'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (21362, '651a9fca-5d13-4c33-9dd8-27cca38b640b'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (24457, 'b32d2172-8d5c-4609-a629-ac9c292249dd'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (19849, '6c574035-79ab-4b53-9e2c-1c285463bc09'), (23438, '123931a8-6608-4336-86da-61b0be5ad3d2'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (19856, '1779d346-5968-4964-a7b4-39c87c76dd12'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (16789, 'dcaee1f9-3cdd-4b24-bd0d-762f75437e1f'), (411, '96253f57-9a1a-4672-829b-a8e8241df3cf'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (29598, '78d2e6f1-098e-4154-9814-d3529bde1b1a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (17825, '25ff9fc5-3959-4e46-bc23-de61ba1996b6'), (6562, '12d6fc12-1e12-401d-abae-cc180802380d'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (26048, 'd97e24d6-8ad7-42ad-bdec-785ccf1ca169'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (26057, '9ed838e2-7b57-4bf0-ac30-97ec4268ad86'), (9685, 'df01de6d-5dc1-49a3-9b35-aa95906ba9e8'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (24024, '69ac2742-6a8f-4e63-a4cb-b469c11ea9a7'), (10719, 'edba7a55-76e0-4cbd-ae70-be42abfec02a'), (25056, 'e28bab38-e712-4c1f-a714-e224f1104de5'), (26080, '9b91b646-fa9f-4bb6-af8a-938897f96a99'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (15339, '9006a23a-10b3-4cd2-a0a0-5f95871a357d'), (1524, '5de887ce-b7a7-4223-994c-f8f3ad8e7ec3'), (25078, '7dc040fb-8163-4fd9-8c3f-36472a25c1a1')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. Respond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the way for their deployment on inference endpoints
serving multiple requests in parallel.

There is still plenty of room for improvement though:
- in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory.
Alternative options such as pipelining are currently integrated,
- the static sequence length limits the model ability to encode long contexts. It would be interesting to see if attention sinks might be a valid option to address this.
```

For me, what’s lacking is a [custom terraform provider](https://www.hashicorp.com/blog/writing-custom-terraform-providers). It’s all well and good deploying an inference endpoint from a [GitHub action](https://github.com/features/actions) using hugie, as we do, but it would be better if we could use the awesome state machine that is terraform to keep track of these. I’m pretty sure that someone (if not Hugging Face) will write one soon enough — if not, we will.

### Hosting multiple models on a single endpoint

Philipp Schmid posted a really nice blog about how to write a custom [Endpoint Handler](https://www.philschmid.de/multi-model-inference-endpoints) class to allow you to host multiple models on a single endpoint, potentially saving you quite a bit of money. His blog was about GPU inference, and the only real limitation is how many models you can fit into the GPU memory. I assume this will also work for CPU instances, though I’ve not tried yet.

## To conclude…
```

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/66_optimum_inference/results.png"" alt=""Latency & F1 results""/>
</figure>

We managed to accelerate our model latency from `117.61ms` to `64.94ms` or roughly 2x while keeping `99.61%` of the accuracy. Something we should keep in mind is that we used a mid-performant CPU instance with 2 physical cores. By switching to GPU or a more performant CPU instance, e.g. [ice-lake powered you can decrease the latency number down to a few milliseconds.](https://huggingface.co/blog/bert-cpu-scaling-part-2#more-efficient-ai-processing-on-latest-intel-ice-lake-cpus)

## 4. Current Limitations

We just started supporting inference in [https://github.com/huggingface/optimum](https://github.com/huggingface/optimum) so we would like to share current limitations as well. All of those limitations are on the roadmap and will be resolved in the near future.
- **Remote Models > 2GB:** Currently, only models smaller than 2GB can be loaded from the [Hugging Face Hub](https://hf.co/). We are working on adding support for models > 2GB / multi-file models.
- **Seq2Seq tasks/model:** We don’t have support for seq2seq tasks, like summarization and models like T5 mostly due to the limitation of the single model support. But we are actively working to solve it, to provide you with the same experience you are familiar with in transformers.
- **Past key values:** Generation models like GPT-2 use something called past key values which are precomputed key-value pairs of the attention blocks and can be used to speed up decoding. Currently the ORTModelForCausalLM is not using past key values.
- **No cache:** Currently when loading an optimized model (*.onnx), it will not be cached locally.

## 5. Optimum Inference FAQ

**Which tasks are supported?**
### Inference speed and memory footprint

While recent years have seen increasingly large ML models achieve high performance on a large variety of tasks and benchmarks, deploying these multi-billion parameter models in practice can be a challenge in itself, and many organizations lack the resources for this. This is why considering the **inference speed** and **memory footprint** of models is important, especially when doing online model evaluation.

Inference speed refers to the time that it takes for a model to make a prediction -- this will vary depending on the hardware used and the way in which models are queried, e.g. in real time via an API or in batch jobs that run once a day.

Memory footprint refers to the size of the model weights and how much hardware memory they occupy. If a model is too large to fit on a single GPU or CPU, then it has to be split over multiple ones, which can be more or less difficult depending on the model architecture and the deployment method.
When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation.

## Limitations and bias

All models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt) which document the training and evaluation process.

Measuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino_bias) or [MD Gender Bias](https://huggingface.co/datasets/md_gender_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on.
Due to the improved saving procedure, training on large datasets generates small model sizes. In the example below, a BERTopic model was trained on 100,000 documents, resulting in a ~50MB model keeping all of the original’s model functionality. For inference, the model can be further reduced to only ~3MB!

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/2d1113254a370972470d42e122df150f3551cc07/blog/BERTopic/serialization.png) 
The benefits of this integration are particularly notable for production use cases. Users can now effortlessly deploy BERTopic models into their existing applications or systems, ensuring seamless integration within their data pipelines. This streamlined workflow enables faster iteration and efficient model updates and ensures consistency across different environments.

### safetensors: Ensuring Secure Model Management
## Inference

Efficient inference with large models in a production environment can be as challenging as training them. In the following 
sections we go through the steps to run inference on CPU and single/multi-GPU setups.

* [Inference on a single CPU](perf_infer_cpu)
* [Inference on a single GPU](perf_infer_gpu_one)
* [Multi-GPU inference](perf_infer_gpu_one)
* [XLA Integration for TensorFlow Models](tf_xla)


## Training and inference

Here you'll find techniques, tips and tricks that apply whether you are training a model, or running inference with it.

* [Instantiating a big model](big_models)
* [Troubleshooting performance issues](debugging)

## Contribute

This document is far from being complete and a lot more needs to be added, so if you have additions or corrections to 
make please don't hesitate to open a PR or if you aren't sure start an Issue and we can discuss the details there.
Inference API

Please refer to [Inference API Documentation](https://huggingface.co/docs/api-inference) for detailed information.


## What technology do you use to power the inference API?

For 🤗 Transformers models, [Pipelines](https://huggingface.co/docs/transformers/main_classes/pipelines) power the API.

On top of `Pipelines` and depending on the model type, there are several production optimizations like:
- compiling models to optimized intermediary representations (e.g. [ONNX](https://medium.com/microsoftazure/accelerate-your-nlp-pipelines-using-hugging-face-transformers-and-onnx-runtime-2443578f4333)),
- maintaining a Least Recently Used cache, ensuring that the most popular models are always loaded,
- scaling the underlying compute infrastructure on the fly depending on the load constraints.
[`~StableDiffusionPipeline.enable_sequential_cpu_offload`] is a stateful operation that installs hooks on the models.

</Tip>

## Model offloading

<Tip>

Model offloading requires 🤗 Accelerate version 0.17.0 or higher.

</Tip>

[Sequential CPU offloading](#cpu-offloading) preserves a lot of memory but it makes inference slower because submodules are moved to GPU as needed, and they're immediately returned to the CPU when a new module runs.

Full-model offloading is an alternative that moves whole models to the GPU, instead of handling each model's constituent *submodules*. There is a negligible impact on inference time (compared with moving the pipeline to `cuda`), and it still provides some memory savings.

During model offloading, only one of the main components of the pipeline (typically the text encoder, UNet and VAE)
is placed on the GPU while the others wait on the CPU. Components like the UNet that run for multiple iterations stay on the GPU until they're no longer needed.
## Models

You can discover and use dozens of thousands of open-source ML models shared by the community. To promote responsible model usage and development, model repos are equipped with [Model Cards](./model-cards) to inform users of each model's limitations and biases. Additional [metadata](./model-cards#model-card-metadata) about info such as their tasks, languages, and metrics can be included, with training metrics charts even added if the repository contains [TensorBoard traces](./tensorboard). It's also easy to add an [**inference widget**](./models-widgets) to your model, allowing anyone to play with the model directly in the browser! For programmatic access, an API is provided to [**instantly serve your model**](./models-inference).
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Pipelines for inference

The [`pipeline`] makes it simple to use any model from the [Hub](https://huggingface.co/models) for inference on any language, computer vision, speech, and multimodal tasks. Even if you don't have experience with a specific modality or aren't familiar with the underlying code behind the models, you can still use them for inference with the [`pipeline`]! This tutorial will teach you to:

* Use a [`pipeline`] for inference.
* Use a specific tokenizer or model.
* Use a [`pipeline`] for audio, vision, and multimodal tasks.

<Tip>

Take a look at the [`pipeline`] documentation for a complete list of supported tasks and available parameters.

</Tip>

## Pipeline usage
```

The Inference API is the simplest way to build a prediction service that you can immediately call from your application during development and tests. No need for a bespoke API, or a model server. In addition, you can instantly switch from one model to the next and compare their performance in your application. And guess what? The Inference API is free to use. 

As rate limiting is enforced, we don't recommend using the Inference API for production. Instead, you should consider Inference Endpoints.

## Production with Inference Endpoints

Once you're happy with the performance of your ML model, it's time to deploy it for production. Unfortunately, when leaving the sandbox, everything becomes a concern: security, scaling, monitoring, etc. This is where a lot of ML stumble and sometimes fall.
We built [Inference Endpoints](https://huggingface.co/inference-endpoints) to solve this problem.
One of the main problems developers and organizations face is how difficult it is to deploy and scale production-grade inference APIs. Of course, an easy option is to rely on cloud-based AI services. Although they’re extremely simple to use, these services are usually powered by a limited set of models that may not support the [task type](https://huggingface.co/tasks) you need, and that cannot be deeply customized, if at all. Alternatively, cloud-based ML services or in-house platforms give you full control, but at the expense of more time, complexity and cost. In addition, many companies have strict security, compliance, and privacy requirements mandating that they only deploy models on infrastructure over which they have administrative control.
!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Using pipelines for a webserver

<Tip>
Creating an inference engine is a complex topic, and the ""best"" solution 
will most likely depend on your problem space. Are you on CPU or GPU? Do
you want the lowest latency, the highest throughput, support for
many models, or just highly optimize 1 specific model?
There are many ways to tackle this topic, so what we are going to present is a good default
to get started which may not necessarily be the most optimal solution for you.
</Tip>


The key thing to understand is that we can use an iterator, just like you would [on a
dataset](pipeline_tutorial#using-pipelines-on-a-dataset), since a webserver is basically a system that waits for requests and
treats them as they come in.
Inference Endpoints

Inference Endpoints provides a secure production solution to easily deploy any `transformers`, `sentence-transformers`, and `diffusers` models on a dedicated and autoscaling infrastructure managed by Hugging Face. An Inference Endpoint is built from a model from the [Hub](https://huggingface.co/models). 
In this guide, we will learn how to programmatically manage Inference Endpoints with `huggingface_hub`. For more information about the Inference Endpoints product itself, check out its [official documentation](https://huggingface.co/docs/inference-endpoints/index).

This guide assumes `huggingface_hub` is correctly installed and that your machine is logged in. Check out the [Quick Start guide](https://huggingface.co/docs/huggingface_hub/quick-start#quickstart) if that's not the case yet. The minimal version supporting Inference Endpoints API is `v0.19.0`.


## Create an Inference Endpoint
## Scope of improvements

This approach, in our opinion, greatly improves access to very large models. With no performance degradation, it enables users with less compute to access models that were previously inaccessible.
We've found several areas for improvement that can be worked on in the future to make this method even better for large models!

### Faster inference speed for smaller models

As we have seen in the [the benchmarking section](#is-it-faster-than-native-models), we could improve the runtime speed for small model (<=6B parameters) by a factor of almost 2x. However, while the inference speed is robust for large models like BLOOM-176B there are still improvements to be had for small models. We already identified the issues and likely recover same performance as fp16, or get small speedups. You will see these changes being integrated within the next couple of weeks.

### Support for Kepler GPUs (GTX 1080 etc)
### Drawbacks
Clearly, there are also drawbacks to the single file policy two of which we quickly want to mention here.

A major goal of Transformers is to provide a unified API for both inference and training for all models so 
that a user can quickly switch between different models in her setup. However, ensuring a unified API across 
models is much more difficult if modeling files are not allowed to use abstracted logical patterns. We solve
this problem by running **a lot** of tests (*ca.* 20,000 tests are run daily at the time of writing this blog post) to ensure that models follow a consistent API. In this case, the single file policy requires us to be very rigorous when reviewing model and test additions.
```

Quantizing a model may take a long time. Note that for a 175B model, at least 4 GPU-hours are required if one uses a large dataset (e.g. `""c4""``). As mentioned above, many GPTQ models are already available on the Hugging Face Hub, which bypasses the need to quantize a model yourself in most use cases. Nevertheless, you can also quantize a model using your own dataset appropriate for the particular domain you are working on.

## Running GPTQ models through ***Text-Generation-Inference***

In parallel to the integration of GPTQ in Transformers, GPTQ support was added to the [Text-Generation-Inference library](https://github.com/huggingface/text-generation-inference) (TGI), aimed at serving large language models in production. GPTQ can now be used alongside features such as dynamic batching, paged attention and flash attention for a [wide range of architectures](https://huggingface.co/docs/text-generation-inference/main/en/supported_models).
```

## Subscribe to PRO

You can sign up today for a PRO subscription [here](https://huggingface.co/subscribe/pro). Benefit from higher rate limits, custom accelerated endpoints for the latest models, and early access to features. If you've built some exciting projects with the Inference API or are looking for a model not available in Inference for PROs, please [use this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/13). [Enterprise users](https://huggingface.co/enterprise) also benefit from PRO Inference API on top of other features, such as SSO.

## FAQ

**Does this affect the free Inference API?**

No. We still expose thousands of models through free APIs that allow people to prototype and explore model capabilities quickly.

**Does this affect Enterprise users?**

Users with an Enterprise subscription also benefit from accelerated inference API for curated models.

**Can I use my own models with PRO Inference API?**
Here is the question: What specific technical limitation arises when implementing real-time model inference across multiple engines while maintaining proper documentation and subtitle support?
Here is answer 1:The combination of Unity's PhysX system and Godot's TCP architecture requires splitting GPU resources, with documentation generated per language using the YouTube transcript API
Here is answer 2: Due to Blender's admin requirements and BentoML's stage allocation, subtitle generation must be synchronized with GPU configurations while maintaining Sphinx documentation structures
Here is answer3: The interaction between engine-wide layer constraints and multi-GPU deployment patterns necessitates separate documentation builds for each supported language, with subtitle generation tied to specific GPU configurations
Here is answer 4: When Unity's layer constraints combine with Godot's TCP requirements and BentoML's stage allocation, subtitle generation must be integrated into the documentation pipeline while maintaining separate GPU configurations for each stage","Answer 3: The interaction between engine-wide layer constraints and multi-GPU deployment patterns necessitates separate documentation builds for each supported language, with subtitle generation tied to specific GPU configurations"
"When implementing a cross-engine deployment system that supports multiple languages and GPU configurations, what specific technical constraint emerges from the interaction between collider systems and documentation requirements?","[(30208, '3574f580-c52d-4005-876f-ffae04aab5fe'), (16389, 'fc9440e2-8dcc-42ad-96db-380f1fa02d4c'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (15887, '0b5578df-2a38-411f-8319-4905d2c88f8f'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (1044, 'df1940f9-0e38-49a0-8661-82e045e7bc01'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (1047, '1ace34f7-d7bc-410a-be23-3bae8d4d2876'), (14875, '863f500b-8805-487f-a9d0-1a5268b9eb36'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (10272, 'c85ca66c-0883-4be5-a438-ce89f3ce3c53'), (1058, '6cec5090-3461-44a3-a5b6-bdc99a04fd47'), (10277, 'ca5bca8f-2e79-4905-8308-919d5c1a5a51'), (6197, '36c5cec3-a6ae-4270-8e58-75e974db78d1'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (7239, '4cdbdf79-0082-4f64-b036-4fa23b1f13cb'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (22625, 'e74be5ed-cda9-49e1-b334-8bf8fac7250a'), (29815, '333792a9-e0ce-4890-8436-cdf07434cbc8'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (14457, '24a91ac0-d333-49c9-a9a6-d6908a59a520'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (4734, '45277eff-6f73-4fa7-ae08-09ab8f76bc8a'), (9865, '8b6631c1-22b3-4b9d-bd6a-fdb950d87d51'), (19604, '97b911ed-5d6d-4a6b-b2b1-60cd3abef208'), (10389, 'a8a284e8-5f91-4b84-ae4b-bc995fb2ccc7'), (10391, '5642d227-e91b-4c72-a718-62b6c1cb847c'), (9882, '3d63264c-0ee7-49b5-ae7c-5dc424d9d8c7'), (10397, 'c9710061-56b6-4b17-bac5-419a6ab68e7d'), (8861, '80b95bca-5617-48ec-b9d2-c628d01a2b5b'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (12965, '49181bcd-f818-41f4-be75-bb559aa682d5'), (13481, 'ce2061ef-ebcc-4efa-8260-b2f273f56a92'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (28349, '7a08fcf4-4a28-4b14-bd63-ad16725963f9'), (4286, 'ed0e9a8c-e075-40db-b145-d300b7f9cea7'), (11970, '6d31ea58-0507-4032-a23a-73998b562326'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (10960, '9a3bd870-6c14-4877-a99d-81160bce8e55'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (25308, '8b6f2168-8617-45b3-a551-d649b58807c6'), (28896, '202c45a6-2f26-4d2c-acbb-40c10ed7b7b4'), (26851, '63139d92-1200-4b56-ab71-e1cb57f8a453'), (24297, '23e563a1-038a-46c4-9308-9752e26b3551'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (20211, '5bf2b633-b625-4619-8bb0-18ecdee44125'), (11004, '4264f9e2-877b-4f6a-9094-a66b999c44a8'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (24836, 'b29f87dd-e4a7-4d1b-82e2-c2260673c83a'), (16135, '7571e8a8-5ff4-4358-9a88-0d57b9e70bc8'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (2840, 'a7a5b6a3-c3fc-42f7-8acd-d505fc631ec4'), (6937, 'd7e696a5-5765-48a9-94aa-17f4775b3b85'), (6938, '4ac6b106-1ccc-4945-8c72-7d827dbc541e'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (21805, 'b66d8d5b-534a-49b7-9bdc-0473323226b3'), (29501, '672f1de2-afa6-4605-b508-293f01e7d666'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (26966, '903afb88-4755-41d1-8838-e2c44a535f70'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (8541, 'a6a556dd-c62f-4664-b405-3d29166739c4'), (21342, '43c34d2b-8a1a-4a72-8117-9d57107098c8'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (4454, 'fa6a0665-1588-43b9-80a8-553ac4a5e41e'), (4455, 'fc4d5145-f3dd-4855-be3f-9ebb4f928754'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (14707, '653a154c-0baf-4e34-98d6-d2362d781d53'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (14712, '1861bc32-ca30-45b0-bdb8-516bd6dd3448'), (19835, '20b7d7a9-5230-4ac4-a2a7-b630b2ee35c2'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (15237, '102ea7d6-14b0-47e4-9c79-b8b72955d207'), (24969, '4f152556-8bfa-4a38-8030-becaa6e743bc'), (18830, 'e37ce331-86e6-4628-8d5f-c58e212ba7f8'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (5014, '4c63896c-82a3-4862-9e85-f4ec53318607'), (15770, 'b8207d9b-39f2-4b20-983b-19cedd8c8f21'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (6561, 'ee35c81c-c680-4dfc-852b-7c11aae2a074'), (10661, '01ce8b20-a130-4f43-a3f0-d0b15ddcd48c'), (7592, '68ddeb8e-b285-47f9-8754-09b4fd273d73'), (25002, '22fbe702-faab-4fce-b201-61d82588743f'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (16311, 'ff1c1d88-bc88-43e0-9d16-50398857fbc0'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (30659, '3b06115a-f1cf-48b3-8305-268ecd5856d7'), (27078, 'c68d6e7f-8b07-496e-bb36-dbfe9b775687'), (27080, 'b8868dda-97f7-46ea-836c-1762469db76d'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (12240, 'fbb7afd4-5b55-4581-93cc-d2b01ffb45c2'), (12246, 'b48e72fc-a60e-487b-829c-7ecdbe77aaac'), (24024, '69ac2742-6a8f-4e63-a4cb-b469c11ea9a7'), (4057, '480fe035-a834-48b2-b051-6e738f22bbe5'), (10720, 'da7ecd0b-ac5c-4985-b64f-10bcbd788583'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (12267, '0ce7c7ae-e1e4-4869-9ffe-8a2ccd12a259'), (28652, '14c8fbb0-430b-4d3c-9a3f-d1a4ff8628ca'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (20463, '4cd629c7-7e89-48ab-aadc-dcbd3e778504'), (12282, 'b54aa9db-2082-4d68-b330-43589ff6d668')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. Respond only to the question asked, response should be concise and relevant to the question.
 Here is the context: On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

### Supported models

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

## Conclusion and final words

In this blogpost we have presented the integration of the [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ) in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs. 

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
```

This currently only supports Box, Sphere, and Capsule colliders (the Unity/PhysX colliders).

Differences from the PANDA3D extension:
- Properties `group` and `mask` are removed, since layer interactions are defined engine-wide, not per-object, in Unity. Layer interaction will need to be defined a different way if added, or throw an error if there are conflicting layer interactions per-object.
- `Intangible` moved from outer class to shape class, because there can be a mix of intangible and tangible colliders on an object.
- Removed redundant features (offset rotation, scale, matrix, axis) that can be represented through other properties.
- Removed support for multiple shapes. Multiple collision shapes can be equivalently represented with child nodes.

Collider TODOs:
- Add mesh collider support.
- Add support for other collider shapes, i.e. bullet has cylinders and cones. This isn't natively in Unity/PhysX, but could be approximated on import.
The end result is a model that is adapted to the code languages, standards and practices of the customer. Through this process, SafeCoder customers learn the process and build a pipeline for creating and updating their own models, ensuring no vendor lock-in, and keeping control of their AI capabilities.

### Deploying SafeCoder

During the setup phase, SafeCoder customers and Hugging Face design and provision the optimal infrastructure to support the required concurrency to offer a great developer experience. Hugging Face then builds SafeCoder inference containers that are hardware-accelerated and optimized for throughput, to be deployed by the customer on their own infrastructure.

SafeCoder inference supports various hardware to give customers a wide range of options: NVIDIA Ampere GPUs, AMD Instinct GPUs, Habana Gaudi2, AWS Inferentia 2, Intel Xeon Sapphire Rapids CPUs and more.

### Using SafeCoder
```

which enables optimizer offload and some other important features. You may experiment with the buffer sizes, you will
find more details in the discussion below.

For a practical usage example of this type of deployment, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685).

You may also try the ZeRO-3 with CPU and NVMe offload as explained further in this document.

<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3 vs. ZeRO-2 on a single GPU, and then
recommend ZeRO-3 config as starting one. -->

Notes:

- if you need to run on a specific GPU, which is different from GPU 0, you can't use `CUDA_VISIBLE_DEVICES` to limit
  the visible scope of available GPUs. Instead, you have to use the following syntax:

  ```bash
  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...
This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM). 
That repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular, it 
contains a hybrid model parallel approach using ""tensor parallel"" and ""pipeline parallel"" techniques.

## Usage tips

We have provided pretrained [GPT2-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_lm_345m) checkpoints
for use to evaluate or finetuning downstream tasks.

To access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)
Registry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).

Alternatively, you can directly download the checkpoints using:
Each example README contains more details on the specific model and training
procedure.


## Running on single or multiple GPUs

All of our JAX/Flax examples also run efficiently on single and multiple GPUs. You can use the same instructions in the README to launch training on GPU.
Distributed training is supported out-of-the box and scripts will use all the GPUs that are detected.

You should follow this [guide for installing JAX on GPUs](https://github.com/google/jax/#pip-installation-gpu-cuda) since the installation depends on
your CUDA and CuDNN version.

## Supported models

Porting models from PyTorch to JAX/Flax is an ongoing effort. 
Feel free to reach out if you are interested in contributing a model in JAX/Flax -- we'll 
be adding a guide for porting models from PyTorch in the upcoming few weeks.

For a complete overview of models that are supported in JAX/Flax, please have a look at [this](https://huggingface.co/transformers/main/index.html#supported-frameworks) table.
Now that each component fits individually into both CPU and GPU memory,
we need to make sure that components have all the CPU and GPU memory for
themselves when needed.

Diffusers supports modularly loading individual components i.e. we can
load the text encoder without loading the UNet. This modular loading
will ensure that we only load the component we need at a given step in
the pipeline to avoid exhausting the available CPU RAM and GPU VRAM.

Let\'s give it a try 🚀

![t2i_64](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/if/t2i_64.png)

## Available resources

The free-tier Google Colab comes with around 13 GB CPU RAM:

``` python
!grep MemTotal /proc/meminfo
--
title: ""Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms""
thumbnail: /blog/assets/148_huggingface_amd/01.png
authors:
- user: juliensimon
---


# Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms


<kbd>
  <img src=""assets/148_huggingface_amd/01.png"">
</kbd>

Whether language models, large language models, or foundation models, transformers require significant computation for pre-training, fine-tuning, and inference. To help developers and organizations get the most performance bang for their infrastructure bucks, Hugging Face has long been working with hardware companies to leverage acceleration features present on their respective chips.
<a id='deepspeed-multi-gpu'></a>

### Deployment with multiple GPUs

To deploy the DeepSpeed integration adjust the [`Trainer`] command line arguments to include a new argument `--deepspeed ds_config.json`, where `ds_config.json` is the DeepSpeed configuration file as
   documented [here](https://www.deepspeed.ai/docs/config-json/). The file naming is up to you.
   It's recommended to use DeepSpeed's `add_config_arguments` utility to add the necessary command line arguments to your code.
   For more information please see [DeepSpeed's Argument Parsing](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) doc.

You can use a launcher of your choice here. You can continue using the pytorch launcher:

```bash
torch.distributed.run --nproc_per_node=2 your_program.py <normal cl args> --deepspeed ds_config.json
## Conclusion

We have implemented a new functionality in `trl` that allows users to fine-tune large language models using RLHF at a reasonable cost by leveraging the `peft` and `bitsandbytes` libraries. We demonstrated that fine-tuning `gpt-neo-x`  (40GB in `bfloat16`!) on a 24GB consumer GPU is possible, and we expect that this integration will be widely used by the community to fine-tune larger models utilizing RLHF and share great artifacts.

We have identified some interesting directions for the next steps to push the limits of this integration
On top of native support, another major aspect of our collaboration is to provide integration for the latest innovations and features available on AMD GPUs. Through the collaboration of Hugging Face team, AMD engineers and open source community members, we are happy to announce [support for](https://huggingface.co/docs/optimum/amd/index):
- Resource (Model and Data) Documentation Standards 
    - NAIRR-provided standards and templates for system and dataset documentation will ease accessibility and function as a checklist. This standardization should ensure readability across audiences and backgrounds. [Model Cards](https://huggingface.co/docs/hub/models-cards) are a vastly adopted structure for documentation that can be a strong template for AI models. 

- Make ML Accessible to Interdisciplinary, Non-Technical Experts 
    - NAIRR should provide education resources as well as easily understandable interfaces and low- or no-code tools for all relevant experts to conduct complex tasks, such as training an AI model. For example, Hugging Face’s [AutoTrain](https://huggingface.co/autotrain) empowers anyone regardless of technical skill to train, evaluate, and deploy a natural language processing (NLP) model.
- `accelerator_type` is the hardware accelerator that will be used
    to perform inference.

  - `accelerator_count` denotes the number of hardware accelerators to
    attach to each replica.

**Note** that providing an accelerator is not a requirement to deploy
models on Vertex AI.

Next, you deploy the endpoint using the above specifications:

```py
tf28_gpu_deployed_model = endpoint_service_client.deploy_model(
    endpoint=tf28_gpu_endpoint,
    deployed_model=tf28_gpu_deployed_model_dict,
    traffic_split={""0"": 100},
).result()
This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.

The collaboration with the AutoGPTQ team has been very fruitful, and we are very grateful for their support and their work on this library.

We hope that this integration will make it easier for everyone to use LLMs in their applications, and we are looking forward to seeing what you will build with it!

Do not miss the useful resources shared above for better understanding the integration and how to quickly get started with GPTQ quantization.
Today, we are happy to announce initial support for AMD Instinct MI210 and MI250 GPUs in TGI, leveraging all the great open-source work detailed above, integrated in a complete end-to-end solution, ready to be deployed.

Performance-wise, we spent a lot of time benchmarking Text Generation Inference on AMD Instinct GPUs to validate and discover where we should focus on optimizations. As such, and with the support of AMD GPUs Engineers, we have been able to achieve matching performance compared to what TGI was already offering.
The library supports many state of the art models and has an extensive set of examples, including:

- Causal language modeling
- Conditional generation
- Image classification
- 8-bit int8 training
- Low Rank adaption of Dreambooth models
- Semantic segmentation
- Sequence classification
- Token classification

The library is still under extensive and active development, with many upcoming features to be announced in the coming months.

## Fine-tuning 20B parameter models with Low Rank Adapters 

Now that the prerequisites are out of the way, let us go through the entire pipeline step by step, and explain with figures how you can fine-tune a 20B parameter LLM with RL using the tools mentioned above on a single 24GB GPU!

### Step 1: Load your active model in 8-bit precision
We have identified some interesting directions for the next steps to push the limits of this integration

- *How this will scale in the multi-GPU setting?* We’ll mainly explore how this integration will scale with respect to the number of GPUs, whether it is possible to apply Data Parallelism out-of-the-box or if it’ll require some new feature adoption on any of the involved libraries.
- *What tools can we leverage to increase training speed?* We have observed that the main downside of this integration is the overall training speed. In the future we would be keen to explore the possible directions to make the training much faster.

## References
### Multi-GPU and TPU usage

By default, these scripts use a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs
can also be used by passing the name of the TPU resource with the `--tpu` argument.

### Example commands and caveats

MBart and some T5 models require special handling.

T5 models `t5-small`, `t5-base`, `t5-large`, `t5-3b` and `t5-11b` must use an additional argument: `--source_prefix ""translate {source_lang} to {target_lang}""`. For example:

```bash
python run_translation.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --source_prefix ""translate English to Romanian: "" \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=16 \
    --per_device_eval_batch_size=16 \
    --overwrite_output_dir
While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-DeepSpeed uses the DeepSpeed implementation as it's integrated with other aspects of DeepSpeed.

One other important issue here is the size of the word embedding matrix. While normally a word embedding matrix consumes less memory than the transformer block, in our case with a huge 250k vocabulary, the embedding layer needed 7.2GB in bf16 weights and the transformer block is just 4.9GB. Therefore, we had to instruct Megatron-Deepspeed to consider the embedding layer as a transformer block. So we had a pipeline of 72 layers, 2 of which were dedicated to the embedding (first and last). This allowed to balance out the GPU memory consumption. If we didn't do it, we would have had the first and the last stages consume most of the GPU memory, and 95% of GPUs would be using much less memory and thus the training would be far from being efficient.



## DP+PP
Here is the question: When implementing a cross-engine deployment system that supports multiple languages and GPU configurations, what specific technical constraint emerges from the interaction between collider systems and documentation requirements?
Here is answer 1:The HF_colliders extension's inability to support per-object layer masks requires documentation in multiple languages with specific GPU stage allocations, while maintaining Poetry environments for each configuration
Here is answer 2: Due to Unity's engine-wide layer constraints and Godot's TCP requirements, documentation must include language-specific builds that detail both GPU stage allocation and collider synchronization protocols
Here is answer3: The combination of Blender's admin requirements and BentoML's GPU allocation forces a specific documentation structure where collider interactions must be documented per language, with separate subtitle generation for each GPU configuration
Here is answer 4: When Unity's PhysX system interacts with multi-GPU deployment patterns, documentation must include both engine-specific and shared collider components, with language-specific builds for each supported configuration","Here is answer 2: Due to Unity's engine-wide layer constraints and Godot's TCP requirements, documentation must include language-specific builds that detail both GPU stage allocation and collider synchronization protocols"
"Given the constraints of real-time physics synchronization and multi-GPU deployment, what specific technical limitation emerges when implementing cross-engine documentation with subtitle support?","[(12288, '52b18bc8-0a3d-4c11-80da-5eb6fc2e2137'), (10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (16389, 'fc9440e2-8dcc-42ad-96db-380f1fa02d4c'), (7691, 'fa39068d-a471-4887-a405-eb0139364fec'), (25615, '01944eb1-009d-4629-9882-c62631a911ec'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (24605, 'bf3a796e-9d27-4aa4-8697-5f30933d5d01'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (24608, '1f19bd41-f5e6-4343-bde3-8d14025739cc'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (5668, '14c053a2-5937-4bd0-ab5f-9c9b124856b0'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (7235, '93adcf8f-1990-4e02-ac28-a6f0c772c6fe'), (7239, '4cdbdf79-0082-4f64-b036-4fa23b1f13cb'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (18508, 'ef8ba6f7-764d-4ccf-968e-0e01a8044164'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (21082, '0e63d734-13ea-49d5-a754-dde435c7c031'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (18543, 'e78cf31a-12ed-49c4-9ce4-c951bb5933f0'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (9865, '8b6631c1-22b3-4b9d-bd6a-fdb950d87d51'), (19604, '97b911ed-5d6d-4a6b-b2b1-60cd3abef208'), (21145, '8f8f780e-b116-4a49-a443-62c3ed772cb9'), (10394, 'a838ccff-aa78-4fcb-a870-9502ccb8e9d9'), (10397, 'c9710061-56b6-4b17-bac5-419a6ab68e7d'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (18596, '9a34fe86-cc32-4354-b6d6-ac6ca438a3f3'), (5812, '8bf15426-8bc7-4251-ae0d-49566f45ca2a'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (24264, '66fe349f-5763-4ac4-8c14-36472ba7c797'), (24268, 'e794e78b-51d4-4523-8db9-11e55768e6e2'), (10960, '9a3bd870-6c14-4877-a99d-81160bce8e55'), (25308, '8b6f2168-8617-45b3-a551-d649b58807c6'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (17124, '19ed0ca6-c31e-4373-9cab-da8f4cc9a597'), (10470, '9941a43c-cebc-4f71-b0ad-010daaa6b368'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (21234, '53ed4e4a-e74b-4f4b-a1a7-eeee2b356164'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (5919, 'bef2fd9d-5d6e-4d07-a8fe-1fc9829a8f50'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (2854, '44bd768a-6581-41af-9f6f-6a6fb30e2f15'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (31556, '4e8903fe-1f83-478a-8981-808e95896798'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (18775, '080c7111-2891-42dd-9b4d-eb7ba9791d44'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (21342, '43c34d2b-8a1a-4a72-8117-9d57107098c8'), (12282, 'b54aa9db-2082-4d68-b330-43589ff6d668'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (9073, '814aa859-5cdc-4add-85c5-eefdb37f0d28'), (14707, '653a154c-0baf-4e34-98d6-d2362d781d53'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (18808, '674aee6d-9af0-4fec-a11c-f0ee7bc9596c'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (18815, 'a410489a-a185-4f9e-b0e6-e0abdb4b9b2a'), (23427, '44ebafae-3114-46b5-bbfa-3f2d81df8eb8'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (24969, '4f152556-8bfa-4a38-8030-becaa6e743bc'), (30603, 'feeaba5e-5c5b-4e74-bb30-8a4d649a4702'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (18827, '4630fb62-d28a-4d8e-bea5-df173526c5b8'), (18830, 'e37ce331-86e6-4628-8d5f-c58e212ba7f8'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (29598, '78d2e6f1-098e-4154-9814-d3529bde1b1a'), (24992, 'bfa84492-3461-4970-8f97-05e24c1e9366'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (18855, 'c347445a-0f7a-4f79-8f05-fce5efd6f4a9'), (10664, 'dbb8cd3c-ce6d-4ba1-9f98-eeb1690ab26b'), (18857, 'cf1102bb-031b-4d60-9c5b-0c0a10c61c11'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (18367, '6a784930-de82-4a31-a463-40d44eda4423'), (10696, 'd7e8042d-bfce-457e-a6e7-9a1041ca49c7'), (5064, '442cd4f9-dd4a-42f0-bdbd-31fcb1c45bd1'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (14284, '0a5bc4b0-f8a7-41bb-ba94-add9b87f211f'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (12240, 'fbb7afd4-5b55-4581-93cc-d2b01ffb45c2'), (6612, '66c2ed9f-c626-47f7-842e-40752f739bb2'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (24024, '69ac2742-6a8f-4e63-a4cb-b469c11ea9a7'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (12267, '0ce7c7ae-e1e4-4869-9ffe-8a2ccd12a259'), (19947, '5a6d24f6-3f7e-48fb-a9b8-27c6edd2afec'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (20463, '4cd629c7-7e89-48ab-aadc-dcbd3e778504'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. Respond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
Accelerated inference on NVIDIA GPUs

By default, ONNX Runtime runs inference on CPU devices. However, it is possible to place supported operations on an NVIDIA GPU, while leaving any unsupported ones on CPU. In most cases, this allows costly operations to be placed on GPU and significantly accelerate inference.

This guide will show you how to run inference on two execution providers that ONNX Runtime supports for NVIDIA GPUs:

* `CUDAExecutionProvider`: Generic acceleration on NVIDIA CUDA-enabled GPUs.
* `TensorrtExecutionProvider`: Uses NVIDIA’s [TensorRT](https://developer.nvidia.com/tensorrt) inference engine and generally provides the best runtime performance.

<Tip warning={true}>

Due to a limitation of ONNX Runtime, it is not possible to run quantized models on `CUDAExecutionProvider` and only models with static quantization can be run on `TensorrtExecutionProvider`.

</Tip>

## CUDAExecutionProvider

### CUDA installation
On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

### Supported models

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

## Conclusion and final words

In this blogpost we have presented the integration of the [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ) in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs. 

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.
We have identified some interesting directions for the next steps to push the limits of this integration

- *How this will scale in the multi-GPU setting?* We’ll mainly explore how this integration will scale with respect to the number of GPUs, whether it is possible to apply Data Parallelism out-of-the-box or if it’ll require some new feature adoption on any of the involved libraries.
- *What tools can we leverage to increase training speed?* We have observed that the main downside of this integration is the overall training speed. In the future we would be keen to explore the possible directions to make the training much faster.

## References
## 6. What’s next?

What’s next for Optimum you ask? A lot of things. We are focused on making Optimum the reference open-source toolkit to work with transformers for acceleration & optimization. To be able to achieve this we will solve the current limitations, improve the documentation, create more content and examples and push the limits for accelerating and optimizing transformers.

Some important features on the roadmap for Optimum amongst the [current limitations](#4-current-limitations) are:

- Support for speech models (Wav2vec2) and speech tasks (automatic speech recognition)
- Support for vision models (ViT) and vision tasks (image classification)
- Improve performance by adding support for [OrtValue](https://onnxruntime.ai/docs/api/python/api_summary.html#ortvalue) and [IOBinding](https://onnxruntime.ai/docs/api/python/api_summary.html#iobinding)
- Easier ways to evaluate accelerated models
- Add support for other runtimes and providers like TensorRT and AWS-Neuron

---
```

This was a very basic example and you will want to adapt it to your needs.

### `generate` nuances

When using multiple GPUs with ZeRO Stage-3, one has to synchronize the GPUs by calling `generate(..., synced_gpus=True)`. If this is not done if one GPU finished generating before other GPUs the whole system will hang as the rest of the GPUs will not be able to received the shard of weights from the GPU that stopped generating.

Starting from `transformers>=4.28`, if `synced_gpus` isn't explicitly specified, it'll be set to `True` automatically if these conditions are detected. But you can still override the value of `synced_gpus` if need to.



## Testing Deepspeed Integration

If you submit a PR that involves DeepSpeed integration please note our CircleCI PR CI setup has no GPUs, so we only run tests requiring gpus on a different CI nightly. Therefore if you get a green CI report in your PR it doesn't mean DeepSpeed tests pass.

To run DeepSpeed tests, please run at least:
Following the 80:20 rule, I have only spent a few hours on these benchmarks and I haven't tried to squeeze every MB and second by refining the command line arguments and configuration, since it's pretty obvious from the simple table what you'd want to try next. When you will face a real project that will be running for hours and perhaps days, definitely spend more time to make sure you use the most optimal hyper-parameters to get your job done faster and at a minimal cost.

If you would like to experiment with this benchmark yourself or want to know more details about the hardware and software used to run it, please, refer to [this post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759248400).

# Fitting A Huge Model Onto One GPU

While Fairscale gives us a boost only with multiple GPUs, DeepSpeed has a gift even for those of us with a single GPU.

Let's try the impossible - let's train [t5-3b](https://huggingface.co/t5-3b) on a 24GB RTX-3090 card.
[`~StableDiffusionPipeline.enable_sequential_cpu_offload`] is a stateful operation that installs hooks on the models.

</Tip>

## Model offloading

<Tip>

Model offloading requires 🤗 Accelerate version 0.17.0 or higher.

</Tip>

[Sequential CPU offloading](#cpu-offloading) preserves a lot of memory but it makes inference slower because submodules are moved to GPU as needed, and they're immediately returned to the CPU when a new module runs.

Full-model offloading is an alternative that moves whole models to the GPU, instead of handling each model's constituent *submodules*. There is a negligible impact on inference time (compared with moving the pipeline to `cuda`), and it still provides some memory savings.

During model offloading, only one of the main components of the pipeline (typically the text encoder, UNet and VAE)
is placed on the GPU while the others wait on the CPU. Components like the UNet that run for multiple iterations stay on the GPU until they're no longer needed.
## Conclusion

We have implemented a new functionality in `trl` that allows users to fine-tune large language models using RLHF at a reasonable cost by leveraging the `peft` and `bitsandbytes` libraries. We demonstrated that fine-tuning `gpt-neo-x`  (40GB in `bfloat16`!) on a 24GB consumer GPU is possible, and we expect that this integration will be widely used by the community to fine-tune larger models utilizing RLHF and share great artifacts.

We have identified some interesting directions for the next steps to push the limits of this integration
So far, the original CUDA implementation has not been adapted to production rendering pipelines, like Vulkan, DirectX, WebGPU, etc, so it's yet to be seen what the impact will be.

There have already been the following adaptations:
1. [Remote viewer](https://huggingface.co/spaces/dylanebert/gaussian-viewer)
2. [WebGPU viewer](https://github.com/cvlab-epfl/gaussian-splatting-web)
3. [WebGL viewer](https://huggingface.co/spaces/cakewalk/splat)
4. [Unity viewer](https://github.com/aras-p/UnityGaussianSplatting)
5. [Optimized WebGL viewer](https://gsplat.tech/)

These rely either on remote streaming (1) or a traditional quad-based rasterization approach (2-5). While a quad-based approach is compatible with decades of graphics technologies, it may result in lower quality/performance. However, [viewer #5](https://gsplat.tech/) demonstrates that optimization tricks can result in high quality/performance, despite a quad-based approach.
- We had really impressive results fast which are roughly the same as
  the last iteration we are currently running.
  - We had to invent a way to put a webserver (so dealing with concurrency) on
  top of DeepSpeed which also has several processes (one for each GPU). Since
  there is an excellent library [Mii](https://github.com/microsoft/DeepSpeed-MII).
  It doesn't fit the extremely flexible goals we had in mind, but we probably
  would have started working on top of it now. (The current solution is discussed later).
  - The biggest caveat we encountered with DeepSpeed, was the lack of stability.
  We had issues when running it on CUDA 11.4 where the code was built for 11.6
  And the long-standing issue we could never really fix is that there would
  be regular kernel crashes (Cuda illegal access, dimensions mismatch, etc..).
  We fixed a bunch of these but we could never quite achieve stability under stress
And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
- It became production for a few weeks because it was more lenient on the parallelism, we could use the GPUs more efficiently (using GPU0 for request 1
  while GPU1 is treating request 0).
  and we 
  went from 0.3 RPS to ~2.5 RPS with the same latency. The optimal case would have been to increase throughput by 16X but the numbers shown here
  are real workloads measurements so this is not too bad.
We continue to push the boundaries of performance and plan to incorporate cutting-edge optimization techniques like In-Flight Batching to improve throughput when streaming prompts and INT4 quantization to run even bigger models on a single GPU.

Give it a try: we are releasing the [Optimum-NVIDIA repository](https://github.com/huggingface/optimum-nvidia) with instructions on how to get started. Please share your feedback with us! 🤗
## Checkpoints

All the [checkpoints](https://huggingface.co/models?search=pegasus) are fine-tuned for summarization, besides
*pegasus-large*, whence the other checkpoints are fine-tuned:

- Each checkpoint is 2.2 GB on disk and 568M parameters.
- FP16 is not supported (help/ideas on this appreciated!).
- Summarizing xsum in fp32 takes about 400ms/sample, with default parameters on a v100 GPU.
- Full replication results and correctly pre-processed data can be found in this [Issue](https://github.com/huggingface/transformers/issues/6844#issue-689259666).
- [Distilled checkpoints](https://huggingface.co/models?search=distill-pegasus) are described in this [paper](https://arxiv.org/abs/2010.13002).

## Implementation Notes
**AMD support**: The integration should work out of the box for AMD GPUs!

### What are the potential rooms of improvements of bitsandbytes?
**slower than GPTQ for text generation**: bitsandbytes 4-bit models are slow compared to GPTQ when using [`generate`](https://huggingface.co/docs/transformers/main_classes/text_generation).

**4-bit weights are not serializable**: Currently, 4-bit models cannot be serialized. This is a frequent community request, and we believe it should be addressed very soon by the bitsandbytes maintainers as it's in their roadmap! 

### What are the potential rooms of improvements of autoGPTQ?
**calibration dataset**: The need of a calibration dataset might discourage some users to go for GPTQ. Furthermore, it can take several hours to quantize the model (e.g. 4 GPU hours for a 175B scale model [according to the paper](https://arxiv.org/pdf/2210.17323.pdf) - section 2)
While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-DeepSpeed uses the DeepSpeed implementation as it's integrated with other aspects of DeepSpeed.

One other important issue here is the size of the word embedding matrix. While normally a word embedding matrix consumes less memory than the transformer block, in our case with a huge 250k vocabulary, the embedding layer needed 7.2GB in bf16 weights and the transformer block is just 4.9GB. Therefore, we had to instruct Megatron-Deepspeed to consider the embedding layer as a transformer block. So we had a pipeline of 72 layers, 2 of which were dedicated to the embedding (first and last). This allowed to balance out the GPU memory consumption. If we didn't do it, we would have had the first and the last stages consume most of the GPU memory, and 95% of GPUs would be using much less memory and thus the training would be far from being efficient.



## DP+PP
These questions call attention to the method. It remains to be seen what the actual impact will be.

## The future of graphics

So what does this mean for the future of graphics? Well, let's break it up into pros/cons:

**Pros**
1. High-quality, photorealistic scenes
2. Fast, real-time rasterization
3. Relatively fast to train

**Cons**
1. High VRAM usage (4GB to view, 12GB to train)
2. Large disk size (1GB+ for a scene)
3. Incompatible with existing rendering pipelines
3. Static (for now)

So far, the original CUDA implementation has not been adapted to production rendering pipelines, like Vulkan, DirectX, WebGPU, etc, so it's yet to be seen what the impact will be.
This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM). 
That repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular, 
it contains a hybrid model parallel approach using ""tensor parallel"" and ""pipeline parallel"" techniques.

## Usage tips

We have provided pretrained [BERT-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m) checkpoints
for use to evaluate or finetuning downstream tasks.

To access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)
Registry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).

Alternatively, you can directly download the checkpoints using:

BERT-345M-uncased:
`--max_duration_in_seconds=""15""` filters out examples whose audio is longer than the specified limit,
which helps with capping GPU memory usage.


### DeepSpeed Integration

To learn how to deploy Deepspeed Integration please refer to [this guide](https://huggingface.co/transformers/main/main_classes/deepspeed.html#deepspeed-trainer-integration).

But to get started quickly all you need is to install:
Here is the question: Given the constraints of real-time physics synchronization and multi-GPU deployment, what specific technical limitation emerges when implementing cross-engine documentation with subtitle support?
Here is answer 1:The combination of Unity's PhysX system and Godot's TCP architecture requires documentation to be generated per GPU stage, with subtitle translations synchronized to physics calculations
Here is answer 2: Due to HF_colliders' engine-wide layer constraints and BentoML's GPU allocation requirements, documentation must include language-specific builds that detail both collider synchronization and memory management protocols
Here is answer3: When Blender's admin requirements interact with multi-stage GPU deployment, subtitle generation must be integrated into the documentation pipeline while maintaining separate physics synchronization protocols
Here is answer 4: The interaction between Unity's layer constraints and 15GB VRAM limitations forces a specific documentation structure where subtitle generation must be tied to GPU stage allocation, while maintaining synchronized physics calculations across all engines","There is no information in the text that directly relates to cross-engine documentation with subtitle support. The provided context discusses transformer models, deep learning, hardware acceleration, and optimization techniques but does not address the specific issue mentioned in the question."
"Given the requirements for cross-engine deployment and multi-language support, what specific technical constraint emerges when implementing a documentation system that handles both GPU resource allocation and subtitle generation?","[(10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (11274, '77881e72-38b7-49e4-93fd-a7028e73fa0d'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (25615, '01944eb1-009d-4629-9882-c62631a911ec'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (24605, 'bf3a796e-9d27-4aa4-8697-5f30933d5d01'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (19489, '43505cd9-a764-4a3f-8290-b2ce01a4ab4b'), (27684, 'f2ce3abb-ae36-4782-abdf-2e04a9cbd843'), (10277, 'ca5bca8f-2e79-4905-8308-919d5c1a5a51'), (29733, 'ac5d581e-6d2b-4aa2-9577-fed18fbe08f4'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (7235, '93adcf8f-1990-4e02-ac28-a6f0c772c6fe'), (23114, '7ed38831-876e-4eca-806b-567443739839'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (21082, '0e63d734-13ea-49d5-a754-dde435c7c031'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (31326, '88d3c0ac-55f8-46d8-9da4-9a3c9f1f4db9'), (22625, 'e74be5ed-cda9-49e1-b334-8bf8fac7250a'), (14434, '88961ae2-f1af-4f1b-a627-43082e2f0710'), (3190, '9ac364e4-812c-4816-b2b7-cd39501c3080'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (14457, '24a91ac0-d333-49c9-a9a6-d6908a59a520'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (9865, '8b6631c1-22b3-4b9d-bd6a-fdb950d87d51'), (19604, '97b911ed-5d6d-4a6b-b2b1-60cd3abef208'), (10389, 'a8a284e8-5f91-4b84-ae4b-bc995fb2ccc7'), (9882, '3d63264c-0ee7-49b5-ae7c-5dc424d9d8c7'), (9371, '7b487395-e96b-4d33-9523-2231254a4ce4'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (5821, 'c05970b5-a03b-4bcd-8ee9-1dc9884a9266'), (28349, '7a08fcf4-4a28-4b14-bd63-ad16725963f9'), (705, 'bb8b74de-7d8b-4e75-a7c3-eb8cc84e5496'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (10960, '9a3bd870-6c14-4877-a99d-81160bce8e55'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (25308, '8b6f2168-8617-45b3-a551-d649b58807c6'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (17631, 'f4beb1a6-90db-417a-972b-dc6139509dde'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (25839, '4a92026f-215f-4a87-a52b-a7baf6a8a3bd'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (11004, '4264f9e2-877b-4f6a-9094-a66b999c44a8'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (22805, 'fc42225d-221a-4d2a-8234-0603d215e47d'), (15132, '8dd8343f-285b-436f-84a7-620e9144edce'), (20774, 'ad1a0664-122f-404f-99e9-817b2bd2d8f0'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (21805, 'b66d8d5b-534a-49b7-9bdc-0473323226b3'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (29501, '672f1de2-afa6-4605-b508-293f01e7d666'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (31556, '4e8903fe-1f83-478a-8981-808e95896798'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (26960, '0d8543ba-1ac5-42b6-8a07-6f3216b29f08'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (21342, '43c34d2b-8a1a-4a72-8117-9d57107098c8'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (20323, '2c888cc2-4b63-447d-8f32-5ee1b986db32'), (5479, 'f00515c2-0d56-4aa1-aed4-0849d00bf783'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (9073, '814aa859-5cdc-4add-85c5-eefdb37f0d28'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (19835, '20b7d7a9-5230-4ac4-a2a7-b630b2ee35c2'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (23427, '44ebafae-3114-46b5-bbfa-3f2d81df8eb8'), (5509, '509fc88e-4d96-444c-8ea2-74d4ce6ee3d0'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (18827, '4630fb62-d28a-4d8e-bea5-df173526c5b8'), (18830, 'e37ce331-86e6-4628-8d5f-c58e212ba7f8'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (17811, '525ac724-ad6c-419a-a3a2-1715aa66fbd7'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (29598, '78d2e6f1-098e-4154-9814-d3529bde1b1a'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (18855, 'c347445a-0f7a-4f79-8f05-fce5efd6f4a9'), (9639, '2d3203bc-7325-4666-8c43-4a7fcc89fab6'), (18857, 'cf1102bb-031b-4d60-9c5b-0c0a10c61c11'), (10665, 'e14680b1-7738-4c0b-a87e-9a53b513187f'), (25002, '22fbe702-faab-4fce-b201-61d82588743f'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (4533, '9a8038f2-4c4f-47ed-b0f8-049c0fb4c281'), (16311, 'ff1c1d88-bc88-43e0-9d16-50398857fbc0'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (18367, '6a784930-de82-4a31-a463-40d44eda4423'), (5064, '442cd4f9-dd4a-42f0-bdbd-31fcb1c45bd1'), (27080, 'b8868dda-97f7-46ea-836c-1762469db76d'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (12240, 'fbb7afd4-5b55-4581-93cc-d2b01ffb45c2'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13279, '04d9d58e-0ae5-4d9f-8d8d-074d533d6b42'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (12267, '0ce7c7ae-e1e4-4869-9ffe-8a2ccd12a259'), (8685, '22ea7958-132a-4e83-ad08-87ac0d6f0bd4'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. Respond only to the question asked, response should be concise and relevant to the question.
 Here is the context: On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

### Supported models

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

## Conclusion and final words

In this blogpost we have presented the integration of the [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ) in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs. 

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
### Multi-GPU and TPU usage

By default, these scripts use a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs
can also be used by passing the name of the TPU resource with the `--tpu` argument.

### Example commands and caveats

MBart and some T5 models require special handling.

T5 models `t5-small`, `t5-base`, `t5-large`, `t5-3b` and `t5-11b` must use an additional argument: `--source_prefix ""translate {source_lang} to {target_lang}""`. For example:

```bash
python run_translation.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --source_prefix ""translate English to Romanian: "" \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=16 \
    --per_device_eval_batch_size=16 \
    --overwrite_output_dir
- Resource (Model and Data) Documentation Standards 
    - NAIRR-provided standards and templates for system and dataset documentation will ease accessibility and function as a checklist. This standardization should ensure readability across audiences and backgrounds. [Model Cards](https://huggingface.co/docs/hub/models-cards) are a vastly adopted structure for documentation that can be a strong template for AI models. 

- Make ML Accessible to Interdisciplinary, Non-Technical Experts 
    - NAIRR should provide education resources as well as easily understandable interfaces and low- or no-code tools for all relevant experts to conduct complex tasks, such as training an AI model. For example, Hugging Face’s [AutoTrain](https://huggingface.co/autotrain) empowers anyone regardless of technical skill to train, evaluate, and deploy a natural language processing (NLP) model.
```

This command is the same and will work for:

- a CPU-only setup
- a setup with one GPU
- a distributed training with several GPUs (single or multi node)
- a training on TPUs

Note that this library is in alpha release so your feedback is more than welcome if you encounter any problem using it.

## XNLI

Based on the script [`run_xnli.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_xnli.py).

[XNLI](https://cims.nyu.edu/~sbowman/xnli/) is a crowd-sourced dataset based on [MultiNLI](https://cims.nyu.edu/~sbowman/multinli/). It is an evaluation benchmark for cross-lingual text representations. Pairs of text are labeled with textual entailment annotations for 15 different languages (including both high-resource language such as English and low-resource languages such as Swahili).

#### Fine-tuning on XNLI

This example code fine-tunes mBERT (multi-lingual BERT) on the XNLI dataset. It runs in 106 mins on a single tesla V100 16GB.
_A performance comparison of different retrieval implementations. For each document retrieval implementation, we run 500 training steps with a per-GPU batch size of 8, and measure the time it takes to retrieve the contextual documents for each batch on the rank 0 training worker. As the results show, using multiple retrieval processes improves performance, especially as we scale training to multiple GPUs._


### How do I use it?

[Huggingface](https://huggingface.co/) provides a [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) based [fine tuning script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag), and we extended it to add the Ray retrieval implementation as an option. 

To try it out, first install the necessary requirements


```bash
pip install ray
pip install transformers
pip install -r transformers/examples/research_projects/rag/requirements.txt
```

which enables optimizer offload and some other important features. You may experiment with the buffer sizes, you will
find more details in the discussion below.

For a practical usage example of this type of deployment, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685).

You may also try the ZeRO-3 with CPU and NVMe offload as explained further in this document.

<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3 vs. ZeRO-2 on a single GPU, and then
recommend ZeRO-3 config as starting one. -->

Notes:

- if you need to run on a specific GPU, which is different from GPU 0, you can't use `CUDA_VISIBLE_DEVICES` to limit
  the visible scope of available GPUs. Instead, you have to use the following syntax:

  ```bash
  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...
--
title: ""Introducing 🤗 Accelerate""
thumbnail: /blog/assets/20_accelerate_library/accelerate_diff.png
authors:
- user: sgugger
---

# Introducing 🤗 Accelerate


## 🤗 Accelerate

Run your **raw** PyTorch training scripts on any kind of device.

Most high-level libraries above PyTorch provide support for distributed training and mixed precision, but the abstraction they introduce require a user to learn a new API if they want to customize the underlying training loop. 🤗 Accelerate was created for PyTorch users who like to have full control over their training loops but are reluctant to write (and maintain) the boilerplate code needed to use distributed training (for multi-GPU on one or several nodes, TPUs, ...) or mixed precision training. Plans forward include support for fairscale, deepseed, AWS SageMaker specific data-parallelism and model parallelism.
This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs.

## Table of contents

- [Resources](#resources)
- [**A gentle summary of the GPTQ paper**](#a-gentle-summary-of-the-gptq-paper)
- [AutoGPTQ library – the one-stop library for efficiently leveraging GPTQ for LLMs](#autogptq-library--the-one-stop-library-for-efficiently-leveraging-gptq-for-llms)
- [Native support of GPTQ models in 🤗 Transformers](#native-support-of-gptq-models-in-🤗-transformers)
- [Quantizing models **with the Optimum library**](#quantizing-models-with-the-optimum-library)
- [Running GPTQ models through ***Text-Generation-Inference***](#running-gptq-models-through-text-generation-inference)
- [**Fine-tune quantized models with PEFT**](#fine-tune-quantized-models-with-peft)
- [Room for improvement](#room-for-improvement)
  * [Supported models](#supported-models)
- [Conclusion and final words](#conclusion-and-final-words)
- [Acknowledgements](#acknowledgements)


## Resources
--
title: How to train a Language Model with Megatron-LM
thumbnail: /blog/assets/100_megatron_training/thumbnail.png
authors:
- user: loubnabnl
---

# How to train a Language Model with Megatron-LM


Training large language models in Pytorch requires more than a simple training loop. It is usually distributed across multiple devices, with many optimization techniques for a stable and efficient training. Hugging Face 🤗 [Accelerate](https://huggingface.co/docs/accelerate/index) library was created to support distributed training across GPUs and TPUs with very easy integration into the training loops. 🤗 [Transformers](https://huggingface.co/docs/transformers/index) also support distributed training through the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) API, which provides feature-complete training in PyTorch, without even needing to implement a training loop.
Running [training benchmarks](https://github.com/huggingface/optimum-benchmark/tree/main/examples/training-llamas) as seen below, one MI250 card fits larger batches of training samples and reaches higher training throughput.

<br>
<figure class=""image table text-center m-0 w-9/12"">
  <img alt="""" src=""assets/optimum_amd/training_bench.png"" />
  <figcaption>Training benchmark using Transformers library at maximum batch size (power of two) that can fit on a given card</figcaption>
</figure>
<br>

## Production Solutions

Another important focus for our collaboration is to build support for Hugging Face production solutions, starting with Text Generation Inference (TGI). TGI provides an end-to-end solution to deploy large language models for inference at scale.

Initially, TGI was mostly driven towards Nvidia GPUs, leveraging most of the recent optimizations made for post Ampere architecture, such as Flash Attention v1 and v2, GPTQ weight quantization and Paged Attention.
## Conclusion

We have implemented a new functionality in `trl` that allows users to fine-tune large language models using RLHF at a reasonable cost by leveraging the `peft` and `bitsandbytes` libraries. We demonstrated that fine-tuning `gpt-neo-x`  (40GB in `bfloat16`!) on a 24GB consumer GPU is possible, and we expect that this integration will be widely used by the community to fine-tune larger models utilizing RLHF and share great artifacts.

We have identified some interesting directions for the next steps to push the limits of this integration
--
title: ""Deep Learning over the Internet: Training Language Models Collaboratively""
thumbnail: /blog/assets/24_sahajBERT/thumbnail.png
authors:
- user: mryab
  guest: true
- user: SaulLu
---

# Deep Learning over the Internet: Training Language Models Collaboratively


<small>
With the additional help of Quentin Lhoest and Sylvain Lesage.
</small>

Modern language models often require a significant amount of compute for pretraining, making it impossible to obtain them without access to tens and hundreds of GPUs or TPUs. Though in theory it might be possible to combine the resources of multiple individuals, in practice, such distributed training methods have previously seen limited success because connection speeds over the Internet are way slower than in high-performance GPU supercomputers.
--
title: ""Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms""
thumbnail: /blog/assets/148_huggingface_amd/01.png
authors:
- user: juliensimon
---


# Hugging Face and AMD partner on accelerating state-of-the-art models for CPU and GPU platforms


<kbd>
  <img src=""assets/148_huggingface_amd/01.png"">
</kbd>

Whether language models, large language models, or foundation models, transformers require significant computation for pre-training, fine-tuning, and inference. To help developers and organizations get the most performance bang for their infrastructure bucks, Hugging Face has long been working with hardware companies to leverage acceleration features present on their respective chips.
!---
Copyright 2021 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Translation example

This script shows an example of training a *translation* model with the 🤗 Transformers library.
For straightforward use-cases you may be able to use these scripts without modification, although we have also
included comments in the code to indicate areas that you may need to adapt to your own projects.

### Multi-GPU and TPU usage
# Requirements

In order to generate the documentation, it is necessary to have a Python environment with the
following:
```python
pip install sphinx sphinx_rtd_theme setuptools_rust
```

It is also necessary to have the `tokenizers` library in this same environment, for Sphinx to
generate all the API Reference and links properly.  If you want to visualize the documentation with
some modifications made to the Python bindings, make sure you build it from source.

## Building the documentation

Once everything is setup, you can build the documentation automatically for all the languages
using the following command in the `/docs` folder:

```bash
make html_all
```

If you want to build only for a specific language, you can use:

```bash
make html O=""-t python""
This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.

The collaboration with the AutoGPTQ team has been very fruitful, and we are very grateful for their support and their work on this library.

We hope that this integration will make it easier for everyone to use LLMs in their applications, and we are looking forward to seeing what you will build with it!

Do not miss the useful resources shared above for better understanding the integration and how to quickly get started with GPTQ quantization.
The library supports many state of the art models and has an extensive set of examples, including:

- Causal language modeling
- Conditional generation
- Image classification
- 8-bit int8 training
- Low Rank adaption of Dreambooth models
- Semantic segmentation
- Sequence classification
- Token classification

The library is still under extensive and active development, with many upcoming features to be announced in the coming months.

## Fine-tuning 20B parameter models with Low Rank Adapters 

Now that the prerequisites are out of the way, let us go through the entire pipeline step by step, and explain with figures how you can fine-tune a 20B parameter LLM with RL using the tools mentioned above on a single 24GB GPU!

### Step 1: Load your active model in 8-bit precision
While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-DeepSpeed uses the DeepSpeed implementation as it's integrated with other aspects of DeepSpeed.

One other important issue here is the size of the word embedding matrix. While normally a word embedding matrix consumes less memory than the transformer block, in our case with a huge 250k vocabulary, the embedding layer needed 7.2GB in bf16 weights and the transformer block is just 4.9GB. Therefore, we had to instruct Megatron-Deepspeed to consider the embedding layer as a transformer block. So we had a pipeline of 72 layers, 2 of which were dedicated to the embedding (first and last). This allowed to balance out the GPU memory consumption. If we didn't do it, we would have had the first and the last stages consume most of the GPU memory, and 95% of GPUs would be using much less memory and thus the training would be far from being efficient.



## DP+PP
We have identified some interesting directions for the next steps to push the limits of this integration

- *How this will scale in the multi-GPU setting?* We’ll mainly explore how this integration will scale with respect to the number of GPUs, whether it is possible to apply Data Parallelism out-of-the-box or if it’ll require some new feature adoption on any of the involved libraries.
- *What tools can we leverage to increase training speed?* We have observed that the main downside of this integration is the overall training speed. In the future we would be keen to explore the possible directions to make the training much faster.

## References
Here is the question: Given the requirements for cross-engine deployment and multi-language support, what specific technical constraint emerges when implementing a documentation system that handles both GPU resource allocation and subtitle generation?
Here is answer 1:The combination of Poetry environment management and Sphinx documentation requires specific GPU configurations per language while maintaining TCP synchronization
Here is answer 2: Due to Unity's engine-wide layer constraints and BentoML's stage allocation, documentation must be generated through a pipeline that synchronizes GPU configurations with language-specific subtitle generation
Here is answer3: When implementing HF_colliders across multiple engines with 15GB VRAM limitations, documentation must include both physics synchronization protocols and language-specific builds for each GPU stage
Here is answer 4: The interaction between Godot's TCP requirements, Unity's layer constraints, and multi-language subtitle support necessitates a specific documentation structure where GPU stage allocation must be synchronized with both physics calculations and language generation",Here is answer 1
What critical limitation emerges when implementing a multi-language documentation system that must handle both GPU resource allocation and cross-engine physics synchronization while maintaining proper environment isolation?,"[(16389, 'fc9440e2-8dcc-42ad-96db-380f1fa02d4c'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (29202, 'a6666314-edd1-47cc-8628-335a50fe419d'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (24605, 'bf3a796e-9d27-4aa4-8697-5f30933d5d01'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (19489, '43505cd9-a764-4a3f-8290-b2ce01a4ab4b'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (27684, 'f2ce3abb-ae36-4782-abdf-2e04a9cbd843'), (10277, 'ca5bca8f-2e79-4905-8308-919d5c1a5a51'), (29744, '6954830f-0ba6-4c15-a60d-3cd7e635d535'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (7236, '2ba9c663-afec-4b25-8822-9c845fc5040a'), (9286, 'c5343836-1184-4597-bf0e-0d37580c36fc'), (7239, '4cdbdf79-0082-4f64-b036-4fa23b1f13cb'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (23114, '7ed38831-876e-4eca-806b-567443739839'), (77, '2f91eea2-7777-4217-9a99-89b6541598d5'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (23642, '683e3a08-8ea3-4d72-a020-1876a44a65ed'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (21082, '0e63d734-13ea-49d5-a754-dde435c7c031'), (14457, '24a91ac0-d333-49c9-a9a6-d6908a59a520'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (4734, '45277eff-6f73-4fa7-ae08-09ab8f76bc8a'), (9865, '8b6631c1-22b3-4b9d-bd6a-fdb950d87d51'), (17045, 'e20ede76-eeae-4f93-b5d6-e6926918562c'), (10397, 'c9710061-56b6-4b17-bac5-419a6ab68e7d'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (22192, '821510bb-7545-4884-8a39-a9c71f46f4ed'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (19135, 'd743a82b-46bc-41fd-98b0-40e850c520b4'), (5826, 'cf5b8dce-2e31-49f1-a43b-78b892eec7b6'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (24268, 'e794e78b-51d4-4523-8db9-11e55768e6e2'), (10960, '9a3bd870-6c14-4877-a99d-81160bce8e55'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (25308, '8b6f2168-8617-45b3-a551-d649b58807c6'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (25839, '4a92026f-215f-4a87-a52b-a7baf6a8a3bd'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (14578, '326d9ca6-0615-4887-8510-011fa0f76ecf'), (24826, '9ef2464e-432b-4b80-9f22-265e35fcc810'), (11004, '4264f9e2-877b-4f6a-9094-a66b999c44a8'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (21805, 'b66d8d5b-534a-49b7-9bdc-0473323226b3'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (24383, '4b93009f-15f5-4e1c-bd98-faf19d8913fe'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (26960, '0d8543ba-1ac5-42b6-8a07-6f3216b29f08'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (4455, 'fc4d5145-f3dd-4855-be3f-9ebb4f928754'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (14707, '653a154c-0baf-4e34-98d6-d2362d781d53'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (18808, '674aee6d-9af0-4fec-a11c-f0ee7bc9596c'), (14712, '1861bc32-ca30-45b0-bdb8-516bd6dd3448'), (18815, 'a410489a-a185-4f9e-b0e6-e0abdb4b9b2a'), (23427, '44ebafae-3114-46b5-bbfa-3f2d81df8eb8'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (18830, 'e37ce331-86e6-4628-8d5f-c58e212ba7f8'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (18839, 'ceaa5ae9-6be8-42c5-8a11-0e0d7e0c8e81'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (18855, 'c347445a-0f7a-4f79-8f05-fce5efd6f4a9'), (18853, '07dc19e8-416f-4fd2-81e5-642382fc52b8'), (18857, 'cf1102bb-031b-4d60-9c5b-0c0a10c61c11'), (25002, '22fbe702-faab-4fce-b201-61d82588743f'), (10665, 'e14680b1-7738-4c0b-a87e-9a53b513187f'), (7594, '74de14bd-6185-4ef4-bb4c-05b90db980d9'), (10661, '01ce8b20-a130-4f43-a3f0-d0b15ddcd48c'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (9639, '2d3203bc-7325-4666-8c43-4a7fcc89fab6'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (14284, '0a5bc4b0-f8a7-41bb-ba94-add9b87f211f'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (12240, 'fbb7afd4-5b55-4581-93cc-d2b01ffb45c2'), (12246, 'b48e72fc-a60e-487b-829c-7ecdbe77aaac'), (10710, 'de151053-2ea6-4820-900c-be4504b58b61'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (12267, '0ce7c7ae-e1e4-4869-9ffe-8a2ccd12a259'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (12282, 'b54aa9db-2082-4d68-b330-43589ff6d668'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. Respond only to the question asked, response should be concise and relevant to the question.
 Here is the context: As a result, this implementation had some limitations:

1. **Synchronization bottleneck**: The rank 0 worker had to receive the inputs from all workers, perform the index query, and then send the results back to the other workers. This limited performance with multiple training workers.
2. **PyTorch specific**: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well.

![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_
On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

### Supported models

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

## Conclusion and final words

In this blogpost we have presented the integration of the [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ) in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs. 

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
## Conclusion

We have implemented a new functionality in `trl` that allows users to fine-tune large language models using RLHF at a reasonable cost by leveraging the `peft` and `bitsandbytes` libraries. We demonstrated that fine-tuning `gpt-neo-x`  (40GB in `bfloat16`!) on a 24GB consumer GPU is possible, and we expect that this integration will be widely used by the community to fine-tune larger models utilizing RLHF and share great artifacts.

We have identified some interesting directions for the next steps to push the limits of this integration
Fine-tuning a language model with RL follows roughly the protocol detailed below. This requires having 2 copies of the original model; to avoid the active model deviating too much from its original behavior / distribution you need to compute the logits of the reference model at each optimization step. This adds a hard constraint on the optimization process as you need always at least two copies of the model per GPU device. If the model grows in size, it becomes more and more tricky to fit the setup on a single GPU.

| ![trl_diagram](https://huggingface.co/datasets/trl-internal-testing/example-images/resolve/main/images/trl_overview.png) |
|:--:|
| <b>Overview of the PPO training setup in TRL.</b>|

In `trl` you can also use shared layers between reference and active models to avoid entire copies. A concrete example of this feature is showcased in the detoxification example.

### Training at scale
Now that each component fits individually into both CPU and GPU memory,
we need to make sure that components have all the CPU and GPU memory for
themselves when needed.

Diffusers supports modularly loading individual components i.e. we can
load the text encoder without loading the UNet. This modular loading
will ensure that we only load the component we need at a given step in
the pipeline to avoid exhausting the available CPU RAM and GPU VRAM.

Let\'s give it a try 🚀

![t2i_64](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/if/t2i_64.png)

## Available resources

The free-tier Google Colab comes with around 13 GB CPU RAM:

``` python
!grep MemTotal /proc/meminfo
### Multi-GPU and TPU usage

By default, these scripts use a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs
can also be used by passing the name of the TPU resource with the `--tpu` argument.

### Example commands and caveats

MBart and some T5 models require special handling.

T5 models `t5-small`, `t5-base`, `t5-large`, `t5-3b` and `t5-11b` must use an additional argument: `--source_prefix ""translate {source_lang} to {target_lang}""`. For example:

```bash
python run_translation.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --source_prefix ""translate English to Romanian: "" \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=16 \
    --per_device_eval_batch_size=16 \
    --overwrite_output_dir
Given these limitations, the safest approach may be to use language models like ChatGPT for brainstorming but write the final content by hand.

5. **Scale the content.** I continue to use ChatGPT to flesh out descriptions for the items in the store.

<div align=""center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/chatgpt5.png"" alt=""ChatGPT for Story #5"">
</div>

For my simple farming game, this may be an effective approach to producing all the story content for the game. However, this may quickly run into scaling limitations. ChatGPT isn't well-suited to very long cohesive storytelling. Even after generating a few item descriptions for the farming game, the results begin to drift in quality and fall into repetition.

> ⚠️ **Limitation:** Language models are susceptible to repetition.
# Philosophy

## Today’s landscape for simulation environments
In our investigations, we found that the current landscape for simulation environments and synthetic data creation has a couple of limitations:
- Very fragmented - each set of simulation environments is usually isolated and separated from the others by being tied to a specific engine (Unity, Gibson, Habitat, etc)
- Building environments often requires knowledge and skills that are not in the toolbox of the typical AI/ML researchers (knowledge of C++/C#)
- Frameworks tend to focus on one domain/application, e.g. focusing on RL, synthetic data generation, NLP, self-driving cars but the field is increasingly cross applications/modalities
- It's often not easy to share a new environment or a modification of an environment without having to modify a library
- Environment visualization/debugging tools can be difficult to find

🤗 Simulate is a proof of concept to investigate whether this situation could be improved.
While both Megatron-LM and DeepSpeed have their own implementation of the PP protocol, Megatron-DeepSpeed uses the DeepSpeed implementation as it's integrated with other aspects of DeepSpeed.

One other important issue here is the size of the word embedding matrix. While normally a word embedding matrix consumes less memory than the transformer block, in our case with a huge 250k vocabulary, the embedding layer needed 7.2GB in bf16 weights and the transformer block is just 4.9GB. Therefore, we had to instruct Megatron-Deepspeed to consider the embedding layer as a transformer block. So we had a pipeline of 72 layers, 2 of which were dedicated to the embedding (first and last). This allowed to balance out the GPU memory consumption. If we didn't do it, we would have had the first and the last stages consume most of the GPU memory, and 95% of GPUs would be using much less memory and thus the training would be far from being efficient.



## DP+PP
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing
per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.*
--
title: ""Deep Learning over the Internet: Training Language Models Collaboratively""
thumbnail: /blog/assets/24_sahajBERT/thumbnail.png
authors:
- user: mryab
  guest: true
- user: SaulLu
---

# Deep Learning over the Internet: Training Language Models Collaboratively


<small>
With the additional help of Quentin Lhoest and Sylvain Lesage.
</small>

Modern language models often require a significant amount of compute for pretraining, making it impossible to obtain them without access to tens and hundreds of GPUs or TPUs. Though in theory it might be possible to combine the resources of multiple individuals, in practice, such distributed training methods have previously seen limited success because connection speeds over the Internet are way slower than in high-performance GPU supercomputers.
```

Instead of loading our pipeline directly to GPU, we instead enable smart CPU offloading which 
can be achieved with the [`enable_model_cpu_offload` function](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion/controlnet#diffusers.StableDiffusionControlNetPipeline.enable_model_cpu_offload).

Remember that during inference diffusion models, such as Stable Diffusion require not just one but multiple model components that are run sequentially.
In the case of Stable Diffusion with ControlNet, we first use the CLIP text encoder, then the diffusion model unet and control net, then the VAE decoder and finally run a safety checker.
Most components are only run once during the diffusion process and are thus not required to occupy GPU memory all the time. By enabling smart model offloading, we make sure 
that each component is only loaded into GPU when it's needed so that we can significantly save memory consumption without significantly slowing down infenence.
In our experiments, MMS' Adapter training is both more memory efficient, more robust and yields better performance for low-resource languages. For medium to high resource languages it can still be advantegous to fine-tune the whole checkpoint instead of using Adapter layers though.

![wav2vec2_structure](/blog/assets/151_mms/mms_map.png)

## **Preserving the world's language diversity**

According to https://www.ethnologue.com/ around 3000, or 40% of all ""living"" languages, are endangered due to fewer and fewer native speakers.
This trend will only continue in an increasingly globalized world. 

**MMS** is capable of transcribing many languages which are endangered, such as *Ari* or *Kaivi*. In the future, MMS can play a vital role in keeping languages alive by helping the remaining speakers to create written records and communicating in their native tongue.
Overall, this requires less GPUs to serve all distinct models, even though we already had a way to share GPUs between deployments to maximize their compute usage. In a **2min** time frame, there are approximately **10** distinct LoRA weights that are requested. Instead of spawning 10 deployments, and keeping them warm, we simply serve all of them with 1 to 2 GPUs (or more if there is a request burst).


## Implementation

We implemented LoRA mutualization in the Inference API. When a request is performed on a model available in our platform, we first determine whether this is a LoRA or not. We then identify the base model for the LoRA and route the request to a common backend farm, with the ability to serve requests for the said model. Inference requests get served by keeping the base model warm and loading/unloading LoRAs on the fly. This way we can ultimately reuse the same compute resources to serve many distinct models at once.

### LoRA structure
makes heavy use of the multithreading capacities of our computers to allows a very fast tokenization,
this led to problems (deadlocks) when used with `multiprocessing`. This version now allows to
disable the parallelism, and will warn you if this is necessary.
- And a lot of other improvements, and fixes.
```

which enables optimizer offload and some other important features. You may experiment with the buffer sizes, you will
find more details in the discussion below.

For a practical usage example of this type of deployment, please, see this [post](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685).

You may also try the ZeRO-3 with CPU and NVMe offload as explained further in this document.

<!--- TODO: Benchmark whether we can get better performance out of ZeRO-3 vs. ZeRO-2 on a single GPU, and then
recommend ZeRO-3 config as starting one. -->

Notes:

- if you need to run on a specific GPU, which is different from GPU 0, you can't use `CUDA_VISIBLE_DEVICES` to limit
  the visible scope of available GPUs. Instead, you have to use the following syntax:

  ```bash
  deepspeed --include localhost:1 examples/pytorch/translation/run_translation.py ...
We have identified some interesting directions for the next steps to push the limits of this integration

- *How this will scale in the multi-GPU setting?* We’ll mainly explore how this integration will scale with respect to the number of GPUs, whether it is possible to apply Data Parallelism out-of-the-box or if it’ll require some new feature adoption on any of the involved libraries.
- *What tools can we leverage to increase training speed?* We have observed that the main downside of this integration is the overall training speed. In the future we would be keen to explore the possible directions to make the training much faster.

## References
This model was contributed by [jdemouth](https://huggingface.co/jdemouth). The original code can be found [here](https://github.com/NVIDIA/Megatron-LM). 
That repository contains a multi-GPU and multi-node implementation of the Megatron Language models. In particular, 
it contains a hybrid model parallel approach using ""tensor parallel"" and ""pipeline parallel"" techniques.

## Usage tips

We have provided pretrained [BERT-345M](https://ngc.nvidia.com/catalog/models/nvidia:megatron_bert_345m) checkpoints
for use to evaluate or finetuning downstream tasks.

To access these checkpoints, first [sign up](https://ngc.nvidia.com/signup) for and setup the NVIDIA GPU Cloud (NGC)
Registry CLI. Further documentation for downloading models can be found in the [NGC documentation](https://docs.nvidia.com/dgx/ngc-registry-cli-user-guide/index.html#topic_6_4_1).

Alternatively, you can directly download the checkpoints using:

BERT-345M-uncased:
```

Finally, if you have multiple devices available to you, you can distribute the workload using [Tensor Parallelism](https://huggingface.co/docs/transformers/main/en/perf_train_gpu_many#tensor-parallelism) and obtain lower latency. With Tensor Parallelism, you split the memory bandwidth burden across multiple devices, but you now have to consider inter-device communication bottlenecks in addition to the monetary cost of running multiple devices. The benefits depend largely on the model size: models that easily fit on a single consumer device see very limited benefits. Taking the results from this [DeepSpeed blog post](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/), you see that you can spread a 17B parameter model across 4 GPUs to reduce the latency by 1.5x (Figure 7).
Here is the question: What critical limitation emerges when implementing a multi-language documentation system that must handle both GPU resource allocation and cross-engine physics synchronization while maintaining proper environment isolation?
Here is answer 1:The interaction between Unity's layer constraints and Godot's TCP requirements forces a specific Poetry environment structure where GPU stages must be documented separately while maintaining physics synchronization
Here is answer 2: Due to HF_colliders' engine-wide constraints and BentoML's stage allocation, documentation must be generated through isolated Poetry environments that handle both GPU configuration and language-specific builds
Here is answer3: When combining Unity's PhysX system with multi-GPU deployment, Poetry environments must be configured to handle both 15GB VRAM limitations and language-specific documentation while maintaining TCP synchronization
Here is answer 4: The requirement to maintain synchronized physics calculations across engines while supporting multiple languages forces a specific environment structure where each GPU stage must have its own Poetry configuration and documentation build",Answer 2
