### Question 1:
**Which statement best explains why the Advantage Actor-Critic (A2C) method uses the Advantage function instead of directly using a Q-function in reinforcement learning?**

1. It reduces the variance of policy updates by subtracting the state-value function from the action-value function, which is directly proportional to TD targets.  
2. The Q-function inherently provides biased results for stochastic policies due to its inability to generalize across episodes.  
3. The Advantage function is computationally simpler than using the state-value function alone for training.  
4. Using the Advantage function enables a Monte Carlo update instead of a Temporal Difference (TD) update.

**Correct Answer**: 1  
**Reasoning**: The Advantage function stabilizes training by reducing variance and focusing on the relative benefit of an action compared to the state average.  

**Document Usage**:  
- **Document 83**: Discusses the relationship between state-value and action-value functions, providing context for subtracting the mean value.  
- **Document 468**: Summarizes value-based methods and their reliance on expected returns.  
- **Document 485**: Defines strategies like TD learning and Advantage function nuances.  
- **Document 729**: Explains the role of action-value functions in off-policy learning.  
- **Document 971**: Recaps RL's goal of maximizing expected cumulative reward, relevant to variance.  
- **Document 1306**: Specifically introduces the Advantage Actor-Critic (A2C) approach and the importance of variance reduction.  
- **Document 1674**: Covers Bellman equations and their relevance to calculating state-value functions.  
- **Document 1954**: Discusses Q-learning and contrasts it with Advantage updates.  
- **Document 1968**: Describes indirect training of policies via value functions.  
- **Document 2011**: Reinforces action-value calculations via Q-functions.  
- **Document 2143**: Revisits Q-learning updates that are contrasted in A2C.  
- **Document 2451**: Emphasizes learning strategies like TD learning and Monte Carlo in reducing computational cost.  
- **Document 2517**: Explicitly details the role of Advantage functions in A2C, emphasizing variance reduction.

**Distractors**:  
- (2) Incorrect, as Q-functions are not inherently biased.  
- (3) Misleading, Advantage requires both Q and state-value computations.  
- (4) Incorrectly associates Advantage with Monte Carlo updates.  

---

### Question 2:
**Why is the Epsilon-Greedy Strategy particularly suitable for Q-learning but less emphasized in Advantage Actor-Critic (A2C)?**

1. Q-learning requires a balance between exploration and exploitation, which A2C handles through dynamic policy gradient updates.  
2. Advantage Actor-Critic (A2C) relies entirely on deterministic policies, making exploration redundant.  
3. Epsilon-Greedy Strategy is incompatible with continuous action spaces, which A2C focuses on.  
4. Epsilon decay over time is not compatible with A2C's value-based computations.

**Correct Answer**: 1  
**Reasoning**: Q-learning uses Epsilon-Greedy to manage exploration, while A2C learns dynamically through policy updates.  

**Document Usage**:  
- **Document 83**: Explains Greedy Policy and its relationship with Q-learning.  
- **Document 468**: Summarizes Q-learning's exploration-exploitation trade-off.  
- **Document 485**: Defines Epsilon-Greedy explicitly.  
- **Document 729**: Shows Q-learning's reliance on Epsilon-Greedy.  
- **Document 971**: Highlights exploration-exploitation as core RL challenges.  
- **Document 1306**: Describes how A2C reduces dependence on predefined exploration strategies.  
- **Document 1674**: Bellman insights highlight Q-learning’s reliance on specific policies.  
- **Document 1954**: Tests Epsilon-Greedy application in Q-learning quizzes.  
- **Document 1968**: Differentiates policy-based and value-based strategies.  
- **Document 2011**: Reinforces Epsilon-Greedy in Q-learning.  
- **Document 2143**: Connects Q-learning’s trade-off to Epsilon strategies.  
- **Document 2451**: Notes differences in methods like TD and Monte Carlo.  
- **Document 2517**: A2C’s approach minimizes direct exploration need.  

**Distractors**:  
- (2) Incorrect, as A2C can handle stochastic policies.  
- (3) Misleading, Epsilon-Greedy could be adapted to continuous spaces but isn’t used in A2C.  
- (4) Incorrect, Epsilon decay is unrelated to value-based calculations.  

---

### Question 3:
**What is the fundamental computational advantage of the Bellman equation in reinforcement learning methods?**

1. It transforms the expected cumulative return calculation into a recursive formula involving immediate rewards and discounted future values.  
2. It eliminates the need for discount factors by iteratively solving state values.  
3. It computes the optimal policy directly without requiring policy updates.  
4. It averages state-action values across all possible policies to improve learning efficiency.

**Correct Answer**: 1  
**Reasoning**: Bellman simplifies cumulative calculations using recursion.  

**Document Usage**:  
- **Document 83**: Introduces value functions and Bellman applications.  
- **Document 468**: Summarizes Bellman’s role in value-based methods.  
- **Document 485**: Defines Bellman as central to RL.  
- **Document 729**: Shows Bellman in Q-function updates.  
- **Document 971**: Provides foundational RL context for Bellman’s purpose.  
- **Document 1306**: Differentiates between policy updates and Bellman recursion.  
- **Document 1674**: Details Bellman equations thoroughly.  
- **Document 1954**: Quiz tests Bellman applications in Q-learning.  
- **Document 1968**: Contrasts Bellman with direct policy methods.  
- **Document 2011**: Highlights recursive Q-value updates via Bellman.  
- **Document 2143**: Builds on Bellman for Q-learning.  
- **Document 2451**: Recaps Bellman’s computational advantages.  
- **Document 2517**: Discusses TD error’s use in approximating Bellman-derived advantages.  

**Distractors**:  
- (2) Misleading, Bellman relies on discount factors.  
- (3) Incorrect, Bellman doesn’t directly compute policies.  
- (4) Misinterprets averaging as Bellman’s function.  

---

### Question 4:
**Why does Temporal Difference (TD) Learning update the value function after a single step rather than waiting for an entire episode, as in Monte Carlo methods?**

1. TD Learning uses an estimate of the return based on the next state’s value to reduce computational cost and enable online learning.  
2. Monte Carlo methods are inherently biased, whereas TD Learning uses unbiased value estimates for updates.  
3. TD Learning is restricted to deterministic policies, making single-step updates essential.  
4. Monte Carlo updates are incompatible with environments that have continuous state spaces, unlike TD Learning.

**Correct Answer**: 1  
**Reasoning**: TD Learning approximates returns incrementally, allowing for computational efficiency and online learning.  

**Document Usage**:  
- **Document 83**: Introduces the Bellman equation, crucial for TD target calculations.  
- **Document 468**: Summarizes RL processes and methods, contrasting Monte Carlo and TD updates.  
- **Document 485**: Defines and compares Monte Carlo and TD Learning.  
- **Document 729**: Explains TD’s role in Q-learning updates.  
- **Document 971**: Describes iterative learning methods in RL, emphasizing TD's role in efficient learning.  
- **Document 1306**: Differentiates Actor-Critic updates from TD’s step-by-step value updates.  
- **Document 1674**: Covers Bellman recursion, forming the basis of TD Learning.  
- **Document 1954**: Includes quiz questions comparing TD and Monte Carlo learning.  
- **Document 1968**: Highlights TD as foundational in value-based approaches.  
- **Document 2011**: TD updates in Q-learning are detailed here.  
- **Document 2143**: Discusses TD as a fundamental concept in RL.  
- **Document 2451**: Recaps learning strategies, emphasizing TD targets.  
- **Document 2517**: Uses TD error in Advantage Actor-Critic (A2C), emphasizing its efficiency.  

**Distractors**:  
- (2) Incorrect, Monte Carlo isn’t inherently biased.  
- (3) Misleading, TD works with stochastic policies as well.  
- (4) Incorrect, both methods can handle continuous spaces with proper approximations.  

---

### Question 5:
**What is the primary reason Q-Learning is considered an off-policy algorithm?**

1. The acting policy during training differs from the policy used to evaluate future states.  
2. Q-Learning explicitly avoids exploration and relies solely on exploitation during updates.  
3. The updates depend entirely on the agent’s optimal actions at every state without stochasticity.  
4. The Q-function is calculated after the agent completes an episode, unlike other RL algorithms.

**Correct Answer**: 1  
**Reasoning**: Q-Learning uses a greedy policy for updates and an epsilon-greedy policy for acting.  

**Document Usage**:  
- **Document 83**: Differentiates greedy policies from epsilon-greedy policies.  
- **Document 468**: Introduces off-policy learning in RL.  
- **Document 485**: Defines off-policy algorithms, emphasizing Q-Learning.  
- **Document 729**: Explains how Q-Learning uses off-policy TD updates.  
- **Document 971**: Describes RL processes where acting and updating policies differ.  
- **Document 1306**: Highlights differences between off-policy (Q-Learning) and policy-based methods.  
- **Document 1674**: Bellman’s equation as used in Q-learning is central to off-policy behavior.  
- **Document 1954**: Quiz questions reinforce Q-learning’s off-policy nature.  
- **Document 1968**: Contrasts policy-based and off-policy value-based methods.  
- **Document 2011**: Reiterates Q-learning’s off-policy strategy.  
- **Document 2143**: Discusses greedy updates in off-policy algorithms.  
- **Document 2451**: Recaps Q-learning as off-policy learning.  
- **Document 2517**: Compares Q-Learning’s off-policy behavior with A2C’s on-policy updates.  

**Distractors**:  
- (2) Incorrect, Q-learning balances exploration and exploitation.  
- (3) Misleading, Q-learning supports stochasticity in acting policies.  
- (4) Incorrect, updates occur incrementally during episodes.  

---

### Question 6:
**How does the Advantage function improve upon the simple use of Q-values in Actor-Critic methods?**

1. It measures how much better an action is compared to the average value of a state, improving stability and variance.  
2. It entirely replaces the state-value function to reduce computation time and memory requirements.  
3. The Advantage function allows deterministic policy gradients, which is not feasible with Q-values.  
4. It ensures that actions leading to rewards are reinforced without considering state values.

**Correct Answer**: 1  
**Reasoning**: The Advantage function highlights relative benefits of actions, stabilizing updates.  

**Document Usage**:  
- **Document 83**: Discusses value functions and relative state-action evaluations.  
- **Document 468**: Summarizes methods improving policy stability.  
- **Document 485**: Defines the Advantage function’s role in variance reduction.  
- **Document 729**: Contrasts Q-value and Advantage-based updates.  
- **Document 971**: Explains RL goals, requiring efficient policy updates.  
- **Document 1306**: Introduces Advantage Actor-Critic and variance reduction.  
- **Document 1674**: Bellman insights relevant for Q-function usage.  
- **Document 1954**: Covers quiz applications of Advantage and Q-values.  
- **Document 1968**: Differentiates policy-based methods reliant on gradients.  
- **Document 2011**: Explains Q-value use in policy derivation.  
- **Document 2143**: Reinforces Q-learning’s distinct reliance on Q-values.  
- **Document 2451**: Recaps variance reduction strategies.  
- **Document 2517**: Details Advantage-based updates in A2C.  

**Distractors**:  
- (2) Incorrect, Advantage supplements, not replaces, state-value functions.  
- (3) Misleading, stochastic gradients can also use Advantage functions.  
- (4) Ignores critical state-value considerations.  

---

### Question 7:
**Why does Q-Learning perform better than Monte Carlo methods in environments with a large state space?**

1. Q-Learning incrementally updates the Q-values at each step using the Bellman equation, avoiding the need to store entire episodes.  
2. Monte Carlo methods are inherently biased, leading to suboptimal policies in large state spaces.  
3. Q-Learning uses deterministic updates that are better suited to large state spaces, while Monte Carlo requires stochastic sampling.  
4. Monte Carlo methods cannot calculate Q-values accurately for large state spaces because they do not rely on immediate rewards.

**Correct Answer**: 1  
**Reasoning**: Q-Learning’s incremental updates avoid the memory and computational burden of storing complete episodes, which is advantageous in large state spaces.  

**Document Usage**:  
- **Document 83**: Explains Bellman equation’s recursive role in simplifying updates.  
- **Document 468**: Summarizes differences between Monte Carlo and TD-based methods like Q-Learning.  
- **Document 485**: Defines Monte Carlo and Q-Learning, emphasizing incremental learning.  
- **Document 729**: Explains Q-Learning’s step-by-step TD updates.  
- **Document 971**: Discusses state-action value computation challenges in RL.  
- **Document 1306**: Highlights advantages of incremental updates in policy evaluation.  
- **Document 1674**: Details Bellman’s efficiency in state-action calculations.  
- **Document 1954**: Quiz questions contrast Monte Carlo and Q-Learning.  
- **Document 1968**: Contrasts value-based methods with complete trajectory-based methods.  
- **Document 2011**: Reinforces Bellman-driven updates in Q-Learning.  
- **Document 2143**: Discusses Q-Learning’s suitability for large state spaces.  
- **Document 2451**: Recaps efficiency differences between Monte Carlo and TD learning.  
- **Document 2517**: Notes how Actor-Critic mitigates large state space issues.  

**Distractors**:  
- (2) Incorrect, Monte Carlo methods are unbiased, though computationally expensive.  
- (3) Misleading, both methods can handle stochastic environments.  
- (4) Incorrect, Monte Carlo does use immediate rewards for its return calculations.  

---

### Question 8:
**How does the Actor in Actor-Critic methods differ fundamentally from a policy learned via Q-Learning?**

1. The Actor explicitly learns a probability distribution over actions, whereas Q-Learning uses a derived greedy policy.  
2. The Actor operates only in continuous action spaces, while Q-Learning is restricted to discrete spaces.  
3. The Actor is trained using state-value functions, while Q-Learning relies exclusively on action-value functions.  
4. The Actor calculates optimal actions based on full episodes, unlike Q-Learning’s step-based updates.

**Correct Answer**: 1  
**Reasoning**: Actor-Critic methods directly optimize a policy function (the Actor), which produces action probabilities, unlike the derived greedy policies in Q-Learning.  

**Document Usage**:  
- **Document 83**: Differentiates policy and value-based methods, introducing greedy policies.  
- **Document 468**: Highlights policy-based methods that optimize distributions.  
- **Document 485**: Defines strategies like Actor-Critic that blend policy and value methods.  
- **Document 729**: Details Q-Learning’s reliance on derived policies.  
- **Document 971**: Recaps differences between directly trained policies and derived ones.  
- **Document 1306**: Explains Actor-Critic’s dual-function approach.  
- **Document 1674**: Discusses value function use in Q-learning versus policy updates in Actor-Critic.  
- **Document 1954**: Quiz compares policy derivation in Q-Learning and Actor-Critic.  
- **Document 1968**: Contrasts direct policy optimization with value-function approaches.  
- **Document 2011**: Reinforces greedy policy derivation in Q-Learning.  
- **Document 2143**: Connects Q-Learning’s derived policies to greedy strategies.  
- **Document 2451**: Recaps foundational differences between policy and value methods.  
- **Document 2517**: Describes Actor’s probabilistic action selection.  

**Distractors**:  
- (2) Misleading, Actor-Critic methods work in both action spaces.  
- (3) Incorrect, Actor-Critic combines value and action functions.  
- (4) Misleading, Actor updates can occur step-by-step.  

---

### Question 9:
**Why is the Bellman equation critical for both Q-Learning and Advantage Actor-Critic methods, despite their differences in implementation?**

1. It provides the recursive framework for estimating value functions used in both methods.  
2. It ensures that policies derived from Q-functions are always deterministic, even in stochastic environments.  
3. It calculates the full expected return for each state-action pair, essential for both methods.  
4. It serves as the foundation for Monte Carlo methods, which both approaches rely on.

**Correct Answer**: 1  
**Reasoning**: The Bellman equation supports the recursive value estimations needed in both Q-Learning and A2C.  

**Document Usage**:  
- **Document 83**: Introduces Bellman as foundational to value-based methods.  
- **Document 468**: Summarizes Bellman’s role in RL frameworks.  
- **Document 485**: Discusses recursive value estimations in RL.  
- **Document 729**: Shows Bellman’s direct application in Q-Learning.  
- **Document 971**: Recaps RL goals tied to Bellman’s structure.  
- **Document 1306**: Highlights Bellman recursion in Actor-Critic updates.  
- **Document 1674**: Elaborates on Bellman’s derivations for state and action values.  
- **Document 1954**: Quiz reinforces Bellman’s applications.  
- **Document 1968**: Contrasts Bellman-driven methods with policy-based methods.  
- **Document 2011**: Details Bellman’s integral role in Q-learning.  
- **Document 2143**: Applies Bellman in Q-Learning and A2C contexts.  
- **Document 2451**: Recaps Bellman equation’s importance.  
- **Document 2517**: Describes Advantage calculations linked to Bellman-based TD errors.  

**Distractors**:  
- (2) Misleading, Bellman supports both deterministic and stochastic policies.  
- (3) Incorrect, Bellman does not compute full returns in one step.  
- (4) Incorrect, Bellman is not a Monte Carlo method.  

---

### Question 10:
**Why is the Advantage function’s variance reduction particularly beneficial in environments with high stochasticity?**

1. It focuses updates on the relative benefit of actions rather than absolute rewards, stabilizing learning in variable environments.  
2. It eliminates the need for discount factors, simplifying calculations in stochastic environments.  
3. The Advantage function replaces the need for state-value functions, reducing computational overhead.  
4. It enables off-policy learning by relying on greedy action selection, essential in stochastic systems.

**Correct Answer**: 1  
**Reasoning**: Advantage captures relative action benefits, reducing variance and enhancing stability under stochasticity.  

**Document Usage**:  
- **Document 83**: Discusses value functions and stochastic environment considerations.  
- **Document 468**: Summarizes variance reduction methods.  
- **Document 485**: Defines Advantage and variance reduction.  
- **Document 729**: Shows Q-value and Advantage distinctions.  
- **Document 971**: Highlights RL goals under stochasticity.  
- **Document 1306**: Describes A2C’s focus on variance reduction.  
- **Document 1674**: Discusses Bellman updates stabilizing variance.  
- **Document 1954**: Quiz covers variance reduction techniques.  
- **Document 1968**: Differentiates Advantage-based updates from other methods.  
- **Document 2011**: Relates Q-values to Advantage in updates.  
- **Document 2143**: Connects variance reduction to Advantage functions.  
- **Document 2451**: Recaps Advantage’s role in improving learning efficiency.  
- **Document 2517**: Details A2C’s variance stabilization through Advantage.  

**Distractors**:  
- (2) Misleading, Advantage retains discount factors.  
- (3) Incorrect, Advantage supplements rather than replaces state-value functions.  
- (4) Misleading, Advantage is unrelated to off-policy learning.  

---

### Question 11:
**Why does Advantage Actor-Critic (A2C) reduce training time compared to the Reinforce algorithm?**

1. A2C uses a value function to estimate the advantage, reducing variance compared to Monte Carlo-based Reinforce.  
2. A2C skips action-value function computations, using only state-value functions for updates.  
3. A2C combines Monte Carlo sampling with dynamic programming to bypass trajectory dependency.  
4. A2C replaces all policy updates with a single deterministic update per episode.

**Correct Answer**: 1  
**Reasoning**: A2C stabilizes training by using Advantage estimates based on the TD error, reducing variance inherent in Monte Carlo methods like Reinforce.  

**Document Usage**:  
- **Document 83**: Discusses value and policy-based methods, foundational to A2C.  
- **Document 468**: Highlights RL goals of efficiency and stability.  
- **Document 485**: Defines Monte Carlo methods and introduces variance challenges.  
- **Document 729**: Explains TD approaches in Q-Learning, relevant to A2C.  
- **Document 971**: Recaps policy learning with reduced variance as a goal.  
- **Document 1306**: Explicitly describes A2C’s variance reduction strategies.  
- **Document 1674**: Details recursive calculations (Bellman equations) that inform Advantage estimation.  
- **Document 1954**: Quiz contrasts Reinforce with A2C.  
- **Document 1968**: Differentiates Monte Carlo and TD methods.  
- **Document 2011**: Links Bellman to faster learning strategies.  
- **Document 2143**: Builds on Q-Learning TD methods to highlight differences.  
- **Document 2451**: Recaps RL strategies, emphasizing efficient learning.  
- **Document 2517**: Discusses A2C’s variance mitigation and time efficiency.  

**Distractors**:  
- (2) Incorrect, A2C uses both state and action-value functions.  
- (3) Misleading, A2C does not combine Monte Carlo and dynamic programming.  
- (4) Incorrect, A2C performs updates at every step, not once per episode.  

---

### Question 12:
**How does the epsilon parameter in the Epsilon-Greedy Strategy impact Q-Learning’s ability to learn optimal policies?**

1. A high epsilon ensures extensive exploration, which is critical in the early stages of training.  
2. A high epsilon avoids overfitting by reducing reliance on the Q-function.  
3. A low epsilon ensures more diverse trajectories, improving state-value coverage.  
4. A low epsilon prevents premature convergence by encouraging more random actions.

**Correct Answer**: 1  
**Reasoning**: A high epsilon encourages exploration, allowing the agent to visit more states and learn their values early in training.  

**Document Usage**:  
- **Document 83**: Introduces Epsilon-Greedy policies for exploration.  
- **Document 468**: Summarizes exploration-exploitation trade-offs.  
- **Document 485**: Defines Epsilon-Greedy explicitly.  
- **Document 729**: Details Q-Learning’s reliance on epsilon to explore state-action spaces.  
- **Document 971**: Explains the need for exploration in RL.  
- **Document 1306**: Differentiates A2C’s policy gradients from Q-Learning exploration.  
- **Document 1674**: Bellman’s recursive updates rely on well-explored spaces.  
- **Document 1954**: Includes quiz scenarios emphasizing epsilon decay.  
- **Document 1968**: Discusses exploration in value-based methods.  
- **Document 2011**: Highlights epsilon's role in Q-Learning dynamics.  
- **Document 2143**: Describes Epsilon-Greedy’s importance in Q-Learning.  
- **Document 2451**: Recaps exploration strategies like Epsilon-Greedy.  
- **Document 2517**: Contrasts exploration needs of Q-Learning and A2C.  

**Distractors**:  
- (2) Incorrect, overfitting is unrelated to epsilon in Q-Learning.  
- (3) Misleading, diverse trajectories depend on exploration, but high epsilon avoids premature exploitation.  
- (4) Incorrect, a high epsilon supports exploration, not low epsilon.  

---

### Question 13:
**Why does Q-Learning use the greedy policy for updates while still exploring during training?**

1. The greedy policy ensures that the updates move toward optimal Q-values based on maximum future rewards.  
2. The greedy policy minimizes variance by reducing dependency on stochastic policies.  
3. Exploration during training compensates for the greedy policy’s deterministic action selection.  
4. Greedy updates eliminate the need for Bellman-based calculations.

**Correct Answer**: 1  
**Reasoning**: The greedy policy focuses updates on maximizing future rewards, while exploration ensures sufficient state-action space coverage.  

**Document Usage**:  
- **Document 83**: Differentiates greedy policies from exploration strategies.  
- **Document 468**: Summarizes policy and value interactions in RL.  
- **Document 485**: Defines off-policy and greedy updates.  
- **Document 729**: Explains Q-Learning’s use of greedy policies for updates.  
- **Document 971**: Highlights the importance of balancing deterministic and exploratory strategies.  
- **Document 1306**: Contrasts greedy updates with A2C’s probabilistic policies.  
- **Document 1674**: Discusses Bellman equations supporting greedy updates.  
- **Document 1954**: Reinforces Q-learning’s greedy policy in quizzes.  
- **Document 1968**: Explains policy derivation from value functions.  
- **Document 2011**: Highlights greedy policy usage during updates.  
- **Document 2143**: Describes exploration-exploitation in Q-Learning.  
- **Document 2451**: Recaps the integration of greedy updates with exploration.  
- **Document 2517**: Contrasts deterministic updates in Q-Learning with A2C.  

**Distractors**:  
- (2) Misleading, greedy updates do not address variance.  
- (3) Partial truth, but greedy updates are for optimization, not compensating exploration.  
- (4) Incorrect, Bellman calculations are integral to Q-Learning.  

---

### Question 14:
**What is the primary reason Advantage functions are used in A2C instead of raw Q-values?**

1. Advantage functions reduce variance by focusing on relative action quality rather than absolute values.  
2. Q-values require additional normalization, making them less efficient for training.  
3. Advantage functions eliminate the need for state-value functions in Actor-Critic.  
4. Q-values cannot be estimated dynamically, unlike Advantage functions.

**Correct Answer**: 1  
**Reasoning**: Advantage functions highlight relative benefits, stabilizing learning and reducing variance.  

**Document Usage**:  
- **Document 83**: Discusses action-value relationships.  
- **Document 468**: Highlights value-based method stability needs.  
- **Document 485**: Introduces Advantage functions.  
- **Document 729**: Explains Q-value limitations.  
- **Document 971**: Emphasizes policy learning goals.  
- **Document 1306**: Describes Advantage in A2C.  
- **Document 1674**: Bellman relevance for relative estimations.  
- **Document 1954**: Quiz contrasts Q-value and Advantage methods.  
- **Document 1968**: Explains the role of Advantage in policy updates.  
- **Document 2011**: Reinforces Q-value shortcomings for variance.  
- **Document 2143**: Contrasts Q-value reliance with A2C.  
- **Document 2451**: Recaps Advantage's role in learning efficiency.  
- **Document 2517**: Highlights variance reduction with Advantage.  

**Distractors**:  
- (2) Incorrect, normalization is not a Q-value limitation.  
- (3) Misleading, state-value functions are still used in A2C.  
- (4) Incorrect, Q-values are dynamically estimated in Q-Learning.  

---

### Question 15:
**Why is the Bellman equation critical in estimating future rewards in both Q-Learning and A2C?**

1. It recursively decomposes future rewards into immediate rewards and the discounted value of future states.  
2. It simplifies the computation of policies by eliminating the need for action-value functions.  
3. It ensures variance reduction by averaging rewards over complete episodes.  
4. It eliminates the need for exploration in Q-Learning and A2C.

**Correct Answer**: 1  
**Reasoning**: The Bellman equation provides a recursive framework that connects immediate rewards to the expected discounted future rewards, crucial for both Q-Learning and A2C.  

**Document Usage**:  
- **Document 83**: Discusses how Bellman simplifies value function calculations.  
- **Document 468**: Summarizes Bellman’s importance in RL.  
- **Document 485**: Highlights Bellman’s relevance for recursive updates.  
- **Document 729**: Explains its direct application in Q-Learning.  
- **Document 971**: Describes the importance of estimating cumulative rewards.  
- **Document 1306**: Details its role in Advantage estimation for A2C.  
- **Document 1674**: Elaborates on Bellman equation dynamics.  
- **Document 1954**: Quiz emphasizes Bellman’s role in policy learning.  
- **Document 1968**: Explains Bellman’s application in value-based methods.  
- **Document 2011**: Highlights its role in RL computations.  
- **Document 2143**: Reinforces its necessity for future reward estimation.  
- **Document 2451**: Recaps its centrality in value-based RL.  
- **Document 2517**: Connects Bellman to efficient learning in A2C.  

**Distractors**:  
- (2) Incorrect, Bellman does not eliminate action-value functions.  
- (3) Misleading, Bellman focuses on step-based updates, not averaging.  
- (4) Incorrect, exploration remains necessary in RL.  

---

### Question 16:
**What differentiates A2C’s Advantage function from the Q-values used in Q-Learning?**

1. The Advantage function calculates the relative benefit of actions, whereas Q-values estimate absolute value for state-action pairs.  
2. Q-values require Monte Carlo sampling, while the Advantage function does not.  
3. The Advantage function is derived from TD error, whereas Q-values are not.  
4. Q-values cannot be used for policy updates, while the Advantage function is designed for this purpose.

**Correct Answer**: 1  
**Reasoning**: The Advantage function highlights the relative benefit of actions compared to the state’s average value, while Q-values estimate the total expected return of state-action pairs.  

**Document Usage**:  
- **Document 83**: Discusses Q-values in value-based methods.  
- **Document 468**: Summarizes RL goals of stability and efficiency.  
- **Document 485**: Introduces the Advantage function and Q-values.  
- **Document 729**: Explains Q-value updates in Q-Learning.  
- **Document 971**: Differentiates absolute and relative value concepts.  
- **Document 1306**: Details Advantage’s role in A2C.  
- **Document 1674**: Relates Bellman recursion to Advantage and Q-value calculations.  
- **Document 1954**: Quiz explores Advantage versus Q-value distinctions.  
- **Document 1968**: Explains policy updates via value functions.  
- **Document 2011**: Highlights Q-values in Q-Learning.  
- **Document 2143**: Connects Advantage and Q-value applications.  
- **Document 2451**: Recaps relative versus absolute value learning.  
- **Document 2517**: Emphasizes Advantage’s benefits in reducing variance.  

**Distractors**:  
- (2) Incorrect, Q-values do not depend on Monte Carlo sampling.  
- (3) Misleading, both Advantage and Q-values can involve TD error.  
- (4) Incorrect, Q-values are integral to policy derivation in Q-Learning.  

---

### Question 17:
**Why does the Advantage function require a state-value function to calculate relative benefits in A2C?**

1. The state-value function provides the baseline against which action values are compared.  
2. It normalizes the action-value function to ensure variance reduction.  
3. It replaces the need for Bellman-based updates in A2C.  
4. It ensures on-policy learning by restricting updates to current states.

**Correct Answer**: 1  
**Reasoning**: The state-value function serves as a baseline, allowing the Advantage function to calculate the relative benefit of actions at a state.  

**Document Usage**:  
- **Document 83**: Explains state-value functions as key to RL.  
- **Document 468**: Summarizes state-value functions' role in learning.  
- **Document 485**: Defines the connection between state-value and action-value functions.  
- **Document 729**: Details Q-value derivation from state-action values.  
- **Document 971**: Emphasizes baselines in learning efficiency.  
- **Document 1306**: Details Advantage calculation in A2C.  
- **Document 1674**: Relates state-value functions to Bellman equations.  
- **Document 1954**: Explores state-value and Advantage connections.  
- **Document 1968**: Discusses baseline concepts in policy learning.  
- **Document 2011**: Reinforces baseline’s importance in RL.  
- **Document 2143**: Connects baselines to Advantage variance reduction.  
- **Document 2451**: Recaps state-value function applications.  
- **Document 2517**: Highlights baselines' importance in A2C.  

**Distractors**:  
- (2) Incorrect, normalization is not the primary role.  
- (3) Incorrect, Bellman updates remain central.  
- (4) Misleading, on-policy learning is not ensured by state-value functions alone.  

---

### Question 18:
**Why is the Epsilon-Greedy Strategy necessary in Q-Learning but not in A2C?**

1. Q-Learning relies on exploration to discover optimal state-action pairs, while A2C explores naturally through stochastic policies.  
2. A2C directly optimizes actions using Advantage values, bypassing exploration needs.  
3. Epsilon-Greedy encourages on-policy learning, which A2C already implements.  
4. Q-Learning requires additional randomness to offset deterministic Bellman updates.

**Correct Answer**: 1  
**Reasoning**: Q-Learning requires explicit exploration (Epsilon-Greedy) since it derives deterministic policies, while A2C’s stochastic policies inherently explore.  

**Document Usage**:  
- **Document 83**: Differentiates exploration in policy and value methods.  
- **Document 468**: Summarizes Epsilon-Greedy’s necessity in Q-Learning.  
- **Document 485**: Introduces Epsilon-Greedy strategies.  
- **Document 729**: Highlights exploration challenges in Q-Learning.  
- **Document 971**: Discusses exploration as critical in RL.  
- **Document 1306**: Contrasts exploration in Q-Learning and A2C.  
- **Document 1674**: Relates exploration to Bellman updates.  
- **Document 1954**: Quiz reinforces Epsilon-Greedy’s role.  
- **Document 1968**: Explains exploration-exploitation trade-offs.  
- **Document 2011**: Highlights deterministic nature of Q-Learning policies.  
- **Document 2143**: Discusses Epsilon-Greedy applications in Q-Learning.  
- **Document 2451**: Recaps Q-Learning’s need for explicit exploration.  
- **Document 2517**: Explores A2C’s inherent stochasticity.  

**Distractors**:  
- (2) Misleading, Advantage does not eliminate exploration needs.  
- (3) Incorrect, Epsilon-Greedy does not enforce on-policy learning.  
- (4) Misleading, randomness offsets deterministic policies but is not Bellman-specific.  

---

### Question 19:
**What is a key reason Q-Learning is considered an off-policy algorithm?**

1. It updates Q-values using a greedy policy while acting based on an exploratory policy.  
2. It calculates values for all state-action pairs regardless of the current policy.  
3. It alternates between on-policy and off-policy learning during updates.  
4. It does not use a Bellman-based recursive update formula.

**Correct Answer**: 1  
**Reasoning**: Q-Learning uses an exploratory policy for actions during training but updates Q-values using a greedy policy, making it off-policy.  

**Document Usage**:  
- **Document 83**: Explains policy behavior in value-based methods.  
- **Document 468**: Defines off-policy algorithms.  
- **Document 485**: Differentiates Q-Learning as off-policy.  
- **Document 729**: Highlights Q-Learning’s greedy update mechanism.  
- **Document 971**: Explains off-policy characteristics.  
- **Document 1306**: Contrasts A2C on-policy nature with Q-Learning.  
- **Document 1674**: Discusses recursive updates and policies.  
- **Document 1954**: Quiz reinforces off-policy mechanisms.  
- **Document 1968

**: Explains policy differences in RL.  
- **Document 2011**: Links Q-Learning to off-policy definitions.  
- **Document 2143**: Explains policy differences in Q-Learning.  
- **Document 2451**: Recaps on-policy versus off-policy distinctions.  
- **Document 2517**: Highlights Q-Learning’s update dynamics.  

**Distractors**:  
- (2) Incorrect, value updates depend on the current policy indirectly.  
- (3) Misleading, Q-Learning is consistently off-policy.  
- (4) Incorrect, Bellman updates are used in Q-Learning.  

---

### Question 20:
**What is the primary goal of reducing variance in A2C using Advantage functions?**

1. To stabilize training by improving gradient estimates for policy updates.  
2. To eliminate the need for state-action values in learning.  
3. To simplify the Bellman equation by avoiding recursion.  
4. To ensure policies converge to deterministic actions.

**Correct Answer**: 1  
**Reasoning**: Reducing variance improves policy gradient estimates, stabilizing training.  

**Document Usage**:  
- **Document 83**: Highlights the importance of value-based stability.  
- **Document 468**: Emphasizes stability in RL.  
- **Document 485**: Connects variance reduction to Advantage functions.  
- **Document 729**: Links Bellman updates to training stability.  
- **Document 971**: Describes the importance of variance reduction in learning.  
- **Document 1306**: Details Advantage’s role in A2C variance reduction.  
- **Document 1674**: Explains Bellman equations and their impact on variance.  
- **Document 1954**: Quiz links variance reduction to A2C success.  
- **Document 1968**: Highlights Advantage in policy stability.  
- **Document 2011**: Relates Bellman equations to training variance.  
- **Document 2143**: Contrasts Q-Learning and A2C on variance issues.  
- **Document 2451**: Recaps variance reduction strategies.  
- **Document 2517**: Highlights Advantage’s impact on A2C efficiency.  

**Distractors**:  
- (2) Incorrect, state-action values remain central to A2C.  
- (3) Incorrect, recursion remains key.  
- (4) Misleading, deterministic convergence is not a primary goal.  

