query,ids,prompt,rag_response
What is the most effective approach for implementing DreamBooth training while ensuring ethical compliance and proper documentation?,"[(10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (11278, 'e713c8e5-c745-44dd-bc3d-29c7e29c2189'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (3601, '5b751b55-fa30-4ca3-9f29-0c2b5992aee0'), (11288, '2451d2fc-6bb5-4e2f-ad80-c64efaba643f'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (12314, 'e385ad95-96ab-4d32-aa88-01c826234363'), (26657, '4f236930-ae24-4ef1-b075-ccb4482a4900'), (26663, '0e555191-162a-45da-ad99-03e99d0199a3'), (26665, 'db193de6-8255-464f-9149-be7af6c03c58'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (26675, '7badede5-4860-404f-8bb6-ab60db480be1'), (26678, 'd17335d9-50c5-400d-bb0e-4d1d02ce9662'), (27707, 'f9141539-50a9-43da-a5c3-bb31ca858ef8'), (26696, '3318e3a1-8c29-4b7d-a5f7-1588fb10b135'), (26697, '80c3fc5d-b1d4-4875-a92a-61f60362bcc9'), (26700, '976e20d2-c980-40d5-ac8c-55241d8fa57c'), (5710, 'f21f0562-b66c-41c2-8ed3-d515e34e88ed'), (4692, '697715c5-2ee3-4da5-9b88-90354e284c9a'), (4696, '6996fe5b-0705-49f1-9069-85ef3e39d463'), (4697, '3714f4e1-21ae-49eb-9ec0-9221657c1046'), (4702, '1b460a9c-a144-4b70-b4a3-0ba6ece900ca'), (4706, '680cae3d-7edf-4b97-9961-e420b0cc6adf'), (4719, '7325e015-6e64-402a-bdcf-d35da35b0310'), (26228, 'cd2ce019-c648-4434-8d98-916726eb00f4'), (18047, '88a9e52b-eec2-4a57-933d-f10b57098f70'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (9860, 'b127b619-65e1-4d79-844d-ade94f6c03a8'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (26265, 'cc640035-a212-471c-b79a-5b2b8e33d260'), (24743, '7c967f07-5e4b-4076-896b-2d21640ef09e'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (17600, '2199f9af-4c5a-4ec3-b773-2e9af00240d4'), (22723, '73aca2a4-7e12-4a27-8803-298209ed9076'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (9428, '5b9c8a03-16ed-45d6-9ce6-2ab388d1892b'), (9429, 'f9b10760-2b6c-4577-999d-9ce25869b8f0'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (9431, '185997ab-1d50-4fe7-88c0-896da8ec8036'), (29400, '4b3191cf-c2a7-4a85-989f-9fd267cb522a'), (12007, '9c867710-e722-43c0-b004-df6f4069ac5e'), (21225, 'e69e5399-a1d0-43f8-a7b8-e134c3416e16'), (12011, '417eb897-bb81-4c51-94d7-0906db08bbd3'), (12012, '8c88d004-cc76-4ac3-abfe-0f5ae57b7ad3'), (4361, '69a44467-ab86-4ff5-bd68-f4fcb4287814'), (4362, 'a3614fd5-f69b-44a3-90b0-01f9eb078f1a'), (6411, '787941ff-6634-421a-a05d-b3c2c1d4cda4'), (6412, 'e491859b-0c3a-451c-8d54-e5536a56bf5f'), (6413, '245f056f-3cce-4401-8130-8bf6522fd9dd'), (9486, '40c09d9f-3207-4542-b6b5-4e56a2db18db'), (6415, 'aaf6f9e4-e2b5-4d91-8c2a-89cbe4f2110b'), (6414, '591d58fa-2c3e-4261-a58a-950eee1e09d1'), (9489, '722afe74-ffcc-4ca5-9439-526a915ab9cc'), (6418, 'e13e35b8-05d3-4f14-bb06-8efe54105256'), (6419, '42aef9bd-e9bd-469a-ac13-036843c85c48'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (6420, 'a0d6a281-060a-42e7-b028-400eafafb07f'), (11558, 'f0e0e76d-1ed1-4de4-bed4-b83eb8297fc7'), (11562, 'a3abc2a8-536f-49c3-937d-a7167bc0f240'), (11563, 'e0a485f8-e425-4c38-8a42-7075440250af'), (19764, '3964a67e-698f-4796-91c3-29d446e12522'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (30541, '66e657f3-787a-43f4-a2ca-ecc504697610'), (30544, '2a74ff39-5f3d-449f-a5ff-10bc8f7dcd3d'), (13158, 'd5de80e9-25fb-4ef5-8868-7bb0688c1611'), (26982, '9679942c-f81f-459d-af4e-a2c4bd5c1912'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (363, 'a89eaa05-3ff1-49d2-8124-aff7fd09fdaa'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (4466, '9aa4d965-f7aa-47c9-9ef0-99efd730ae50'), (30579, '9ca0751a-33ff-4f8b-a7ee-49343630ad20'), (4467, 'e220a748-bccb-46f5-8533-dbf4767b9d3b'), (12286, '2a89892b-4a32-4801-9d33-14f95f02fce5'), (4475, 'a93fa4e5-6ae7-4d7a-befe-e29f4fed3c27'), (14716, '493155e3-f96f-48c6-be10-b3d2c158a052'), (4476, 'b80eba33-6fae-4fe9-8ba0-b91a48e87a9e'), (21374, '5f22e6c9-db26-49d7-ac1a-d830f7203d82'), (21377, '11698850-263a-4917-8e03-3d7c601995b7'), (13699, 'e6673957-64d4-4ab9-a6ab-290eebf4e65d'), (21383, '41387f0d-02b9-48a4-a7f4-e8318d3417a3'), (15243, '4189372b-3be4-4bca-b934-f8d9d37da251'), (21388, '7dec6a76-7037-46ee-8c41-096b85b4d1ae'), (23459, 'e479dae7-075a-43ca-aee7-6683ca38ae18'), (11684, '9cfcfbdb-6441-4284-8ef3-d14780da146f'), (11685, 'a18ef3de-ff80-4514-b1dd-0a2a98939675'), (11687, '69e7636e-6b7e-4dc2-9a53-908bca40991f'), (11689, 'f514bd1a-ea82-4bb1-a46f-23e244b0ce87'), (8618, 'b72c1623-10fb-4fdc-99e7-31e030260a85'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (30123, '35db152e-2826-467e-a7dc-b616f217bb13'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (433, '02562a97-e1ae-4a5a-b71c-02b035ab095a'), (3507, 'e3d2edd7-6062-4b3f-9ae2-63e8a07a4b96'), (9656, 'a77713b2-b75f-474e-8b1c-b4abc596b9bf'), (11705, 'ebf07ac5-f930-4b51-b548-ba02e7dd5d14'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (12241, '5f95fcd3-3f90-4165-ae7a-424c50eb60e9'), (27602, '7ac74ea0-7594-4d73-9158-90afe51597ea'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27101, 'a0b5c992-ca35-43fb-8604-666943c50f3c'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (23533, '354bcc39-ac3c-4a0f-ac68-3b5a873cc33b'), (31227, 'a993a1ba-0a3e-44ca-b6ee-aae21b1be246'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (12285, 'ba282827-386a-4cba-a8fb-866207b82420'), (31230, '98a71ccc-41b5-417b-82c4-fcf53961bfde')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: DreamBooth training example for Stable Diffusion XL (SDXL)

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few (3~5) images of a subject.

The `train_dreambooth_lora_sdxl.py` script shows how to implement the training procedure and adapt it for [Stable Diffusion XL](https://huggingface.co/papers/2307.01952).

> üí° **Note**: For now, we only allow DreamBooth fine-tuning of the SDXL UNet via LoRA. LoRA is a parameter-efficient fine-tuning technique introduced in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*. 

## Running locally with PyTorch

### Installing the dependencies

Before running the scripts, make sure to install the library's training dependencies:

**Important**
DreamBooth training example

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few(3~5) images of a subject.
The `train_dreambooth.py` script shows how to implement the training procedure and adapt it for stable diffusion.


## Running locally with PyTorch

### Installing the dependencies

Before running the scripts, make sure to install the library's training dependencies:

**Important**

To make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:
```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install -e .
```

Then cd in the example folder and run
```bash
pip install -r requirements.txt
Multi Subject DreamBooth training

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few(3~5) images of a subject.
This `train_multi_subject_dreambooth.py` script shows how to implement the training procedure for one or more subjects and adapt it for stable diffusion. Note that this code is based off of the `examples/dreambooth/train_dreambooth.py` script as of 01/06/2022.

This script was added by @kopsahlong, and is not actively maintained. However, if you come across anything that could use fixing, feel free to open an issue and tag @kopsahlong.

## Running locally with PyTorch
### Installing the dependencies

Before running the script, make sure to install the library's training dependencies:

To start, execute the following steps in a new virtual environment:
```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install -e .
* Dreambooth tends to overfit quickly. To get good-quality images, we must find a 'sweet spot' between the number of training steps and the learning rate. We recommend using a low learning rate and progressively increasing the number of steps until the results are satisfactory.
* Dreambooth needs more training steps for faces. In our experiments, 800-1200 steps worked well when using a batch size of 2 and LR of 1e-6.
* Prior preservation is important to avoid overfitting when training on faces. For other subjects, it doesn't seem to make a huge difference.
* If you see that the generated images are noisy or the quality is degraded, it likely means overfitting. First, try the steps above to avoid it. If the generated images are still noisy, use the DDIM scheduler or run more inference steps (~100 worked well in our experiments).
[DreamBooth](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) by [colossalai](https://github.com/hpcaitech/ColossalAI.git)

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few(3~5) images of a subject.
The `train_dreambooth_colossalai.py` script shows how to implement the training procedure and adapt it for stable diffusion.
```

## Dreamboothing with LoRA

Dreambooth allows you to ""teach"" new concepts to a Stable Diffusion model. LoRA is compatible with Dreambooth and the process is similar to fine-tuning, with a couple of advantages:

- Training is faster.
- We only need a few images of the subject we want to train (5 or 10 are usually enough).
- We can tweak the text encoder, if we want, for additional fidelity to the subject.

To train Dreambooth with LoRA you need to use [this diffusers script](https://github.com/huggingface/diffusers/blob/main/examples/dreambooth/train_dreambooth_lora.py). Please, take a look at [the README](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth#training-with-low-rank-adaptation-of-large-language-models-lora), [the documentation](https://huggingface.co/docs/diffusers/main/en/training/lora) and [our hyperparameter exploration blog post](https://huggingface.co/blog/dreambooth) for details.
### Summary of Initial Results

To get good results training Stable Diffusion with Dreambooth, it's important to tune the learning rate and training steps for your dataset.

* High learning rates and too many training steps will lead to overfitting. The model will mostly generate images from your training data, no matter what prompt is used.
* Low learning rates and too few steps will lead to underfitting: the model will not be able to generate the concept we were trying to incorporate.

Faces are harder to train. In our experiments, a learning rate of `2e-6` with `400` training steps works well for objects but faces required `1e-6` (or `2e-6`) with ~1200 steps.

Image quality degrades a lot if the model overfits, and this happens if:
* The learning rate is too high.
* We run too many training steps.
* In the case of faces, when no prior preservation is used, as shown in the next section.

## Using Prior Preservation when training Faces
For a quick, cheap and easy way to train your Dreambooth models with LoRA, please [check this Space](https://huggingface.co/spaces/lora-library/LoRA-DreamBooth-Training-UI) by [`hysts`](https://twitter.com/hysts12321). You need to duplicate it and assign a GPU so it runs fast. This process will save you from having to set up your own training environment and you'll be able to train your models in minutes!

## Other Methods

The quest for easy fine-tuning is not new. In addition to Dreambooth, [_textual inversion_](https://huggingface.co/docs/diffusers/main/en/training/text_inversion) is another popular method that attempts to teach new concepts to a trained Stable Diffusion Model. One of the main reasons for using Textual Inversion is that trained weights are also small and easy to share. However, they only work for a single subject (or a small handful of them), whereas LoRA can be used for general-purpose fine-tuning, meaning that it can be adapted to new domains or datasets.
**Training**
An example of using LoRA for parameter efficient dreambooth training is given in [`examples/lora_dreambooth/train_dreambooth.py`](examples/lora_dreambooth/train_dreambooth.py)

```bash
export MODEL_NAME= ""CompVis/stable-diffusion-v1-4"" #""stabilityai/stable-diffusion-2-1""
export INSTANCE_DIR=""path-to-instance-images""
export CLASS_DIR=""path-to-class-images""
export OUTPUT_DIR=""path-to-save-model""
We conducted a lot of experiments to analyze the effect of different settings in Dreambooth. This post presents our findings and some tips to improve your results when fine-tuning Stable Diffusion with Dreambooth.

Before we start, please be aware that this method should never be used for malicious purposes, to generate harm in any way, or to impersonate people without their knowledge. Models trained with it are still bound by the [CreativeML Open RAIL-M license](https://huggingface.co/spaces/CompVis/stable-diffusion-license) that governs distribution of Stable Diffusion models.

_Note: a previous version of this post was published [as a W&B report](https://wandb.ai/psuraj/dreambooth/reports/Dreambooth-Training-Analysis--VmlldzoyNzk0NDc3)_.

## TL;DR: Recommended Settings
```

Then navigate to the folder of the training script (for example, [DreamBooth](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth)) and install the `requirements.txt` file. Some training scripts have a specific requirement file for SDXL, LoRA or Flax. If you're using one of these scripts, make sure you install its corresponding requirements file.

```bash
cd examples/dreambooth
pip install -r requirements.txt
# to train SDXL with DreamBooth
pip install -r requirements_sdxl.txt
```

To speedup training and reduce memory-usage, we recommend:

- using PyTorch 2.0 or higher to automatically use [scaled dot product attention](../optimization/torch2.0#scaled-dot-product-attention) during training (you don't need to make any changes to the training code)
- installing [xFormers](../optimization/xformers) to enable memory-efficient attention
**NEW** ‚ú® Dreambooth training for Stable Diffusion using LoHa and LoKr adapters [`examples/stable_diffusion/train_dreambooth.py`](examples/stable_diffusion/train_dreambooth.py)

### Parameter Efficient Tuning of LLMs for RLHF components such as Ranker and Policy
- Here is an example in [trl](https://github.com/lvwerra/trl) library using PEFT+INT8 for tuning policy model: [gpt2-sentiment_peft.py](https://github.com/lvwerra/trl/blob/main/examples/sentiment/scripts/gpt2-sentiment_peft.py) and corresponding [Blog](https://huggingface.co/blog/trl-peft)
- Example using PEFT for Instruction finetuning, reward model and policy : [stack_llama](https://github.com/lvwerra/trl/tree/main/examples/research_projects/stack_llama/scripts) and corresponding [Blog](https://huggingface.co/blog/stackllama) 

### INT8 training of large models in Colab using PEFT LoRA and bitsandbytes
!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# DreamBooth

[DreamBooth](https://huggingface.co/papers/2208.12242) is a training technique that updates the entire diffusion model by training on just a few images of a subject or style. It works by associating a special word in the prompt with the example images.
* Training the text encoder in addition to the UNet has a big impact on quality. Our best results were obtained using a combination of text encoder fine-tuning, low LR, and a suitable number of steps. However, fine-tuning the text encoder requires more memory, so a GPU with at least 24 GB of RAM is ideal. Using techniques like 8-bit Adam, `fp16` training or gradient accumulation, it is possible to train on 16 GB GPUs like the ones provided by Google Colab or Kaggle.
* Fine-tuning with or without EMA produced similar results.
* There's no need to use the `sks` word to train Dreambooth. One of the first implementations used it because it was a rare token in the vocabulary, but it's actually a kind of rifle. Our experiments, and those by for example [@nitrosocke](https://huggingface.co/nitrosocke) show that it's ok to select terms that you'd naturally use to describe your target.
</Tip>

## DreamBooth

[DreamBooth](https://dreambooth.github.io/) finetunes an *entire diffusion model* on just several images of a subject to generate images of that subject in new styles and settings. This method works by using a special word in the prompt that the model learns to associate with the subject image. Of all the training methods, DreamBooth produces the largest file size (usually a few GBs) because it is a full checkpoint model.

Let's load the [herge_style](https://huggingface.co/sd-dreambooth-library/herge-style) checkpoint, which is trained on just 10 images drawn by Herg√©, to generate images in that style. For it to work, you need to include the special word `herge_style` in your prompt to trigger the checkpoint:

```py
from diffusers import AutoPipelineForText2Image
import torch
## Using Prior Preservation when training Faces

Prior preservation is a technique that uses additional images of the same class we are trying to train as part of the fine-tuning process. For example, if we try to incorporate a new person into the model, the _class_ we'd want to preserve could be _person_. Prior preservation tries to reduce overfitting by using photos of the new person combined with photos of other people. The nice thing is that we can generate those additional class images using the Stable Diffusion model itself! The training script takes care of that automatically if you want, but you can also provide a folder with your own prior preservation images.

Prior preservation, 1200 steps, lr=`2e-6`.
![Faces, prior preservation](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/dreambooth-assets/7_faces_with_prior.jpg)
```

<div class=""flex justify-center"">
  <img class=""rounded-xl"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-text-inversion.png""/>
</div>

## DreamBooth

[DreamBooth](../training/dreambooth) is a technique for generating contextualized images of a subject given just a few images of the subject to train on. It is similar to textual inversion, but DreamBooth trains the full model whereas textual inversion only fine-tunes the text embeddings. This means you should use [`~DiffusionPipeline.from_pretrained`] to load the DreamBooth model (feel free to browse the [Stable Diffusion Dreambooth Concepts Library](https://huggingface.co/sd-dreambooth-library) for 100+ trained models):

```py
import torch
from diffusers import DiffusionPipeline, UniPCMultistepScheduler
from compel import Compel
!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# DreamBooth fine-tuning with LoRA

This guide demonstrates how to use LoRA, a low-rank approximation technique, to fine-tune DreamBooth with the 
`CompVis/stable-diffusion-v1-4` model.
--
title: Training Stable Diffusion with Dreambooth using Diffusers
thumbnail: /blog/assets/sd_dreambooth_training/thumbnail.jpg
authors:
- user: valhalla
- user: pcuenq
- user: 9of9
  guest: true
---

# Training Stable Diffusion with Dreambooth using üß® Diffusers


[Dreambooth](https://dreambooth.github.io/) is a technique to teach new concepts to [Stable Diffusion](https://huggingface.co/blog/stable_diffusion) using a specialized form of fine-tuning. Some people have been using it with a few of their photos to place themselves in fantastic situations, while others are using it to incorporate new styles. [üß® Diffusers](https://github.com/huggingface/diffusers) provides a Dreambooth [training script](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth). It doesn't take long to train, but it's hard to select the right set of hyperparameters and it's easy to overfit.
```

### Inference from a training checkpoint

You can also perform inference from one of the checkpoints saved during the training process, if you used the `--checkpointing_steps` argument. Please, refer to [the documentation](https://huggingface.co/docs/diffusers/main/en/training/dreambooth#performing-inference-using-a-saved-checkpoint) to see how to do it.

## Additional Dreambooth documentation
Because the `train_multi_subject_dreambooth.py` script here was forked from an original version of `train_dreambooth.py` in the `examples/dreambooth` folder, I've included the original applicable training documentation for single subject examples below.

Here is the question: What is the most effective approach for implementing DreamBooth training while ensuring ethical compliance and proper documentation?
Here is answer 1:Maximizing training speed while ignoring ethical considerations
Here is answer 2: Following ethical guidelines without technical optimization
Here is answer3: Implementing a balanced approach with ethical monitoring, technical optimization, and comprehensive documentation
Here is answer 4: Focusing solely on model performance metrics",Accordingly I believe Answer3 would seem most plausible .
"When implementing a real-time dashboard for model monitoring, what combination of features would best support both ethical oversight and technical performance?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (28677, '45f0fcb4-e1cb-4497-808a-e3001963761e'), (30727, '5a7a6803-dbbe-48fe-8ccd-60d7bb61992d'), (13322, 'b0a48bd2-b0c8-4a35-a6a6-918c274e6820'), (13324, 'e927c83d-2a0f-4409-a00f-c0599d0ce5aa'), (13325, '16d0f9ea-2690-41bb-b5b7-ea15fa1b01e6'), (15886, '1f277e8b-db79-4468-af4b-e11deb15d54a'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (30739, 'fda68c03-224d-4b2c-8385-15822bc392ef'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (12314, 'e385ad95-96ab-4d32-aa88-01c826234363'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (5683, '3fbd0aa6-3836-4bba-965c-ec5e95b6213f'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (5682, 'fb18c66c-cdd7-47e6-a4bf-022dc4f0f8af'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (4192, 'fc859fbf-f289-4177-a3a6-21b11cb22ad3'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (9859, '3e6fc260-35b7-46ab-af85-5de4ca79b906'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (9862, '33a9f610-ef4f-48c2-8df9-388fa0fd2a8f'), (19080, '94934921-1e26-4d2b-a418-99560516f2d0'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (7356, '7384d51b-c1f5-4e52-aee5-7401f6562bf2'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (15057, 'dd7c818f-cdea-48c7-bced-a336200df22b'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (29400, '4b3191cf-c2a7-4a85-989f-9fd267cb522a'), (29401, 'ffc0aed6-1b92-4799-baef-1e0edcb9587b'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (7901, '17026dcd-41fe-493c-8a87-b0ddc5ea48cb'), (19689, '6402466f-c27d-48ba-9c09-4a777b71c17b'), (10999, '773e67b3-120d-4606-8cb8-6270fa3f5b46'), (4361, '69a44467-ab86-4ff5-bd68-f4fcb4287814'), (4362, 'a3614fd5-f69b-44a3-90b0-01f9eb078f1a'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (16143, 'bcbfd73d-4617-42c4-ac93-60e0ae7f56d2'), (29503, '8d8f1ab2-d503-4300-a5cb-65e057381f71'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (23390, 'a0d6b001-969e-45ef-9770-61309e5be4e1'), (21853, '474cca01-f1f8-4899-ae29-3cbc7f3913c7'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8624, '339598bc-e183-4af9-9283-072e1969ff78'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (29638, '090cbe12-3fd1-4622-b456-102ca3ae6d96'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27101, 'a0b5c992-ca35-43fb-8604-666943c50f3c'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (25058, 'f06d5733-7ce2-443d-85cb-88a01b88ca4f'), (15334, 'd746f337-15b7-4205-80bb-7b72bc0e202f'), (15335, 'fdc7d8df-5f34-4684-94ed-70f19e6041e3'), (15336, '807f8cec-83e0-450b-aa1a-a12265b5545d'), (15339, '9006a23a-10b3-4cd2-a0a0-5f95871a357d'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (5622, 'ccda777f-15b6-4a2a-9ba6-f00ffacce0d2'), (5623, '8fdc94b4-3360-405e-b141-430c2e6a5ebc'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ## Examples of implementations: Safety features and Mechanisms

The team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.

- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.

- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.

- **Encouraging safety in deployment**
If specific ad hoc practices devoted to documentation, transparency and ethical usage of ML models are already present and improving each day (e.g., model cards, evaluation benchmarks), why shouldn't open licensing practices also be adapted to the specific capabilities and challenges stemming from ML models?
### Other key new insights

* Model cards are best filled out when done by people with different roles: Technical specifications can generally only be filled out by the developers; ethical considerations throughout are generally best informed by people who tend to work on ethical issues.
* Model users care a lot about licences -- specifically, whether a model can legally be used for a specific task.


## Appendix B: Landscape Analysis
_Overview of the state of model documentation in Machine Learning_

### MODEL CARD EXAMPLES
Examples of model cards and closely-related variants include:
In our response, we encourage the Task Force to:  

- Appoint Technical and Ethical Experts as Advisors
    - Technical experts with a track record of ethical innovation should be prioritized as advisors; they can calibrate NAIRR on not only what is technically feasible, implementable, and necessary for AI systems, but also on how to avoid exacerbating harmful biases and other malicious uses of AI systems. [Dr. Margaret Mitchell](https://www.m-mitchell.com/), one of the most prominent technical experts and ethics practitioners in the AI field and Hugging Face‚Äôs Chief Ethics Scientist, is a natural example of an external advisor.
How to minimize the risk of harm? Here are some practical recommendations.

### Best practices for responsible use

- Make sure people know which parts of their user experience depend on the outputs of the ML system  
- If possible, give users the ability to opt-out  
- Provide a mechanism for users to give feedback on the model decision, and to override it  
- Monitor feedback, especially model failures, for groups of users that may be disproportionately affected  

What needs most to be avoided is to use the model to automatically make decisions for, or about, a user, without opportunity for a human to provide input or correct the output. Several regulations, such as [GDPR](https://gdpr-info.eu/) in Europe, require that users be provided an explanation for automatic decisions made about them.

---
Once the contract was signed, we began the migration of moving off our regex based system to direct an increasing amount of critical path traffic to the transformer model. Internally, we had to build some new telemetry for both model and production data monitoring. Given that this system is positioned so early in the product experience, any inaccuracies in model outcomes could significantly impact business metrics. We ran an extensive experiment where new users were split equally between the old system and the new model. We assessed model performance in conjunction with broader business metrics, such as paid user retention and engagement. The ML model clearly outperformed in terms of retention, leading us to confidently make the decision to scale the system - first to new users and then to existing users - ramping to 100% over a span of two months.
To learn more about how we optimize inference with ü§ó¬†Diffusers, check out the [docs](https://huggingface.co/docs/diffusers/optimization/opt_overview)!

## Ethics and safety

Generative models are cool, but they also have the ability to produce harmful and NSFW content. To help users interact with these models responsibly and ethically, we‚Äôve added a [`safety_checker`](https://github.com/huggingface/diffusers/blob/main/src/diffusers/pipelines/stable_diffusion/safety_checker.py) component that flags inappropriate content generated during inference. Model creators can choose to incorporate this component into their models if they want.
Note: While StarCoder is the inspiration and model powering the initial version of SafeCoder, an important benefit of building a LLM solution upon open source models is that it can adapt to the latest and greatest open source models available. In the future, SafeCoder may offer other similarly commercially permissible open source models built upon ethically sourced and transparent datasets as the base LLM available for fine-tuning.

## Privacy and Security as a Core Principle

For any company, the internal codebase is some of its most important and valuable intellectual property. A core principle of SafeCoder is that the customer internal codebase will never be accessible to any third party (including Hugging Face) during training or inference.
The integration of use-based restrictions clauses into open AI licenses brings up the ability to better control the use of AI artifacts and the capacity of enforcement to the licensor of the ML model, standing up for a responsible use of the released AI artifact, in case a misuse of the model is identified. If behavioral-use restrictions were not present in open AI licenses, how would licensors even begin to think about responsible use-related legal tools when openly releasing their AI artifacts? OpenRAILs and RAILs are the first step towards enabling ethics-informed behavioral restrictions.
Moderating ML artifacts presents unique challenges due to the dynamic and rapidly evolving nature of these systems. In fact, as ML models become more advanced and capable of producing increasingly diverse content, the potential for harmful or unintended outputs grows, necessitating the development of robust moderation and evaluation strategies. Moreover, the complexity of ML models and the vast amounts of data they process exacerbate the challenge of identifying and addressing potential biases and ethical concerns.
## Scope

The Diffusers community will apply the following ethical guidelines to the project‚Äôs development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.


## Ethical guidelines

The following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.

- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.

- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.
## _Machine Bias:_ from ML Systems to Personal and Social Risks

ML systems allow us to automate complex tasks at a scale never seen before as they are deployed in more sectors and use cases. When the technology works at its best, it can help smooth interactions between people and technical systems, remove the need for highly repetitive work, or unlock new ways of processing information to support research.

These same systems are also likely to reproduce discriminatory and abusive behaviors represented in their training data, especially when the data encodes human behaviors.
The technology then has the potential to make these issues significantly worse. Automation and deployment at scale can indeed:
## Detecting Bias

Most of the issues that we describe above cannot be solved with a single solution ‚Äì indeed, [bias is a complex topic](https://huggingface.co/blog/ethics-soc-2) that cannot be meaningfully addressed with technology alone. Bias is deeply intertwined with the broader social, cultural, and historical context in which it exists. Therefore, addressing bias in AI systems is not only a technological challenge but also a socio-technical one that demands multidisciplinary attention. However, a combination of approaches including tools, red-teaming and evaluations can help glean important insights that can inform both model creators and downstream users about the biases contained in TTI and other multimodal models.

We present some of these approaches below:
Furthermore, as the field of AI and machine learning continues to expand, the variety of use cases and applications proliferates. This makes it essential for us to stay up-to-date with the latest research, ethical considerations, and best practices. For this reason, promoting user collaboration is also vital to the sustainability of our platform. Namely, through our community features, such as the Community Tab, we encourage and foster collaborative solutions between repository authors, users, organizations, and our team.

## Consent as a Core Value
Models fine-tuned with CoT have shown to perform much better on tasks involving commonsense, arithmetic, and symbolic reasoning as in [Chung et al., ‚Äò22](https://arxiv.org/pdf/2210.11416.pdf). 

CoT fine-tuning have also shown to be very effective for harmlessness (sometimes doing better than RLHF) without the model being evasive and generating ‚ÄúSorry, I cannot respond to this question,‚Äù for prompts that are sensitive as shown by [Bai et al.,‚Äô22](https://www.anthropic.com/constitutional.pdf). See Appendix D of their paper for more examples.

![Comparing CoT and RLHF](assets/dialog-agents/rlhf.png)

## Takeaways:

1. You only need a very tiny fraction of data for instruction fine-tuning (order of few hundreds) compared to the pre-training data.
2. Supervised fine-tuning uses human annotations to make model outputs safer and helpful.
3. CoT fine-tuning improves model performance on tasks requiring step-by-step thinking and makes them less evasive on sensitive topics.
In the coming months, we will be putting together several other pieces on values, tensions, and ethics operationalization. We welcome (and want!) feedback on any and all of our work, and hope to continue engaging with the AI community through technical and values-informed lenses. 

Thanks for reading! ü§ó

~ Meg, on behalf of the Ethics and Society regulars
* Accountability mechanisms should **combine internal requirements with external access** and transparency. Internal requirements such as good documentation practices shape more responsible development and provide clarity on the developers‚Äô responsibility in enabling safer and more reliable technology. External access to the internal processes and development choices is still necessary to verify claims and documentation, and to empower the many stakeholders of the technology who reside outside of its development chain to meaningfully shape its evolution and promote their interest.
- Rigorous work pays special attention to developing with best practices in mind. In ML, this can mean examining failure cases (including conducting bias and fairness audits), protecting privacy through security measures, and ensuring that potential users (technical and non-technical) are informed about the project's limitations. 
- Consentful work [supports](https://www.consentfultech.io/) the self-determination of people who use and are affected by these technologies.
- Socially Conscious work shows us how technology can support social, environmental, and scientific efforts.
- Sustainable work highlights and explores techniques for making machine learning ecologically sustainable.
- Inclusive work broadens the scope of who builds and benefits in the machine learning world. 
- Inquisitive work shines a light on inequities and power structures which challenge the community to rethink its relationship to technology.

Read more at https://huggingface.co/ethics
- **Be fair:** We define fairness as the equal treatment of all human beings. Being fair implies monitoring and mitigating unwanted biases that are based on characteristics such as race, gender, disabilities, and sexual orientation. To limit as much as possible negative outcomes, especially outcomes that impact marginalized and vulnerable groups, reviews of unfair biases - such as racism for predictive policing algorithms - should be conducted on both the data and the model outputs.
- **Be self-critical:** We are aware of our imperfections and we should constantly lookout for ways to better operationalize ethical values and other responsible AI decisions. For instance, this includes better strategies for curating and filtering training data. We should not overclaim or entertain spurious discourses and hype.
- **Give credit:** We should respect and acknowledge people's work through proper licensing and credit attribution.
It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it ü§©.

The new ""Community"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.

Some example use cases for discussions and pull requests:

- Propose suggestions in model cards to improve disclosures of ethical biases.
- Let users flag concerning generations of a given Space demo.
- Provide a venue through which model and dataset authors can have a direct discussion with community members.
- Allow others to improve your repositories! For example, users might want to provide TensorFlow weights!

## Discussions

![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)

Here is the question: When implementing a real-time dashboard for model monitoring, what combination of features would best support both ethical oversight and technical performance?
Here is answer 1:Technical metrics only with basic visualization
Here is answer 2: Ethical compliance tracking without performance metrics
Here is answer3: Integrated monitoring system with ethical compliance, performance metrics, and automated alerts
Here is answer 4: Basic logging system with manual review","I choose answer 3: Integrated monitoring system avec ethic compliance ,performance metric ans automated alerts"
"What is the primary benefit of employing Ray for distributed retrieval in RAG models, as opposed to PyTorch's torch.distributed?","[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (28673, '5d042da5-0429-46b6-bc0e-f68cfd69fe52'), (16390, 'b500f262-fd36-4f33-8fcf-b0ed2bbcbd1c'), (26634, 'b08afb02-fff2-4df4-b310-09795b760eda'), (10251, '31409786-ddd4-49ae-94d2-bea926b83be4'), (24603, '78522c83-d8fe-4f56-8cbe-0c5d741f50f1'), (30761, '683724e2-567a-46f0-9254-202ca2f88a96'), (9261, '198885ca-f8cc-4c0e-b829-e1a58632ac3f'), (9262, '740244f7-7949-41ec-911d-c5ac6ce7a64f'), (9263, '2e1affcb-5fd4-4b9b-a787-6ddb6248fbfb'), (29744, '6954830f-0ba6-4c15-a60d-3cd7e635d535'), (29745, 'ed5bb6d8-e497-4707-8feb-1b30b4362980'), (17458, '29ac6c95-d3e2-470e-a268-83fd7c395ddd'), (14387, 'dd664e1e-e7c7-41fa-8f28-7ec57f939569'), (29749, '99e4c28c-49ec-4ff4-8d8d-5d6e72b18962'), (26167, '571fd306-3198-44ef-a1d7-198d6a6d5854'), (29752, '866f9889-e3da-4559-b3d6-343baf84f073'), (3128, 'cd116de5-9c26-431b-aacf-ee4e4b6e9331'), (2618, '8c25f1c0-1ce2-488c-a60e-9ecdd7e2403b'), (29751, '09b22d0e-1989-4985-8183-c14df816cb62'), (29769, 'b7739621-6687-45e5-964d-cb69e022c6c8'), (4085, '8a08cc47-8750-4040-b9cf-d499f02be598'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (28755, '8ac51c04-dd98-4293-ac4f-7a630c9784db'), (23635, '8da5d714-1b83-4977-8b30-0d99b8236dac'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (23637, 'c1c8a809-12af-48eb-9d4e-166d5a66bf0e'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (6744, '0932c511-c713-4152-8929-d5fbbf574904'), (18522, 'a7bc8f48-cf4a-4fde-9b50-cf7db4d17089'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (3167, '3da1ff3e-1da3-42ef-97e6-51557bddcbd4'), (2658, 'd27df11c-c6b8-4473-b960-2902742a35d0'), (19555, '9bb43118-305f-403d-8b60-5c20b13a050c'), (22118, '36a5c490-635c-46e4-bbff-9f2186d8cb06'), (19561, 'bd6d1959-f94f-4a44-b2c4-a94c6950eb8c'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (23665, 'e42074b5-3215-4016-9f95-becc8a65bdeb'), (25715, '4ffee9ef-08fd-4cad-8cc0-2649906d7721'), (11383, '7d1776e7-3738-41fc-ae14-19c3f95d3c25'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (24711, 'ff60e964-aa24-44ba-9ac0-e72b18947ea2'), (24715, '6a7a1cae-5191-48e3-a13d-6d456647ffac'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (31374, '9ed70363-2221-4572-96f7-8b14c9fb8411'), (17046, '660ba43d-2e1a-42b0-88a6-ca4d50a71f29'), (17049, '8685cd9b-60eb-4a2b-8cce-43d96d0c264d'), (17050, '94372e5d-1e3c-4f95-9307-d1444bf39269'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (26291, '49c9c734-3e96-4cdc-a308-fc5f84f4264a'), (21171, 'c2adcd5d-814f-42b8-bac7-8989008f0c48'), (26294, 'e270afea-3092-480b-ba50-52d43d31bf2b'), (29374, '221cd942-7551-4df5-9a02-d0f73e055431'), (22761, '5728fbcb-454d-473d-9a7b-47d0f24a646f'), (22772, 'd0b704d9-ea88-403d-8797-758ba929ebd4'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (28938, 'c79cdb2e-622b-41c8-bde6-21c9ca8d958a'), (25874, 'd8b23baa-4b3f-49d9-b14c-77bdf69ddb1e'), (6433, '510f65f8-5c52-495f-ad44-a069704bb9ff'), (6434, 'da7f20d6-ff0f-435c-a62d-4e0b137881a6'), (13097, 'd0ae3415-29e2-4a0a-a9e4-9f788040397f'), (31533, '19fbc1ec-55bd-4264-b56a-1b6e60093779'), (31535, 'fcd12fd9-deed-453d-bbf0-f6bad6552817'), (31536, '37cc5143-44ba-4aa9-a04f-6f0006a85b20'), (12080, '9dbbfe60-bc76-41a7-abd6-f3511f64fb33'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (9540, '8cfc2be7-ce6e-4ee3-9fa7-d4da603fc5cf'), (23365, '26869439-d39c-4f25-96bc-fa6efd3cffb6'), (12104, '91971395-1e61-4f8d-bc69-6e8b6191c233'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15723, '27f5205f-669c-45a9-912d-c8bb8f74efd5'), (15724, 'cf08e3ee-cc1f-469a-864c-615037aa2306'), (24433, 'bebc2a4b-2754-442d-8616-0f6f184afc3c'), (24434, 'bb7afb46-7718-4546-95b2-cc62410b73cb'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (3961, 'efa02508-c542-4ad1-8b61-608f4b1415de'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (19842, '177c114f-7a6c-4116-9c04-d60f549b7fac'), (31108, 'b2a6db3a-19cd-4f85-8ebb-367f85973aa0'), (8086, '1c1af7f1-c19d-46a1-a25f-8fe6d479a26c'), (15768, 'f0a867f0-3313-47f8-a4d9-49cb78888982'), (15772, '65169dba-99db-485c-9bff-b711a9a78463'), (15773, 'fdad774c-45be-4ff9-acc2-9e93ace35a0a'), (17822, '20439f0d-de85-4a4f-91e9-58d6c81c72a7'), (15776, '5792c81e-163d-4045-9e92-aa5c22b8bdc9'), (17826, '25fa36a1-9fd9-4630-b761-739de4353aff'), (25009, 'e191215c-3849-428b-93c3-94c92cc197bb'), (12252, 'b6a1ac6c-8f54-462c-bcbb-00e3d41d94d7'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (27638, '57f803ac-4276-4362-9712-42f92f35f465'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (29177, '619af8a9-fbe3-421d-9eca-be6d9044cb79'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4092, '49e8126f-6930-48ec-abbd-632bcb85a3df'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Instead, a framework-agnostic and a more flexible implementation for ad-hoc concurrent programming is required. [Ray](https://ray.io/) fits the bill perfectly. Ray is a simple, yet powerful Python library for general-purpose distributed and parallel programming. Using Ray for distributed document retrieval, we achieved a **2x speedup per retrieval call compared to `torch.distributed`**, and overall better fine-tuning scalability.

### Ray for Document Retrieval
![alt_text](assets/12_ray_rag/torch_distributed_document_retrieval.png ""image_tooltip"")
_Document retrieval with the torch.distributed implementation_


The main drawback of the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) implementation for document retrieval was that it latched onto the same process group used for training and only the rank 0 training worker loaded the index into memory.

As a result, this implementation had some limitations:
```

Using Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than
just the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate
processes than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.

# Evaluation
Our evaluation script enables two modes of evaluation (controlled by the `eval_mode` argument): `e2e` - end2end evaluation, returns EM (exact match) and F1 scores calculated for the downstream task and `retrieval` - which returns precision@k of the documents retrieved for provided inputs.
```
You will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.

## Document Retrieval
When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other
with [`Ray`](https://docs.ray.io/en/master/).

This option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.
By default this flag is set to `pytorch`.

For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.
![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_

To overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Ray‚Äôs stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG.
And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
For the Ray implementation, the index is loaded in *separate* process(es). The training workers randomly select which
retriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed_retriever` arg to `ray`.
To configure the number of retrieval workers (the number of processes that load the index), you can set the `num_retrieval_workers` flag.
Also make sure to start the Ray cluster before running fine-tuning.

```bash
# Start a single-node Ray cluster.
ray start --head

python examples/research_projects/rag/finetune_rag.py \
    --data_dir $DATA_DIR \
    --output_dir $OUTPUT_DIR \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_type rag_sequence \
    --fp16 \
    --gpus 8
    --distributed_retriever ray \
    --num_retrieval_workers 4

# Stop the ray cluster once fine-tuning has finished.
ray stop
### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
_A performance comparison of different retrieval implementations. For each document retrieval implementation, we run 500 training steps with a per-GPU batch size of 8, and measure the time it takes to retrieve the contextual documents for each batch on the rank 0 training worker. As the results show, using multiple retrieval processes improves performance, especially as we scale training to multiple GPUs._


### How do I use it?

[Huggingface](https://huggingface.co/) provides a [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) based [fine tuning script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag), and we extended it to add the Ray retrieval implementation as an option. 

To try it out, first install the necessary requirements


```bash
pip install ray
pip install transformers
pip install -r transformers/examples/research_projects/rag/requirements.txt
As a result, this implementation had some limitations:

1. **Synchronization bottleneck**: The rank 0 worker had to receive the inputs from all workers, perform the index query, and then send the results back to the other workers. This limited performance with multiple training workers.
2. **PyTorch specific**: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well.

![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_
```

## What‚Äôs next?

Using RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks.
Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune‚Äôs integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.

And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!
```


Then, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh)!


```bash
# Sample script to finetune RAG using Ray for distributed retrieval.

# Add parent directory to python path to access lightning_base.py
export PYTHONPATH=""../"":""${PYTHONPATH}""

# Start a single-node Ray cluster.
ray start --head

# A sample finetuning run, you need to specify data_dir, output_dir and model_name_or_path
# run ./examples/rag/finetune_rag_ray.sh --help to see all the possible options
Key differences include:
1. DDP performs only a single communication per batch - sending gradients, while DP performs five different data exchanges per batch.
DDP copies data using [torch.distributed](https://pytorch.org/docs/master/distributed.html), while DP copies data within 
the process via Python threads (which introduces limitations associated with GIL). As a result, **`DistributedDataParallel` (DDP) is generally faster than `DataParallel` (DP)** unless you have slow GPU card inter-connectivity.
2. Under DP, GPU 0 performs significantly more work than other GPUs, resulting in GPU under-utilization. 
3. DDP supports distributed training across multiple machines, whereas DP does not.

This is not an exhaustive list of differences between DP and DDP, however, other nuances are out of scope of this guide.
You can get a deeper understanding of these methods by reading this [article](https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/).
# Retrieval Augmented Generation with Huggingface Transformers and Ray


##### A guest blog post by <a href=""/amogkam"">Amog Kamsetty</a> from the Anyscale team

[Huggingface Transformers](https://huggingface.co/) recently added the [Retrieval Augmented Generation (RAG)](https://twitter.com/huggingface/status/1310597560906780680) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks. In this blog post, we introduce the integration of [Ray](https://docs.ray.io/en/master/), a library for building scalable applications, into the RAG contextual document retrieval mechanism. This speeds up retrieval calls by 2x and improves the scalability of RAG distributed [fine-tuning](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag).



### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")
We started a 3-day job to reimplement the necessary parts of `torch.distributed`
To get up and running in the Rust world [nccl-rs](https://github.com/Narsil/nccl-rs).
We had the version working but something was off in the generations compared to its 
Python counterpart. During the investigation of the issues, we figured...
**that we had forgotten to remove the profiler in the Pytorch measurements**...
```

### Distributed usage

When you use [`~Dataset.map`] in a distributed setting, you should also use [torch.distributed.barrier](https://pytorch.org/docs/stable/distributed?highlight=barrier#torch.distributed.barrier). This ensures the main process performs the mapping, while the other processes load the results, thereby avoiding duplicate work.

The following example shows how you can use `torch.distributed.barrier` to synchronize the processes:

```py
>>> from datasets import Dataset
>>> import torch.distributed

>>> dataset1 = Dataset.from_dict({""a"": [0, 1, 2]})

>>> if training_args.local_rank > 0:
...     print(""Waiting for main process to perform the mapping"")
...     torch.distributed.barrier()

>>> dataset2 = dataset1.map(lambda x: {""a"": x[""a""] + 1})

>>> if training_args.local_rank == 0:
...     print(""Loading results from main process"")
...     torch.distributed.barrier()
### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")

_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._



Recently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library.
## Overview

Retrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and
sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate
outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing
both retrieval and generation to adapt to downstream tasks.

It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela.

The abstract from the paper is the following:
```

<Tip>

To learn more, take a look at the [Distributed Inference with ü§ó Accelerate](https://huggingface.co/docs/accelerate/en/usage_guides/distributed_inference#distributed-inference-with-accelerate) guide.

</Tip>

## PyTorch Distributed

PyTorch supports [`DistributedDataParallel`](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) which enables data parallelism.

To start, create a Python file and import `torch.distributed` and `torch.multiprocessing` to set up the distributed process group and to spawn the processes for inference on each GPU. You should also initialize a [`DiffusionPipeline`]:

```py
import torch
import torch.distributed as dist
import torch.multiprocessing as mp

from diffusers import DiffusionPipeline

sd = DiffusionPipeline.from_pretrained(
    ""runwayml/stable-diffusion-v1-5"", torch_dtype=torch.float16, use_safetensors=True
)
## Webserver part

Just like the Rust counterpart, we had to implement the batching of requests
with different parameters. Since we were in the `PyTorch` world, we have pretty
much full control of what's going on. 
Since we're in Python, we have the limiting factor that the `torch.distributed` 
needs to run on several processes instead of threads, which means it's slightly
harder to communicate between processes. In the end, we opted to communicate
raw strings over a Redis pub/sub to distribute the requests to all processes at once.
Since we are in different processes it's easier to do it that way than communicating
tensors (which are way bigger) for instance.

Here is the question: What is the primary benefit of employing Ray for distributed retrieval in RAG models, as opposed to PyTorch's torch.distributed?
Here is answer 1:Ray reduces memory usage through shared GPU access
Here is answer 2: Ray improves scalability and allows for framework independence
Here is answer3: Ray enhances model accuracy by focusing on inference tuning
Here is answer 4: Ray simplifies data preprocessing for large corpora",Here istheanswer : Hereisanswer2 : Rayscalabilityandallowsforframeworkindependence
"In value-based reinforcement learning, how does the Bellman equation reduce computational complexity, and how might it relate to modern document retrieval mechanisms?","[(6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (29213, 'ecfbdc64-5f35-4f83-8641-32ac46207dca'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (30816, '199d05d3-a124-431e-98c0-73eb3b9d9e94'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (13926, 'c6436b34-5943-4b25-b196-20d9695a4c1d'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (11365, 'fe8da28b-5e19-460e-be89-ccd9c18b8b30'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17025, '69f0e2e7-b149-4523-a038-d069193eeec2'), (17040, '3c81255d-fba2-425b-add0-66180682f513'), (14486, '1d0b077e-bd05-4bb6-9239-cdebc73e9813'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (21186, '31cd25b9-75f3-43f6-b771-f47d73dafddd'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (20179, '43cd6317-d563-4694-aa5f-8715ac76a2cc'), (20181, 'c3d5e236-999f-4476-b4ae-60717e645952'), (20182, 'ba1d7c23-8372-4dcc-879a-a18edc0a90a6'), (20183, '0bc8e3d4-5308-48a5-a6f1-343318328e56'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (5866, '4108faf4-0e48-4871-8e5b-31983d6c0129'), (6900, '0ae7974c-5457-40e4-a353-fde028bee870'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (1335, '268a1193-d635-4530-92e3-3146245d10a0'), (1336, '329c759f-4fc3-49ae-89bd-b24d835a593b'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (20806, 'b65455ed-1e56-4734-8384-734771a4e1ef'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (13153, 'ceaaeca1-e37c-45bd-870c-17fc8edf6ab6'), (16743, '28949d3c-ff9f-4e2c-8682-daa6d669b12c'), (16746, '45080cea-724d-4d98-8278-7cd29fd71f9b'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23958, '82ffb2ac-f889-482f-bd2b-090b36dbcce1'), (23959, '46501f70-bf28-440b-ba81-9c88be735e91'), (23960, '85cca8b8-d29e-4219-a5ff-f837b064f343'), (23961, '13b18911-853f-4362-aad9-9563a8885e44'), (23962, '35661ef2-1497-4bbd-af50-38276811d7ce'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (925, '2b19d79e-1c48-4d05-a48d-19f5138ae7c1'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (23454, 'bb59e1c0-8193-40ce-a450-9efc2ecc6226'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (23971, '02fa64da-7776-42d4-a529-913fbf0f4c8b'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (13756, '9c192e5b-e67e-45d1-b202-4edddebfc1bd'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13764, 'c2619e99-58b4-4943-a672-c368132c540e'), (13766, '324b6368-b377-4ac8-920b-ae7106ccadf1'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (23506, '526dbd3b-8d49-4c69-b33f-3da88e48f7fa'), (13777, '679244ce-c5e4-4649-ade1-f8d91fa4825c'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13782, 'cc4cbb46-9352-4787-bd3e-b5aec0c09d4e'), (13784, 'df3f2f2b-7b61-43c5-9f8b-7c20249b29e4'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (13786, 'ce79d072-ab99-4290-b96b-8252d9463234'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (1004, '7bae84e8-1b63-41df-828f-817d61593d05'), (23535, '0de47e64-f26d-4a3b-bb54-a6517f3e5e5b'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: However, most Transformer models continued to trend towards more parameters, leading to new models focused on improving training efficiency. [ALBERT](model_doc/albert) reduces memory consumption by lowering the number of parameters in two ways: separating the larger vocabulary embedding into two smaller matrices and allowing layers to share parameters. [DeBERTa](model_doc/deberta) added a disentangled attention mechanism where the word and its position are separately encoded in two vectors. The attention is computed from these separate vectors instead of a single vector containing the word and position embeddings. [Longformer](model_doc/longformer) also focused on making attention more efficient, especially for processing documents with longer sequence lengths. It uses a combination of local windowed attention (attention only calculated from fixed window size around each token) and global attention (only for specific task tokens like `[CLS]` for classification) to create a sparse
Mid-way Quiz [[mid-way-quiz]]

The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.


### Q1: What are the two main approaches to find optimal policy?


<Question
	choices={[
		{
			text: ""Policy-based methods"",
			explain: ""With Policy-Based methods, we train the policy directly to learn which action to take given a state."",
      correct: true
		},
		{
			text: ""Random-based methods"",
			explain: """"
		},
    {
			text: ""Value-based methods"",
			explain: ""With value-based methods, we train a value function to learn which state is more valuable and use this value function to take the action that leads to it."",
      correct: true
		},
		{
			text: ""Evolution-strategies methods"",
      explain: """"
		}
	]}
/>


### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>
- The value of  \\(V(S_{t+1}) \\)  = Immediate reward  \\(R_{t+2}\\)  + Discounted value of the next state ( \\(gamma * V(S_{t+2})\\) ).
- And so on.

To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return,¬†**which is a long process.**¬†This is equivalent¬†**to the sum of immediate reward + the discounted value of the state that follows.**

## **Monte Carlo vs Temporal Difference Learning**

The last thing we need to talk about before diving into Q-Learning is the two ways of learning.

Remember that an RL agent¬†**learns by interacting with its environment.**¬†The idea is that¬†**using the experience taken**, given the reward it gets, will¬†**update its value or policy.**

Monte Carlo and Temporal Difference Learning are two different¬†**strategies on how to train our value function or our policy function.**¬†Both of them¬†**use experience to solve the RL problem.**
## **The ‚ÄúDeep‚Äù in Reinforcement Learning**

‚áí What we've talked about so far is Reinforcement Learning. But where does the ""Deep"" come into play?

Deep Reinforcement Learning introduces¬†**deep neural networks to solve Reinforcement Learning problems**¬†‚Äî hence the name ‚Äúdeep‚Äù.

For instance, in the next article, we‚Äôll work¬†on¬†Q-Learning¬†(classic Reinforcement Learning) and then¬†Deep Q-Learning¬†both are value-based RL algorithms.

You‚Äôll see the difference is that in the first approach,¬†**we use a traditional algorithm**¬†to create a Q table that helps us find what action to take for each state.

In the second approach,¬†**we will use a Neural Network**¬†(to approximate the q value).

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/deep.jpg"" alt=""Value based RL""/>
  <figcaption>Schema inspired by the Q learning notebook by Udacity
</figcaption>
</figure>
Summary [[summary]]

That was a lot of information! Let's summarize:

- Reinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.

- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**

- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**

- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
Before DDPO, Reward-weighted regression (RWR) was an established way of using Reinforcement Learning to fine-tune diffusion models. RWR reuses the denoising loss function of the diffusion model along with training data sampled from the model itself and per-sample loss weighting that depends on the reward associated with the final samples. This algorithm ignores the intermediate denoising steps/samples. While this works, two things should be noted:

1. Optimizing by weighing the associated loss, which is a maximum likelihood objective, is an approximate optimization
2. The associated loss is not an exact maximum likelihood objective but an approximation that is derived from a reweighed variational bound

The two orders of approximation have a significant impact on both performance and the ability to handle complex objectives.
- Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.
- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**
- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**
- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
*Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models,
leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence
modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards.
Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well
in other domains, such as natural-language processing, can also provide effective solutions to the RL problem.
To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture
to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence
modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common
So let's get started!

- [What is RL? A short recap](#what-is-rl-a-short-recap)
- [The two types of value-based methods](#the-two-types-of-value-based-methods)
  - [The State-Value function](#the-state-value-function)
  - [The Action-Value function](#the-action-value-function)
- [The Bellman Equation: simplify our value estimation](#the-bellman-equation-simplify-our-value-estimation)
- [Monte Carlo vs Temporal Difference Learning](#monte-carlo-vs-temporal-difference-learning)
  - [Monte Carlo: learning at the end of the episode](#monte-carlo-learning-at-the-end-of-the-episode)
  - [Temporal Difference Learning: learning at each step](#temporal-difference-learning-learning-at-each-step)


## **What is RL? A short recap**
### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>

**The Bellman equation is a recursive equation** that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:

Rt+1 + gamma * V(St+1)

The immediate reward + the discounted value of the state that follows

</details>

### Q3: Define each part of the Bellman Equation

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4-quiz.jpg"" alt=""Bellman equation quiz""/>


<details>
<summary>Solution</summary>

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg"" alt=""Bellman equation solution""/>

</details>

### Q4: What is the difference between Monte Carlo and Temporal Difference learning methods?
Writing a loss function to capture these attributes seems intractable and most language models are still trained with a simple next token prediction loss (e.g. cross entropy). To compensate for the shortcomings of the loss itself people define metrics that are designed to better capture human preferences such as [BLEU](https://en.wikipedia.org/wiki/BLEU) or [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)). While being better suited than the loss function itself at measuring performance these metrics simply compare generated text to references with simple rules and are thus also limited. Wouldn't it be great if we use human feedback for generated text as a measure of performance or go even one step further and use that feedback as a loss to optimize the model? That's the idea of Reinforcement Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language model with human feedback. RLHF has enabled language models to begin to align a
### Among the value-based methods, we can find two main strategies

- **The state-value function.** For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.
- **The action-value function.** In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.

### Epsilon-greedy strategy:

- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.
- Chooses the action with the highest expected reward with a probability of 1-epsilon.
- Chooses a random action with a probability of epsilon.
- Epsilon is typically decreased over time to shift focus towards exploitation.

### Greedy strategy:
The Bellman Equation: simplify our value estimation [[bellman-equation]]

The Bellman equation¬†**simplifies our state value or state-action value calculation.**


<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg"" alt=""Bellman equation""/>

With what we have learned so far, we know that if we calculate \\(V(S_t)\\) (the value of a state), we need to calculate the return starting at that state and then follow the policy forever after.¬†**(The policy we defined in the following example is a Greedy Policy; for simplification, we don't discount the reward).**

So to calculate \\(V(S_t)\\), we need to calculate the sum of the expected rewards. Hence:
Introduction [[introduction]]

  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png"" alt=""thumbnail""/>

In the last unit, we learned about Deep Q-Learning. In this value-based deep reinforcement learning algorithm, we¬†**used a deep neural network to approximate the different Q-values for each possible action at a state.**

Since the beginning of the course, we have only studied value-based methods,¬†**where we estimate a value function as an intermediate step towards finding an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy"" />

In value-based methods, the policy ** \(œÄ\) only exists because of the action value estimates since the policy is just a function** (for instance, greedy-policy) that will select the action with the highest value given a state.
Training a language model with reinforcement learning was, for a long time, something that people would have thought as impossible both for engineering and algorithmic reasons. 
What multiple organizations seem to have gotten to work is fine-tuning some or all of the parameters of a **copy of the initial LM** with a policy-gradient RL algorithm, Proximal Policy Optimization (PPO). 
Some parameters of the LM are frozen because fine-tuning an entire 10B or 100B+ parameter model is prohibitively expensive (for more, see Low-Rank Adaptation ([LoRA](https://arxiv.org/abs/2106.09685)) for LMs or the [Sparrow](https://arxiv.org/abs/2209.14375) LM from DeepMind) -- depending on the scale of the model and infrastructure being used. The exact dynamics of how many parameters to freeze, or not, is considered an open research problem.
### Q1: Which of the following interpretations of bias-variance tradeoff is the most accurate in the field of Reinforcement Learning?

<Question
	choices={[
		{
			text: ""The bias-variance tradeoff reflects how my model is able to generalize the knowledge to previously tagged data we give to the model during training time."",
			explain: ""This is the traditional bias-variance tradeoff in Machine Learning. In our specific case of Reinforcement Learning, we don't have previously tagged data, but only a reward signal."",
      			correct: false,
		},
   		{
			text: ""The bias-variance tradeoff reflects how well the reinforcement signal reflects the true reward the agent should get from the enviromment"",
			explain: """",
      			correct: true,
		},		
	]}
/>

### Q2: Which of the following statements are true, when talking about models with bias and/or variance in RL?
## **The Bellman Equation: simplify our value estimation**

The Bellman equation¬†**simplifies our state value or state-action value calculation.**

  
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/bellman.jpg"" alt=""Bellman equation""/>
</figure>
                                                                
With what we learned from now, we know that if we calculate the \\(V(S_t)\\) (value of a state), we need to calculate the return starting at that state and then follow the policy forever after.¬†**(Our policy that we defined in the following example is a Greedy Policy, and for simplification, we don't discount the reward).**
  
So to calculate \\(V(S_t)\\), we need to make the sum of the expected rewards. Hence:
Here we see that our value function¬†**defined value for each possible state.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/value_2.jpg"" alt=""Value based RL""/>
  <figcaption>Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.
</figcaption>
</figure>

Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.

If we recap:

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/vbm_1.jpg"" alt=""Vbm recap""/>
</figure>
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/vbm_2.jpg"" alt=""Vbm recap""/>
</figure>


## **The ‚ÄúDeep‚Äù in Reinforcement Learning**

Here is the question: In value-based reinforcement learning, how does the Bellman equation reduce computational complexity, and how might it relate to modern document retrieval mechanisms?
Here is answer 1:By simplifying the value estimation process; provides a basis for greedy policies
Here is answer 2: By breaking down the retrieval task; supports action-value approximations
Here is answer3: By iteratively refining retrieval indices; analogous to optimal policy updates
Here is answer 4: By prioritizing states with the highest returns; aligns with state-value approximation","Q1 Answer : Both Value-Based Methods (""Policy-Based Methods"", False , 
                      {""text"":=Value-Based Methods,""explain"": With Value-Based METHODS,""We Train"",""correct"":true} ,
                       {""text"":=Random-Based Methods} ,
                       {""text"":=Evolution-strategies Methods } )"
Which feature of Gradio enhances real-time model interaction and how does it align with reinforcement learning principles for optimal decision-making?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (2053, '9354950f-a962-4289-a102-9a4de80854fa'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (5660, 'f060d5f9-0dce-4d78-8298-2524cde9ccdb'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (16469, 'aba54ccf-519e-4018-816e-984adec99ff9'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (1145, 'abe44225-c493-44fb-82d3-494085e90422'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (14465, '83b269a3-f505-41d3-82ff-1eb665eca1a9'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (14478, '583d434f-41d4-4529-bca3-6a3c23fe8382'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (18584, '815482b7-2249-4e35-a25e-18ccc16fd17b'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27349, 'ab899ec0-c793-4e15-997a-3c85ad2d12f9'), (24789, '6151fc10-9ec2-41ab-8af7-99a5aa43b076'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (27927, 'ea460800-536f-4a56-948a-42c18e8375be'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2860, 'cd0fbf96-6263-4efc-9922-f67f4cce3a51'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (5474, '1158d6f0-fad9-4bbb-babf-d6178165afc7'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (16745, 'bbcf4d12-db52-4214-bd20-fbb0a28f27e1'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (16762, '0a98d923-bf03-4726-a7c3-4f2c56457e83'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (21907, '0e50272c-345a-4857-a282-c33dc7daecf5'), (14231, '38cb7f87-85dc-4acf-950b-b56bbf7a2a7b'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (20391, '9de4cdee-68cc-43dd-8926-27cabdf0d172'), (14249, '3bde59f7-ba94-4ec5-8768-6195057f7e50'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (14257, '04266ef8-e616-469e-b903-4a3381ca634c'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (13247, '7151a6c0-468c-45c7-9cb8-418c3953caaf'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (3541, '50f4f4c3-4378-4cff-8b04-2e6443145fad'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (5608, 'a7a2b842-db8e-4097-857c-0f4dca7d024b'), (29674, '81aa5082-4e7c-42b6-8684-491f31bb23aa'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
## Setting up a Gradio Demo for EfficientNet-Lite4

EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)

Here we walk through setting up a example demo for EfficientNet-Lite4 using Gradio

First we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio interface for a user to interact with. See the full code below.
```

## 5. Create Gradio Chatbot backed by Amazon SageMaker

We can also create a gradio application to chat with our model. Gradio is a python library that allows you to quickly create customizable UI components around your machine learning models. You can find more about gradio [here](https://gradio.app/).

```python
!pip install gradio  --upgrade
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.1.0

### Features

- [#5005](https://github.com/gradio-app/gradio/pull/5005) [`f5539c76`](https://github.com/gradio-app/gradio/commit/f5539c7618e31451420bd3228754774da14dc65f) - Enhancement: Add focus event to textbox and number component. Thanks [@JodyZ0203](https://github.com/JodyZ0203)!

### Fixes
Demos and GUIs built with Gradio give the power of machine learning to more and more people because they allow non-technical users to access, use, and give feedback on models. And our acquisition by Hugging Face is the next step in this ongoing journey of accessibility. Hugging Face has already radically democratized machine learning so that any software engineer can use state-of-the-art models with a few lines of code. By working together with Hugging Face, we're taking this even further so that machine learning is accessible to literally anyone with an internet connection and a browser. With Hugging Face, we are going to keep growing Gradio and make it the best way to share your machine learning model with anyone, anywhere üöÄ
Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API

Hugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load(""Helsinki-NLP/opus-mt-en-es"", src=""models"")

demo.launch()
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
I recruited my talented housemates Ali Abdalla, Ali Abid, and Dawood Khan to release the first version of Gradio in 2019. We steadily expanded to cover more areas of machine learning including text, speech, and video. We found that it wasn't just researchers who needed to share machine learning models: interdisciplinary teams in industry, from startups to public companies, were building models and needed to debug them internally or showcase them externally. Gradio could help with both. Since we first released the library, more than 300,000 demos have been built with Gradio. We couldn't have done this without our community of contributors, our supportive investors, and the amazing Ahsen Khaliq who joined our company this year.
Hugging Face has always worked to make models accessible and easy to use. The `transformers` library makes it possible to load a model in a few lines of code. After a model is loaded, it can be used to make predictions on new data programmatically.  _But it‚Äôs not just programmers that are using machine learning models!_ An increasingly common scenario in machine learning is **demoing models to interdisciplinary teams** or letting **non-programmers use models** (to help discover biases, failure points, etc.). 

The **[Gradio library](https://gradio.app/)** lets machine learning developers create demos and GUIs from machine learning models very easily, and share them for free with your collaborators as easily as sharing a Google docs link. Now, we‚Äôre excited to share that the Gradio 2.0 library lets you **_load and use almost any Hugging Face model_ _with a GUI_** **_in just 1 line of code_**. Here‚Äôs an example:

![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)
Using Flagging

Related spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic
Tags: FLAGGING, DATA

## Introduction

When you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these ""hard"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.

Gradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.

## The **Flag** button in `gradio.Interface`
his demo shows how you can build a live interactive dashboard with gradio.
The current time is refreshed every second and the plot every half second by using the 'every' keyword in the event handler.
Changing the value of the slider will control the period of the sine curve (the distance between peaks).
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features

Here is the question: Which feature of Gradio enhances real-time model interaction and how does it align with reinforcement learning principles for optimal decision-making?
Here is answer 1:Dynamic type-checking for API calls; supports state-action evaluation
Here is answer 2: Predefined Greedy Policy for decision rendering; aligns with action-value methods
Here is answer3: Interactive UI with flexible pipeline configurations; mirrors exploration-exploitation
Here is answer 4: Type-safe code generation; enforces static policy behavior","The correct answer(s): 
Interactive UI"
How do licensing practices on the Hugging Face Hub promote ethical compliance and user transparency in model deployment?,"[(12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (22547, '51e87e51-6544-440b-9057-4201251fc27d'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (5687, '56f80e00-a026-4153-8220-a3fd825d93c4'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (12348, '57a3c79a-af62-43be-bebc-4ef70c6c1495'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (22588, 'a59c4df1-bdaa-4faa-b0f4-f3b5aeff16a6'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (28741, '50733951-b5db-4e9b-83a4-b2911af5c989'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (25685, 'e7551698-0040-4251-b71c-50be7bc42a95'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (14944, 'cd075ce4-d02e-47be-ad35-21b38e850692'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (17509, '56ea90b0-f162-4bf5-8c05-654f3a183cd9'), (1127, '61bce4e3-ba06-42fd-82da-5f4b1ae2562a'), (21629, '98fbe65d-63b8-42b8-b534-0cd39021b65d'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (19603, '561460ee-7e32-4394-b0d3-d59f4b7790dc'), (27284, '344b1139-9e69-4a8a-a5b7-593e7b089268'), (26785, 'a539b4a7-925a-4038-bb46-23b14c6dbb78'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (30380, 'b4858a62-bfab-4374-b4e5-c287a54d33be'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (7868, 'fc0025e1-0005-48de-97c7-5fe250a873d5'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (13510, '0e4bf28c-0eed-49c8-a4a5-724ce73aa1fd'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (11975, 'c977182b-d101-4944-95e4-2c832d9bf6fd'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (14535, 'b9835dda-ff2d-4d55-a497-5553a72948a6'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16070, 'c25e9600-d460-45ec-9d7b-54be9e3a1dc1'), (208, 'ca329095-eb87-48a9-9393-7dafb6980978'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (10461, '791d16b9-ac09-45b6-a426-0eea3d3b4063'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (5873, '9d5439bb-61db-43f2-ba27-d82d1ce267c5'), (14577, '03a094b5-b29c-4840-a743-d2ceb896c494'), (23284, '59c5d501-a716-4200-a252-695f07ef83dc'), (11526, '994250e0-f714-40fe-b8ff-52c2610db830'), (8970, 'd5af4c95-97c8-4196-bab1-0d74644248f4'), (4366, 'fa03c662-5522-4732-a0ba-6b673062d9af'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (10530, '04d72a8e-2ec5-4e32-86ed-6aad0064f400'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (29512, '8b6ee5ce-9e58-4501-b82e-25ec855e7c6f'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (24410, '63102f98-f7b8-4a0b-8034-3e2687269cb4'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (8549, '7ee9801f-9160-4b67-ab5e-ccacabf36b60'), (17767, 'a7af65e5-6000-4846-b469-ecc521b89dfa'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (24436, '62eee91c-545a-404b-97af-1d99deab920c'), (897, 'f387e8a2-b1bf-48fa-a6f3-c7a015a21ae7'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (19335, 'e7a25af1-ba2a-4e2e-af04-f55deda54356'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (22421, 'bdae70d4-1482-4a7a-aa11-6e2aa1cd81c2'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (17301, '9e130d4e-12df-4941-91c1-6e5fd07be093'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (11163, '395c4c3d-8c49-42dc-9fda-2e3de70b64bc'), (5027, 'd4bbbeb5-0f59-45d8-b525-8c4180353a26'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (3510, '4e9735e1-0a03-42b4-a237-4ea82a96730d'), (7096, '3ba57845-c986-4965-b8a2-53c99388658b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (12740, '2c345101-5eb0-4245-879d-78b6cff152f2'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (21463, 'e97bfba4-a14f-477a-9f94-afd38c01d4c5'), (9690, 'c221a30d-3ba1-4731-af02-978855235bc7'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (8671, '6d52ab73-7503-465a-8ecd-7a7b98b95c22'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (25065, '0e262c4e-5cc0-4b85-9743-25d55e76332e'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (10232, '27e77ecf-9ddd-433f-bcda-08fa044e1cdc'), (22012, '5c55b2ec-3284-45a4-9f86-cf514fba02e0')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it ü§©.

The new ""Community"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.

Some example use cases for discussions and pull requests:

- Propose suggestions in model cards to improve disclosures of ethical biases.
- Let users flag concerning generations of a given Space demo.
- Provide a venue through which model and dataset authors can have a direct discussion with community members.
- Allow others to improve your repositories! For example, users might want to provide TensorFlow weights!

## Discussions

![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
Hugging Face‚Äôs mission is to [‚Äúdemocratize good machine learning‚Äù](https://huggingface.co/about). We understand the term ‚Äúdemocratization‚Äù in this context to mean making Machine Learning systems not just easier to develop and deploy, but also easier for its many stakeholders to understand, interrogate, and critique. To that end, we have worked on fostering transparency and inclusion through our [education efforts](https://huggingface.co/learn/nlp-course/chapter1/1), [focus on documentation](https://huggingface.co/docs/hub/model-cards), [community guidelines](https://huggingface.co/blog/content-guidelines-update) and approach to [responsible openness](https://huggingface.co/blog/ethics-soc-3), as well as developing no- and low-code tools to allow people with all levels of technical background to analyze [ML datasets](https://huggingface.co/spaces/huggingface/data-measurements-tool) and [models](https://huggingface.co/spaces/society-ethics/StableBias). We believe this helps everyone
Hello, world!

Originating as an open-source company, Hugging Face was founded on some key ethical values in tech: _collaboration_, _responsibility_, and _transparency_. To code in an open environment means having your code ‚Äì and the choices within ‚Äì viewable to the world, associated with your account and available for others to critique and add to.  As the research community began using the Hugging Face Hub to host models and data, the community directly integrated _reproducibility_ as another fundamental value of the company. And as the number of datasets and models on Hugging Face grew, those working at Hugging Face implemented [documentation requirements](https://huggingface.co/docs/hub/models-cards) and [free instructive courses](https://huggingface.co/course/chapter1/1), meeting the newly emerging values defined by the research community with complementary values around _auditability_ and _understanding_ the math, code, processes and people that lead to current technology.
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
This consideration for people's consent and experiences on the platform extends to Community Content and people's behaviors on the Hub. To maintain a safe and welcoming environment, we do not allow aggressive or harassing language directed at our users and/or the Hugging Face staff. We focus on fostering collaborative resolutions for any potential conflicts between users and repository authors, intervening only when necessary. To promote transparency, we encourage open discussions to occur within our Community tab.

Our approach is a reflection of our ongoing efforts to adapt and progress, which is made possible by the invaluable input of our users who actively collaborate and share their feedback. We are committed to being receptive to comments and constantly striving for improvement. We encourage you to reach out to [feedback@huggingface.co](mailto:feedback@huggingface.co) with any questions or concerns.
## Examples of implementations: Safety features and Mechanisms

The team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.

- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.

- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.

- **Encouraging safety in deployment**
Organizations

The Hugging Face Hub offers **Organizations**, which can be used to group accounts and manage datasets, models, and Spaces. The Hub also allows admins to set user roles to [**control access to repositories**](./organizations-security) and manage their organization's [payment method and billing info](https://huggingface.co/pricing).

If an organization needs to track user access to a dataset due to licensing or privacy issues, an organization can enable [user access requests](./datasets-gated).

## Contents

- [Managing Organizations](./organizations-managing)
- [Organization Cards](./organizations-cards)
- [Access Control in Organizations](./organizations-security)
- [Enterprise Hub features](./enterprise-hub)
  - [SSO in Organizations](./enterprise-sso)
  - [Audit Logs](./audit-logs)
  - [Storage Regions](./storage-regions)
- monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
- robustly document our most-downloaded models with model cards that detail social impacts, biases, and intended and out-of-scope use cases,
- create audience-guiding tags, such as the ‚ÄúNot For All Audiences‚Äù tag that can be added to the repository‚Äôs card metadata to avoid un-requested violent and sexual content,
- promote use of [Open Responsible AI Licenses (RAIL)](https://huggingface.co/blog/open_rail) for [models](https://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license), such as with LLMs ([BLOOM](https://huggingface.co/spaces/bigscience/license), [BigCode](https://huggingface.co/spaces/bigcode/license)),
- conduct research that [analyzes](https://arxiv.org/abs/2302.04844) which models and datasets have the highest potential for, or track record of, misuse and malicious use.
Building from these basics, we are taking an approach to operationalizing values that center the context-specific nature of our projects and the foreseeable effects they may have. As such, we offer no global list of values or principles here; instead, we continue to share [project-specific thinking](https://huggingface.co/blog/ethical-charter-multimodal), such as this newsletter, and will share more as we understand more. Since we believe that community discussion is key to identifying different values at play and who is impacted, we have recently opened up the opportunity for anyone who can connect to the Hugging Face Hub online to provide [direct feedback on models, data, and Spaces](https://huggingface.co/blog/community-update). Alongside tools for open discussion, we have created a [Code of Conduct](https://huggingface.co/code-of-conduct) and [content guidelines](https://huggingface.co/content-guidelines) to help guide discussions along dimensions we believe to be important for an
- **On-prem Private Hub**: on-premise deployment of the Hugging Face Hub on your own infrastructure. For customers with strict compliance rules and/or workloads where they don't want or are not allowed to run on a public cloud.

Now that we have covered the basics of what the Private Hub is, let's go over how companies are using it to accelerate their ML development. 

## 3. How Are Companies Using the Private Hub to Accelerate Their ML Roadmap?

[ü§ó Transformers](https://github.com/huggingface/transformers) is one of the [fastest growing open source projects of all time](https://star-history.com/#tensorflow/tensorflow&nodejs/node&kubernetes/kubernetes&pytorch/pytorch&huggingface/transformers&Timeline). We now offer [25+ open source libraries](https://github.com/huggingface) and over 10,000 companies are now using Hugging Face to build technology with machine learning.
Model cards were originally proposed by [(Mitchell et al., 2019)](https://dl.acm.org/doi/10.1145/3287560.3287596) and provide a framework for model reporting that showcases information relevant to bias risks, including broad ethical considerations, disaggregated evaluation, and use case recommendation. The Hugging Face Hub provides even more tools for model documentation, with a [model card guidebook](https://huggingface.co/docs/hub/model-cards) in the Hub documentation, and an [app that lets you create extensive model cards](https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool) easily for your new model.
* **Encouraging safety in deployment**
    * **[Safe Stable Diffusion](https://huggingface.co/docs/diffusers/main/en/api/pipelines/stable_diffusion_safe)**: It mitigates the well-known issue that models, like Stable Diffusion, that are trained on unfiltered, web-crawled datasets tend to suffer from inappropriate degeneration. Related paper: [Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models](https://arxiv.org/abs/2211.05105).

    * **Staged released on the Hub**: in particularly sensitive situations, access to some repositories should be restricted. This staged release is an intermediary step that allows the repository‚Äôs authors to have more control over its use.

* **Licensing**: [OpenRAILs](https://huggingface.co/blog/open_rail), a new type of licensing, allow us to ensure free access while having a set of restrictions that ensure more responsible use.
Hugging Face also offers Business Associate Addendum or GDPR data processing agreement through the Inference Endpoint enterprise plan.

## Model Security/Privacy:

You can set a model repository as private if you do not want to publicly expose it. Hugging Face does not own any model or data you upload to the Hugging Face hub. Hugging Face does provide malware and pickle scans over the contents of the model repository as with all items in the Hub.

## Inference Endpoints and Hub Security

The Hugging Face Hub, which Inference Endpoints is part, is also SOC2 Type 2 certified. The Hugging Face Hub offers Role Based Access Control. For more on hub security: https://huggingface.co/docs/hub/security

<img width=""150"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg"">

## Inference Endpoint Security level

We currently offer three types of endpoints, in order or increasing security level:
To upload models to the Hub, or download models and integrate them into your work, explore the [**Models documentation**](./models). You can also choose from [**over a dozen libraries**](./models-libraries) such as ü§ó Transformers, Asteroid, and ESPnet that support the Hub.

## Datasets

The Hub is home to over 5,000 datasets in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. The Hub makes it simple to find, download, and upload datasets. Datasets are accompanied by extensive documentation in the form of [**Dataset Cards**](./model-cards) and [**Dataset Preview**](./datasets-overview#datasets-on-the-hub) to let you explore the data directly in your browser. While many datasets are public, [**organizations**](./organizations) and individuals can create private datasets to comply with licensing or privacy issues. You can learn more about [**Datasets here on Hugging Face Hub documentation**](./datasets-overview).
of Conduct](https://huggingface.co/code-of-conduct) and [content guidelines](https://huggingface.co/content-guidelines) to help guide discussions along dimensions we believe to be important for an inclusive community space. We have developed a [Private Hub](https://huggingface.co/blog/introducing-private-hub) for secure ML development, a [library for evaluation](https://huggingface.co/blog/eval-on-the-hub) to make it easier for developers to evaluate their models rigorously, [code for analyzing data for skews and biases](https://github.com/huggingface/data-measurements-tool), and [tools for tracking carbon emissions when training a model](https://huggingface.co/blog/carbon-emissions-on-the-hub). We are also developing [new open and responsible AI licensing](https://huggingface.co/blog/open_rail), a modern form of licensing that directly addresses the harms that AI systems can create. And this week, we made it possible to [‚Äúflag‚Äù model and Spaces
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
[Explore existing organizations](https://huggingface.co/organizations), create a new organization [here](https://huggingface.co/organizations/new), and then visit the [**Organizations documentation**](./organizations) to learn more.

## Security

The Hugging Face Hub supports security and access control features to give you the peace of mind that your code, models, and data are safe. Visit the [**Security**](./security) section in these docs to learn about:

- User Access Tokens
- Access Control for Organizations
- Signing commits with GPG
- Malware scanning

<img width=""150"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/security-soc-1.jpg"">

Here is the question: How do licensing practices on the Hugging Face Hub promote ethical compliance and user transparency in model deployment?
Here is answer 1:By enforcing strict legal contracts through the Hub
Here is answer 2: By categorizing licenses into predefined templates for open-source models
Here is answer3: By standardizing metadata for custom licenses and supporting discoverability
Here is answer 4: By requiring proprietary licenses for all high-performance models",The correct answer is: By standardizing metadata for custom licenses and supporting discoverability.
"When fine-tuning RAG models, how does incorporating Ray processes affect the efficiency of contextual document retrieval?","[(3584, '50687ee5-f560-462d-8edd-ca97b26579c1'), (4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (5123, '3c49348e-9d5c-46a4-aaf6-e5f3275adbc0'), (10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (17912, '6c000270-627d-44ae-bac1-9ab0640951e5'), (6679, '5355dc06-3d44-4916-a09c-fc5839bb0ca7'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (9244, '8fa4d35f-4e98-4682-8ec6-82ae0b95bd93'), (2613, '8ed34ac1-8e5c-4f4d-a4e5-765f43230e11'), (2618, '8c25f1c0-1ce2-488c-a60e-9ecdd7e2403b'), (2619, '024156c9-9a01-45e7-8318-ad7aea597207'), (24122, '3967b12b-479f-49b4-867a-35745704a595'), (24124, '639bd8a5-b100-403a-959e-b14dcd51135e'), (6726, '795b94d0-39c6-474d-8d73-0e5cb0319395'), (2638, 'fd31dc0c-6b80-49bb-9662-e72e93f6a307'), (21074, '808ca6dc-63be-4d6c-b1e5-63d43d71fae2'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (19028, '2b2cd572-c0b0-4841-8f1d-41c6eaf19944'), (19029, '408d71df-7e47-408f-a66e-f6c9e3711b88'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (6749, '81c8a3fc-8f17-4481-92ab-f3d126518cc2'), (8797, '0855748e-ea7f-43e6-b941-cd339499c56f'), (30306, '22273990-7b23-4785-accd-84c550db4f53'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (26743, '7b7813e8-f2db-4dc4-aa11-195a3d0475da'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (24711, 'ff60e964-aa24-44ba-9ac0-e72b18947ea2'), (30343, '6b30a0cf-b903-492c-aba3-abc1809c663c'), (6280, '895b9441-5bad-4ad3-8118-16fa7c02947d'), (24714, '7a6500d3-a84e-491a-b1e1-a650b28bcbe9'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (21646, '4cde0740-3b9b-4bd8-a2df-5a3fb1d427e2'), (9872, 'f0242619-9c2e-487d-88a3-a40e416bc97c'), (2705, 'c2f73906-1242-4776-b210-f4af9061b8ad'), (24722, '9a565944-9d37-481d-a9b1-09c1be33068b'), (24724, '6bd040c2-e0aa-498a-bb77-a9de2eb9c42c'), (25257, 'ef9a46af-a650-4ab1-a69e-65e287089adf'), (18089, 'a0ef15e1-8cdf-46a2-b27e-a15791eb2418'), (11437, 'bcf6c108-66d8-4c82-8e0b-90a0f98d8602'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (22202, '2aa42eca-37dd-4c52-ac50-71d43028fb51'), (10941, '3a713f0f-1a17-4b52-8cd4-d35ebf71072e'), (14526, '50ba7f0b-b5bc-4765-95b8-6b353b288b4f'), (8180, 'b61dffa7-89c2-4e92-9853-68e7782fc0c2'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (16588, '28e0b657-bebd-4eeb-a96e-fed578a3cefa'), (24273, '9d89b90d-0743-475c-8c30-c52c8b3feee9'), (24280, 'ef680020-6ecc-48bf-a65e-0761dfb2d098'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (23779, 'dd9b1fb6-c1ea-4613-82a3-8457708a0756'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (8947, '7672a471-5549-49f3-aebe-5677229568e0'), (6900, '0ae7974c-5457-40e4-a353-fde028bee870'), (21251, '31eb9d16-0e2a-43a4-938b-4c96a7366457'), (18692, 'a4f174fa-f21a-4a5e-b591-ae5e955a9ef4'), (21253, 'dd8c8a62-e782-4212-8d05-16b0914b678c'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (14084, '975cef0c-44a3-4f42-bb19-d2ea4270c921'), (20232, '668b86d9-f6af-41ad-8730-c6c574c4a338'), (19724, '33ebed95-e2dc-4ff2-96f7-60f2c17586ce'), (1806, '7e4b23b4-6f6d-4dfa-a292-cab055399716'), (18194, 'db26ab63-8606-4fce-adeb-9039344f080a'), (3861, 'b29474d6-97e9-4a9a-87ad-fa2fb569cd9f'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (30505, '9e92a10f-02d1-4093-aa4b-1504134b9dcc'), (21290, 'e04fb004-041f-4f28-863d-a2968988f978'), (13611, '338896a5-047e-40e7-b52a-7336ea945618'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (31536, '37cc5143-44ba-4aa9-a04f-6f0006a85b20'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (310, 'ba32687f-1ed4-4efe-8c70-d47465e2733b'), (4922, '72fa3aeb-aff1-4f00-ad30-1abc8348f40a'), (24898, 'ca194eff-6753-4dc6-8cf8-0ec1a9a8ba00'), (1352, 'aa7129be-e5fa-4311-9cc6-39ddb946184a'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (14684, '5ae632de-fdb4-46f2-8f90-67cc8c042104'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (22897, '9384708d-94c1-4390-9c65-337486bab960'), (23924, '2f66f36a-dc72-4c6f-807d-265a585dea5b'), (3958, '239b64f8-77de-410a-9385-deb7c1b65e54'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (3961, 'efa02508-c542-4ad1-8b61-608f4b1415de'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (7550, '7d55e630-3187-47ea-a06e-4ac35f9a8265'), (8068, 'd79b15bd-7976-444e-ae07-e20e9a89aa6e'), (3465, 'df743924-ed1e-4203-8840-9b440ee9aea1'), (11148, 'b8aba780-8123-47d8-98f8-834d14204672'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (3472, '78eddeb6-be46-435a-b0bd-cc2263b4f9db'), (24467, '4ddbf49c-138b-42ef-8062-54f8e2303ce9'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (14330, '8ae393b3-c96e-4225-adde-4207e307b32a'), (12198, 'c617332f-ef8f-477a-803c-147aed04f986'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (2985, '6ad9cc82-7643-4e0a-8a93-d62ad4e98de4'), (30635, '1e9f8414-71ee-406e-bfe0-2d26dd64e9ac'), (9132, 'acbb6bfd-6d43-4a24-a2a8-92f7765350e9'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (2502, '8e358fab-31fa-4f90-b90d-a95834e06f52'), (31182, '55ded0b6-0c39-4e5b-b3e1-50e845ac89d9'), (25040, 'e13749b3-38a6-4cfa-adf3-d39d677c38e3'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (17878, '64394a15-79e8-4191-adb9-2f885f52e3b0'), (2519, 'ea219184-5b7d-4421-b252-b5bd2876d98e'), (16367, 'd67c6d67-fcc1-467f-8631-57e9b9929de2'), (17905, 'b7afa931-d869-43b2-aa19-1f7a79b23124'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (4085, '8a08cc47-8750-4040-b9cf-d499f02be598'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (18426, 'e2f413c4-5d19-4219-baa1-bc1e3f068ff1'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
```
You will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.

## Document Retrieval
When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other
with [`Ray`](https://docs.ray.io/en/master/).

This option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.
By default this flag is set to `pytorch`.

For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.
# Retrieval Augmented Generation with Huggingface Transformers and Ray


##### A guest blog post by <a href=""/amogkam"">Amog Kamsetty</a> from the Anyscale team

[Huggingface Transformers](https://huggingface.co/) recently added the [Retrieval Augmented Generation (RAG)](https://twitter.com/huggingface/status/1310597560906780680) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks. In this blog post, we introduce the integration of [Ray](https://docs.ray.io/en/master/), a library for building scalable applications, into the RAG contextual document retrieval mechanism. This speeds up retrieval calls by 2x and improves the scalability of RAG distributed [fine-tuning](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag).



### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")
And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
For the Ray implementation, the index is loaded in *separate* process(es). The training workers randomly select which
retriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed_retriever` arg to `ray`.
To configure the number of retrieval workers (the number of processes that load the index), you can set the `num_retrieval_workers` flag.
Also make sure to start the Ray cluster before running fine-tuning.

```bash
# Start a single-node Ray cluster.
ray start --head

python examples/research_projects/rag/finetune_rag.py \
    --data_dir $DATA_DIR \
    --output_dir $OUTPUT_DIR \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_type rag_sequence \
    --fp16 \
    --gpus 8
    --distributed_retriever ray \
    --num_retrieval_workers 4

# Stop the ray cluster once fine-tuning has finished.
ray stop
_A performance comparison of different retrieval implementations. For each document retrieval implementation, we run 500 training steps with a per-GPU batch size of 8, and measure the time it takes to retrieve the contextual documents for each batch on the rank 0 training worker. As the results show, using multiple retrieval processes improves performance, especially as we scale training to multiple GPUs._


### How do I use it?

[Huggingface](https://huggingface.co/) provides a [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) based [fine tuning script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag), and we extended it to add the Ray retrieval implementation as an option. 

To try it out, first install the necessary requirements


```bash
pip install ray
pip install transformers
pip install -r transformers/examples/research_projects/rag/requirements.txt
![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_

To overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Ray‚Äôs stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG.
Instead, a framework-agnostic and a more flexible implementation for ad-hoc concurrent programming is required. [Ray](https://ray.io/) fits the bill perfectly. Ray is a simple, yet powerful Python library for general-purpose distributed and parallel programming. Using Ray for distributed document retrieval, we achieved a **2x speedup per retrieval call compared to `torch.distributed`**, and overall better fine-tuning scalability.

### Ray for Document Retrieval
![alt_text](assets/12_ray_rag/torch_distributed_document_retrieval.png ""image_tooltip"")
_Document retrieval with the torch.distributed implementation_


The main drawback of the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) implementation for document retrieval was that it latched onto the same process group used for training and only the rank 0 training worker loaded the index into memory.

As a result, this implementation had some limitations:
## Overview

Retrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and
sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate
outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing
both retrieval and generation to adapt to downstream tasks.

It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela.

The abstract from the paper is the following:
```

## What‚Äôs next?

Using RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks.
### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")

_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._



Recently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library.
Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune‚Äôs integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.

And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!
Intro

Authors: @patrickvonplaten and @lhoestq

Aimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.

A RAG model encapsulates two core components: a question encoder and a generator.
During a forward pass, we encode the input with the question encoder and pass it
to the retriever to extract relevant context documents. The documents are then prepended to the input.
Such contextualized inputs are passed to the generator.

Read more about RAG  at https://arxiv.org/abs/2005.11401.

# Note

‚ö†Ô∏è This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability

# Finetuning
```

Using Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than
just the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate
processes than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.

# Evaluation
Our evaluation script enables two modes of evaluation (controlled by the `eval_mode` argument): `e2e` - end2end evaluation, returns EM (exact match) and F1 scores calculated for the downstream task and `retrieval` - which returns precision@k of the documents retrieved for provided inputs.
# Latest Update

‚ö†Ô∏è Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)

# Note

‚ö†Ô∏è This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability

# Testing

The following two bash scripts can be used to quickly test the implementation.
1. sh ./test_run/test_finetune.sh script
    - Tests the full end-to-end fine-tuning ability with a dummy knowlendge-base and dummy training dataset (check test_dir directory).
    - Users can replace the dummy dataset and knowledge-base with their own to do their own finetuning.
    - Please read the comments in the test_finetune.sh file.
2. sh ./test_run/test_rag_new_features.sh
    - Tests the newly added functions (set_context_encoder and set_context_encoder_tokenizer) related to modeling rag.
    - This is sufficient to check the model's ability to use the set functions correctly.
*Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve
state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely
manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind
task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge
remain open research problems. Pre-trained models with a differentiable access mechanism to explicit nonparametric
memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a
general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) ‚Äî models which combine pre-trained
parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a
As a result, this implementation had some limitations:

1. **Synchronization bottleneck**: The rank 0 worker had to receive the inputs from all workers, perform the index query, and then send the results back to the other workers. This limited performance with multiple training workers.
2. **PyTorch specific**: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well.

![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_
This information retrieval step allows [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models/) to make use of multiple sources of knowledge -- those that are baked into the model parameters and the information that is contained in the contextual passages, allowing it to outperform other state-of-the-art models in tasks like question answering. You can try it for yourself using this [demo provided by Huggingface](https://huggingface.co/rag/)!
This model was contributed by [ola13](https://huggingface.co/ola13).

## Usage tips

Retrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. 
RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq 
modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt 
to downstream tasks.

## RagConfig

[[autodoc]] RagConfig

## RagTokenizer

[[autodoc]] RagTokenizer

## Rag specific outputs

[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput

[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput

## RagRetriever

[[autodoc]] RagRetriever

<frameworkcontent>
<pt>

## RagModel

[[autodoc]] RagModel
    - forward

## RagSequenceForGeneration

[[autodoc]] RagSequenceForGeneration
    - forward
    - generate

## RagTokenForGeneration

[[autodoc]] RagTokenForGeneration
    - forward
    - generate
parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a
pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a
pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages
across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our
models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks,
outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation
tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art
parametric-only seq2seq baseline.*

Here is the question: When fine-tuning RAG models, how does incorporating Ray processes affect the efficiency of contextual document retrieval?
Here is answer 1:It reduces retrieval latency by batching index lookups across workers
Here is answer 2: It eliminates the need for external knowledge sources
Here is answer3: It increases memory consumption by duplicating indices
Here is answer 4: It simplifies single-GPU fine-tuning pipelines","The correct answer choice would be:

Answer #1: It reduces latency reduction enhances efficiency"
"What makes epsilon-greedy policies suitable for managing exploration and exploitation in value-based methods, and how might this concept inform RAG training strategies?","[(14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30292, '523f2e0b-8c7c-4d43-81b4-0bf745ad3379'), (25687, 'e0602f1c-f1fd-4fa9-a1d2-0fc616b3fb9a'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (28785, '6a4793d6-328d-4198-8f28-cb61dd76346c'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (23162, 'adfa1aae-7eea-4d74-b807-13110e10c22f'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (17547, '91c00572-19ed-4c45-a397-6e45a63c5956'), (17548, 'aefb1f79-29ee-4e4c-9c9e-fee9108f9e90'), (17549, '6cea314d-9588-4cbe-9e0c-dea7dc8ff9f0'), (17550, 'f7ea19ba-2641-4b2f-89db-c031f1abbd53'), (17552, 'e6dc2f24-99a1-4fd8-a073-c0a439c5d184'), (17553, 'a45cc6c7-9ac2-4e91-97bb-7b24a815d88f'), (17555, '96bccc70-063c-4751-acd8-ac03d5f93056'), (14486, '1d0b077e-bd05-4bb6-9239-cdebc73e9813'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (25777, '222d6fa5-c0e6-4075-bb20-83f0dfe99f5c'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25280, '8c7846b3-bc36-403f-b031-66f0a9105705'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (20701, '04553fa1-c85e-4533-9f15-8a7240e55fc3'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11040, 'd3ac7329-8173-4897-b351-0738ff068eaf'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (22819, 'a415afd3-d643-4e9a-bb7a-478941d50298'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (23339, 'acc2c624-ff31-433f-be5f-f1000a8f48dc'), (20805, '751dabdb-968e-45e0-8286-ed51ace44972'), (20806, 'b65455ed-1e56-4734-8384-734771a4e1ef'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19299, '58e3889c-4edb-4806-9064-e951799e80cb'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (19313, '43028562-976a-43d3-8062-60062555356c'), (19314, '77062fd7-8f64-4bc1-9a5c-db9ac13a4e5d'), (19315, '91b8e9dc-2998-430a-9fcc-55fc69552b76'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19317, 'cab6254e-890a-47c4-a68b-1b64258cc023'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23952, 'ec7797a6-1617-49dc-9737-a162e926580a'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (23965, '50a1c8ae-9ee4-4afe-a86f-7583efa72965'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (961, 'f8df8306-bd61-414c-92fb-2f5f3e706930'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

## Define the epsilon-greedy policy ü§ñ

Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.

The idea with epsilon-greedy:

- With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).

- With *probability …õ*: we do **exploration** (trying a random action).

As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg"" alt=""Q-Learning"" width=""100%""/>
The epsilon-greedy strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that, with an initial value of …õ = 1.0:

- *With probability 1 ‚Äî …õ*¬†: we do¬†**exploitation**¬†(aka our agent selects the action with the highest state-action pair value).
- With probability …õ:¬†**we do exploration**¬†(trying random action).

At the beginning of the training,¬†**the probability of doing exploration will be huge since …õ is very high, so most of the time, we'll explore.**¬†But as the training goes on, and consequently our¬†**Q-table gets better and better in its estimations, we progressively reduce the epsilon value**¬†since we will need less and less exploration and more exploitation.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg"" alt=""Q-learning""/>


### Step 3: Perform action At, get reward Rt+1 and next state St+1 [[step3]]
Consequently, whatever method you use to solve your problem,¬†**you will have a policy**. In the case of value-based methods, you don't train the policy: your policy¬†**is just a simple pre-specified function**¬†(for instance, the Greedy Policy) that¬†uses the values given by the value-function to select its actions.

So the difference is:

- In policy-based training,¬†**the optimal policy (denoted œÄ\*) is found by training the policy directly.**
- In value-based training,¬†**finding an optimal value function (denoted Q\* or V\*, we'll study the difference below) leads to having an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link between value and policy""/>

In fact, most of the time, in value-based methods, you'll use¬†**an Epsilon-Greedy Policy**¬†that handles the exploration/exploitation trade-off; we'll talk about this when we talk about Q-Learning in the second part of this unit.
</details>

### Q4: Can you explain what is Epsilon-Greedy Strategy?

<details>
<summary>Solution</summary>
Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that we define epsilon …õ = 1.0:

- With *probability 1 ‚Äî …õ* : we do exploitation (aka our agent selects the action with the highest state-action pair value).
- With *probability …õ* : we do exploration (trying random action).

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg"" alt=""Epsilon Greedy""/>


</details>

### Q5: How do we update the Q value of a state, action pair?
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-update-ex.jpg"" alt=""Q Update exercise""/>

<details>
<summary>Solution</summary>
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-update-solution.jpg"" alt=""Q Update exercise""/>
**Step 1: We initialize the Q-Table**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-3.jpg"" alt=""Q-learning""/>
</figure>

We need to initialize the Q-Table for each state-action pair.¬†**Most of the time, we initialize with values of 0.**

**Step 2: Choose action using Epsilon Greedy Strategy**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-4.jpg"" alt=""Q-learning""/>
</figure>

Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that we define epsilon …õ = 1.0:

- *With probability 1 ‚Äî …õ*¬†: we do¬†**exploitation**¬†(aka our agent selects the action with the highest state-action pair value).
- With probability …õ:¬†**we do exploration**¬†(trying random action).
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg"" alt=""Q-learning""/>

### Step 1: We initialize the Q-table [[step1]]

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-3.jpg"" alt=""Q-learning""/>


We need to initialize the Q-table for each state-action pair.¬†**Most of the time, we initialize with values of 0.**

### Step 2: Choose an action using the epsilon-greedy strategy [[step2]]

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg"" alt=""Q-learning""/>


The epsilon-greedy strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that, with an initial value of …õ = 1.0:
### Q2: What is a Q-table?

<Question
	choices={[
		{
			text: ""An algorithm we use in Q-Learning"",
			explain: """",
		},
		{
			text: ""Q-table is the internal memory of our agent"",
			explain: """",
      correct: true
		},
    {
			text: ""In Q-table each cell corresponds a state value"",
			explain: ""Each cell corresponds to a state-action value pair value. Not a state value."",
		}
	]}
/>

### Q3: Why if we have an optimal Q-function Q* we have an optimal policy?

<details>
<summary>Solution</summary>

Because if we have an optimal Q-function, we have an optimal policy since we know for each state what is the best action to take.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""link value policy""/>

</details>

### Q4: Can you explain what is Epsilon-Greedy Strategy?

<details>
<summary>Solution</summary>
Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.
So the difference is:

- In policy-based,¬†**the optimal policy is found by training the policy directly.**
- In value-based,¬†**finding an optimal value function leads to having an optimal policy.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/link-value-policy.jpg"" alt=""Link between value and policy""/>
</figure>

In fact, most of the time, in value-based methods, you'll use¬†**an Epsilon-Greedy Policy**¬†that handles the exploration/exploitation trade-off; we'll talk about it when we talk about Q-Learning in the second part of this unit.


So, we have two types of value-based functions:

### **The State-Value function**

We write the state value function under a policy œÄ like this:

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/state-value-function-1.jpg"" alt=""State value function""/>
</figure>
### Among the value-based methods, we can find two main strategies

- **The state-value function.** For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.
- **The action-value function.** In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.

### Epsilon-greedy strategy:

- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.
- Chooses the action with the highest expected reward with a probability of 1-epsilon.
- Chooses a random action with a probability of epsilon.
- Epsilon is typically decreased over time to shift focus towards exploitation.

### Greedy strategy:
### Greedy strategy:

- Involves always choosing the action that is expected to lead to the highest reward, based on the current knowledge of the environment. (Only exploitation)
- Always chooses the action with the highest expected reward.
- Does not include any exploration.
- Can be disadvantageous in environments with uncertainty or unknown optimal actions.

### Off-policy vs on-policy algorithms

- **Off-policy algorithms:** A different policy is used at training time and inference time
- **On-policy algorithms:** The same policy is used during training and inference

### Monte Carlo and Temporal Difference learning strategies

- **Monte Carlo (MC):** Learning at the end of the episode. With Monte Carlo, we wait until the episode ends and then we update the value function (or policy function) from a complete episode.
```

```python
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

## Define the greedy policy ü§ñ

Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.

- Epsilon-greedy policy (acting policy)
- Greedy-policy (updating policy)

The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg"" alt=""Q-Learning"" width=""100%""/>


```python
def greedy_policy(Qtable, state):
  # Exploitation: take the action with the highest state, action value
  action =

  return action
Under a deterministic policy, the policy will either always move right when in a red state or always move left. **Either case will cause our agent to get stuck and never suck the dust**.

Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic policy** (""greedy epsilon strategy""). Consequently, our agent can **spend a lot of time before finding the dust**.

On the other hand, an optimal stochastic policy **will randomly move left or right in red (colored) states**. Consequently, **it will not be stuck and will reach the goal state with a high probability**.

<figure class=""image table text-center m-0 w-full"">
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg"" alt=""Hamster 1""/>
</figure>

### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces
Under a deterministic policy, the policy will either move right when in a red state or move left. **Either case will cause our agent to get stuck and never suck the dust**.

Under a value-based RL algorithm, we learn a quasi-deterministic policy (""greedy epsilon strategy""). Consequently, our agent can spend a lot of time before finding the dust.

On the other hand, an optimal stochastic policy will randomly move left or right in grey states. Consequently, **it will not be stuck and will reach the goal state with a high probability**.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/hamster3.jpg"" alt=""Hamster 1""/>
</figure>

3. Policy gradients are **more effective in high-dimensional action spaces and continuous actions spaces**

Indeed, the problem with Deep Q-learning is that their **predictions assign a score (maximum expected future reward) for each possible action**, at each time step, given the current state.
```

#### Solution

```python
def epsilon_greedy_policy(Qtable, state, epsilon):
    # Randomly generate a number between 0 and 1
    random_num = random.uniform(0, 1)
    # if random_num > greater than epsilon --> exploitation
    if random_num > epsilon:
        # Take the action with the highest value given a state
        # np.argmax can be useful here
        action = greedy_policy(Qtable, state)
    # else --> exploration
    else:
        action = env.action_space.sample()

    return action
Training timestep 2:

**Step 2: Choose action using Epsilon Greedy Strategy**

**I take a random action again, since epsilon is big 0.99**¬†(since we decay it a little bit because as the training progress, we want less and less exploration).

I took action down.¬†**Not a good action since it leads me to the poison.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/q-ex-6.jpg"" alt=""Maze-Example""/>
</figure>

**Step 3: Perform action At, gets \\(R_{t+1}\\) and St+1**

Because I go to the poison state,¬†**I get \\(R_{t+1} = -10\\), and I die.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/q-ex-7.jpg"" alt=""Maze-Example""/>
</figure>

**Step 4: Update \\(Q(S_t, A_t)\\)**
### Tasks

- **Episodic**: Has a starting point and an ending point.
- **Continuous**: Has a starting point but no ending point.

### Exploration v/s Exploitation Trade-Off

- **Exploration**: It's all about exploring the environment by trying random actions and receiving feedback/returns/rewards from the environment.
- **Exploitation**: It's about exploiting what we know about the environment to gain maximum rewards.
- **Exploration-Exploitation Trade-Off**: It balances how much we want to **explore** the environment and how much we want to **exploit** what we know about the environment.

### Policy

- **Policy**: It is called the agent's brain. It tells us what action to take, given the state.
- **Optimal Policy**: Policy that **maximizes** the **expected return** when an agent acts according to it. It is learned through *training*.

### Policy-based Methods:
If we take an example:

  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/MC-2.jpg"" alt=""Monte Carlo""/>


- We always start the episode¬†**at the same starting point.**
- **The agent takes actions using the policy**. For instance, using an Epsilon Greedy Strategy, a policy that alternates between exploration (random actions) and exploitation.
- We get¬†**the reward and the next state.**
- We terminate the episode if the cat eats the mouse or if the mouse moves > 10 steps.

- At the end of the episode,¬†**we have a list of State, Actions, Rewards, and Next States tuples**
For instance [[State tile 3 bottom, Go Left, +1, State tile 2 bottom], [State tile 2 bottom, Go Left, +0, State tile 1 bottom]...]

- **The agent will sum the total rewards \\(G_t\\)**¬†(to see how well it did).
- It will then¬†**update \\(V(s_t)\\) based on the formula**
```

## Create the training loop method

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg"" alt=""Q-Learning"" width=""100%""/>

The training loop goes like this:

```
For episode in the total of training episodes:

Reduce epsilon (since we need less and less exploration)
Reset the environment

  For step in max timesteps:
    Choose the action At using epsilon greedy policy
    Take the action (a) and observe the outcome state(s') and reward (r)
    Update the Q-value Q(s,a) using Bellman equation Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]
    If done, finish the episode
    Our next state is the new state
And consequently,¬†**we don't define by hand the behavior of our policy; it's the training that will define it.**

- *Value-based methods:*¬†**Indirectly, by training a value function**¬†that outputs the value of a state or a state-action pair. Given this value function, our policy¬†**will take an action.**

Since the policy is not trained/learned,¬†**we need to specify its behavior.**¬†For instance, if we want a policy that, given the value function, will take actions that always lead to the biggest reward,¬†**we'll create a Greedy Policy.**

<figure>
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/two-approaches-3.jpg"" alt=""Two RL approaches""/>
  <figcaption>Given a state, our action-value function (that we train) outputs the value of each action at that state. Then, our pre-defined Greedy Policy selects the action that will yield the highest value given a state or a state action pair.</figcaption>
</figure>
- *On-policy:*¬†using the¬†**same policy for acting and updating.**

For instance, with Sarsa, another value-based algorithm,¬†**the Epsilon-Greedy Policy selects the next_state-action pair, not a greedy policy.**


<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/off-on-3.jpg"" alt=""Off-on policy""/>
    <figcaption>Sarsa</figcaption>
</figure>

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/off-on-4.jpg"" alt=""Off-on policy""/>
</figure>

## **A Q-Learning example**

To better understand Q-Learning, let's take a simple example:

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Maze-Example-2.jpg"" alt=""Maze-Example""/>
</figure>

Here is the question: What makes epsilon-greedy policies suitable for managing exploration and exploitation in value-based methods, and how might this concept inform RAG training strategies?
Here is answer 1:They maximize reward predictability; useful for deterministic tasks
Here is answer 2: They ensure probabilistic fairness; critical for document indexing
Here is answer3: They balance exploration and exploitation trade-offs; applicable to retrieval calls
Here is answer 4: They prioritize high-value actions; improve training throughput",Answer : They balance explore-exploit trade-offs ; applicableto retrieval calls
"What role does metadata play in Hugging Face's license tagging system, and how does this enhance collaborative AI development?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (22547, '51e87e51-6544-440b-9057-4201251fc27d'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (2597, '6458f359-e453-452e-93f0-0a72efc03c98'), (23078, '773ea43e-498d-4d36-8809-1e19c760b774'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (6184, 'ce23f107-580c-4726-839d-7d174da13172'), (28718, '20436458-7cd8-4a96-8331-99513560667b'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (24657, '37943db2-8a2c-4369-80c8-e75eee95f201'), (1620, '5dd0aa93-2fc4-430b-b511-62f9aee29d8e'), (14424, '557cf24b-d009-4bf3-ad9a-bc944e44be56'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (22124, 'e862ae05-e350-458c-893b-9b4a3d0b9f3f'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (21623, '32bea9e3-44f8-4464-a8dd-d2dcbc3a2102'), (24184, 'a7a6758e-20d3-4752-8f8a-952376a05592'), (21624, 'cf165b8c-348b-4ccc-acd1-944ae9c89877'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (19582, '763ea5e2-b3ae-4de8-be1b-62fe897fd4a3'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25728, 'e9a67dae-3e05-4640-afbb-816d75639376'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (27775, '56108da4-6463-4bb1-8393-42e02e4b2cc9'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (12935, '78e0073f-75a8-4831-8522-3bbca5d23602'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (29354, 'eb6365fe-b9fc-400e-866f-6a21ed2a810f'), (22709, '0d067896-b515-4643-b7de-b83eaba05997'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (25805, 'fcaebe7b-c2ec-4995-829f-39de8f874270'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (3329, '64a72f1a-f303-41c1-97ea-b32aeeac5fbc'), (16644, '48292295-18de-4870-999a-cf16ec29e3c1'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (1297, '0110063d-229c-4fc0-a7c3-f875aa009eb2'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (11054, '397721c5-71da-4c64-ba5c-e20f0d7b0544'), (31023, '238bb199-dbdb-4206-b206-07eeba69ea3c'), (2864, 'bd4a8a28-05e5-486b-8aff-f74ae7a91a34'), (7989, '7bf24f34-1a48-44c6-b805-592b4c1b9e1d'), (31031, '306dd725-df38-43a2-a289-44917df0a6f1'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (11616, '48ca75eb-8bd5-4a2e-b04e-27b04be80149'), (3937, '7450b67c-b9da-4dd2-a9a5-2772aa7ae9bf'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (15738, '3bd17398-a99e-4c37-8fc7-42bb853cb70e'), (8578, 'b6835785-bd61-41a6-94d0-a59988d7384f'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (8590, 'd83fb1f9-3d9c-409e-a570-5a3a708f8463'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8596, 'd07e02e0-f50e-4524-90c9-9b3cae858d33'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (13728, '91c1a2c4-7e1d-4ddf-8e7c-768b77a28bbc'), (5539, '0c889c50-6523-4611-8f7b-428625f7d0f7'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (31670, '86835c8a-fa41-4c7a-90b5-e28107d5fcc9'), (13239, 'f13538dd-edce-4970-a1cc-83ab6fd1a5d6'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (2512, '922c050f-9165-4d33-9529-8a72a7e9ed40'), (2513, '46aef253-eecf-48f6-8c8a-26e8d72a48f7'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (6105, '9efc65f9-7879-4b2c-8da5-721897d26d7d'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (13817, 'f8a28592-da18-4073-9696-21473e18cb2c'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: #### Next steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
#### Next Steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
- monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
- robustly document our most-downloaded models with model cards that detail social impacts, biases, and intended and out-of-scope use cases,
- create audience-guiding tags, such as the ‚ÄúNot For All Audiences‚Äù tag that can be added to the repository‚Äôs card metadata to avoid un-requested violent and sexual content,
- promote use of [Open Responsible AI Licenses (RAIL)](https://huggingface.co/blog/open_rail) for [models](https://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license), such as with LLMs ([BLOOM](https://huggingface.co/spaces/bigscience/license), [BigCode](https://huggingface.co/spaces/bigcode/license)),
- conduct research that [analyzes](https://arxiv.org/abs/2302.04844) which models and datasets have the highest potential for, or track record of, misuse and malicious use.
We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#bias-risks-and-limitations), recently applied to the new Hugging Face multimodal model [IDEFICS](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct). We've worked on how to operationalize *transparency* responsibly, including [updating our Content Policy](https://huggingface.co/blog/content-guidelines-update) (spearheaded by [Giada](https://huggingface.co/giadap)). We've advanced our support of language *diversity* on the Hub by [using machine learning to improve metadata](https://huggingface.co/blog/huggy-lingo) (spearheaded by [Daniel](https://huggingface.co/davanstrien)), and our support of *rigour* in AI by [adding more descriptive statistics to datasets](https://twitter.com/polinaeterna/status/1707447966355563000) (spearheaded by [Polina](https://huggingface.co/polinaeterna)) to foster a better understanding of what AI
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
### Using Librarian-Bot to Update Metadata

To ensure this valuable language metadata is incorporated back into the Hub, we turn to Librarian-Bot! Librarian-Bot takes the language predictions generated by Meta's [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model and opens pull requests to add this information to the metadata of each respective dataset. 

This system not only updates the datasets with language information, but also does it swiftly and efficiently, without requiring manual work from humans. If the owner of a repo decided to approve and merge the pull request, then the language metadata becomes available for all users, significantly enhancing the usability of the Hugging Face Hub. You can keep track of what the librarian-bot is doing [here](https://huggingface.co/librarian-bot/activity/community)! 

#### Next Steps
### Using Librarian-Bot to Update Metadata

To ensure this valuable language metadata is incorporated back into the Hub, we turn to Librarian-Bot! Librarian-Bot takes the language predictions generated by Meta's [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model and opens pull requests to add this information to the metadata of each respective dataset. 

This automated system not only updates the datasets with language information, but also does it swiftly and efficiently, without requiring manual work from humans. Once these pull requests are approved and merged, the language metadata becomes available for all users, significantly enhancing the usability of the Hugging Face Hub. You can keep track of what the librarian-bot is doing [here](https://huggingface.co/librarian-bot/activity/community)! 


#### Next steps
Hugging Face is where it is today thanks to its community of developers, so we‚Äôve seen firsthand what open development brings to the table
to support more robust innovation for more diverse and context-specific use cases;
where developers can easily share innovative new techniques, mix and match ML components to suit their own needs,
and reliably work with full visibility into their entire stack.
We‚Äôre also acutely aware of the necessary role of transparency in supporting more accountability and inclusivity of the technology ‚Äì
which we‚Äôve worked on fostering through better documentation and accessibility of ML artifacts, education efforts,
and hosting large-scale multidisciplinary collaborations, among others.
Thus, as the EU AI Act moves toward its final phase, we believe accounting for the specific needs and strengths of open and open-source development of ML systems will be instrumental in supporting its long-term goals.
--
title: ""Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub""
thumbnail: blog/assets/156_huggylingo/Huggy_Lingo.png
authors:
- user: davanstrien
---

## Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub



**tl;dr**: We're using machine learning to detect the language of Hub datasets with no language metadata, and [librarian-bots](https://huggingface.co/librarian-bots) to make pull requests to add this metadata. 

The Hugging Face Hub has become the repository where the community shares machine learning models, datasets, and applications. As the number of datasets grows, metadata becomes increasingly important as a tool for finding the right resource for your use case.

In this blog post, I'm excited to share some early experiments which seek to use machine learning to improve the metadata for datasets hosted on the Hugging Face Hub.

### Language metadata for datasets on the Hub
--
title: ""Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub""
thumbnail: blog/assets/156_huggylingo/Huggy_Lingo.png
authors:
- user: davanstrien
---

## Huggy Lingo: Using Machine Learning to Improve Language Metadata on the Hugging Face Hub



**tl;dr**: We're using machine learning to detect the language of Hub datasets with no language metadata, and [librarian-bots](https://huggingface.co/librarian-bots) to make pull requests to add this metadata. 

The Hugging Face Hub has become the repository where the community shares machine learning models, datasets, and applications. As the number of datasets grows, metadata becomes increasingly important as a tool for finding the right resource for your use case.

In this blog post, I'm excited to share some early experiments which seek to use machine learning to improve the metadata for datasets hosted on the Hugging Face Hub.

### Language Metadata for Datasets on the Hub
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One ‚Äì or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) ‚Äì specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable information, including a `license` tag.
reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable information, including a `license` tag.
#### Why is Language Metadata Important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. 

Many people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.
On the Hugging Face Hub, this information is stored in each dataset repository's *README.md* file. There are two main steps you should take before creating this file:

1. Use the [`datasets-tagging` application](https://huggingface.co/datasets/tagging/) to create metadata tags in YAML format. These tags are used for a variety of search features on the Hugging Face Hub and ensure your dataset can be easily found by members of the community. Since we have created a custom dataset here, you'll need to clone the `datasets-tagging` repository and run the application locally. Here's what the interface looks like:

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter5/datasets-tagger.png"" alt=""The `datasets-tagging` interface."" width=""80%""/>
</div>
There have been significant advances in new Transformer and Diffuser machine learning models that process and generate text, audio, and images. However, most of these popular generative AI models are not publicly available, widening the gap of machine learning capabilities between the largest tech companies and everyone else. To counter this trend, AWS and Hugging Face are partnering to contribute next-generation models to the global AI community and democratize machine learning. Through the strategic partnership, Hugging Face will leverage AWS as a preferred cloud provider so developers in Hugging Face‚Äôs community can access AWS‚Äôs state-of-the-art tools (e.g., [Amazon SageMaker](https://aws.amazon.com/sagemaker), [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)) to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their
--
title: ""Introducing Hugging Face for Education ü§ó""
thumbnail: /blog/assets/61_education/thumbnail.png
authors:
- user: Violette
---

# Introducing Hugging Face for Education ü§ó


Given that machine learning will make up the overwhelming majority of software development and that non-technical people will be exposed to AI systems more and more, one of the main challenges of AI is adapting and enhancing employee skills. It is also becoming necessary to support teaching staff in proactively taking AI's ethical and critical issues into account. 

As an open-source company democratizing machine learning, [Hugging Face](https://huggingface.co/) believes it is essential to educate people from all backgrounds worldwide.
If you want to accelerate your Document AI efforts, Hugging Face can help. Through our [Enterprise Acceleration Program](https://huggingface.co/support) we partner with enterprises to provide guidance on AI use cases. For Document AI, this could involve helping build a pre-train model, improving accuracy on a fine-tuning task, or providing overall guidance on tackling your first Document AI use case.

We can also provide bundles of compute credits to use our training (AutoTrain) or inference (Spaces or Inference Endpoints) products at scale.

### Resources

Notebooks and tutorials for many Document AI models can be found at: 
- Niels' [Transformers-Tutorials](https://github.com/NielsRogge/Transformers-Tutorials)
- Philipp's [Document AI with Hugging Face Transformers](https://github.com/philschmid/document-ai-transformers)

Here is the question: What role does metadata play in Hugging Face's license tagging system, and how does this enhance collaborative AI development?
Here is answer 1:Facilitates reproducibility by linking licenses to datasets
Here is answer 2: Ensures strict compliance through automated legal checks
Here is answer3: Promotes transparency by standardizing license information
Here is answer 4: Prevents proprietary use of public models",Answer 3
What is the computational advantage of using value-based functions in reinforcement learning compared to direct policy-based methods?,"[(24580, '9e0b974c-4de1-4c4e-9ad2-c81dd70f9d85'), (24581, '22244e17-6683-4b12-bf73-b4e4b5377b96'), (1037, 'ee236590-8d9d-4d1a-b76d-0cee9e154cdf'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (20017, '7f1adcff-7661-482a-83a6-f88d6491fb7b'), (12850, 'c6e54b94-1bc1-4c66-8959-7c3a3451782c'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30298, '4e59aebc-b2c4-4323-9471-a87bce987887'), (30299, '2fc3d546-824f-49c4-9d8d-066e5c7cda4e'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (11371, '7a730a29-98ba-4ed3-8803-2deedc8c54a0'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (14487, '4f24c302-152f-4c11-b198-4be00041e6a0'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21210, 'a7d914db-0a82-4bef-889e-b11938bed886'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23294, '1936ebac-0df8-44c6-a638-cb0f634059de'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11047, '939d2f75-31f8-4c36-92da-3b8dadf6c83a'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19299, '58e3889c-4edb-4806-9064-e951799e80cb'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19317, 'cab6254e-890a-47c4-a68b-1b64258cc023'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23952, 'ec7797a6-1617-49dc-9737-a162e926580a'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23956, '9dec7d5b-756e-4a6c-ac15-1b544069a789'), (23957, '666ef815-1fab-4378-93d7-9c5e8c83e825'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (923, '3f1f99cb-3e95-4732-9582-c75dfc21e2bb'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23964, '80a4ae6e-a7f4-4201-845f-0e0e4cfcdfed'), (924, 'a954e80d-bcae-4191-ad77-dc130d6f2b5a'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (23970, '02e2356f-f06a-4242-9e9a-01da5226883c'), (23971, '02fa64da-7776-42d4-a529-913fbf0f4c8b'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13248, '60d563de-39e5-4163-89e6-86fa1da80be6'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (964, '65fb34e4-bfe0-4990-b85f-bf4e0f542e42'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
### Among the value-based methods, we can find two main strategies

- **The state-value function.** For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.
- **The action-value function.** In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.

### Epsilon-greedy strategy:

- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.
- Chooses the action with the highest expected reward with a probability of 1-epsilon.
- Chooses a random action with a probability of epsilon.
- Epsilon is typically decreased over time to shift focus towards exploitation.

### Greedy strategy:
The advantages and disadvantages of policy-gradient methods

At this point, you might ask, ""but Deep Q-Learning is excellent! Why use policy-gradient methods?"". To answer this question, let's study the **advantages and disadvantages of policy-gradient methods**.

## Advantages

There are multiple advantages over value-based methods. Let's see some of them:

### The simplicity of integration

We can estimate the policy directly without storing additional data (action values).

### Policy-gradient methods can learn a stochastic policy

Policy-gradient methods can¬†**learn a stochastic policy while value functions can't**.

This has two consequences:

1. We **don't need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores¬†**the state space without always taking the same trajectory.**
Because, in value-based,¬†**œÄ exists only because of the action value estimates, since policy is just a function**¬†(for instance, greedy-policy) that will select the action with the highest value given a state.

But, with policy-based methods, we want to optimize the policy directly¬†**without having an intermediate step of learning a value function.**

So today,¬†**we'll study our first Policy-Based method**: Reinforce. And we'll implement it from scratch using PyTorch. Before testing its robustness using CartPole-v1, PixelCopter, and Pong.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/envs.gif"" alt=""Environments""/>
</figure>

Let's get started,
What are the policy-based methods?

The main goal of Reinforcement learning is to **find the optimal policy \\(\pi^{*}\\) that will maximize the expected cumulative reward**.
Because Reinforcement Learning is based on the *reward hypothesis*: **all goals can be described as the maximization of the expected cumulative reward.**

For instance, in a soccer game (where you're going to train the agents in two units), the goal is to win the game. We can describe this goal in reinforcement learning as
**maximizing the number of goals scored** (when the ball crosses the goal line) into your opponent's soccer goals. And **minimizing the number of goals in your soccer goals**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg"" alt=""Soccer"" />

## Value-based, Policy-based, and Actor-critic methods

In the first unit, we saw two methods to find (or, most of the time, approximate) this optimal policy \\(\pi^{*}\\).
There are **two** ways to find your optimal policy:

- By **training your policy directly**: policy-based methods.
- By **training a value function** that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.

- Finally, we spoke about Deep RL because **we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name ""deep.""**

# Let's train our first Deep Reinforcement Learning agent and upload it to the Hub üöÄ

## Get a certificate üéì

To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push your trained model to the Hub and **get a result of >= 200**.
The benefits of using policy-gradient methods over value-based methods include: 
    - simplicity of integration: no need to store action values;
    - ability to learn a stochastic policy: the agent explores the state space without always taking the same trajectory, and avoids the problem of perceptual aliasing;
    - effectiveness in high-dimensional and continuous action spaces; and
    - improved convergence properties.

- **Policy Gradient:** A subset of policy-based methods where the objective is to maximize the performance of a parameterized policy using gradient ascent. The goal of a policy-gradient is to control the probability distribution of actions by tuning the policy such that good actions (that maximize the return) are sampled more frequently in the future. 

- **Monte Carlo Reinforce:** A policy-gradient algorithm that uses an estimated return from an entire episode to update the policy parameter.
---


‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit6/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*

[In Unit 5](https://huggingface.co/blog/deep-rl-pg), we learned about our first Policy-Based algorithm called **Reinforce**. 
In Policy-Based methods, **we aim to optimize the policy directly without using a value function**. More precisely, Reinforce is part of a subclass of *Policy-Based Methods* called *Policy-Gradient methods*. This subclass optimizes the policy directly by **estimating the weights of the optimal policy using Gradient Ascent**.
- There are two ways to find your optimal policy:
  1. By training your policy directly: **policy-based methods.**
  2. By training a value function that tells us the expected return the agent will get at each state and use this function to define our policy: **value-based methods.**

- Finally, we speak about Deep RL because we introduces **deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based)** hence the name ‚Äúdeep.‚Äù

---
Now that you've studied the bases of Reinforcement Learning, you‚Äôre ready to train your first lander agent to¬†**land correctly on the Moon üåï and share it with the community through the Hub** üî•

<figure class=""image table text-center m-0 w-full"">
    <video
        alt=""LunarLander""
        style=""max-width: 70%; margin: auto;""
        autoplay loop autobuffer muted playsinline
    >
      <source src=""assets/63_deep_rl_intro/lunarlander.mp4"" type=""video/mp4"">
  </video>
</figure>
With policy-based methods, we want to optimize the policy directly¬†**without having an intermediate step of learning a value function.**

So today,¬†**we'll learn about policy-based methods and study a subset of these methods called policy gradient**. Then we'll implement our first policy gradient algorithm called Monte Carlo **Reinforce** from scratch using PyTorch.
Then, we'll test its robustness using the CartPole-v1 and PixelCopter environments.

You'll then be able to iterate and improve this implementation for more advanced environments.

<figure class=""image table text-center m-0 w-full"">
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif"" alt=""Environments""/>
</figure>

Let's get started!
Instead, with policy-gradient methods, we output a¬†**probability distribution over actions.**

### Policy-gradient methods have better convergence properties

In value-based methods, we use an aggressive operator to **change the value function: we take the maximum over Q-estimates**.
Consequently, the action probabilities may change dramatically for an arbitrarily small change in the estimated action values if that change results in a different action having the maximal value.

For instance, if during the training, the best action was left (with a Q-value of 0.22) and the training step after it's right (since the right Q-value becomes 0.23), we dramatically changed the policy since now the policy will take most of the time right instead of left.

On the other hand, in policy-gradient methods, stochastic policy action preferences (probability of taking action) **change smoothly over time**.

## Disadvantages

Naturally, policy-gradient methods also have some disadvantages:
## What are Policy-Gradient Methods?
Policy-Gradient is a subclass of Policy-Based Methods, a category of algorithms that¬†**aims to optimize the policy directly without using a value function using different techniques.** The difference with Policy-Based Methods is that Policy-Gradient methods are a series of algorithms that aim to optimize the policy directly¬†**by estimating the weights of the optimal policy using Gradient Ascent.**

### An Overview of Policy Gradients
Why do we optimize the policy directly by estimating the weights of an optimal policy using Gradient Ascent in Policy Gradients Methods?

Remember that reinforcement learning aims¬†**to find an optimal behavior strategy (policy) to maximize its expected cumulative reward.**

We also need to remember that a policy is a function that **given a state, outputs, a distribution over actions** (in our case using a stochastic policy).
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png"" alt=""thumbnail""/>


In this notebook, you'll code your first Deep Reinforcement Learning algorithm from scratch: Reinforce (also called Monte Carlo Policy Gradient).

Reinforce is a *Policy-based method*: a Deep Reinforcement Learning algorithm that tries **to optimize the policy directly without using an action-value function**.

More precisely, Reinforce is a *Policy-gradient method*, a subclass of *Policy-based methods* that aims **to optimize the policy directly by estimating the weights of the optimal policy using gradient ascent**.

To test its robustness, we're going to train it in 2 different simple environments:
- Cartpole-v1
- PixelcopterEnv

‚¨áÔ∏è Here is an example of what **you will achieve at the end of this notebook.** ‚¨áÔ∏è
Introduction [[introduction]]

  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png"" alt=""thumbnail""/>

In the last unit, we learned about Deep Q-Learning. In this value-based deep reinforcement learning algorithm, we¬†**used a deep neural network to approximate the different Q-values for each possible action at a state.**

Since the beginning of the course, we have only studied value-based methods,¬†**where we estimate a value function as an intermediate step towards finding an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy"" />

In value-based methods, the policy ** \(œÄ\) only exists because of the action value estimates since the policy is just a function** (for instance, greedy-policy) that will select the action with the highest value given a state.
</details>

### Q5: Which of the following statements are true about the Actor-Critic Method?

<Question
	choices={[
   		 {
			text: ""The Critic does not learn any function during the training process"",
			explain: ""Both the Actor and the Critic function parameters are updated during training time"",
      			correct: false,
		},
		{
			text: ""The Actor learns a policy function, while the Critic learns a value function"",
			explain: """",
      			correct: true,
		},
    		{
			text: ""It adds resistance to stochasticity and reduces high variance"",
			explain: """",
      			correct: true,
		},	    
	]}
/>



### Q6: What is `Advantage` in the A2C method?

<details>
<summary>Solution</summary>

Instead of using directly the Action-Value function of the Critic as it is, we could use an `Advantage` function. The idea behind an `Advantage` function is that we calculate the relative advantage of an action compared to the others possible at a state, averaging them.
Quiz

The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.


### Q1: What are the advantages of policy-gradient over value-based methods? (Check all that apply)

<Question
	choices={[
		{
			text: ""Policy-gradient methods can learn a stochastic policy"",
			explain: """",
      			correct: true,
		},
		{
			text: ""Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces"",
			explain: """",
      			correct: true,
		},
    {
			text: ""Policy-gradient converges most of the time on a global maximum."",
			explain: ""No, frequently, policy-gradient converges on a local maximum instead of a global optimum."",
		},
	]}
/>

### Q2: What is the Policy Gradient Theorem?

<details>
<summary>Solution</summary>
## Adding Advantage in Actor-Critic (A2C)
We can stabilize learning further by **using the Advantage function as Critic instead of the Action value function**.

The idea is that the Advantage function calculates the relative advantage of an action compared to the others possible at a state: **how taking that action at a state is better compared to the average value of the state**. It's subtracting the mean value of the state from the state action pair:

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit8/advantage1.jpg"" alt=""Advantage Function""/>

In other words, this function calculates **the extra reward we get if we take this action at that state compared to the mean reward we get at that state**.
The Policy Gradient algorithm (simplified) looks like this:
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/pg_bigpicture.jpg"" alt=""Policy Gradient Big Picture""/>
</figure>

But Deep Q-Learning is excellent! Why use policy gradient methods?

### The Advantages of Policy-Gradient Methods

There are multiple advantages over Deep Q-Learning methods. Let's see some of them:

1. The simplicity of the integration:¬†**we can estimate the policy directly without storing additional data (action values).**
  
2. Policy gradient methods can¬†**learn a stochastic policy while value functions can't**.

This has two consequences:

a. We **don't need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores¬†**the state space without always taking the same trajectory.**
## Value-based, Policy-based, and Actor-critic methods

In the first unit, we saw two methods to find (or, most of the time, approximate) this optimal policy \\(\pi^{*}\\).

- In *value-based methods*, we learn a value function.
  - The idea is that an optimal value function leads to an optimal policy \\(\pi^{*}\\).
  - Our objective is to **minimize the loss between the predicted and target value** to approximate the true action-value function.
  - We have a policy, but it's implicit since it **is generated directly from the value function**. For instance, in Q-Learning, we used an (epsilon-)greedy policy.
---

‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit4/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*

[In the last unit](https://huggingface.co/blog/deep-rl-dqn), we learned about Deep Q-Learning. In this value-based Deep Reinforcement Learning algorithm, we¬†**used a deep neural network to approximate the different Q-values for each possible action at a state.**

Indeed, since the beginning of the course, we only studied value-based methods,¬†**where we estimate a value function as an intermediate step towards finding an optimal policy.**
  
<img src=""https://huggingface.co/blog/assets/70_deep_rl_q_part1/link-value-policy.jpg"" alt=""Link value policy"" />

Here is the question: What is the computational advantage of using value-based functions in reinforcement learning compared to direct policy-based methods?
Here is answer 1:Reduces training data requirements by focusing on action sampling
Here is answer 2: Simplifies policy definition through precomputed value lookups
Here is answer3: Enhances reward signals through stochastic policy actions
Here is answer 4: Improves parallelism in distributed environments","After analyzing all options provided I think I know which one should be picked up when asking 'What happens when both actor-critic enters into account'

Answer : Both Answers referring respectively 'Actor' & 'Critic' describe accurately how both ones work together within AC approach which mix.elements coming either direct(method usable selecting particular above described good fit(like act Ïï∞who doesRLrelated tasksfromscratchtakingcurrentstateasinputandgeneratingsuiteofpossibleactions authoritarian decisions Andr autofocusonchosen directions containing information"
How does the modular design of Gradio contribute to scalable reinforcement learning simulations?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (27654, 'b251137d-d7d4-4fa5-be14-69555f6edf11'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (29197, '596dee43-da92-457c-82fc-9bced93586f3'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (5660, 'f060d5f9-0dce-4d78-8298-2524cde9ccdb'), (5662, '9acf2b54-137b-40ce-a6f8-eec06d51c4da'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (16941, '55ad21a6-df67-43c6-ab75-7c99a97f9b1a'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (3119, '6a16877a-d058-484e-adf4-1e3130b324ae'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (11829, '38bd3f56-e5b3-4e10-8cb9-2c02e7602038'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (9312, '14034176-8319-4870-8e63-8ce542f58d45'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (25206, '9db87039-b249-40f0-a7a4-0f0a0ffad669'), (7799, '90107451-0822-4205-96bc-e19ca4488723'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (6791, 'caea27fd-5810-4f6f-afa1-5befc362cced'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (21675, 'f912b25a-5222-43ff-86a2-960b77ef8b41'), (6826, '8aa2b833-292f-452f-a64e-c2b51b5ee38a'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (21743, '093f3b5e-006e-467e-b1c9-5120230b2150'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (24842, '0d694989-93f1-48d3-b256-e21d25ab982b'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (12055, '7dc896f5-36c3-4193-9361-7ef771809049'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18247, '74315d00-318c-4e2d-84b1-cb46200e84ce'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (2464, '54179c43-bf4a-4c14-b098-b5140e0a1a4b'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (12721, '26fbe79d-0795-4bcf-a444-8f56c357cc4c'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (7095, '46239874-f414-40b3-bf11-c8ec6587d46c'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
I recruited my talented housemates Ali Abdalla, Ali Abid, and Dawood Khan to release the first version of Gradio in 2019. We steadily expanded to cover more areas of machine learning including text, speech, and video. We found that it wasn't just researchers who needed to share machine learning models: interdisciplinary teams in industry, from startups to public companies, were building models and needed to debug them internally or showcase them externally. Gradio could help with both. Since we first released the library, more than 300,000 demos have been built with Gradio. We couldn't have done this without our community of contributors, our supportive investors, and the amazing Ahsen Khaliq who joined our company this year.
### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
## Components

Gradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.

Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[""webcam"", ""upload""]`).

**Static and Interactive Components**
Introduction to Gradio[[introduction-to-gradio]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

In this chapter we will be learning about how to build **interactive demos** for your machine learning models.

Why build a demo or a GUI for your machine learning model in the first place? Demos allow:

- **Machine learning developers** to easily present their work to a wide audience including non-technical teams or customers
- **Researchers** to more easily reproduce machine learning models and behavior
- **Quality testers** or **end users** to more easily identify and debug failure points of models
- **Diverse users** to discover algorithmic biases in models

We'll be using the Gradio library to build demos for our models. Gradio allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.

Here are some examples of machine learning demos built with Gradio:
Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
Running a Gradio App on your Web Server with Nginx

Tags: DEPLOYMENT, WEB SERVER, NGINX

## Introduction

Gradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.

In some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).

In this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.

**Prerequisites**
**Static and Interactive Components**

Every component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.

**Preprocessing and Postprocessing**

When a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).
### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


### The Gradio Python & JavaScript Ecosystem
Quickstart

Gradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif"" style=""padding-bottom: 10px"">

It just takes a few lines of Python to create a beautiful demo like the one above, so let's get started üí´

## Installation

**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)


We recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:

```bash
pip install gradio
Hugging Face has always worked to make models accessible and easy to use. The `transformers` library makes it possible to load a model in a few lines of code. After a model is loaded, it can be used to make predictions on new data programmatically.  _But it‚Äôs not just programmers that are using machine learning models!_ An increasingly common scenario in machine learning is **demoing models to interdisciplinary teams** or letting **non-programmers use models** (to help discover biases, failure points, etc.). 

The **[Gradio library](https://gradio.app/)** lets machine learning developers create demos and GUIs from machine learning models very easily, and share them for free with your collaborators as easily as sharing a Google docs link. Now, we‚Äôre excited to share that the Gradio 2.0 library lets you **_load and use almost any Hugging Face model_ _with a GUI_** **_in just 1 line of code_**. Here‚Äôs an example:

![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)
#### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


#### The Gradio Python & JavaScript Ecosystem
The 4 Kinds of Gradio Interfaces

So far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.

It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:
Setting Up a Demo for Maximum Performance

Tags: CONCURRENCY, LATENCY, PERFORMANCE

Let's say that your Gradio demo goes _viral_ on social media -- you have lots of users trying it out simultaneously, and you want to provide your users with the best possible experience or, in other words, minimize the amount of time that each user has to wait in the queue to see their prediction.

How can you configure your Gradio demo to handle the most traffic? In this Guide, we dive into some of the parameters of Gradio's `.queue()` method as well as some other related parameters, and discuss how to set these parameters in a way that allows you to serve lots of users simultaneously with minimal latency.
So far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?

### Chatbots with `gr.ChatInterface`

Gradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).

### Custom Demos with `gr.Blocks`
So far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?

#### Chatbots with `gr.ChatInterface`

Gradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).

#### Custom Demos with `gr.Blocks`
Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API

Hugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load(""Helsinki-NLP/opus-mt-en-es"", src=""models"")

demo.launch()
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.1.0

### Features

- [#5005](https://github.com/gradio-app/gradio/pull/5005) [`f5539c76`](https://github.com/gradio-app/gradio/commit/f5539c7618e31451420bd3228754774da14dc65f) - Enhancement: Add focus event to textbox and number component. Thanks [@JodyZ0203](https://github.com/JodyZ0203)!

### Fixes
Custom Components in 5 minutes

Gradio 4.0 introduces Custom Components -- the ability for developers to create their own custom components and use them in Gradio apps.
You can publish your components as Python packages so that other users can use them as well.
Users will be able to use all of Gradio's existing functions, such as `gr.Blocks`, `gr.Interface`, API usage, themes, etc. with Custom Components.
This guide will cover how to get started making custom components.

## Installation

You will need to have:

* Python 3.8+ (<a href=""https://www.python.org/downloads/"" target=""_blank"">install here</a>)
* Node.js v16.14+ (<a href=""https://nodejs.dev/en/download/package-manager/"" target=""_blank"">install here</a>)
* npm 9+ (<a href=""https://docs.npmjs.com/downloading-and-installing-node-js-and-npm/"" target=""_blank"">install here</a>)
* Gradio 4.0+ (`pip install --upgrade gradio`)

## The Workflow

The Custom Components workflow consists of 4 steps: create, dev, build, and publish.

Here is the question: How does the modular design of Gradio contribute to scalable reinforcement learning simulations?
Here is answer 1:By separating UI rendering from backend computations
Here is answer 2: By enabling pre-trained policies for large action spaces
Here is answer3: By automating parameter tuning during experiments
Here is answer 4: By integrating real-time debugging tools",None
"Why is multi-license support essential in Hugging Face's ecosystem, and how does it facilitate ethical AI use?","[(14857, '7a1dc41a-f45a-4d69-bd0b-2342658d4486'), (22547, '51e87e51-6544-440b-9057-4201251fc27d'), (14358, '4732caf5-9d60-4123-a2c2-94d1b5ca78fe'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (2597, '6458f359-e453-452e-93f0-0a72efc03c98'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (558, '093e0bfc-98b8-4d7b-a428-9ec96209387f'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (22588, 'a59c4df1-bdaa-4faa-b0f4-f3b5aeff16a6'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (11839, '1295e8ba-675b-4e41-9660-6a758ea89426'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (24657, '37943db2-8a2c-4369-80c8-e75eee95f201'), (94, 'db2da404-7a48-42c3-b246-9a0cc202fe4c'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (14944, 'cd075ce4-d02e-47be-ad35-21b38e850692'), (30821, '721f4d5f-3a80-403c-b5bd-8d94a87543e5'), (17509, '56ea90b0-f162-4bf5-8c05-654f3a183cd9'), (21611, '63264b8d-88bb-4d37-b0a4-051d519848ad'), (4208, '6510a508-9d37-48d7-96b5-cea2f637feae'), (21624, 'cf165b8c-348b-4ccc-acd1-944ae9c89877'), (14972, '1a9cc31a-a8ea-47b5-803e-fa461faaa9b5'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25728, 'e9a67dae-3e05-4640-afbb-816d75639376'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (12935, '78e0073f-75a8-4831-8522-3bbca5d23602'), (4745, '1a92f7c2-3ab9-4ff1-89af-f0ab573bdec0'), (5625, 'c79981e0-e8ac-454a-a1ec-1e4b2c4b1ba2'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (3229, 'bf82b68e-7d33-457e-b6d8-fbd0466d6271'), (7837, 'fb0db92b-f560-4130-a389-df2018f97ada'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (29354, 'eb6365fe-b9fc-400e-866f-6a21ed2a810f'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (18093, '5513757a-36b3-47a3-8270-f75a1d76df5e'), (28335, '994c72af-e053-48a3-9059-b9662f76400a'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (13510, '0e4bf28c-0eed-49c8-a4a5-724ce73aa1fd'), (14535, 'b9835dda-ff2d-4d55-a497-5553a72948a6'), (11975, 'c977182b-d101-4944-95e4-2c832d9bf6fd'), (17608, '12e880e8-366a-4118-bb57-c2f40c0793cd'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (3789, '6c31804d-45f5-468e-a44c-e7060de4fdd8'), (208, 'ca329095-eb87-48a9-9393-7dafb6980978'), (21713, '18782689-5845-4f1e-92b4-3ed0000a08e1'), (22224, '8d654242-8580-4742-9b96-a0b250750feb'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24286, '6bd381bd-f2c3-4435-bec4-43843d989605'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (14577, '03a094b5-b29c-4840-a743-d2ceb896c494'), (3329, '64a72f1a-f303-41c1-97ea-b32aeeac5fbc'), (11526, '994250e0-f714-40fe-b8ff-52c2610db830'), (8970, 'd5af4c95-97c8-4196-bab1-0d74644248f4'), (30994, '79a1b793-603c-4141-87d4-270e01bb91f7'), (8491, '3cb0ff2d-cd24-47e1-b368-9800cbb99b0f'), (31031, '306dd725-df38-43a2-a289-44917df0a6f1'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (12099, '97456597-42ed-4914-bc4a-1146c5fc4b6f'), (9030, 'f38d1e50-b06a-4f7b-b7ff-67ae2899320e'), (24905, 'f62d7196-fd88-4740-b825-0375a9e0c9f7'), (10576, 'c05e1ee5-97a4-4a5f-a403-219c022099f6'), (24410, '63102f98-f7b8-4a0b-8034-3e2687269cb4'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4964, '87a02f1c-9369-4f63-8050-567d148301e0'), (17767, 'a7af65e5-6000-4846-b469-ecc521b89dfa'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (24436, '62eee91c-545a-404b-97af-1d99deab920c'), (2940, '41b2ed97-f869-45fa-a2b1-445971ef0afd'), (897, 'f387e8a2-b1bf-48fa-a6f3-c7a015a21ae7'), (1409, 'e2e144e9-b204-40e4-8e05-92b8fdf21a19'), (15761, '19f353ed-20f2-48e1-8cb1-4a719927586c'), (10641, '69905c2d-26d2-4ad5-84ac-f7f6aabdb846'), (17301, '9e130d4e-12df-4941-91c1-6e5fd07be093'), (8599, 'b9ceb8e8-cbcd-4cd4-b341-73c69d823263'), (5020, '9c9df608-6264-4ae6-9f4c-394daed65dde'), (18334, 'c504a58a-f5ca-405b-9dbb-5b5f06fd1cf0'), (31136, '51caed7d-0da0-49a7-b6a2-8ee3113b8459'), (22433, 'f1052a2a-7fe3-49f0-beeb-740f4ecaa31e'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (6571, '524ec6e8-072e-4f5f-8879-d89812239e9d'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (3510, '4e9735e1-0a03-42b4-a237-4ea82a96730d'), (13239, 'f13538dd-edce-4970-a1cc-83ab6fd1a5d6'), (7096, '3ba57845-c986-4965-b8a2-53c99388658b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29121, 'a79d6e54-b6d5-4e9e-9617-8d8cf4ef5569'), (15300, 'ea0bd10e-50d7-46c2-84eb-1307151c2efb'), (5572, 'c7410d55-fc34-4411-841a-9379e5f391d8'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (3032, '222fc5a1-961d-4b64-9e4a-a414fa382d75'), (25563, 'd0969367-f5d0-4564-9ef6-1e386f979f8f'), (27611, '7cf54765-35e5-4ee1-9681-3e8a82b3c92f'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (27114, 'bc032949-d8d4-4c0e-8898-fdcb416b7e1b'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (22012, '5c55b2ec-3284-45a4-9f86-cf514fba02e0')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Hugging Face is where it is today thanks to its community of developers, so we‚Äôve seen firsthand what open development brings to the table
to support more robust innovation for more diverse and context-specific use cases;
where developers can easily share innovative new techniques, mix and match ML components to suit their own needs,
and reliably work with full visibility into their entire stack.
We‚Äôre also acutely aware of the necessary role of transparency in supporting more accountability and inclusivity of the technology ‚Äì
which we‚Äôve worked on fostering through better documentation and accessibility of ML artifacts, education efforts,
and hosting large-scale multidisciplinary collaborations, among others.
Thus, as the EU AI Act moves toward its final phase, we believe accounting for the specific needs and strengths of open and open-source development of ML systems will be instrumental in supporting its long-term goals.
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One ‚Äì or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) ‚Äì specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
of Conduct](https://huggingface.co/code-of-conduct) and [content guidelines](https://huggingface.co/content-guidelines) to help guide discussions along dimensions we believe to be important for an inclusive community space. We have developed a [Private Hub](https://huggingface.co/blog/introducing-private-hub) for secure ML development, a [library for evaluation](https://huggingface.co/blog/eval-on-the-hub) to make it easier for developers to evaluate their models rigorously, [code for analyzing data for skews and biases](https://github.com/huggingface/data-measurements-tool), and [tools for tracking carbon emissions when training a model](https://huggingface.co/blog/carbon-emissions-on-the-hub). We are also developing [new open and responsible AI licensing](https://huggingface.co/blog/open_rail), a modern form of licensing that directly addresses the harms that AI systems can create. And this week, we made it possible to [‚Äúflag‚Äù model and Spaces
Hello, world!

Originating as an open-source company, Hugging Face was founded on some key ethical values in tech: _collaboration_, _responsibility_, and _transparency_. To code in an open environment means having your code ‚Äì and the choices within ‚Äì viewable to the world, associated with your account and available for others to critique and add to.  As the research community began using the Hugging Face Hub to host models and data, the community directly integrated _reproducibility_ as another fundamental value of the company. And as the number of datasets and models on Hugging Face grew, those working at Hugging Face implemented [documentation requirements](https://huggingface.co/docs/hub/models-cards) and [free instructive courses](https://huggingface.co/course/chapter1/1), meeting the newly emerging values defined by the research community with complementary values around _auditability_ and _understanding_ the math, code, processes and people that lead to current technology.
‚ÄúThe future of AI is here, but it‚Äôs not evenly distributed,‚Äù said Clement Delangue, CEO of Hugging Face. ‚ÄúAccessibility and transparency are the keys to sharing progress and creating tools to use these new capabilities wisely and responsibly. Amazon SageMaker and AWS-designed chips will enable our team and the larger machine learning community to convert the latest research into openly reproducible models that anyone can build on.‚Äù

## Collaborating to scale AI in the cloud

This expanded strategic partnership enables Hugging Face and AWS to accelerate machine learning adoption using the latest models hosted on Hugging Face with the industry-leading capabilities of Amazon SageMaker. Customers can now easily fine-tune and deploy state-of-the-art Hugging Face models in just a few clicks on Amazon SageMaker and Amazon Elastic Computing Cloud (EC2), taking advantage of purpose-built machine learning accelerators including AWS Trainium and AWS Inferentia.
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
The integration of use-based restrictions clauses into open AI licenses brings up the ability to better control the use of AI artifacts and the capacity of enforcement to the licensor of the ML model, standing up for a responsible use of the released AI artifact, in case a misuse of the model is identified. If behavioral-use restrictions were not present in open AI licenses, how would licensors even begin to think about responsible use-related legal tools when openly releasing their AI artifacts? OpenRAILs and RAILs are the first step towards enabling ethics-informed behavioral restrictions.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
Demos and GUIs built with Gradio give the power of machine learning to more and more people because they allow non-technical users to access, use, and give feedback on models. And our acquisition by Hugging Face is the next step in this ongoing journey of accessibility. Hugging Face has already radically democratized machine learning so that any software engineer can use state-of-the-art models with a few lines of code. By working together with Hugging Face, we're taking this even further so that machine learning is accessible to literally anyone with an internet connection and a browser. With Hugging Face, we are going to keep growing Gradio and make it the best way to share your machine learning model with anyone, anywhere üöÄ
# Putting ethical principles at the core of the research lifecycle

## Ethical charter - Multimodal project


## Purpose of the ethical charter

It has been well documented that machine learning research and applications can potentially lead to ""data privacy issues, algorithmic biases, automation risks and malicious uses"" (NeurIPS 2021 [ethics guidelines](https://nips.cc/public/EthicsGuidelines)). The purpose of this short document is to formalize the ethical principles that we (the multimodal learning group at Hugging Face) adopt for the project we are pursuing. By defining these ethical principles at the beginning of the project, we make them core to our machine learning lifecycle.
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
‚ÄúOur collaboration with Hugging Face around SafeCoder fully aligns to VMware‚Äôs goal of enabling customer choice of solutions while maintaining privacy and control of their business data. In fact, we have been running SafeCoder internally for months and have seen excellent results. Best of all, our collaboration with Hugging Face is just getting started, and I‚Äôm excited to take our solution to our hundreds of thousands of customers worldwide,‚Äù says Chris Wolf, Vice President of VMware AI Labs. Learn more about private AI and VMware‚Äôs differentiation in this emerging space [here](https://octo.vmware.com/vmware-private-ai-foundation/).

---

If you‚Äôre interested in SafeCoder for your company, please contact us [here](mailto:api-enterprise@huggingface.co?subject=SafeCoder) - our team will contact you to discuss your requirements!
By being transparent about the decisions we're making in the project, who is working on which aspects of the system, and how the team can be contacted, we hope to receive feedback early enough in the process to make meaningful changes, and ground discussions about choices in an awareness of the goals we aim to achieve and the values we hope to incorporate.

This document is the result of discussions led by the multimodal learning group at Hugging Face (composed of machine learning researchers and engineers), with the contributions of multiple experts in ethics operationalization, data governance, and personal privacy.

## Limitations of this ethical charter
## 2. What is the Private Hub?

The [Private Hub](https://huggingface.co/platform) allows companies to use Hugging Face‚Äôs complete ecosystem in their own private and compliant environment to accelerate their machine learning development. It brings ML tools for every step of the ML lifecycle together in one place to make collaborating in ML simpler and more productive, while having a compliant environment that companies need for building ML securely:

<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""The Private Hub"" src=""assets/92_introducing_private_hub/private-hub.png""></medium-zoom>
  <figcaption>The Private Hub</figcaption>
</figure>
There have been significant advances in new Transformer and Diffuser machine learning models that process and generate text, audio, and images. However, most of these popular generative AI models are not publicly available, widening the gap of machine learning capabilities between the largest tech companies and everyone else. To counter this trend, AWS and Hugging Face are partnering to contribute next-generation models to the global AI community and democratize machine learning. Through the strategic partnership, Hugging Face will leverage AWS as a preferred cloud provider so developers in Hugging Face‚Äôs community can access AWS‚Äôs state-of-the-art tools (e.g., [Amazon SageMaker](https://aws.amazon.com/sagemaker), [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)) to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their
In keeping with Hugging Face‚Äôs core values of *openness* and *accountability*, we are sharing a collection of what we‚Äôve said and done here.  This includes our CEO [Clem](https://huggingface.co/clem)‚Äôs [testimony to U.S. Congress](https://twitter.com/ClementDelangue/status/1673348676478025730) and [statements at the U.S. Senate AI Insight Forum](https://twitter.com/ClementDelangue/status/1702095553503412732); our advice on the [E.U. AI Act](https://huggingface.co/blog/eu-ai-act-oss); our [comments to the NTIA on AI Accountability](https://huggingface.co/blog/policy-ntia-rfc); and our Chief Ethics Scientist [Meg](https://huggingface.co/meg)‚Äôs [comments to the Democratic Caucus](assets/164_ethics-soc-5/meg_dem_caucus.pdf). Common to many of these discussions were questions about why openness in AI can be beneficial, and we share a collection of our answers to this question [here](assets/164_ethics-soc-5/why_open.md).
Open licensing is one of the cornerstones of AI innovation. Licenses as social and legal institutions should be well taken care of. They should not be conceived as burdensome legal technical mechanisms, but rather as a communication instrument among AI communities bringing stakeholders together by sharing common messages on how the licensed artifact can be used.

Let's invest in a healthy open and responsible AI licensing culture, the future of AI innovation and impact depends on it, on all of us, on you.

Author: Carlos Mu√±oz Ferrandis

Blog acknowledgments: Yacine Jernite, Giada Pistilli, Irene Solaiman, Clementine Fourrier, Cl√©ment D√©lange

Here is the question: Why is multi-license support essential in Hugging Face's ecosystem, and how does it facilitate ethical AI use?
Here is answer 1:It simplifies the deployment of open-source models
Here is answer 2: It ensures compatibility with proprietary datasets
Here is answer3: It allows nuanced legal frameworks to be reflected in metadata
Here is answer 4: It enables automatic updates of model weights",I choose answer3 : It allows nuanced legal frameworks
"What is a key limitation of torch.distributed for document retrieval in RAG fine-tuning, and how does Ray overcome it?","[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (10251, '31409786-ddd4-49ae-94d2-bea926b83be4'), (6679, '5355dc06-3d44-4916-a09c-fc5839bb0ca7'), (24603, '78522c83-d8fe-4f56-8cbe-0c5d741f50f1'), (12317, '271019f8-cd28-4e51-814f-981ed9de6798'), (24606, '0b632df8-fda6-4a3f-9b26-aa4ce8eb417d'), (7208, 'd7164bc1-bc8d-4ca1-9309-e5a8b834f98d'), (20521, 'c8d9c234-55d3-4552-91fc-5030005cd9bf'), (17458, '29ac6c95-d3e2-470e-a268-83fd7c395ddd'), (2613, '8ed34ac1-8e5c-4f4d-a4e5-765f43230e11'), (9782, '738e1e6a-9b5c-4f95-8f37-baec1c198116'), (29752, '866f9889-e3da-4559-b3d6-343baf84f073'), (2618, '8c25f1c0-1ce2-488c-a60e-9ecdd7e2403b'), (5694, 'dee73012-4ba6-4015-8099-1ff6348fc9b2'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (1100, 'b79e9cec-adcc-4a9b-981b-5a6e9fbcd005'), (6738, '001fbe14-a409-4006-b8c8-e19952cdccbc'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (19547, '8caa2fcc-7a1d-4007-9db2-8e7011fd9033'), (22110, '60c67161-62ba-4bb8-88e1-6a0592f80287'), (22111, '6cb41ff3-1249-49c2-b70e-3668ab947b7a'), (22112, '72ebbfd3-6f7e-4e49-aeee-7692ef491d72'), (22113, '14d11c06-e512-45ae-8775-9c31eed1e9d7'), (22114, '0a5ba4bb-e50a-4171-82ba-b95d1f4b573c'), (19555, '9bb43118-305f-403d-8b60-5c20b13a050c'), (4707, '0aeaff5d-3eb8-40d0-b8da-3a399195ab50'), (22116, 'e561f1a9-e1b1-4da2-9db7-3485e7719c79'), (22118, '36a5c490-635c-46e4-bbff-9f2186d8cb06'), (22119, 'b0e6ddbb-4f06-4aaa-a05e-0c06c29dcd0d'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (19561, 'bd6d1959-f94f-4a44-b2c4-a94c6950eb8c'), (22121, 'ebdea3ea-7be3-4bbe-bee4-0b68571dc003'), (19563, '505b868d-35d2-4d6c-8268-8e729abd6f63'), (11383, '7d1776e7-3738-41fc-ae14-19c3f95d3c25'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (10873, 'a4e244ba-2eea-4b67-a7e0-2be6376bcc7a'), (1662, '9463d366-70d4-4377-8f4c-a43126487694'), (24711, 'ff60e964-aa24-44ba-9ac0-e72b18947ea2'), (24712, 'e34b93ed-59ef-42e7-9129-b252acbf40f8'), (5257, '01286d7b-65ff-41e0-b0b1-3ec8dcb42148'), (8330, 'b2bd4024-ac0b-42c0-9840-c85ddda39140'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (11918, '0533046d-82d3-411e-b1fa-90da52cc0fe8'), (9876, 'b5c62f13-7a9a-411b-9d22-0935ded13372'), (22177, 'fe470ead-825d-4432-ab27-1f1f4a497250'), (19106, 'f9485ca5-0c07-4a79-b3ae-363c2ee36a2b'), (30382, 'c4079e2a-5915-47e2-b0f5-6dadaf8d6c3c'), (26294, 'e270afea-3092-480b-ba50-52d43d31bf2b'), (6338, 'f8556800-ba9c-4c54-b96e-56d695a0da64'), (709, '68b25497-c5e6-494d-aebf-9f31f7c98cc0'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (17635, 'cb4c9589-721d-4162-839c-0a37be083184'), (22772, 'd0b704d9-ea88-403d-8797-758ba929ebd4'), (761, '85426fd8-7ddb-45c5-b275-4bda9bfb9212'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (28938, 'c79cdb2e-622b-41c8-bde6-21c9ca8d958a'), (5392, 'd987ceef-ce76-4097-bf39-4b68e4e49f77'), (16673, '19dd7de4-3435-4cd6-80bc-6a7dc5ac2cda'), (13097, 'd0ae3415-29e2-4a0a-a9e4-9f788040397f'), (31536, '37cc5143-44ba-4aa9-a04f-6f0006a85b20'), (17208, '2203f22a-0a57-4589-bd1b-a9d45aa91f1f'), (17209, '637b92d4-eb98-4446-b500-74fe9871a4dd'), (17210, 'bf0844da-5cf9-43fa-8032-2845dfe427c1'), (17211, '7ce7337b-fcc4-41d3-bbc0-478753c50a6a'), (17212, '3639fc6d-edf2-464b-b1f8-6a8147fa1270'), (17213, 'f849ea19-994a-4645-821c-d8c7dcea2c4d'), (17214, 'c24d07fd-0008-4815-9ccd-66957ce33488'), (17215, 'b86bbccd-3b71-4a3f-993f-a8f72ce10054'), (17216, 'e14f50a6-9791-4a08-91f2-bae04ec2151a'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (9540, '8cfc2be7-ce6e-4ee3-9fa7-d4da603fc5cf'), (11770, '2d23cee2-4878-40e3-8d34-c17acb4ff6a8'), (25944, 'f58e15a6-a061-45de-8d88-27354eac2711'), (17770, 'e7622f83-c164-4281-816b-b028d8eab303'), (29177, '619af8a9-fbe3-421d-9eca-be6d9044cb79'), (14190, '52eac1aa-1ce8-4e04-85df-009833ff0153'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (31108, 'b2a6db3a-19cd-4f85-8ebb-367f85973aa0'), (15768, 'f0a867f0-3313-47f8-a4d9-49cb78888982'), (17822, '20439f0d-de85-4a4f-91e9-58d6c81c72a7'), (15776, '5792c81e-163d-4045-9e92-aa5c22b8bdc9'), (17826, '25fa36a1-9fd9-4630-b761-739de4353aff'), (17315, '42e6b701-ef1a-4a5b-adbe-9e296483d4f5'), (2980, '1f273dec-8863-42c5-804e-8246f09b1434'), (2984, '59ae3b4c-4388-43a0-9dd8-80a1c32f1434'), (2985, '6ad9cc82-7643-4e0a-8a93-d62ad4e98de4'), (1471, '0be7ac99-eca3-4e0d-86bd-8c7451edbbb6'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (31719, '9413008c-5af5-444b-ba79-f5ba91409f43'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (4085, '8a08cc47-8750-4040-b9cf-d499f02be598'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4092, '49e8126f-6930-48ec-abbd-632bcb85a3df'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Instead, a framework-agnostic and a more flexible implementation for ad-hoc concurrent programming is required. [Ray](https://ray.io/) fits the bill perfectly. Ray is a simple, yet powerful Python library for general-purpose distributed and parallel programming. Using Ray for distributed document retrieval, we achieved a **2x speedup per retrieval call compared to `torch.distributed`**, and overall better fine-tuning scalability.

### Ray for Document Retrieval
![alt_text](assets/12_ray_rag/torch_distributed_document_retrieval.png ""image_tooltip"")
_Document retrieval with the torch.distributed implementation_


The main drawback of the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) implementation for document retrieval was that it latched onto the same process group used for training and only the rank 0 training worker loaded the index into memory.

As a result, this implementation had some limitations:
![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_

To overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Ray‚Äôs stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG.
### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
```
You will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.

## Document Retrieval
When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other
with [`Ray`](https://docs.ray.io/en/master/).

This option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.
By default this flag is set to `pytorch`.

For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.
As a result, this implementation had some limitations:

1. **Synchronization bottleneck**: The rank 0 worker had to receive the inputs from all workers, perform the index query, and then send the results back to the other workers. This limited performance with multiple training workers.
2. **PyTorch specific**: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well.

![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_
And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
# Retrieval Augmented Generation with Huggingface Transformers and Ray


##### A guest blog post by <a href=""/amogkam"">Amog Kamsetty</a> from the Anyscale team

[Huggingface Transformers](https://huggingface.co/) recently added the [Retrieval Augmented Generation (RAG)](https://twitter.com/huggingface/status/1310597560906780680) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks. In this blog post, we introduce the integration of [Ray](https://docs.ray.io/en/master/), a library for building scalable applications, into the RAG contextual document retrieval mechanism. This speeds up retrieval calls by 2x and improves the scalability of RAG distributed [fine-tuning](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag).



### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")
For the Ray implementation, the index is loaded in *separate* process(es). The training workers randomly select which
retriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed_retriever` arg to `ray`.
To configure the number of retrieval workers (the number of processes that load the index), you can set the `num_retrieval_workers` flag.
Also make sure to start the Ray cluster before running fine-tuning.

```bash
# Start a single-node Ray cluster.
ray start --head

python examples/research_projects/rag/finetune_rag.py \
    --data_dir $DATA_DIR \
    --output_dir $OUTPUT_DIR \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_type rag_sequence \
    --fp16 \
    --gpus 8
    --distributed_retriever ray \
    --num_retrieval_workers 4

# Stop the ray cluster once fine-tuning has finished.
ray stop
```

## What‚Äôs next?

Using RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks.
## Overview

Retrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and
sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate
outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing
both retrieval and generation to adapt to downstream tasks.

It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela.

The abstract from the paper is the following:
### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")

_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._



Recently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library.
## Webserver part

Just like the Rust counterpart, we had to implement the batching of requests
with different parameters. Since we were in the `PyTorch` world, we have pretty
much full control of what's going on. 
Since we're in Python, we have the limiting factor that the `torch.distributed` 
needs to run on several processes instead of threads, which means it's slightly
harder to communicate between processes. In the end, we opted to communicate
raw strings over a Redis pub/sub to distribute the requests to all processes at once.
Since we are in different processes it's easier to do it that way than communicating
tensors (which are way bigger) for instance.
Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune‚Äôs integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.

And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!
```


Then, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh)!


```bash
# Sample script to finetune RAG using Ray for distributed retrieval.

# Add parent directory to python path to access lightning_base.py
export PYTHONPATH=""../"":""${PYTHONPATH}""

# Start a single-node Ray cluster.
ray start --head

# A sample finetuning run, you need to specify data_dir, output_dir and model_name_or_path
# run ./examples/rag/finetune_rag_ray.sh --help to see all the possible options
Intro

Authors: @patrickvonplaten and @lhoestq

Aimed at tackling the knowledge-intensive NLP tasks (think tasks a human wouldn't be expected to solve without access to external knowledge sources), RAG models are seq2seq models with access to a retrieval mechanism providing relevant context documents at training and evaluation time.

A RAG model encapsulates two core components: a question encoder and a generator.
During a forward pass, we encode the input with the question encoder and pass it
to the retriever to extract relevant context documents. The documents are then prepended to the input.
Such contextualized inputs are passed to the generator.

Read more about RAG  at https://arxiv.org/abs/2005.11401.

# Note

‚ö†Ô∏è This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability

# Finetuning
_A performance comparison of different retrieval implementations. For each document retrieval implementation, we run 500 training steps with a per-GPU batch size of 8, and measure the time it takes to retrieve the contextual documents for each batch on the rank 0 training worker. As the results show, using multiple retrieval processes improves performance, especially as we scale training to multiple GPUs._


### How do I use it?

[Huggingface](https://huggingface.co/) provides a [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) based [fine tuning script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag), and we extended it to add the Ray retrieval implementation as an option. 

To try it out, first install the necessary requirements


```bash
pip install ray
pip install transformers
pip install -r transformers/examples/research_projects/rag/requirements.txt
```

Using Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than
just the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate
processes than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.

# Evaluation
Our evaluation script enables two modes of evaluation (controlled by the `eval_mode` argument): `e2e` - end2end evaluation, returns EM (exact match) and F1 scores calculated for the downstream task and `retrieval` - which returns precision@k of the documents retrieved for provided inputs.
# Latest Update

‚ö†Ô∏è Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)

# Note

‚ö†Ô∏è This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability

# Testing

The following two bash scripts can be used to quickly test the implementation.
1. sh ./test_run/test_finetune.sh script
    - Tests the full end-to-end fine-tuning ability with a dummy knowlendge-base and dummy training dataset (check test_dir directory).
    - Users can replace the dummy dataset and knowledge-base with their own to do their own finetuning.
    - Please read the comments in the test_finetune.sh file.
2. sh ./test_run/test_rag_new_features.sh
    - Tests the newly added functions (set_context_encoder and set_context_encoder_tokenizer) related to modeling rag.
    - This is sufficient to check the model's ability to use the set functions correctly.
This model was contributed by [ola13](https://huggingface.co/ola13).

## Usage tips

Retrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. 
RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq 
modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt 
to downstream tasks.

## RagConfig

[[autodoc]] RagConfig

## RagTokenizer

[[autodoc]] RagTokenizer

## Rag specific outputs

[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput

[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput

## RagRetriever

[[autodoc]] RagRetriever

<frameworkcontent>
<pt>

## RagModel

[[autodoc]] RagModel
    - forward

## RagSequenceForGeneration

[[autodoc]] RagSequenceForGeneration
    - forward
    - generate

## RagTokenForGeneration

[[autodoc]] RagTokenForGeneration
    - forward
    - generate
The original RAG code has also been modified to work with the latest versions of pytorch lightning (version 1.2.10) and RAY (version 1.3.0). All other implementation details remain the same as the [original RAG code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag).
Read more about RAG  at https://arxiv.org/abs/2005.11401.

This code can be modified to experiment with other research on retrival augmented models which include training of the retriever (e.g. [REALM](https://arxiv.org/abs/2002.08909) and [MARGE](https://arxiv.org/abs/2006.15020)).

To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script also includes descriptions on each command-line argument used.

# Latest Update

‚ö†Ô∏è Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)

# Note

Here is the question: What is a key limitation of torch.distributed for document retrieval in RAG fine-tuning, and how does Ray overcome it?
Here is answer 1:Torch.distributed struggles with synchronization; Ray uses independent processes to manage retrieval
Here is answer 2: Torch.distributed cannot handle large datasets; Ray compresses indices for efficiency
Here is answer3: Torch.distributed lacks framework integration; Ray integrates directly with TensorFlow
Here is answer 4: Torch.distributed supports only single-GPU training; Ray supports multi-GPU setups",Answer: Torch-distribution struggles synchronization; Abstract measure manages remedies
How does the action-value function in reinforcement learning influence RAG‚Äôs document retrieval strategies?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (15941, 'b55be617-b94e-461c-9aef-0ae569761bf9'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (6215, '6e5bbcdf-c067-4f3c-8622-e073530f78e8'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (30298, '4e59aebc-b2c4-4323-9471-a87bce987887'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (11365, 'fe8da28b-5e19-460e-be89-ccd9c18b8b30'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17533, 'f2d73255-0ee5-4a2b-a2a4-b10e4d2c0612'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (14486, '1d0b077e-bd05-4bb6-9239-cdebc73e9813'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25777, '222d6fa5-c0e6-4075-bb20-83f0dfe99f5c'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (1320, 'd5a5263a-a570-474e-8d13-92d45d7a6e95'), (1336, '329c759f-4fc3-49ae-89bd-b24d835a593b'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (20810, '118abb64-11e7-49fe-a65e-bdee25933d33'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (19304, 'b47f3bd6-724f-4cb0-adff-c4fbefb8903c'), (16746, '45080cea-724d-4d98-8278-7cd29fd71f9b'), (20335, '24699ce0-932d-4081-a7d8-ed55918efde0'), (19311, '6e5dbd8e-f471-4533-b4a4-74571e8d6abf'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23956, '9dec7d5b-756e-4a6c-ac15-1b544069a789'), (23957, '666ef815-1fab-4378-93d7-9c5e8c83e825'), (23958, '82ffb2ac-f889-482f-bd2b-090b36dbcce1'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (923, '3f1f99cb-3e95-4732-9582-c75dfc21e2bb'), (924, 'a954e80d-bcae-4191-ad77-dc130d6f2b5a'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23454, 'bb59e1c0-8193-40ce-a450-9efc2ecc6226'), (23966, '5ad52301-e36d-4241-a74f-c251f1bf2470'), (11160, 'da0b4465-7e50-41b1-9bb2-d6ce2483d770'), (23970, '02e2356f-f06a-4242-9e9a-01da5226883c'), (23971, '02fa64da-7776-42d4-a529-913fbf0f4c8b'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (13755, 'c07affda-1794-4844-a683-eb253231c213'), (961, 'f8df8306-bd61-414c-92fb-2f5f3e706930'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13764, 'c2619e99-58b4-4943-a672-c368132c540e'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ### Among the value-based methods, we can find two main strategies

- **The state-value function.** For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.
- **The action-value function.** In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.

### Epsilon-greedy strategy:

- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.
- Chooses the action with the highest expected reward with a probability of 1-epsilon.
- Chooses a random action with a probability of epsilon.
- Epsilon is typically decreased over time to shift focus towards exploitation.

### Greedy strategy:
```

The action space is a vector with 3 values:
- Control x, y, z movement


### Normalize observation and rewards

A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).

For that purpose, there is a wrapper that will compute a running average and standard deviation of input features.

We also normalize rewards with this same wrapper by adding `norm_reward = True`

[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)

```python
env = make_vec_env(env_id, n_envs=4)

# Adding this wrapper to normalize the observation and the reward
env = # TODO: Add the wrapper
```

#### Solution

```python
env = make_vec_env(env_id, n_envs=4)

env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
Introduction [[introduction]]

  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png"" alt=""thumbnail""/>

In the last unit, we learned about Deep Q-Learning. In this value-based deep reinforcement learning algorithm, we¬†**used a deep neural network to approximate the different Q-values for each possible action at a state.**

Since the beginning of the course, we have only studied value-based methods,¬†**where we estimate a value function as an intermediate step towards finding an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy"" />

In value-based methods, the policy ** \(œÄ\) only exists because of the action value estimates since the policy is just a function** (for instance, greedy-policy) that will select the action with the highest value given a state.
The agent's goal is to maximize its cumulative reward,¬†**called the expected return.**

### **The reward hypothesis: the central idea of Reinforcement Learning**

‚áí Why is the goal of the agent to maximize the expected return?

Because RL is based on the¬†**reward hypothesis**, which is that all goals can be described as the¬†**maximization of the expected return**¬†(expected cumulative reward).

That‚Äôs why in Reinforcement Learning,¬†**to have the best behavior,**¬†we need to¬†**maximize the expected cumulative reward.**

### **Markov Property**

In papers, you‚Äôll see that the RL process is called the¬†**Markov Decision Process**¬†(MDP).

We‚Äôll talk again about the Markov Property in the following units. But if you need to remember something today about it, Markov Property implies that our agent needs¬†**only the current state to decide**¬†what action to take and¬†**not the history of all the states**¬†**and actions**¬†they took before.

### **Observations/States Space**
### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
The agent's goal is to _maximize_ its cumulative reward, **called the expected return.**

## The reward hypothesis: the central idea of Reinforcement Learning [[reward-hypothesis]]

‚áí Why is the goal of the agent to maximize the expected return?

Because RL is based on the¬†**reward hypothesis**, which is that all goals can be described as the¬†**maximization of the expected return**¬†(expected cumulative reward).

That‚Äôs why in Reinforcement Learning,¬†**to have the best behavior,**¬†we aim to learn to take actions that¬†**maximize the expected cumulative reward.**


## Markov Property [[markov-property]]

In papers, you‚Äôll see that the RL process is called a¬†**Markov Decision Process**¬†(MDP).
## The Reward Function [[reward-function]]

The reward function is designed so that **Huggy will fulfill his goal**: fetch the stick.

Remember that one of the foundations of Reinforcement Learning is the *reward hypothesis*: a goal can be described as the **maximization of the expected cumulative reward**.

Here, our goal is that Huggy **goes towards the stick but without spinning too much**. Hence, our reward function must translate this goal.

Our reward function:

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit-bonus1/reward.jpg"" alt=""Huggy reward function"" width=""100%"">
- Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.
- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**
- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**
- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
Here we see that our value function¬†**defined value for each possible state.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/value_2.jpg"" alt=""Value based RL""/>
  <figcaption>Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.
</figcaption>
</figure>

Thanks to our value function, at each step our policy will select the state with the biggest value defined by the value function: -7, then -6, then -5 (and so on) to attain the goal.

If we recap:

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/vbm_1.jpg"" alt=""Vbm recap""/>
</figure>
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/vbm_2.jpg"" alt=""Vbm recap""/>
</figure>


## **The ‚ÄúDeep‚Äù in Reinforcement Learning**
---

‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit4/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*

[In the last unit](https://huggingface.co/blog/deep-rl-dqn), we learned about Deep Q-Learning. In this value-based Deep Reinforcement Learning algorithm, we¬†**used a deep neural network to approximate the different Q-values for each possible action at a state.**

Indeed, since the beginning of the course, we only studied value-based methods,¬†**where we estimate a value function as an intermediate step towards finding an optimal policy.**
  
<img src=""https://huggingface.co/blog/assets/70_deep_rl_q_part1/link-value-policy.jpg"" alt=""Link value policy"" />
Under a deterministic policy, the policy will either always move right when in a red state or always move left. **Either case will cause our agent to get stuck and never suck the dust**.

Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic policy** (""greedy epsilon strategy""). Consequently, our agent can **spend a lot of time before finding the dust**.

On the other hand, an optimal stochastic policy **will randomly move left or right in red (colored) states**. Consequently, **it will not be stuck and will reach the goal state with a high probability**.

<figure class=""image table text-center m-0 w-full"">
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg"" alt=""Hamster 1""/>
</figure>

### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces
Summary [[summary]]

That was a lot of information! Let's summarize:

- Reinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.

- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**

- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**

- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
**Q-Learning is the algorithm we use to train our Q-function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.

<figure>
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg"" alt=""Q-function""/>
  <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption>
</figure>

The¬†**Q comes from ""the Quality"" (the value) of that action at that state.**

Let's recap the difference between value and reward:

- The *value of a state*, or a *state-action pair* is the expected cumulative reward our agent gets if it starts at this state (or state-action pair) and then acts accordingly to its policy.
- The *reward* is the **feedback I get from the environment** after performing an action at a state.
## **Introducing Q-Learning**
### **What is Q-Learning?**

Q-Learning is an¬†**off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end¬†of this chapter.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us¬†**the value of each state or each state-action pair.**
- *Uses a TD approach:*¬†**updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-Function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-function.jpg"" alt=""Q-function""/>
  <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption>
</figure>
---

‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*
  
**[In the last Unit](https://huggingface.co/blog/deep-rl-a2c)**, we learned about Advantage Actor Critic (A2C), a hybrid architecture combining value-based and policy-based methods that help to stabilize the training by reducing the variance with:

- *An Actor*¬†that controls¬†**how our agent behaves**¬†(policy-based method).
- *A Critic*¬†that measures¬†**how good the action taken is**¬†(value-based method).
Because, in value-based,¬†**œÄ exists only because of the action value estimates, since policy is just a function**¬†(for instance, greedy-policy) that will select the action with the highest value given a state.

But, with policy-based methods, we want to optimize the policy directly¬†**without having an intermediate step of learning a value function.**

So today,¬†**we'll study our first Policy-Based method**: Reinforce. And we'll implement it from scratch using PyTorch. Before testing its robustness using CartPole-v1, PixelCopter, and Pong.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/envs.gif"" alt=""Environments""/>
</figure>

Let's get started,
### Q5: What is the exploration/exploitation tradeoff?

<details>
<summary>Solution</summary>

In Reinforcement Learning, we need to **balance how much we explore the environment and how much we exploit what we know about the environment**.

- *Exploration* is exploring the environment by **trying random actions in order to find more information about the environment**.

- *Exploitation* is **exploiting known information to maximize the reward**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg"" alt=""Exploration Exploitation Tradeoff"" width=""100%"">

</details>


### Q6: What is a policy?

<details>
<summary>Solution</summary>

- The Policy œÄ **is the brain of our Agent**. It‚Äôs the function that tells us what action to take given the state we are in. So it defines the agent‚Äôs behavior at a given time.
Introducing Q-Learning [[q-learning]]
## What is Q-Learning? [[what-is-q-learning]]

Q-Learning is an¬†**off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end¬†of this unit.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us¬†**the value of each state or each state-action pair.**
- *TD approach:*¬†**updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.
So now, before diving on Q-Learning, let's summarise what we just learned:

We have two types of value-based functions:

- State-Value function: outputs the expected return if¬†**the agent starts at a given state and acts accordingly to the policy forever after.**
- Action-Value function: outputs the expected return if¬†**the agent starts in a given state, takes a given action at that state**¬†and then acts accordingly to the policy forever after.
- In value-based methods,¬†**we define the policy by hand**¬†because we don't train it, we train a value function. The idea is that if we have an optimal value function, we¬†**will have an optimal policy.**

There are two types of methods to learn a policy for a value function:

Here is the question: How does the action-value function in reinforcement learning influence RAG‚Äôs document retrieval strategies?
Here is answer 1:It provides probabilistic guidance for action selection in retrieval pipelines
Here is answer 2: It enables retrieval strategies to maximize immediate rewards
Here is answer3: It incorporates state-action pair values for sequential retrieval optimization
Here is answer 4: It limits exploration by focusing on deterministic retrieval outputs","Based on provided context alone I would say 

Answer : It incorporates sequential-state-pair values into sequential optimization"
What makes Gradio‚Äôs quality checks valuable for maintaining model deployment standards?,"[(24065, 'e3e1132e-e823-411b-b7cc-d755700630aa'), (24067, '157c15d9-4820-40cb-9366-e7b656d413a7'), (28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (27654, 'b251137d-d7d4-4fa5-be14-69555f6edf11'), (24073, '6d6d9a99-657b-4749-a031-7d443a05252c'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (4109, '97503a54-b6a1-43d3-8868-3be0c0ed379a'), (5651, 'fbea4144-f67b-4eaa-a37e-347dfab99d2d'), (5653, '96183704-8c3b-4f40-abcb-e94d83092caf'), (28183, 'a8cab7ae-c71e-44bd-8d55-8406dc0b8660'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (28186, '2715ef7b-62a4-4ae9-95c5-944fe97fd438'), (5660, 'f060d5f9-0dce-4d78-8298-2524cde9ccdb'), (28199, '5d792e30-a6af-483e-bf0d-836c80cb5097'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (28209, '68d2780c-5f9b-479f-9212-72a4754e9969'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28232, 'edcdb710-1075-4024-937a-e1e9b421cd61'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (28248, '14667f8d-196e-48a1-9b90-85075c677427'), (12890, 'd760aaaa-a33f-407b-a4fd-7069770cdf32'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28257, 'cfc6625e-3cab-4f72-8584-37a13847b65a'), (9312, '14034176-8319-4870-8e63-8ce542f58d45'), (2150, 'f095c104-a3da-40e4-8826-58c85512a8c0'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (28267, '6012369a-6c18-4403-85a8-b58f59c33f3c'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (7799, '90107451-0822-4205-96bc-e19ca4488723'), (25206, '9db87039-b249-40f0-a7a4-0f0a0ffad669'), (7801, '280e989b-e836-4f8c-b88b-e647aecc80bb'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (25226, '9c7b12af-d7dc-4a79-8194-5bee1f723220'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (25228, 'baea8d1e-5d4c-41d5-967b-7acff9b12ce8'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (2208, 'ba593b1c-6f5d-49c6-b377-4b55c72484e4'), (26787, '32682646-5b86-49ea-b44b-dcffacbf726f'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (2214, '025e63ca-22c5-4fa2-9b8e-44065d8d2701'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (6826, '8aa2b833-292f-452f-a64e-c2b51b5ee38a'), (2220, '700e60d0-c612-424f-b157-205616f9f254'), (21675, 'f912b25a-5222-43ff-86a2-960b77ef8b41'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (2238, '29adadfd-79f3-46a4-93d5-9cb57573ec81'), (2249, 'b6c12764-f869-4323-a0ab-efd1cbcb6ee1'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (2252, 'cf2e843d-3e19-4217-b439-3fcdb5926a08'), (2253, '528c0950-87eb-4ce0-9db7-aeaf9089c226'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (2263, 'de23a944-32ae-4aca-9342-ed441d8275b4'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (2285, '5b7edfae-d1bb-4c5d-8d8f-83b616f05d0d'), (27887, '69311dd3-e6f7-4e37-a620-9ae98462dcfd'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (27897, '00ee70f5-c8bc-45d4-9a8b-c379cafc9c97'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (2309, '49057658-3d2a-48f6-bcb5-968964f12996'), (2312, '51224360-7358-4c07-ac89-332e6ee010f8'), (26898, 'ba831bc1-46d4-4934-9a8f-689aad480ec3'), (2325, 'e39e098f-cf1e-4087-812e-2136ccec4ec7'), (12055, '7dc896f5-36c3-4193-9361-7ef771809049'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (2335, '12335f0e-4907-4225-a481-d525a0c2e614'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (6954, 'dc0a4a82-5286-4d59-ac85-fe642c49151e'), (3382, 'f57e911f-3558-4cec-a7c5-dc8c73ec91ef'), (2358, '61c00040-18d9-47df-b54f-732d42c2c5f9'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (2374, '3c2fc425-71a9-4574-8e7e-9ea6de5e161a'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (2383, 'c6ad6da4-4a77-4db0-b926-6c1b77b6db81'), (2393, 'ab888fa8-fd7f-4996-8db2-bab36ec7709b'), (29028, '15a78b30-fe85-4a8b-885d-1bf6e46e88b1'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (12146, '6b4f5910-201d-414b-825a-1abf827d8df1'), (12148, 'eb4e71b6-274a-443a-850f-cda16f106a1e'), (28024, '6c568b56-28c0-4eeb-b9c5-1618913cb943'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (28082, '8c86f9c1-60c8-40ed-8084-d82bfb3fc00a'), (14259, 'a39a8b28-d140-469b-bcf6-4087261c872f'), (15795, 'aabab3d6-cbff-41a3-827f-a25abfc8958f'), (13746, '0bb29eea-90a5-4fd6-8177-0ccbbf32e132'), (27062, 'cd158b22-9fc5-49d1-afc0-8a5dfd595f1e'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (28088, '86a70b0f-cff2-4271-8ac0-c271371d3049'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (28094, 'a923dd7b-a562-4641-9f9b-f09697c8b6ee'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (3525, 'b0b4a2df-8e7d-43b7-8046-5668bfe2ea02'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (28112, '62dd089d-8e58-4803-b275-9ae30db8b7b5'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (11734, 'd2266dda-9dcd-4bce-b11d-eea9e47bb1e6'), (28123, '1e0f48d2-52b2-4fe6-a624-509a98f026ae'), (2013, '8247fa80-5295-40ba-80ca-5ab308278cc0'), (28126, '82468f2e-776c-4d15-8134-4912158c36b3'), (28127, 'bd655a7b-d09b-4b55-8ba0-932251f68bcf'), (29664, '55395fd7-b89c-4823-abee-a2b5747fe3af'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29665, 'dd02a0cc-76a4-4b6b-a7f2-2430754de480'), (19939, '964731bb-bf8d-4d64-b54d-ae3ce10c46e1'), (2023, 'd20a455f-f235-41b7-a1c5-5b15b10c5065'), (28137, '5116f0d1-8fcf-4fed-971e-06051bd8b4b1'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283'), (28159, '82652842-43af-40f0-9e66-fbd1e5adac72')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
gradio-ui

This folder contains all of the Gradio UI and component source code.

- [set up](#setup)
- [running the application](#running-the-application)
- [local development](#local-development)
- [building for production](#building-for-production)
- [quality checks](#quality-checks)
- [ci checks](#ci-checks)

## setup

This folder is managed as 'monorepo' a multi-package repository which make dependency management very simple. In order to do this we use `pnpm` as our package manager.

Make sure [`pnpm`](https://pnpm.io/) is installed by [following the installation instructions for your system](https://pnpm.io/installation).

You will also need `node` which you probably already have

## running the application

Install all dependencies:

```bash
pnpm i
Test Strategy

Very brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.

This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).

## Objectives

The purposes of all testing activities on Gradio fit one of the following objectives:

1. Ensure that the Gradio library functions as we expect it to.
2. Enable the maintenance team to quickly identify both the presence and source of defects.
3. Prevent regressions, i.e. if we fix something it should stay fixed.
4. Improve the quality of the codebase in order to ease maintenance efforts.
5. Reduce the amount of manual testing required.

## Scope
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
We should not focus on code coverage but on test coverage following the below criteria:

- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)
- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)
- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)
- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)
- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)
- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)

## Types of testing
## Do I need to host my custom component on HuggingFace Spaces?
You can develop and build your custom component without hosting or connecting to HuggingFace.
If you would like to share your component with the gradio community, it is recommended to publish your package to PyPi and host a demo on HuggingFace so that anyone can install it or try it out.

## What methods are mandatory for implementing a custom component in Gradio?

You must implement the `preprocess`, `postprocess`, `as_example`, `api_info`, `example_inputs`, `flag`, and `read_from_flag` methods. Read more in the [backend guide](./backend).

## What is the purpose of a `data_model` in Gradio custom components?

A `data_model` defines the expected data format for your component, simplifying the component development process and self-documenting your code. It streamlines API usage and example caching.

## Why is it important to use `FileData` for components dealing with file uploads?
Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API

Hugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load(""Helsinki-NLP/opus-mt-en-es"", src=""models"")

demo.launch()
As one of the founders of Gradio, I couldn't be more excited about the next step in our journey. I still remember clearly how we started in 2019: as a PhD student at Stanford, I struggled to share a medical computer vision model with one of my collaborators, who was a doctor. I needed him to test my machine learning model, but he didn't know Python and couldn't easily run the model on his own images. I envisioned a tool that could make it super simple for machine learning engineers to build and share demos of computer vision models, which in turn would lead to better feedback and more reliable models üîÅ
For instructions on how to write and run tests see the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).

## Managing defects

As we formalise our testing strategy and bring / keep our test up to standard, it is important that we have some principles on managing defects as they occur/ are reported. For now we can have one very simple rule:

- Every bug fix should be accompanied by a test that failed before the fix and passes afterwards. This test should _typically_ be a dynamic code test but it could be a linting rule or new type if that is appropriate. There are always exceptions but we should think very carefully before ignoring this rule.
## 0.4.1

### Patch Changes

- Updated dependencies [[`bca6c2c80`](https://github.com/gradio-app/gradio/commit/bca6c2c80f7e5062427019de45c282238388af95), [`3cdeabc68`](https://github.com/gradio-app/gradio/commit/3cdeabc6843000310e1a9e1d17190ecbf3bbc780), [`fad92c29d`](https://github.com/gradio-app/gradio/commit/fad92c29dc1f5cd84341aae417c495b33e01245f)]:
  - @gradio/client@0.7.2
  - @gradio/atoms@0.2.1
  - @gradio/upload@0.3.3
  - @gradio/statustracker@0.3.1

## 0.4.0

### Features

- [#6240](https://github.com/gradio-app/gradio/pull/6240) [`dd901c1b0`](https://github.com/gradio-app/gradio/commit/dd901c1b0af73a78fca8b6875b2bb00f84071ac8) - Model3D panning, improved UX. Thanks [@dylanebert](https://github.com/dylanebert)!
- [#6255](https://github.com/gradio-app/gradio/pull/6255) [`e3ede2ff7`](https://github.com/gradio-app/gradio/commit/e3ede2ff7d4a36fb21bb0b146b8d5ad239c0e086) - Ensure Model 3D updates when attributes change. Thanks [@hannahblair](https://github.com/hannahblair)!
## 0.2.0

### Features

- [#5373](https://github.com/gradio-app/gradio/pull/5373) [`79d8f9d8`](https://github.com/gradio-app/gradio/commit/79d8f9d891901683c5a1b7486efb44eab2478c96) - Adds `height` and `zoom_speed` parameters to `Model3D` component, as well as a button to reset the camera position. Thanks [@abidlabs](https://github.com/abidlabs)!
- [#5266](https://github.com/gradio-app/gradio/pull/5266) [`4ccb9a86`](https://github.com/gradio-app/gradio/commit/4ccb9a86f194c6997f80a09880edc3c2b0554aab) - Makes it possible to set the initial camera position for the `Model3D` component as a tuple of (alpha, beta, radius). Thanks [@mbahri](https://github.com/mbahri)!

## 0.1.0

### Highlights

#### Improve startup performance and markdown support ([#5279](https://github.com/gradio-app/gradio/pull/5279) [`fe057300`](https://github.com/gradio-app/gradio/commit/fe057300f0672c62dab9d9b4501054ac5d45a4ec))

##### Improved markdown support
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.0.2

### Patch Changes

- Updated dependencies []:
  - @gradio/utils@0.0.2
- Fixed Named API Requests by [@abidlabs](https://github.com/abidlabs) in [PR 2151](https://github.com/gradio-app/gradio/pull/2151)
- Quick Fix: Cannot upload Model3D image after clearing it by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2168](https://github.com/gradio-app/gradio/pull/2168)
- Fixed misleading log when server_name is '0.0.0.0' by [@lamhoangtung](https://github.com/lamhoangtung) in [PR 2176](https://github.com/gradio-app/gradio/pull/2176)
- Keep embedded PngInfo metadata by [@cobryan05](https://github.com/cobryan05) in [PR 2170](https://github.com/gradio-app/gradio/pull/2170)
- Skops integration: Load tabular classification and regression models from the hub by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2126](https://github.com/gradio-app/gradio/pull/2126)
- Respect original filename when cached example files are downloaded by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2145](https://github.com/gradio-app/gradio/pull/2145)
- Fixed Named API Requests by [@abidlabs](https://github.com/abidlabs) in [PR 2151](https://github.com/gradio-app/gradio/pull/2151)
- Quick Fix: Cannot upload Model3D image after clearing it by [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2168](https://github.com/gradio-app/gradio/pull/2168)
- Fixed misleading log when server_name is '0.0.0.0' by [@lamhoangtung](https://github.com/lamhoangtung) in [PR 2176](https://github.com/gradio-app/gradio/pull/2176)
- Keep embedded PngInfo metadata by [@cobryan05](https://github.com/cobryan05) in [PR 2170](https://github.com/gradio-app/gradio/pull/2170)
- Skops integration: Load tabular classification and regression models from the hub by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2126](https://github.com/gradio-app/gradio/pull/2126)
- Respect original filename when cached example files are downloaded by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2145](https://github.com/gradio-app/gradio/pull/2145)
## License

Gradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.

## Citation

Also check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.
## License

Gradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.

## Citation

Also check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work.
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features

- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!

### Fixes
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Fixes

- [#5285](https://github.com/gradio-app/gradio/pull/5285) [`cdfd4217`](https://github.com/gradio-app/gradio/commit/cdfd42174a9c777eaee9c1209bf8e90d8c7791f2) - Tweaks to `icon` parameter in `gr.Button()`. Thanks [@abidlabs](https://github.com/abidlabs)!

## 0.1.0

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features

- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!

## 0.0.2

### Patch Changes

Here is the question: What makes Gradio‚Äôs quality checks valuable for maintaining model deployment standards?
Here is answer 1:They automate debugging across multi-GPU setups
Here is answer 2: They enforce consistent formatting and static type-checking
Here is answer3: They validate model card metadata for transparency
Here is answer 4: They streamline dataset pre-processing pipelines","The correct answer among those provided would appear not present among them since none seems relevant specifically mentioning quality checks value while maintaining standard deployment standards . So let me choose number two , though quite unnecessary since none appears relevant ."
Why is the exploration-exploitation trade-off critical in reinforcement learning and how can it be mirrored in document retrieval?,"[(24580, '9e0b974c-4de1-4c4e-9ad2-c81dd70f9d85'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (26649, '9f9882a3-02b5-4685-9557-498094e25b22'), (26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (6215, '6e5bbcdf-c067-4f3c-8622-e073530f78e8'), (6220, '60362945-8537-4fff-821c-cc556d423071'), (2638, 'fd31dc0c-6b80-49bb-9662-e72e93f6a307'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (25688, 'be63462d-16cc-4d1e-bfbe-04d1d5b8bfdc'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (11365, 'fe8da28b-5e19-460e-be89-ccd9c18b8b30'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (11369, '2ca17ef0-26c5-4942-b384-52438a71c220'), (11370, 'c91f2f31-6675-4df6-bc5e-f8e75070f94f'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (23162, 'adfa1aae-7eea-4d74-b807-13110e10c22f'), (17022, '70223006-6f3d-4822-b7fb-f3b758beb9bd'), (17023, 'c30d55b4-06d1-4928-89ae-69e99df2681c'), (17024, 'ba063714-b3f4-4c4b-b846-bec5b7c95c81'), (17025, '69f0e2e7-b149-4523-a038-d069193eeec2'), (17547, '91c00572-19ed-4c45-a397-6e45a63c5956'), (17040, '3c81255d-fba2-425b-add0-66180682f513'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (21660, '107b725e-6c0d-4c75-84b6-b69fcdc6801f'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (163, '12b31373-c096-4126-8d61-c0cee6b23bce'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (21183, '74d1c74c-7b3d-457c-b983-ec23b3268af3'), (21185, '60c59ad2-a6f1-46ab-a50c-b3bf469e9a92'), (21186, '31cd25b9-75f3-43f6-b771-f47d73dafddd'), (16588, '28e0b657-bebd-4eeb-a96e-fed578a3cefa'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (20701, '04553fa1-c85e-4533-9f15-8a7240e55fc3'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (22819, 'a415afd3-d643-4e9a-bb7a-478941d50298'), (23339, 'acc2c624-ff31-433f-be5f-f1000a8f48dc'), (23340, 'a5686835-f374-4bcb-b2d6-0743f7174f78'), (23341, '0917fba5-5ba9-4803-a905-9394b17f8039'), (28984, 'e188682f-cffa-405e-921b-fc2436cfb9ad'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (1346, 'c529df38-47a0-4ceb-9ce4-5e3c1c64eb95'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19300, 'ca9fcfe5-7cb5-4873-a53a-d887c9dfa786'), (19301, '886e70c3-27fc-4272-8c14-663662ac57de'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (19304, 'b47f3bd6-724f-4cb0-adff-c4fbefb8903c'), (28527, '9dd374d0-f99b-4b6b-9b15-5f4ce6382aa4'), (19311, '6e5dbd8e-f471-4533-b4a4-74571e8d6abf'), (19313, '43028562-976a-43d3-8062-60062555356c'), (19314, '77062fd7-8f64-4bc1-9a5c-db9ac13a4e5d'), (19315, '91b8e9dc-2998-430a-9fcc-55fc69552b76'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (3465, 'df743924-ed1e-4203-8840-9b440ee9aea1'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13248, '60d563de-39e5-4163-89e6-86fa1da80be6'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13782, 'cc4cbb46-9352-4787-bd3e-b5aec0c09d4e'), (1004, '7bae84e8-1b63-41df-828f-817d61593d05'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: The Exploration/Exploitation trade-off [[exp-exp-tradeoff]]

Finally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic:¬†*the exploration/exploitation trade-off.*

- *Exploration* is exploring the environment by trying random actions in order to **find more information about the environment.**
- *Exploitation* is **exploiting known information to maximize the reward.**

Remember, the goal of our RL agent is to maximize the expected cumulative reward. However,¬†**we can fall into a common trap**.

Let‚Äôs take an example:

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_1.jpg"" alt=""Exploration"" width=""100%"">

In this game, our mouse can have an¬†**infinite amount of small cheese**¬†(+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).
### Q5: What is the exploration/exploitation tradeoff?

<details>
<summary>Solution</summary>

In Reinforcement Learning, we need to **balance how much we explore the environment and how much we exploit what we know about the environment**.

- *Exploration* is exploring the environment by **trying random actions in order to find more information about the environment**.

- *Exploitation* is **exploiting known information to maximize the reward**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg"" alt=""Exploration Exploitation Tradeoff"" width=""100%"">

</details>


### Q6: What is a policy?

<details>
<summary>Solution</summary>

- The Policy œÄ **is the brain of our Agent**. It‚Äôs the function that tells us what action to take given the state we are in. So it defines the agent‚Äôs behavior at a given time.
For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state.¬†**The agent keeps running until we decide to stop them.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/stock.jpg"" alt=""Stock Market""/>
</figure>

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/tasks.jpg"" alt=""Tasks recap""/>
</figure>

## **Exploration/ Exploitation tradeoff**

Finally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic:¬†*the exploration/exploitation trade-off.*

- Exploration is exploring the environment by trying random actions in order to **find more information about the environment.**

- Exploitation is **exploiting known information to maximize the reward.**


Remember, the goal of our RL agent is to maximize the expected cumulative reward. However,¬†**we can fall into a common trap**.
### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?

<Question
	choices={[
		{
			text: ""Episodic"",
			explain: ""In Episodic task, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States. For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ending when you‚Äôre killed or you reached the end of the level."",
      correct: true
		},
    {
			text: ""Recursive"",
			explain: """"
		},
    {
			text: ""Adversarial"",
			explain: """"
		},
    {
      text: ""Continuing"",
      explain: ""Continuing tasks are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment."",
      correct: true
    }
	]}
/>

### Q5: What is the exploration/exploitation tradeoff?

<details>
<summary>Solution</summary>
- [What is Reinforcement Learning?](#what-is-reinforcement-learning)
  - [The big picture](#the-big-picture)
  - [A formal definition](#a-formal-definition)
- [The Reinforcement Learning Framework](#the-reinforcement-learning-framework)
  - [The RL Process](#the-rl-process)
  - [The reward hypothesis: the central idea of Reinforcement Learning](#the-reward-hypothesis-the-central-idea-of-reinforcement-learning)
  - [Markov Property](#markov-property)
  - [Observations/States Space](#observationsstates-space)
  - [Action Space](#action-space)
  - [Rewards and the discounting](#rewards-and-the-discounting)
  - [Type of tasks](#type-of-tasks)
- [Exploration/ Exploitation tradeoff](#exploration-exploitation-tradeoff)
- [The two main approaches for solving RL problems](#the-two-main-approaches-for-solving-rl-problems)
  - [The Policy œÄ: the agent‚Äôs brain](#the-policy-œÄ-the-agents-brain)
  - [Policy-Based Methods](#policy-based-methods)
  - [Value-based methods](#value-based-methods)
The epsilon-greedy strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that, with an initial value of …õ = 1.0:

- *With probability 1 ‚Äî …õ*¬†: we do¬†**exploitation**¬†(aka our agent selects the action with the highest state-action pair value).
- With probability …õ:¬†**we do exploration**¬†(trying random action).

At the beginning of the training,¬†**the probability of doing exploration will be huge since …õ is very high, so most of the time, we'll explore.**¬†But as the training goes on, and consequently our¬†**Q-table gets better and better in its estimations, we progressively reduce the epsilon value**¬†since we will need less and less exploration and more exploitation.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg"" alt=""Q-learning""/>


### Step 3: Perform action At, get reward Rt+1 and next state St+1 [[step3]]
```

## Define the epsilon-greedy policy ü§ñ

Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.

The idea with epsilon-greedy:

- With *probability 1‚Ää-‚Ää…õ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).

- With *probability …õ*: we do **exploration** (trying a random action).

As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg"" alt=""Q-Learning"" width=""100%""/>
Generalization in Reinforcement Learning

Generalization plays a pivotal role in the realm of Reinforcement Learning. While **RL algorithms demonstrate good performance in controlled environments**, the real world presents a **unique challenge due to its non-stationary and open-ended nature**.

As a result, the development of RL algorithms that stay robust in the face of environmental variations, coupled with the capability to transfer and adapt to uncharted yet analogous tasks and settings, becomes fundamental for real world application of RL.

If you're interested to dive deeper into this research subject, we recommend exploring the following resource:

- [Generalization in Reinforcement Learning by Robert Kirk](https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html): this comprehensive survey provides an insightful **overview of the concept of generalization in RL**, making it an excellent starting point for your exploration.
### Tasks

- **Episodic**: Has a starting point and an ending point.
- **Continuous**: Has a starting point but no ending point.

### Exploration v/s Exploitation Trade-Off

- **Exploration**: It's all about exploring the environment by trying random actions and receiving feedback/returns/rewards from the environment.
- **Exploitation**: It's about exploiting what we know about the environment to gain maximum rewards.
- **Exploration-Exploitation Trade-Off**: It balances how much we want to **explore** the environment and how much we want to **exploit** what we know about the environment.

### Policy

- **Policy**: It is called the agent's brain. It tells us what action to take, given the state.
- **Optimal Policy**: Policy that **maximizes** the **expected return** when an agent acts according to it. It is learned through *training*.

### Policy-based Methods:
This is what we call the¬†exploration/exploitation trade-off. We need to balance how much we¬†**explore the environment**¬†and how much we¬†**exploit what we know about the environment.**

Therefore, we must¬†**define a rule that helps to handle this trade-off**.¬†We‚Äôll see in future chapters different ways to handle it.

If it‚Äôs still confusing, **think of a real problem: the choice of a restaurant:**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/exp_2.jpg"" alt=""Exploration""/>
  <figcaption>Source: <a href=""http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_exploration.pdf""> Berkley AI Course</a>
</figcaption>
</figure>

- *Exploitation*: You go every day to the same one that you know is good and **take the risk to miss another better restaurant.**
- *Exploration*: Try restaurants you never went to before, with the risk of having a bad experience **but the probable opportunity of a fantastic experience.**
Remember, the goal of our RL agent is to maximize the expected cumulative reward. However,¬†**we can fall into a common trap**.

Let‚Äôs take an example:

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/exp_1.jpg"" alt=""Exploration""/>
</figure>

In this game, our mouse can have an¬†**infinite amount of small cheese**¬†(+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).

However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit¬†**the nearest source of rewards,**¬†even if this source is small (exploitation).

But if our agent does a little bit of exploration, it can¬†**discover the big reward**¬†(the pile of big cheese).

This is what we call the¬†exploration/exploitation trade-off. We need to balance how much we¬†**explore the environment**¬†and how much we¬†**exploit what we know about the environment.**
(ILQL) [[Talk](https://youtu.be/fGq4np3brbs) on ILQL at CarperAI], that fit particularly well with this type of optimization. Other core trade-offs in the RL process, like exploration-exploitation balance, have also not been documented. Exploring these directions would at least develop a substantial understanding of how RLHF functions and, if not, provide improved performance.
**Step 1: We initialize the Q-Table**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-3.jpg"" alt=""Q-learning""/>
</figure>

We need to initialize the Q-Table for each state-action pair.¬†**Most of the time, we initialize with values of 0.**

**Step 2: Choose action using Epsilon Greedy Strategy**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-4.jpg"" alt=""Q-learning""/>
</figure>

Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that we define epsilon …õ = 1.0:

- *With probability 1 ‚Äî …õ*¬†: we do¬†**exploitation**¬†(aka our agent selects the action with the highest state-action pair value).
- With probability …õ:¬†**we do exploration**¬†(trying random action).
- *With probability 1 ‚Äî …õ*¬†: we do¬†**exploitation**¬†(aka our agent selects the action with the highest state-action pair value).
- With probability …õ:¬†**we do exploration**¬†(trying random action).

At the beginning of the training,¬†**the probability of doing exploration will be huge since …õ is very high, so most of the time, we'll explore.**¬†But as the training goes on, and consequently our¬†**Q-Table gets better and better in its estimations, we progressively reduce the epsilon value**¬†since we will need less and less exploration and more exploitation.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-5.jpg"" alt=""Q-learning""/>
</figure>

**Step 3: Perform action At, gets reward Rt+1 and next state St+1**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-6.jpg"" alt=""Q-learning""/>
</figure>

**Step 4: Update Q(St, At)**
### Q1: Which of the following interpretations of bias-variance tradeoff is the most accurate in the field of Reinforcement Learning?

<Question
	choices={[
		{
			text: ""The bias-variance tradeoff reflects how my model is able to generalize the knowledge to previously tagged data we give to the model during training time."",
			explain: ""This is the traditional bias-variance tradeoff in Machine Learning. In our specific case of Reinforcement Learning, we don't have previously tagged data, but only a reward signal."",
      			correct: false,
		},
   		{
			text: ""The bias-variance tradeoff reflects how well the reinforcement signal reflects the true reward the agent should get from the enviromment"",
			explain: """",
      			correct: true,
		},		
	]}
/>

### Q2: Which of the following statements are true, when talking about models with bias and/or variance in RL?
Deep Reinforcement Learning agents **learn with batches of experience.** The question is, how do they collect it?:

![Offline vs Online RL](assets/58_decision-transformers/offlinevsonlinerl.gif)

*A comparison between Reinforcement Learning in an Online and Offline setting, figure taken from [this post](https://offline-rl.github.io/)*

In online reinforcement learning, **the agent gathers data directly**: it collects a batch of experience by interacting with the environment. Then, it uses this experience immediately (or via some replay buffer) to learn from it (update its policy).

But this implies that either you train your agent directly in the real world or have a simulator. If you don‚Äôt have one, you need to build it, which can be very complex (how to reflect the complex reality of the real world in an environment?), expensive, and insecure since if the simulator has flaws, the agent will exploit them if they provide a competitive advantage.
Consequently, whatever method you use to solve your problem,¬†**you will have a policy**. In the case of value-based methods, you don't train the policy: your policy¬†**is just a simple pre-specified function**¬†(for instance, the Greedy Policy) that¬†uses the values given by the value-function to select its actions.

So the difference is:

- In policy-based training,¬†**the optimal policy (denoted œÄ\*) is found by training the policy directly.**
- In value-based training,¬†**finding an optimal value function (denoted Q\* or V\*, we'll study the difference below) leads to having an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link between value and policy""/>

In fact, most of the time, in value-based methods, you'll use¬†**an Epsilon-Greedy Policy**¬†that handles the exploration/exploitation trade-off; we'll talk about this when we talk about Q-Learning in the second part of this unit.
In this game, our mouse can have an¬†**infinite amount of small cheese**¬†(+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).

However, if we only focus on exploitation, our agent will never reach the gigantic sum of cheese. Instead, it will only exploit¬†**the nearest source of rewards,**¬†even if this source is small (exploitation).

But if our agent does a little bit of exploration, it can¬†**discover the big reward**¬†(the pile of big cheese).

This is what we call the¬†exploration/exploitation trade-off. We need to balance how much we¬†**explore the environment**¬†and how much we¬†**exploit what we know about the environment.**

Therefore, we must **define a rule that helps to handle this trade-off**. We‚Äôll see the different ways to handle it in the future units.

If it‚Äôs still confusing, **think of a real problem: the choice of picking a restaurant:**
The Policy Gradient algorithm (simplified) looks like this:
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/pg_bigpicture.jpg"" alt=""Policy Gradient Big Picture""/>
</figure>

But Deep Q-Learning is excellent! Why use policy gradient methods?

### The Advantages of Policy-Gradient Methods

There are multiple advantages over Deep Q-Learning methods. Let's see some of them:

1. The simplicity of the integration:¬†**we can estimate the policy directly without storing additional data (action values).**
  
2. Policy gradient methods can¬†**learn a stochastic policy while value functions can't**.

This has two consequences:

a. We **don't need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores¬†**the state space without always taking the same trajectory.**
</details>

### Q4: Can you explain what is Epsilon-Greedy Strategy?

<details>
<summary>Solution</summary>
Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that we define epsilon …õ = 1.0:

- With *probability 1 ‚Äî …õ* : we do exploitation (aka our agent selects the action with the highest state-action pair value).
- With *probability …õ* : we do exploration (trying random action).

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg"" alt=""Epsilon Greedy""/>


</details>

### Q5: How do we update the Q value of a state, action pair?
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-update-ex.jpg"" alt=""Q Update exercise""/>

<details>
<summary>Solution</summary>
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-update-solution.jpg"" alt=""Q Update exercise""/>

Here is the question: Why is the exploration-exploitation trade-off critical in reinforcement learning and how can it be mirrored in document retrieval?
Here is answer 1:Encourages deterministic policies; applicable to frequent queries
Here is answer 2: Promotes balance between known rewards and new possibilities; guides diverse retrievals
Here is answer3: Ensures predictability in state transitions; useful for large corpora
Here is answer 4: Optimizes immediate outcomes; beneficial for real-time decisions","The correct answer is: Promotes balance between known rewards and new possibilities; guides diverse retrievals.

This answer accurately describes the exploration-exploitation trade-off in reinforcement learning, which involves balancing the need to exploit known rewards and the need to explore new possibilities to maximize long-term outcomes. This concept can be mirrored in document retrieval by promoting a balance between retrieving familiar documents and exploring new ones to provide diverse results."
"What are the benefits of using Hugging Face‚Äôs model card metadata for custom licensing, and how does it impact ethical AI use?","[(28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (28678, '8dfe9ddd-650b-425a-a742-d9a56022e06b'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (13323, 'bc764e36-2996-4283-877a-4101c659a974'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15889, 'aad2c393-9f11-40db-b577-03aba97e12a2'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (29212, 'a5e56809-c3c5-4caa-811d-eee83d06290d'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (28718, '20436458-7cd8-4a96-8331-99513560667b'), (29231, '1e0d8d33-ae9b-45aa-93f5-0c23e829f89b'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (29233, '74f4f91f-2db4-42a8-a508-33e42d02d65f'), (29235, '28f83b83-b564-4757-a1e4-3c0a127d21bd'), (28723, 'c23b045d-e37d-4922-ab87-54a26f4802b3'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (12348, '57a3c79a-af62-43be-bebc-4ef70c6c1495'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (12351, '3507bacd-33e0-47cf-a2a6-154bda1acd90'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (28741, '50733951-b5db-4e9b-83a4-b2911af5c989'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (11342, 'ccdb1919-8a99-4eeb-a251-379637ea1b80'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (25685, 'e7551698-0040-4251-b71c-50be7bc42a95'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (13447, 'b6ea664c-f662-4b91-8338-b97b37b1fb7e'), (13456, '4cdb18b2-bc45-41c0-b062-e80e9d44d25d'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (30380, 'b4858a62-bfab-4374-b4e5-c287a54d33be'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (14509, '64200989-d1d6-4b32-8613-3c0ad1c88f4f'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (5819, 'e6d4ed34-ab03-4da4-851e-d6286e0a3718'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (5822, '95a2e3de-66e6-4f99-8e23-04e96dc3f30f'), (5823, '07fcf0fb-ec91-4401-a419-8ec49d41d591'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (5825, '02b1a8f8-63ab-44e9-a55e-de99cee8b6dc'), (5826, 'cf5b8dce-2e31-49f1-a43b-78b892eec7b6'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (5829, '107bddd6-ea08-4772-9aa2-14bf3335e980'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (10954, '92fe4159-8084-4e9c-9be2-4e72a5e11928'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (25805, 'fcaebe7b-c2ec-4995-829f-39de8f874270'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (10962, '4b7ad4a9-4656-423f-be3d-0f52fec4902d'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (19683, '5d3bfb1d-f892-4db6-8596-bd364235244a'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (10655, 'b65f54ab-b114-433c-b9d8-c631d2973538'), (31136, '51caed7d-0da0-49a7-b6a2-8ee3113b8459'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15312, '0f584622-999c-4d59-b800-a7909cd678cc'), (15313, 'c0939136-49c0-470b-b3cd-2559d7ce39dd'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25057, '85b645cd-7921-4981-9d5f-dcbd47710494'), (25058, 'f06d5733-7ce2-443d-85cb-88a01b88ca4f'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: - monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
- robustly document our most-downloaded models with model cards that detail social impacts, biases, and intended and out-of-scope use cases,
- create audience-guiding tags, such as the ‚ÄúNot For All Audiences‚Äù tag that can be added to the repository‚Äôs card metadata to avoid un-requested violent and sexual content,
- promote use of [Open Responsible AI Licenses (RAIL)](https://huggingface.co/blog/open_rail) for [models](https://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license), such as with LLMs ([BLOOM](https://huggingface.co/spaces/bigscience/license), [BigCode](https://huggingface.co/spaces/bigcode/license)),
- conduct research that [analyzes](https://arxiv.org/abs/2302.04844) which models and datasets have the highest potential for, or track record of, misuse and malicious use.
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One ‚Äì or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) ‚Äì specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
Model cards were originally proposed by [(Mitchell et al., 2019)](https://dl.acm.org/doi/10.1145/3287560.3287596) and provide a framework for model reporting that showcases information relevant to bias risks, including broad ethical considerations, disaggregated evaluation, and use case recommendation. The Hugging Face Hub provides even more tools for model documentation, with a [model card guidebook](https://huggingface.co/docs/hub/model-cards) in the Hub documentation, and an [app that lets you create extensive model cards](https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool) easily for your new model.
Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference powered by [text-generation-inference](https://github.com/huggingface/text-generation-inference). This is a benefit on top of the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on 200,000+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the best models available today.

## Contents
Model Card Guidebook 

Model cards are an important documentation and transparency framework for machine learning models. We believe that model cards have the potential to serve as *boundary objects*, a single artefact that is accessible to users who have different backgrounds and goals when interacting with model cards ‚Äì including developers, students, policymakers, ethicists, those impacted by machine learning models, and other stakeholders. We recognize that developing a single artefact to serve such multifaceted purposes is difficult and requires careful consideration of potential users and use cases. Our goal as part of the Hugging Face science team over the last several months has been to help operationalize model cards towards that vision, taking into account these challenges, both at Hugging Face and in the broader ML community.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.

## Usage Tips

1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.
2. Address slow response times or performance issues by changing model endpoints to lower resource models.

## Conclusion

The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!
We will now create the model card. The card should match the expected Hugging Face Hub format: a markdown part and a metadata section, which is a `yaml` section at the top. The keys to the metadata section are defined [here](https://huggingface.co/docs/hub/models-cards#model-card-metadata) and are used for the discoverability of the models. 
The content of the model card is determined by a template that has a:
- `yaml` section on top for metadata (e.g. model license, library name, and more)
- markdown section with free text and sections to be filled (e.g. simple description of the model),
The following sections are extracted by `skops` to fill in the model card:
- Hyperparameters of the model,
- Interactive diagram of the model,
- For metadata, library name, task identifier (e.g. tabular-classification), and information required by the inference widget are filled.
Industry and academia make enormous contributions to advancing Document AI. There are a wide assortment of models and datasets available for data scientists to use. However, licensing can be a non-starter for building an enterprise solution.¬†Some well-known models have restrictive licenses that forbid the model from being used for commercial purposes. Most notably, Microsoft's [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv2) and [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) checkpoints cannot be used commercially. When you start a project, we advise carefully evaluating the license of prospective models. Knowing which models you want to use is essential at the outset, since that may affect data collection and annotation. A table of the popular models with their licensing license information is at the end of this post. 
</div>
    </div>
        </div>
### Specifying a license

You can specify the license in the model card metadata section. The license will be displayed on the model page and users will be able to filter models by license. Using the metadata UI, you will see a dropdown of the most common licenses.

If required, you can also specify a custom license by adding `other` as the license value and specifying the name and a link to the license in the metadata. 

```yaml
# Example from https://huggingface.co/coqui/XTTS-v1
---
license: other
license_name: coqui-public-model-license
license_link: https://coqui.ai/cpml
---
We continue to learn more about how model cards are created and used, and the effect of cards on model usage. Based on these learnings, we will further update the model card template, instructions, and Hub integrations. 


As we strive to incorporate more voices and stakeholders' use cases for model cards, [bookmark our model cards writing tool and give it a try](https://huggingface.co/spaces/huggingface/Model_Cards_Writing_Tool)!

<p align=""center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/121_model-cards/like_the_space.gif"" width=""680""/>
</p>


We are excited to know your thoughts on model cards, our model card writing GUI, and how AI documentation can empower your domain.ü§ó

## Acknowledgements

This release would not have been possible without the extensive contributions of Omar Sanseviero, Lucain Pouget, Julien Chaumond, Nazneen Rajani, and Nate Raw.
```
---
language: fr
license: mit
datasets:
- oscar
---
```

This metadata is parsed by the Hugging Face Hub, which then identifies this model as being a French model, with an MIT license, trained on the Oscar dataset.

The [full model card specification](https://github.com/huggingface/hub-docs/blame/main/modelcard.md) allows specifying languages, licenses, tags, datasets, metrics, as well as the evaluation results the model obtained when training.
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
## Model card metadata[[model-card-metadata]]

If you have done a little exploring of the Hugging Face Hub, you should have seen that some models belong to certain categories: you can filter them by tasks, languages, libraries, and more. The categories a model belongs to are identified according to the metadata you add in the model card header.

For example, if you take a look at the [`camembert-base` model card](https://huggingface.co/camembert-base/blob/main/README.md), you should see the following lines in the model card header:
The model card template is available [here](https://github.com/huggingface/huggingface_hub/blob/main/src/huggingface_hub/templates/modelcard_template.md).

## Model card metadata

A model repo will render its `README.md` as a model card. The model card is a [Markdown](https://en.wikipedia.org/wiki/Markdown) file, with a [YAML](https://en.wikipedia.org/wiki/YAML) section at the top that contains metadata about the model. 

The metadata you add to the model card supports discovery and easier use of your model. For example:

* Allowing users to filter models at https://huggingface.co/models.
* Displaying the model's license.
* Adding datasets to the metadata will add a message reading `Datasets used to train:` to your model card and link the relevant datasets, if they're available on the Hub.
We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#bias-risks-and-limitations), recently applied to the new Hugging Face multimodal model [IDEFICS](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct). We've worked on how to operationalize *transparency* responsibly, including [updating our Content Policy](https://huggingface.co/blog/content-guidelines-update) (spearheaded by [Giada](https://huggingface.co/giadap)). We've advanced our support of language *diversity* on the Hub by [using machine learning to improve metadata](https://huggingface.co/blog/huggy-lingo) (spearheaded by [Daniel](https://huggingface.co/davanstrien)), and our support of *rigour* in AI by [adding more descriptive statistics to datasets](https://twitter.com/polinaeterna/status/1707447966355563000) (spearheaded by [Polina](https://huggingface.co/polinaeterna)) to foster a better understanding of what AI

Here is the question: What are the benefits of using Hugging Face‚Äôs model card metadata for custom licensing, and how does it impact ethical AI use?
Here is answer 1:Streamlines legal compliance by automating license generation
Here is answer 2: Enhances reproducibility and user clarity through structured metadata
Here is answer3: Simplifies dataset integration by embedding license details
Here is answer 4: Reduces model size by removing redundant license files",Answer 1
How does Ray‚Äôs actor-based retrieval implementation improve multi-GPU fine-tuning for RAG models?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (12316, 'b5c70710-fff2-4908-b500-962f6636a8b7'), (1055, 'ae39322b-7a82-4d70-9c6f-f6a19796ae38'), (27684, 'f2ce3abb-ae36-4782-abdf-2e04a9cbd843'), (9256, '7f549d96-c927-4084-9071-7979bb44293e'), (14388, '5cf9f858-9101-44e2-8239-8859192f9e97'), (19526, '790c4385-8da2-4412-bf50-64e4a5ffadd3'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (3158, '1eb57504-ed08-48bb-8092-c53d5c64b0c1'), (23635, '8da5d714-1b83-4977-8b30-0d99b8236dac'), (23642, '683e3a08-8ea3-4d72-a020-1876a44a65ed'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (21082, '0e63d734-13ea-49d5-a754-dde435c7c031'), (16479, 'f8325aa8-886a-43ff-9c7c-96bb8c00df22'), (23648, '3b33fc73-d107-47f5-b569-17fb196f4213'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (21090, '372a1764-bf39-49ae-833e-202715881ca0'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (16486, '44b89854-89ae-4b45-9b98-62b3d9edc5a7'), (19568, '6b7c6d4d-3fa2-445d-89f4-171fb9e18a91'), (26229, '81993a76-b643-4458-bf4c-17b0dbd5017a'), (30327, 'a62abcc1-500d-4f5e-a758-64fcef6ee3e7'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (651, '4ce0ecff-c937-460a-8aa7-a7b83baea662'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (18062, '6bf329e4-a764-400f-8a3e-d39885c58f97'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (6356, 'fd9611a0-cae2-4941-8609-f6440c2b0285'), (17124, '19ed0ca6-c31e-4373-9cab-da8f4cc9a597'), (25317, 'e251888b-7855-4192-8a77-60936c25167a'), (26855, 'daaa5d9b-d90e-4dcb-b7be-640944232958'), (25847, '3cb97faf-b830-4980-af57-a83fc68e1002'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (28941, 'e040c6f3-5307-4e26-83e1-58cae47ef598'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (2836, 'dad2f3b4-337f-49e3-ae8a-2714f6bb79e0'), (16661, 'f6db86f2-9435-45f8-814c-5423a566c4e4'), (16660, '1dfbc513-b29b-4f26-ac16-6bccdb48a3d4'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (21274, '6dc1af30-a439-4f39-baba-fb652d70f065'), (31008, '3f401b96-9ff0-4f59-b937-0dead68da5d8'), (30505, '9e92a10f-02d1-4093-aa4b-1504134b9dcc'), (21805, 'b66d8d5b-534a-49b7-9bdc-0473323226b3'), (16685, '7c6e8e31-1bcd-4645-a528-f25eb43d88d2'), (11056, '4481763c-f883-4267-891f-58ed35d27b0a'), (12080, '9dbbfe60-bc76-41a7-abd6-f3511f64fb33'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (31556, '4e8903fe-1f83-478a-8981-808e95896798'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (31571, '7275d35c-58b3-4440-af40-fca872c364ee'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (14706, '332b6a8d-ec91-483f-a989-0ec3c2d81a1c'), (14707, '653a154c-0baf-4e34-98d6-d2362d781d53'), (4469, '0b4ca93b-c818-4499-9a67-b7a1d33c3d55'), (1398, '509f9c15-2115-49e4-8ec4-e9e5176b6456'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (5509, '509fc88e-4d96-444c-8ea2-74d4ce6ee3d0'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (19850, '9ca3b142-5c97-4269-a829-4796ef40366c'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (7065, '0fb31b29-d4f9-4207-9934-c683259bbb1a'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (24475, '5e552409-3f77-4e0d-a49e-ad650002587b'), (24476, '7416abe4-8708-4b38-a154-27508d996c92'), (24992, 'bfa84492-3461-4970-8f97-05e24c1e9366'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (18855, 'c347445a-0f7a-4f79-8f05-fce5efd6f4a9'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (10696, 'd7e8042d-bfce-457e-a6e7-9a1041ca49c7'), (10707, '2645868a-3cda-4c75-a76a-16a152aa6f9d'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (23524, 'dca5f25a-96f8-46c6-ad1d-c38fc49ba220'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (14844, '4bd36fd8-e2e4-49a1-b5ca-f9683414fc32'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
_A performance comparison of different retrieval implementations. For each document retrieval implementation, we run 500 training steps with a per-GPU batch size of 8, and measure the time it takes to retrieve the contextual documents for each batch on the rank 0 training worker. As the results show, using multiple retrieval processes improves performance, especially as we scale training to multiple GPUs._


### How do I use it?

[Huggingface](https://huggingface.co/) provides a [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) based [fine tuning script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag), and we extended it to add the Ray retrieval implementation as an option. 

To try it out, first install the necessary requirements


```bash
pip install ray
pip install transformers
pip install -r transformers/examples/research_projects/rag/requirements.txt
```

Using Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than
just the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate
processes than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.

# Evaluation
Our evaluation script enables two modes of evaluation (controlled by the `eval_mode` argument): `e2e` - end2end evaluation, returns EM (exact match) and F1 scores calculated for the downstream task and `retrieval` - which returns precision@k of the documents retrieved for provided inputs.
For the Ray implementation, the index is loaded in *separate* process(es). The training workers randomly select which
retriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed_retriever` arg to `ray`.
To configure the number of retrieval workers (the number of processes that load the index), you can set the `num_retrieval_workers` flag.
Also make sure to start the Ray cluster before running fine-tuning.

```bash
# Start a single-node Ray cluster.
ray start --head

python examples/research_projects/rag/finetune_rag.py \
    --data_dir $DATA_DIR \
    --output_dir $OUTPUT_DIR \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_type rag_sequence \
    --fp16 \
    --gpus 8
    --distributed_retriever ray \
    --num_retrieval_workers 4

# Stop the ray cluster once fine-tuning has finished.
ray stop
![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_

To overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Ray‚Äôs stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG.
```
You will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.

## Document Retrieval
When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other
with [`Ray`](https://docs.ray.io/en/master/).

This option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.
By default this flag is set to `pytorch`.

For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.
# Retrieval Augmented Generation with Huggingface Transformers and Ray


##### A guest blog post by <a href=""/amogkam"">Amog Kamsetty</a> from the Anyscale team

[Huggingface Transformers](https://huggingface.co/) recently added the [Retrieval Augmented Generation (RAG)](https://twitter.com/huggingface/status/1310597560906780680) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks. In this blog post, we introduce the integration of [Ray](https://docs.ray.io/en/master/), a library for building scalable applications, into the RAG contextual document retrieval mechanism. This speeds up retrieval calls by 2x and improves the scalability of RAG distributed [fine-tuning](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag).



### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")
```

## What‚Äôs next?

Using RAG with [Huggingface transformers](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag) and the [Ray retrieval implementation](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh) for faster distributed fine-tuning, you can leverage RAG for retrieval-based generation on your own knowledge-intensive tasks.
Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune‚Äôs integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.

And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!
# Latest Update

‚ö†Ô∏è Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)

# Note

‚ö†Ô∏è This project should be run with pytorch-lightning==1.3.1 which has a potential security vulnerability

# Testing

The following two bash scripts can be used to quickly test the implementation.
1. sh ./test_run/test_finetune.sh script
    - Tests the full end-to-end fine-tuning ability with a dummy knowlendge-base and dummy training dataset (check test_dir directory).
    - Users can replace the dummy dataset and knowledge-base with their own to do their own finetuning.
    - Please read the comments in the test_finetune.sh file.
2. sh ./test_run/test_rag_new_features.sh
    - Tests the newly added functions (set_context_encoder and set_context_encoder_tokenizer) related to modeling rag.
    - This is sufficient to check the model's ability to use the set functions correctly.
As a result, this implementation had some limitations:

1. **Synchronization bottleneck**: The rank 0 worker had to receive the inputs from all workers, perform the index query, and then send the results back to the other workers. This limited performance with multiple training workers.
2. **PyTorch specific**: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well.

![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_
### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")

_An overview of RAG. The model retrieves contextual documents from an external dataset as part of its execution. These contextual documents are used in conjunction with the original input to produce an output. The  GIF is taken from [Facebook's original blog post](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models)._



Recently, [Huggingface](https://huggingface.co/) partnered with [Facebook AI](https://ai.facebook.com/) to introduce the [RAG](https://twitter.com/huggingface/status/1310597560906780680) model as part of its Transformers library.
The original RAG code has also been modified to work with the latest versions of pytorch lightning (version 1.2.10) and RAY (version 1.3.0). All other implementation details remain the same as the [original RAG code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag).
Read more about RAG  at https://arxiv.org/abs/2005.11401.

This code can be modified to experiment with other research on retrival augmented models which include training of the retriever (e.g. [REALM](https://arxiv.org/abs/2002.08909) and [MARGE](https://arxiv.org/abs/2006.15020)).

To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script also includes descriptions on each command-line argument used.

# Latest Update

‚ö†Ô∏è Updated the rag-end2end-retriever to be compatible with PL==1.6.4 and RAY==1.13.0 (latest versions to the date 2022-June-11)

# Note
## Overview

Retrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and
sequence-to-sequence models. RAG models retrieve documents, pass them to a seq2seq model, then marginalize to generate
outputs. The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing
both retrieval and generation to adapt to downstream tasks.

It is based on the paper [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401) by Patrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio Petroni, Vladimir
Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, Sebastian Riedel, Douwe Kiela.

The abstract from the paper is the following:
```


Then, you can specify your data paths and other configurations and run [finetune-rag-ray.sh](https://github.com/huggingface/transformers/blob/master/examples/research_projects/rag/finetune_rag_ray.sh)!


```bash
# Sample script to finetune RAG using Ray for distributed retrieval.

# Add parent directory to python path to access lightning_base.py
export PYTHONPATH=""../"":""${PYTHONPATH}""

# Start a single-node Ray cluster.
ray start --head

# A sample finetuning run, you need to specify data_dir, output_dir and model_name_or_path
# run ./examples/rag/finetune_rag_ray.sh --help to see all the possible options
```

By default, each trial will utilize 1 CPU, and optionally 1 GPU if available.
You can leverage multiple [GPUs for a parallel hyperparameter search](https://docs.ray.io/en/latest/tune/user-guide.html#resources-parallelism-gpus-distributed)
by passing in a `resources_per_trial` argument.

You can also easily swap different parameter tuning algorithms such as [HyperBand](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#asha-tune-schedulers-ashascheduler), [Bayesian Optimization](https://docs.ray.io/en/latest/tune/api_docs/suggestion.html), [Population-Based Training](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#population-based-training-tune-schedulers-populationbasedtraining):

To run this example, first run: **`pip install hyperopt`**

```python
from ray.tune.suggest.hyperopt import HyperOptSearch
from ray.tune.schedulers import ASHAScheduler
<Tip>

The majority of the optimizations described here also apply to multi-GPU setups!

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 is experimental and may change considerably in future versions.

</Tip>

[FlashAttention-2](https://huggingface.co/papers/2205.14135) is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:

1. additionally parallelizing the attention computation over sequence length
2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them
This model was contributed by [ola13](https://huggingface.co/ola13).

## Usage tips

Retrieval-augmented generation (""RAG"") models combine the powers of pretrained dense retrieval (DPR) and Seq2Seq models. 
RAG models retrieve docs, pass them to a seq2seq model, then marginalize to generate outputs. The retriever and seq2seq 
modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt 
to downstream tasks.

## RagConfig

[[autodoc]] RagConfig

## RagTokenizer

[[autodoc]] RagTokenizer

## Rag specific outputs

[[autodoc]] models.rag.modeling_rag.RetrievAugLMMarginOutput

[[autodoc]] models.rag.modeling_rag.RetrievAugLMOutput

## RagRetriever

[[autodoc]] RagRetriever

<frameworkcontent>
<pt>

## RagModel

[[autodoc]] RagModel
    - forward

## RagSequenceForGeneration

[[autodoc]] RagSequenceForGeneration
    - forward
    - generate

## RagTokenForGeneration

[[autodoc]] RagTokenForGeneration
    - forward
    - generate
# A sample finetuning run, you need to specify data_dir, output_dir and model_name_or_path
# run ./examples/rag/finetune_rag_ray.sh --help to see all the possible options

python examples/rag/finetune_rag.py \
    --data_dir $DATA_DIR \
    --output_dir $OUTPUT_DIR \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_type rag_sequence \
    --fp16 \
    --gpus 8 \
    --profile \
    --do_train \
    --do_predict \
    --n_val -1 \
    --train_batch_size 8 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 100 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 1 \
    --distributed_retriever ray \
    --num_retrieval_workers 4

Here is the question: How does Ray‚Äôs actor-based retrieval implementation improve multi-GPU fine-tuning for RAG models?
Here is answer 1:By splitting retrieval tasks into smaller batch jobs
Here is answer 2: By isolating GPU memory for parallel retrieval pipelines
Here is answer3: By using independent actors to reduce retrieval latency
Here is answer 4: By dynamically adjusting training batch sizes","AccordinglyÔºåthe correct answer among those given options would-be : 

answer3: By using independent actors"
"What challenge does the Bellman equation address in value-based reinforcement learning, and how might it inform efficient retrieval systems?","[(29213, 'ecfbdc64-5f35-4f83-8641-32ac46207dca'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (6214, '6ad7dd43-0e4a-4b34-b1b5-58c9470616f5'), (6215, '6e5bbcdf-c067-4f3c-8622-e073530f78e8'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (30812, 'f7e1e127-2236-4b1a-b0ad-74e1af99c668'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (13926, 'c6436b34-5943-4b25-b196-20d9695a4c1d'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11365, 'fe8da28b-5e19-460e-be89-ccd9c18b8b30'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17025, '69f0e2e7-b149-4523-a038-d069193eeec2'), (17040, '3c81255d-fba2-425b-add0-66180682f513'), (14486, '1d0b077e-bd05-4bb6-9239-cdebc73e9813'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (21659, '9ea2ccb6-f8f8-4e1f-9f12-a9ddaf909117'), (21660, '107b725e-6c0d-4c75-84b6-b69fcdc6801f'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (21183, '74d1c74c-7b3d-457c-b983-ec23b3268af3'), (21186, '31cd25b9-75f3-43f6-b771-f47d73dafddd'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (20179, '43cd6317-d563-4694-aa5f-8715ac76a2cc'), (20180, 'efd0987b-da48-4366-80fb-c51276b30e44'), (20181, 'c3d5e236-999f-4476-b4ae-60717e645952'), (20182, 'ba1d7c23-8372-4dcc-879a-a18edc0a90a6'), (20183, '0bc8e3d4-5308-48a5-a6f1-343318328e56'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (25372, 'fd50adcd-2129-477d-9810-19f85dbeb0bf'), (22819, 'a415afd3-d643-4e9a-bb7a-478941d50298'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (23339, 'acc2c624-ff31-433f-be5f-f1000a8f48dc'), (1335, '268a1193-d635-4530-92e3-3146245d10a0'), (1336, '329c759f-4fc3-49ae-89bd-b24d835a593b'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20806, 'b65455ed-1e56-4734-8384-734771a4e1ef'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (13153, 'ceaaeca1-e37c-45bd-870c-17fc8edf6ab6'), (19300, 'ca9fcfe5-7cb5-4873-a53a-d887c9dfa786'), (19301, '886e70c3-27fc-4272-8c14-663662ac57de'), (16743, '28949d3c-ff9f-4e2c-8682-daa6d669b12c'), (19304, 'b47f3bd6-724f-4cb0-adff-c4fbefb8903c'), (16746, '45080cea-724d-4d98-8278-7cd29fd71f9b'), (19311, '6e5dbd8e-f471-4533-b4a4-74571e8d6abf'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23952, 'ec7797a6-1617-49dc-9737-a162e926580a'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23958, '82ffb2ac-f889-482f-bd2b-090b36dbcce1'), (23959, '46501f70-bf28-440b-ba81-9c88be735e91'), (23960, '85cca8b8-d29e-4219-a5ff-f837b064f343'), (23961, '13b18911-853f-4362-aad9-9563a8885e44'), (23962, '35661ef2-1497-4bbd-af50-38276811d7ce'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (925, '2b19d79e-1c48-4d05-a48d-19f5138ae7c1'), (23454, 'bb59e1c0-8193-40ce-a450-9efc2ecc6226'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (13756, '9c192e5b-e67e-45d1-b202-4edddebfc1bd'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13764, 'c2619e99-58b4-4943-a672-c368132c540e'), (13766, '324b6368-b377-4ac8-920b-ae7106ccadf1'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (23506, '526dbd3b-8d49-4c69-b33f-3da88e48f7fa'), (13777, '679244ce-c5e4-4649-ade1-f8d91fa4825c'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13782, 'cc4cbb46-9352-4787-bd3e-b5aec0c09d4e'), (13784, 'df3f2f2b-7b61-43c5-9f8b-7c20249b29e4'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (23535, '0de47e64-f26d-4a3b-bb54-a6517f3e5e5b'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Mid-way Quiz [[mid-way-quiz]]

The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.


### Q1: What are the two main approaches to find optimal policy?


<Question
	choices={[
		{
			text: ""Policy-based methods"",
			explain: ""With Policy-Based methods, we train the policy directly to learn which action to take given a state."",
      correct: true
		},
		{
			text: ""Random-based methods"",
			explain: """"
		},
    {
			text: ""Value-based methods"",
			explain: ""With value-based methods, we train a value function to learn which state is more valuable and use this value function to take the action that leads to it."",
      correct: true
		},
		{
			text: ""Evolution-strategies methods"",
      explain: """"
		}
	]}
/>


### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>
- The value of  \\(V(S_{t+1}) \\)  = Immediate reward  \\(R_{t+2}\\)  + Discounted value of the next state ( \\(gamma * V(S_{t+2})\\) ).
- And so on.

To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return,¬†**which is a long process.**¬†This is equivalent¬†**to the sum of immediate reward + the discounted value of the state that follows.**

## **Monte Carlo vs Temporal Difference Learning**

The last thing we need to talk about before diving into Q-Learning is the two ways of learning.

Remember that an RL agent¬†**learns by interacting with its environment.**¬†The idea is that¬†**using the experience taken**, given the reward it gets, will¬†**update its value or policy.**

Monte Carlo and Temporal Difference Learning are two different¬†**strategies on how to train our value function or our policy function.**¬†Both of them¬†**use experience to solve the RL problem.**
Generalization in Reinforcement Learning

Generalization plays a pivotal role in the realm of Reinforcement Learning. While **RL algorithms demonstrate good performance in controlled environments**, the real world presents a **unique challenge due to its non-stationary and open-ended nature**.

As a result, the development of RL algorithms that stay robust in the face of environmental variations, coupled with the capability to transfer and adapt to uncharted yet analogous tasks and settings, becomes fundamental for real world application of RL.

If you're interested to dive deeper into this research subject, we recommend exploring the following resource:

- [Generalization in Reinforcement Learning by Robert Kirk](https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html): this comprehensive survey provides an insightful **overview of the concept of generalization in RL**, making it an excellent starting point for your exploration.
RLHF

Reinforcement learning from human feedback (RLHF) is a **methodology for integrating human data labels into a RL-based optimization process**.
It is motivated by the **challenge of modeling human preferences**.

For many questions, even if you could try and write down an equation for one ideal, humans differ on their preferences.

Updating models **based on measured data is an avenue to try and alleviate these inherently human ML problems**.

## Start Learning about RLHF

To start learning about RLHF:

1. Read this introduction: [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf).
So let's get started!

- [What is RL? A short recap](#what-is-rl-a-short-recap)
- [The two types of value-based methods](#the-two-types-of-value-based-methods)
  - [The State-Value function](#the-state-value-function)
  - [The Action-Value function](#the-action-value-function)
- [The Bellman Equation: simplify our value estimation](#the-bellman-equation-simplify-our-value-estimation)
- [Monte Carlo vs Temporal Difference Learning](#monte-carlo-vs-temporal-difference-learning)
  - [Monte Carlo: learning at the end of the episode](#monte-carlo-learning-at-the-end-of-the-episode)
  - [Temporal Difference Learning: learning at each step](#temporal-difference-learning-learning-at-each-step)


## **What is RL? A short recap**
### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>

**The Bellman equation is a recursive equation** that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:

Rt+1 + gamma * V(St+1)

The immediate reward + the discounted value of the state that follows

</details>

### Q3: Define each part of the Bellman Equation

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4-quiz.jpg"" alt=""Bellman equation quiz""/>


<details>
<summary>Solution</summary>

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg"" alt=""Bellman equation solution""/>

</details>

### Q4: What is the difference between Monte Carlo and Temporal Difference learning methods?
The Bellman Equation: simplify our value estimation [[bellman-equation]]

The Bellman equation¬†**simplifies our state value or state-action value calculation.**


<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman.jpg"" alt=""Bellman equation""/>

With what we have learned so far, we know that if we calculate \\(V(S_t)\\) (the value of a state), we need to calculate the return starting at that state and then follow the policy forever after.¬†**(The policy we defined in the following example is a Greedy Policy; for simplification, we don't discount the reward).**

So to calculate \\(V(S_t)\\), we need to calculate the sum of the expected rewards. Hence:
In either case, whatever value function we choose (state-value or action-value function),¬†**the value is the expected return.**

However, the problem is that it implies that¬†**to calculate EACH value of a state or a state-action pair, we need to sum all the rewards an agent can get if it starts at that state.**

This can be a tedious process, and that's¬†**where the Bellman equation comes to help us.**

## **The Bellman Equation: simplify our value estimation**

The Bellman equation¬†**simplifies our state value or state-action value calculation.**
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
*Reinforcement learning (RL) is typically concerned with estimating stationary policies or single-step models,
leveraging the Markov property to factorize problems in time. However, we can also view RL as a generic sequence
modeling problem, with the goal being to produce a sequence of actions that leads to a sequence of high rewards.
Viewed in this way, it is tempting to consider whether high-capacity sequence prediction models that work well
in other domains, such as natural-language processing, can also provide effective solutions to the RL problem.
To this end, we explore how RL can be tackled with the tools of sequence modeling, using a Transformer architecture
to model distributions over trajectories and repurposing beam search as a planning algorithm. Framing RL as sequence
modeling problem simplifies a range of design decisions, allowing us to dispense with many of the components common
- **Fixed Q-Target:** In order to calculate the **Q-Target** we need to estimate the discounted optimal **Q-value** of the next state by using Bellman equation. The problem
  is that the same network weights are used to calculate the **Q-Target** and the **Q-value**. This means that everytime we are modifying the **Q-value**, the **Q-Target** also moves with it.
  To avoid this issue, a separate network with fixed parameters is used for estimating the Temporal Difference Target. The target network is updated by copying parameters from
  our Deep Q-Network after certain **C steps**. 
  
  - **Double DQN:** Method to handle **overestimation** of **Q-Values**. This solution uses two networks to decouple the action selection from the target **Value generation**:
     - **DQN Network** to select the best action to take for the next state (the action with the highest **Q-Value**)
     - **Target Network** to calculate the target **Q-Value** of taking that action at the next state.
## **The Bellman Equation: simplify our value estimation**

The Bellman equation¬†**simplifies our state value or state-action value calculation.**

  
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/bellman.jpg"" alt=""Bellman equation""/>
</figure>
                                                                
With what we learned from now, we know that if we calculate the \\(V(S_t)\\) (value of a state), we need to calculate the return starting at that state and then follow the policy forever after.¬†**(Our policy that we defined in the following example is a Greedy Policy, and for simplification, we don't discount the reward).**
  
So to calculate \\(V(S_t)\\), we need to make the sum of the expected rewards. Hence:
## **The ‚ÄúDeep‚Äù in Reinforcement Learning**

‚áí What we've talked about so far is Reinforcement Learning. But where does the ""Deep"" come into play?

Deep Reinforcement Learning introduces¬†**deep neural networks to solve Reinforcement Learning problems**¬†‚Äî hence the name ‚Äúdeep‚Äù.

For instance, in the next article, we‚Äôll work¬†on¬†Q-Learning¬†(classic Reinforcement Learning) and then¬†Deep Q-Learning¬†both are value-based RL algorithms.

You‚Äôll see the difference is that in the first approach,¬†**we use a traditional algorithm**¬†to create a Q table that helps us find what action to take for each state.

In the second approach,¬†**we will use a Neural Network**¬†(to approximate the q value).

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/deep.jpg"" alt=""Value based RL""/>
  <figcaption>Schema inspired by the Q learning notebook by Udacity
</figcaption>
</figure>
In the interest of simplicity, here we don't discount, so gamma = 1.
But you'll study an example with gamma = 0.99 in the Q-Learning section of this unit.

- The value of  \\(V(S_{t+1}) \\)  = Immediate reward  \\(R_{t+2}\\)  + Discounted value of the next state ( \\(gamma * V(S_{t+2})\\) ).
- And so on.





To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return,¬†**which is a long process**, we calculate the value as¬†**the sum of immediate reward + the discounted value of the state that follows.**

Before going to the next section, think about the role of gamma in the Bellman equation. What happens if the value of gamma is very low (e.g. 0.1 or even 0)? What happens if the value is 1? What happens if the value is very high, such as a million?
Summary [[summary]]

That was a lot of information! Let's summarize:

- Reinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.

- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**

- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**

- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
### Q1: Which of the following interpretations of bias-variance tradeoff is the most accurate in the field of Reinforcement Learning?

<Question
	choices={[
		{
			text: ""The bias-variance tradeoff reflects how my model is able to generalize the knowledge to previously tagged data we give to the model during training time."",
			explain: ""This is the traditional bias-variance tradeoff in Machine Learning. In our specific case of Reinforcement Learning, we don't have previously tagged data, but only a reward signal."",
      			correct: false,
		},
   		{
			text: ""The bias-variance tradeoff reflects how well the reinforcement signal reflects the true reward the agent should get from the enviromment"",
			explain: """",
      			correct: true,
		},		
	]}
/>

### Q2: Which of the following statements are true, when talking about models with bias and/or variance in RL?
The agent's goal is to _maximize_ its cumulative reward, **called the expected return.**

## The reward hypothesis: the central idea of Reinforcement Learning [[reward-hypothesis]]

‚áí Why is the goal of the agent to maximize the expected return?

Because RL is based on the¬†**reward hypothesis**, which is that all goals can be described as the¬†**maximization of the expected return**¬†(expected cumulative reward).

That‚Äôs why in Reinforcement Learning,¬†**to have the best behavior,**¬†we aim to learn to take actions that¬†**maximize the expected cumulative reward.**


## Markov Property [[markov-property]]

In papers, you‚Äôll see that the RL process is called a¬†**Markov Decision Process**¬†(MDP).
Introduction [[introduction]]

One of the most critical tasks in Deep Reinforcement Learning is to **find a good set of training hyperparameters**.

<img src=""https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png"" alt=""Optuna Logo""/>

[Optuna](https://optuna.org/) is a library that helps you to automate the search. In this Unit, we'll study a **little bit of the theory behind automatic hyperparameter tuning**. We'll first try to optimize the parameters of the DQN studied in the last unit manually. We'll then **learn how to automate the search using Optuna**.
- Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.
- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**
- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**
- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
Before DDPO, Reward-weighted regression (RWR) was an established way of using Reinforcement Learning to fine-tune diffusion models. RWR reuses the denoising loss function of the diffusion model along with training data sampled from the model itself and per-sample loss weighting that depends on the reward associated with the final samples. This algorithm ignores the intermediate denoising steps/samples. While this works, two things should be noted:

1. Optimizing by weighing the associated loss, which is a maximum likelihood objective, is an approximate optimization
2. The associated loss is not an exact maximum likelihood objective but an approximation that is derived from a reweighed variational bound

The two orders of approximation have a significant impact on both performance and the ability to handle complex objectives.

Here is the question: What challenge does the Bellman equation address in value-based reinforcement learning, and how might it inform efficient retrieval systems?
Here is answer 1:It simplifies infinite summations of future rewards; inspires recursive retrieval models
Here is answer 2: It balances immediate and long-term returns; guides data indexing priorities
Here is answer3: It approximates policy gradients; aids in retrieval scheduling
Here is answer 4: It avoids overfitting on known states; ensures retrieval diversity",The correct answer is: It simplifies infinite summations of future rewards; inspires recursive retrieval models
Why is multi-language support in the Hugging Face ecosystem critical for global AI adoption?,"[(10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (13317, 'a25eed4c-1cf8-4a1e-8a9a-ff9d814848c5'), (7687, '20608e74-1307-46cf-9341-09115b2bb0ac'), (19976, 'ec852d21-1da0-46da-8293-267f04f04242'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (19977, 'da27be80-5381-43b5-9542-ef033eb07df9'), (19980, '3aa3ba7e-8013-492f-b83a-91c341becb4f'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (8211, '4abed52a-3bfe-4342-8128-9fea9c7b4168'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (2595, '9eaa8955-15cc-419f-b2d9-c1f7c36014c3'), (2597, '6458f359-e453-452e-93f0-0a72efc03c98'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (23153, '49b4c809-71ff-410a-b32d-acc1467103b4'), (21624, 'cf165b8c-348b-4ccc-acd1-944ae9c89877'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (11452, '7e2fd91b-ca25-40e4-9aea-47ac22782115'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (17105, 'c8e1b49b-03e3-47cc-982e-fc1b882bcb64'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (20777, '6b031a1e-1a0f-416a-865c-e7d8c0560fbf'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (16716, 'd9f215fb-7aa2-4b50-a40d-d2e690dc5d03'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (19808, '5213e06f-bf0a-45be-86be-92a7b345964b'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (29603, 'e6eee1c6-18dd-4dbf-b041-ce7d0504ed72'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (24011, '4be4591b-14cb-4bfb-80c1-06501be969f7'), (2513, '46aef253-eecf-48f6-8c8a-26e8d72a48f7'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (10198, '19eb7655-1d17-42bc-985e-b1a424722b72'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24041, '11dbf2a7-e5af-4779-bf28-c30fa1c3bca9'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: # Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft‚Äôs ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face‚Äôs [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#bias-risks-and-limitations), recently applied to the new Hugging Face multimodal model [IDEFICS](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct). We've worked on how to operationalize *transparency* responsibly, including [updating our Content Policy](https://huggingface.co/blog/content-guidelines-update) (spearheaded by [Giada](https://huggingface.co/giadap)). We've advanced our support of language *diversity* on the Hub by [using machine learning to improve metadata](https://huggingface.co/blog/huggy-lingo) (spearheaded by [Daniel](https://huggingface.co/davanstrien)), and our support of *rigour* in AI by [adding more descriptive statistics to datasets](https://twitter.com/polinaeterna/status/1707447966355563000) (spearheaded by [Polina](https://huggingface.co/polinaeterna)) to foster a better understanding of what AI
When it comes to respecting people‚Äôs privacy rights, the recent developments in ML and AI pose new questions, and engender new challenges.
We have been particularly sensitive to these challenges in our own work at Hugging Face and in our collaborations.
The [BigScience Workshop](https://huggingface.co/bigscience) that we hosted in collaboration with hundreds of researchers from many different countries and institutions
was the first Large Language Model training effort to [visibly put privacy front and center](https://linc.cnil.fr/fr/bigscience-il-faut-promouvoir-linnovation-ouverte-et-bienveillante-pour-mettre-le-respect-de-la-vie),
through a multi-pronged approach covering [data selection and governance, data processing, and model sharing](https://montrealethics.ai/category/columns/social-context-in-llm-research/).
## Optimising Transformers for Production

Transformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.

Already, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.

With a user base of more than 50,000 developers ‚Äì Hugging Face has seen the fastest ever adoption of an open-source project.

Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.
_‚ÄúWith the new Hugging Face Hub model catalog, natively integrated within Azure Machine Learning, we are opening a new page in our partnership with Microsoft, offering a super easy way for enterprise customers to deploy Hugging Face models for real-time inference, all within their secure Azure environment.‚Äù_ said Julien Simon, Chief Evangelist at Hugging Face. 

_""The integration of Hugging Face's open-source models into Azure Machine Learning represents our commitment to empowering developers with industry-leading AI tools,""_ said John Montgomery, Corporate Vice President, Azure AI Platform at Microsoft. _""This collaboration not only simplifies the deployment process of large language models but also provides a secure and scalable environment for real-time inferencing. It's an exciting milestone in our mission to accelerate AI initiatives and bring innovative solutions to the market swiftly and securely, backed by the power of Azure infrastructure.""_
- Empower Diverse Researcher Perspectives via Accessible Tooling and Resources
    - Tooling and resources must be available and accessible to different disciplines as well as the many languages and perspectives needed to drive responsible innovation. This means at minimum providing resources in multiple languages, which can be based on the most spoken languages in the U.S. The [BigScience Research Workshop](https://bigscience.huggingface.co/), a community of over 1000 researchers from different disciplines hosted by Hugging Face and the French government, is a good example of empowering perspectives from over 60 countries to build one of the most powerful open-source multilingual language models. 

Our <a href=""/blog/assets/92_us_national_ai_research_resource/Hugging_Face_NAIRR_RFI_2022.pdf"">memo</a> goes into further detail for each recommendation. We are eager for more resources to make AI broadly accessible in a responsible manner.
By making [Hugging Face available on AWS Marketplace](https://aws.amazon.com/marketplace/pp/prodview-n6vsyhdjkfng2), we are removing barriers to adopting AI and making it easier for companies to leverage large language models. Now with just a few clicks, AWS customers can subscribe and connect their Hugging Face Account with their AWS account. 

By subscribing through AWS Marketplace, Hugging Face organization usage charges for services like Inference Endpoints will automatically appear on your AWS bill, instead of being charged by Hugging Face to the credit card on file for your organization.

We are excited about this launch as it will bring our technology to more developers who rely on AWS, and make it easier for businesses to consume Hugging Face services.

## Getting Started

Before you can connect your AWS Account with your Hugging Face account, you need to fulfill the following prerequisites:
# Databricks ‚ù§Ô∏è Hugging Face: up to 40% faster training and tuning of Large Language Models


Generative AI has been taking the world by storm. As the data and AI company, we have been on this journey with the release of the open source large language model [Dolly](https://huggingface.co/databricks/dolly-v2-12b), as well as the internally crowdsourced dataset licensed for research and commercial use that we used to fine-tune it, the [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k). Both the model and dataset are available on Hugging Face. We‚Äôve learned a lot throughout this process, and today we‚Äôre excited to announce our first of many official commits to the Hugging Face codebase that allows users to easily create a Hugging Face Dataset from an Apache Spark‚Ñ¢ dataframe.
- [AWS: Embracing natural language processing with Hugging Face](https://aws.amazon.com/de/blogs/opensource/embracing-natural-language-processing-with-hugging-face/)
- [Deploy Hugging Face models easily with Amazon SageMaker](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker)
- [AWS and Hugging Face collaborate to simplify and accelerate adoption of natural language processing models](https://aws.amazon.com/blogs/machine-learning/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/)
- [Walkthrough: End-to-End Text Classification](https://youtu.be/ok3hetb42gU)
- [Working with Hugging Face models on Amazon SageMaker](https://youtu.be/leyrCgLAGjMn)
- [Distributed Training: Train BART/T5 for Summarization using ü§ó Transformers and Amazon SageMaker](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq)
- [AWS: Embracing natural language processing with Hugging Face](https://aws.amazon.com/de/blogs/opensource/embracing-natural-language-processing-with-hugging-face/)
- [Deploy Hugging Face models easily with Amazon SageMaker](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker)
- [AWS and Hugging Face collaborate to simplify and accelerate adoption of natural language processing models](https://aws.amazon.com/blogs/machine-learning/aws-and-hugging-face-collaborate-to-simplify-and-accelerate-adoption-of-natural-language-processing-models/)
- [Walkthrough: End-to-End Text Classification](https://youtu.be/ok3hetb42gU)
- [Working with Hugging Face models on Amazon SageMaker](https://youtu.be/leyrCgLAGjMn)
- [Distributed Training: Train BART/T5 for Summarization using ü§ó Transformers and Amazon SageMaker](https://huggingface.co/blog/sagemaker-distributed-training-seq2seq)
Hugging Face is where it is today thanks to its community of developers, so we‚Äôve seen firsthand what open development brings to the table
to support more robust innovation for more diverse and context-specific use cases;
where developers can easily share innovative new techniques, mix and match ML components to suit their own needs,
and reliably work with full visibility into their entire stack.
We‚Äôre also acutely aware of the necessary role of transparency in supporting more accountability and inclusivity of the technology ‚Äì
which we‚Äôve worked on fostering through better documentation and accessibility of ML artifacts, education efforts,
and hosting large-scale multidisciplinary collaborations, among others.
Thus, as the EU AI Act moves toward its final phase, we believe accounting for the specific needs and strengths of open and open-source development of ML systems will be instrumental in supporting its long-term goals.
#### Why is Language Metadata Important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. 

Many people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.
However, running large language models in such an environment can be pretty resource-intensive, especially if you are not able to use hardware acceleration.

### By using an API

Today, various cloud providers propose commercial APIs to use language models. Here is the current Hugging Face offering:

The free [Inference API](https://huggingface.co/docs/api-inference/index) to allow anyone to use small to medium-sized models from the community.

The more advanced and production-ready [Inference Endpoints API](https://huggingface.co/inference-endpoints) for those who require larger models or custom inference code.

These two APIs can be used from Node using the [Hugging Face Inference API library](https://www.npmjs.com/package/@huggingface/inference) on NPM.
## Example uses of the Hugging Face Hub
Individuals and organizations already use the Hugging Face hub to share machine learning models, datasets and demos related to the GLAM sector.

### [BigLAM](https://huggingface.co/biglam)
An initiative developed out of the [BigScience project](https://bigscience.huggingface.co/) is focused on making datasets from GLAM with relevance to machine learning are made more accessible. BigLAM has so far made over 30 datasets related to GLAM available via the Hugging Face hub.

### [Nasjonalbiblioteket AI Lab](https://huggingface.co/NbAiLab) 
The AI lab at the National Library of Norway is a very active user of the Hugging Face hub, with ~120 models, 23 datasets and six machine learning demos shared publicly. These models include language models trained on Norwegian texts from the National Library of Norway and Whisper (speech-to-text) models trained on S√°mi languages.

### [Smithsonian Institution](https://huggingface.co/Smithsonian)
Under the hood, watsonx.ai also integrates many Hugging Face open-source libraries, such as [transformers](https://github.com/huggingface/transformers) (100k+ GitHub stars!), [accelerate](https://github.com/huggingface/accelerate), [peft](https://github.com/huggingface/peft) and our [Text Generation Inference](https://github.com/huggingface/text-generation-inference) server, to name a few. We're happy to partner with IBM and to collaborate on the watsonx AI and data platform so that Hugging Face customers can work natively with their Hugging Face models and datasets to multiply the impact of AI across businesses.

In addition, IBM has also developed its own collection of Large Language Models, and we will work with their team to open-source them and make them easily available in the Hugging Face Hub.
‚ÄúThe future of AI is here, but it‚Äôs not evenly distributed,‚Äù said Clement Delangue, CEO of Hugging Face. ‚ÄúAccessibility and transparency are the keys to sharing progress and creating tools to use these new capabilities wisely and responsibly. Amazon SageMaker and AWS-designed chips will enable our team and the larger machine learning community to convert the latest research into openly reproducible models that anyone can build on.‚Äù

## Collaborating to scale AI in the cloud

This expanded strategic partnership enables Hugging Face and AWS to accelerate machine learning adoption using the latest models hosted on Hugging Face with the industry-leading capabilities of Amazon SageMaker. Customers can now easily fine-tune and deploy state-of-the-art Hugging Face models in just a few clicks on Amazon SageMaker and Amazon Elastic Computing Cloud (EC2), taking advantage of purpose-built machine learning accelerators including AWS Trainium and AWS Inferentia.
Researchers can [now download, run and study BLOOM](https://huggingface.co/bigscience/bloom) to investigate the performance and behavior of recently developed large language models down to their deepest internal operations. More generally, any individual or institution who agrees to the terms of the model‚Äôs [Responsible AI License](https://bigscience.huggingface.co/blog/the-bigscience-rail-license) (developed during the BigScience project itself) can use and build upon the model on a local machine or on a cloud provider. In this spirit of collaboration and continuous improvement, we‚Äôre also releasing, for the first time, the intermediary checkpoints and optimizer states of the training. Don‚Äôt have 8 A100s to play with? An inference API, currently backed by Google‚Äôs TPU cloud and a FLAX version of the model, also allows quick tests, prototyping, and lower-scale use. You can already play with it on the Hugging Face Hub.
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
There have been significant advances in new Transformer and Diffuser machine learning models that process and generate text, audio, and images. However, most of these popular generative AI models are not publicly available, widening the gap of machine learning capabilities between the largest tech companies and everyone else. To counter this trend, AWS and Hugging Face are partnering to contribute next-generation models to the global AI community and democratize machine learning. Through the strategic partnership, Hugging Face will leverage AWS as a preferred cloud provider so developers in Hugging Face‚Äôs community can access AWS‚Äôs state-of-the-art tools (e.g., [Amazon SageMaker](https://aws.amazon.com/sagemaker), [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)) to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their

Here is the question: Why is multi-language support in the Hugging Face ecosystem critical for global AI adoption?
Here is answer 1:It increases dataset size and diversity for training
Here is answer 2: It ensures accessibility across different user groups and applications
Here is answer3: It standardizes tokenization for multilingual tasks
Here is answer 4: It enables seamless integration with proprietary systems",Answer : It ensures accessibility across different user groups
"What distinguishes policy-based methods from value-based methods in reinforcement learning, and how might this distinction influence RAG models?","[(24580, '9e0b974c-4de1-4c4e-9ad2-c81dd70f9d85'), (24581, '22244e17-6683-4b12-bf73-b4e4b5377b96'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (12850, 'c6e54b94-1bc1-4c66-8959-7c3a3451782c'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (21598, 'e113a596-30cc-441f-ac19-90b0009cc7f4'), (21600, '9af1cc4c-21f2-4e01-81b1-d37162920ffb'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (21606, '136ec334-4b38-4957-a9a2-8b75cf91a5d1'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (14487, '4f24c302-152f-4c11-b198-4be00041e6a0'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25274, '0a54e74a-9c72-4f9a-b0f0-c8420921e175'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21210, 'a7d914db-0a82-4bef-889e-b11938bed886'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11047, '939d2f75-31f8-4c36-92da-3b8dadf6c83a'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19299, '58e3889c-4edb-4806-9064-e951799e80cb'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19317, 'cab6254e-890a-47c4-a68b-1b64258cc023'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (19325, 'fb7c67ab-594a-487c-ae87-f94d0dbe7c65'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23952, 'ec7797a6-1617-49dc-9737-a162e926580a'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23956, '9dec7d5b-756e-4a6c-ac15-1b544069a789'), (23957, '666ef815-1fab-4378-93d7-9c5e8c83e825'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (923, '3f1f99cb-3e95-4732-9582-c75dfc21e2bb'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (924, 'a954e80d-bcae-4191-ad77-dc130d6f2b5a'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (961, 'f8df8306-bd61-414c-92fb-2f5f3e706930'), (962, 'c68008e9-dc91-4db0-a2f0-89a793040fff'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (964, '65fb34e4-bfe0-4990-b85f-bf4e0f542e42'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: What are the policy-based methods?

The main goal of Reinforcement learning is to **find the optimal policy \\(\pi^{*}\\) that will maximize the expected cumulative reward**.
Because Reinforcement Learning is based on the *reward hypothesis*: **all goals can be described as the maximization of the expected cumulative reward.**

For instance, in a soccer game (where you're going to train the agents in two units), the goal is to win the game. We can describe this goal in reinforcement learning as
**maximizing the number of goals scored** (when the ball crosses the goal line) into your opponent's soccer goals. And **minimizing the number of goals in your soccer goals**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg"" alt=""Soccer"" />

## Value-based, Policy-based, and Actor-critic methods

In the first unit, we saw two methods to find (or, most of the time, approximate) this optimal policy \\(\pi^{*}\\).
- [The Policy œÄ: the agent‚Äôs brain](#the-policy-œÄ-the-agents-brain)
  - [Policy-Based Methods](#policy-based-methods)
  - [Value-based methods](#value-based-methods)
- [The ‚ÄúDeep‚Äù in Reinforcement Learning](#the-deep-in-reinforcement-learning)
## What are Policy-Gradient Methods?
Policy-Gradient is a subclass of Policy-Based Methods, a category of algorithms that¬†**aims to optimize the policy directly without using a value function using different techniques.** The difference with Policy-Based Methods is that Policy-Gradient methods are a series of algorithms that aim to optimize the policy directly¬†**by estimating the weights of the optimal policy using Gradient Ascent.**

### An Overview of Policy Gradients
Why do we optimize the policy directly by estimating the weights of an optimal policy using Gradient Ascent in Policy Gradients Methods?

Remember that reinforcement learning aims¬†**to find an optimal behavior strategy (policy) to maximize its expected cumulative reward.**

We also need to remember that a policy is a function that **given a state, outputs, a distribution over actions** (in our case using a stochastic policy).
There are **two** ways to find your optimal policy:

- By **training your policy directly**: policy-based methods.
- By **training a value function** that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.

- Finally, we spoke about Deep RL because **we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name ""deep.""**

# Let's train our first Deep Reinforcement Learning agent and upload it to the Hub üöÄ

## Get a certificate üéì

To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push your trained model to the Hub and **get a result of >= 200**.
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
### Among the value-based methods, we can find two main strategies

- **The state-value function.** For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.
- **The action-value function.** In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.

### Epsilon-greedy strategy:

- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.
- Chooses the action with the highest expected reward with a probability of 1-epsilon.
- Chooses a random action with a probability of epsilon.
- Epsilon is typically decreased over time to shift focus towards exploitation.

### Greedy strategy:
Because, in value-based,¬†**œÄ exists only because of the action value estimates, since policy is just a function**¬†(for instance, greedy-policy) that will select the action with the highest value given a state.

But, with policy-based methods, we want to optimize the policy directly¬†**without having an intermediate step of learning a value function.**

So today,¬†**we'll study our first Policy-Based method**: Reinforce. And we'll implement it from scratch using PyTorch. Before testing its robustness using CartPole-v1, PixelCopter, and Pong.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/envs.gif"" alt=""Environments""/>
</figure>

Let's get started,
- [What is Reinforcement Learning?](#what-is-reinforcement-learning)
  - [The big picture](#the-big-picture)
  - [A formal definition](#a-formal-definition)
- [The Reinforcement Learning Framework](#the-reinforcement-learning-framework)
  - [The RL Process](#the-rl-process)
  - [The reward hypothesis: the central idea of Reinforcement Learning](#the-reward-hypothesis-the-central-idea-of-reinforcement-learning)
  - [Markov Property](#markov-property)
  - [Observations/States Space](#observationsstates-space)
  - [Action Space](#action-space)
  - [Rewards and the discounting](#rewards-and-the-discounting)
  - [Type of tasks](#type-of-tasks)
- [Exploration/ Exploitation tradeoff](#exploration-exploitation-tradeoff)
- [The two main approaches for solving RL problems](#the-two-main-approaches-for-solving-rl-problems)
  - [The Policy œÄ: the agent‚Äôs brain](#the-policy-œÄ-the-agents-brain)
  - [Policy-Based Methods](#policy-based-methods)
  - [Value-based methods](#value-based-methods)
---


‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit6/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*

[In Unit 5](https://huggingface.co/blog/deep-rl-pg), we learned about our first Policy-Based algorithm called **Reinforce**. 
In Policy-Based methods, **we aim to optimize the policy directly without using a value function**. More precisely, Reinforce is part of a subclass of *Policy-Based Methods* called *Policy-Gradient methods*. This subclass optimizes the policy directly by **estimating the weights of the optimal policy using Gradient Ascent**.
Under a deterministic policy, the policy will either always move right when in a red state or always move left. **Either case will cause our agent to get stuck and never suck the dust**.

Under a value-based Reinforcement learning algorithm, we learn a **quasi-deterministic policy** (""greedy epsilon strategy""). Consequently, our agent can **spend a lot of time before finding the dust**.

On the other hand, an optimal stochastic policy **will randomly move left or right in red (colored) states**. Consequently, **it will not be stuck and will reach the goal state with a high probability**.

<figure class=""image table text-center m-0 w-full"">
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/hamster3.jpg"" alt=""Hamster 1""/>
</figure>

### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces
Consequently, whatever method you use to solve your problem,¬†**you will have a policy**. In the case of value-based methods, you don't train the policy: your policy¬†**is just a simple pre-specified function**¬†(for instance, the Greedy Policy) that¬†uses the values given by the value-function to select its actions.

So the difference is:

- In policy-based training,¬†**the optimal policy (denoted œÄ\*) is found by training the policy directly.**
- In value-based training,¬†**finding an optimal value function (denoted Q\* or V\*, we'll study the difference below) leads to having an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link between value and policy""/>

In fact, most of the time, in value-based methods, you'll use¬†**an Epsilon-Greedy Policy**¬†that handles the exploration/exploitation trade-off; we'll talk about this when we talk about Q-Learning in the second part of this unit.
With policy-based methods, we want to optimize the policy directly¬†**without having an intermediate step of learning a value function.**

So today,¬†**we'll learn about policy-based methods and study a subset of these methods called policy gradient**. Then we'll implement our first policy gradient algorithm called Monte Carlo **Reinforce** from scratch using PyTorch.
Then, we'll test its robustness using the CartPole-v1 and PixelCopter environments.

You'll then be able to iterate and improve this implementation for more advanced environments.

<figure class=""image table text-center m-0 w-full"">
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif"" alt=""Environments""/>
</figure>

Let's get started!
The benefits of using policy-gradient methods over value-based methods include: 
    - simplicity of integration: no need to store action values;
    - ability to learn a stochastic policy: the agent explores the state space without always taking the same trajectory, and avoids the problem of perceptual aliasing;
    - effectiveness in high-dimensional and continuous action spaces; and
    - improved convergence properties.

- **Policy Gradient:** A subset of policy-based methods where the objective is to maximize the performance of a parameterized policy using gradient ascent. The goal of a policy-gradient is to control the probability distribution of actions by tuning the policy such that good actions (that maximize the return) are sampled more frequently in the future. 

- **Monte Carlo Reinforce:** A policy-gradient algorithm that uses an estimated return from an entire episode to update the policy parameter.
- **Policy-based methods.** The policy is usually trained with a neural network to select what action to take given a state. In this case it is the neural network which outputs the action that the agent should take instead of using a value function. Depending on the experience received by the environment, the neural network will be re-adjusted and will provide better actions.
- **Value-based methods.** In this case, a value function is trained to output the value of a state or a state-action pair that will represent our policy. However, this value doesn't define what action the agent should take. In contrast, we need to specify the behavior of the agent given the output of the value function. For example, we could decide to adopt a policy to take the action that always leads to the biggest reward (Greedy Policy). In summary, the policy is a Greedy Policy (or whatever decision the user takes) that uses the values of the value-function to decide the actions to take.
Introduction [[introduction]]

  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png"" alt=""thumbnail""/>

In the last unit, we learned about Deep Q-Learning. In this value-based deep reinforcement learning algorithm, we¬†**used a deep neural network to approximate the different Q-values for each possible action at a state.**

Since the beginning of the course, we have only studied value-based methods,¬†**where we estimate a value function as an intermediate step towards finding an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy"" />

In value-based methods, the policy ** \(œÄ\) only exists because of the action value estimates since the policy is just a function** (for instance, greedy-policy) that will select the action with the highest value given a state.
Model Based Reinforcement Learning (MBRL)

Model-based reinforcement learning only differs from its model-free counterpart in learning a *dynamics model*, but that has substantial downstream effects on how the decisions are made.

The dynamics model usually models the environment transition dynamics, \\( s_{t+1} = f_\theta (s_t, a_t) \\), but things like inverse dynamics models (mapping from states to actions) or reward models (predicting rewards) can be used in this framework.


## Simple definition

- There is an agent that repeatedly tries to solve a problem, **accumulating state and action data**.
- With that data, the agent creates a structured learning tool, *a dynamics model*, to reason about the world.
- With the dynamics model, the agent **decides how to act by predicting the future**.
- With those actions, **the agent collects more data, improves said model, and hopefully improves future actions**.

## Academic definition
Introducing Q-Learning [[q-learning]]
## What is Q-Learning? [[what-is-q-learning]]

Q-Learning is an¬†**off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end¬†of this unit.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us¬†**the value of each state or each state-action pair.**
- *TD approach:*¬†**updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.
---

‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*
  
**[In the last Unit](https://huggingface.co/blog/deep-rl-a2c)**, we learned about Advantage Actor Critic (A2C), a hybrid architecture combining value-based and policy-based methods that help to stabilize the training by reducing the variance with:

- *An Actor*¬†that controls¬†**how our agent behaves**¬†(policy-based method).
- *A Critic*¬†that measures¬†**how good the action taken is**¬†(value-based method).
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png"" alt=""thumbnail""/>


In this notebook, you'll code your first Deep Reinforcement Learning algorithm from scratch: Reinforce (also called Monte Carlo Policy Gradient).

Reinforce is a *Policy-based method*: a Deep Reinforcement Learning algorithm that tries **to optimize the policy directly without using an action-value function**.

More precisely, Reinforce is a *Policy-gradient method*, a subclass of *Policy-based methods* that aims **to optimize the policy directly by estimating the weights of the optimal policy using gradient ascent**.

To test its robustness, we're going to train it in 2 different simple environments:
- Cartpole-v1
- PixelcopterEnv

‚¨áÔ∏è Here is an example of what **you will achieve at the end of this notebook.** ‚¨áÔ∏è
Mid-way Quiz [[mid-way-quiz]]

The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.


### Q1: What are the two main approaches to find optimal policy?


<Question
	choices={[
		{
			text: ""Policy-based methods"",
			explain: ""With Policy-Based methods, we train the policy directly to learn which action to take given a state."",
      correct: true
		},
		{
			text: ""Random-based methods"",
			explain: """"
		},
    {
			text: ""Value-based methods"",
			explain: ""With value-based methods, we train a value function to learn which state is more valuable and use this value function to take the action that leads to it."",
      correct: true
		},
		{
			text: ""Evolution-strategies methods"",
      explain: """"
		}
	]}
/>


### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>

Here is the question: What distinguishes policy-based methods from value-based methods in reinforcement learning, and how might this distinction influence RAG models?
Here is answer 1:Policy-based methods optimize actions directly; RAG could benefit from this for retrieval routing
Here is answer 2: Value-based methods train deterministic outputs; RAG leverages probabilistic policies
Here is answer3: Policy-based methods require more data; RAG models simplify tasks using action-value functions
Here is answer 4: Value-based methods ensure exploration; RAG avoids this for efficiency","The correct answer is:

Policy-based methods optimize the policy directly without using a value function."
How do Gradio's setup and local development processes support scalable AI workflows?,"[(8203, '6c0cf928-5dfb-4006-ac6d-7ab27cd5875a'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (3102, '6c7f3c2e-5b16-4ca0-8363-ead695f3bae8'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (28246, '4a3bf591-3b47-4caa-bd34-c5f1bc064e2f'), (28248, '14667f8d-196e-48a1-9b90-85075c677427'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (11357, 'a2d06311-cad3-4127-8cc1-1b7e15255d0f'), (11358, 'd8407ad8-7a13-4c9a-901c-6a4d4502993d'), (28255, 'bcb424b2-729f-4991-87ac-45ef5ce77853'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (11360, '5df29451-6693-4cac-9c11-3d3838496bfc'), (28257, 'cfc6625e-3cab-4f72-8584-37a13847b65a'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (27756, '8f9de8cd-281f-441a-b692-3d1b132a6728'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (6259, 'e1dad236-a338-4128-821c-38c369e6a47e'), (6261, 'd0ed0a3b-3fb6-4a46-9e26-d04afabaf82f'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (6791, 'caea27fd-5810-4f6f-afa1-5befc362cced'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (4238, '2e5da79f-70d6-43e2-93ea-032c4d4eae1e'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (17068, 'e8d4188d-34a3-4d81-96d0-85488e9eb25a'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (9403, 'f02196ba-b6c3-4f63-8626-a0d8278fce15'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (14042, '6d5f441d-7e53-469a-af95-3fdc35f8fa6b'), (14043, '969903ce-08d6-4aa6-ae88-ddf707c7c323'), (6876, '83d18f4a-a9ff-488a-a396-55120e57e9f4'), (17119, '3362d998-e1dd-4fe9-838e-5136a991278a'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (27897, '00ee70f5-c8bc-45d4-9a8b-c379cafc9c97'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (19214, '2d9ea80a-98e3-4bca-91ce-fe9e72bdb0ca'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (18718, '22d2a501-6bbd-4f0f-ae74-291e43ad8509'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (6953, '5b5e8c9e-10d8-4ea2-930a-08ac3425c937'), (6954, 'dc0a4a82-5286-4d59-ac85-fe642c49151e'), (30507, '71f21d73-1d7a-46ab-8ace-0c85a938a6c6'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (3382, 'f57e911f-3558-4cec-a7c5-dc8c73ec91ef'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (2372, '03aacefb-6869-419b-80ba-b72212d966d5'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (2374, '3c2fc425-71a9-4574-8e7e-9ea6de5e161a'), (2381, '25022ddc-db5e-48bf-a2bf-918a00646b76'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (2383, 'c6ad6da4-4a77-4db0-b926-6c1b77b6db81'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (15705, 'fc5a9db9-4e60-4317-8a58-015ff53348be'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (16762, '0a98d923-bf03-4726-a7c3-4f2c56457e83'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14247, '99b38e8f-68a6-4442-a93a-76803370e58f'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (13746, '0bb29eea-90a5-4fd6-8177-0ccbbf32e132'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (11192, 'a6746ae6-12c0-4e20-a9c7-16c2d49dc376'), (23483, '48efe26f-2d36-4283-91e0-1e19ee13177c'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (23997, 'f5eb3def-edf7-4dfa-b630-80e186c92aed'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (9177, 'f2603eb7-ed07-4331-8252-28f988a21715'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (2023, 'd20a455f-f235-41b7-a1c5-5b15b10c5065'), (29674, '81aa5082-4e7c-42b6-8684-491f31bb23aa'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: gradio-ui

This folder contains all of the Gradio UI and component source code.

- [set up](#setup)
- [running the application](#running-the-application)
- [local development](#local-development)
- [building for production](#building-for-production)
- [quality checks](#quality-checks)
- [ci checks](#ci-checks)

## setup

This folder is managed as 'monorepo' a multi-package repository which make dependency management very simple. In order to do this we use `pnpm` as our package manager.

Make sure [`pnpm`](https://pnpm.io/) is installed by [following the installation instructions for your system](https://pnpm.io/installation).

You will also need `node` which you probably already have

## running the application

Install all dependencies:

```bash
pnpm i
## The Workflow

The Custom Components workflow consists of 4 steps: create, dev, build, and publish.

1. create: creates a template for you to start developing a custom component.
2. dev: launches a development server with a sample app & hot reloading allowing you to easily develop your custom component
3. build: builds a python package containing to your custom component's Python and JavaScript code -- this makes things official!
4. publish: uploads your package to [PyPi](https://pypi.org/) and/or a sample app to [HuggingFace Spaces](https://hf.co/spaces).

Each of these steps is done via the Custom Component CLI. You can invoke it with `gradio cc` or `gradio component`

Tip: Run `gradio cc --help` to get a help menu of all available commands. You can also append `--help` to any command name to bring up a help page for that command, e.g. `gradio cc create --help`.

## 1. create

Bootstrap a new template by running the following in any working directory:
Custom Components in 5 minutes

Gradio 4.0 introduces Custom Components -- the ability for developers to create their own custom components and use them in Gradio apps.
You can publish your components as Python packages so that other users can use them as well.
Users will be able to use all of Gradio's existing functions, such as `gr.Blocks`, `gr.Interface`, API usage, themes, etc. with Custom Components.
This guide will cover how to get started making custom components.

## Installation

You will need to have:

* Python 3.8+ (<a href=""https://www.python.org/downloads/"" target=""_blank"">install here</a>)
* Node.js v16.14+ (<a href=""https://nodejs.dev/en/download/package-manager/"" target=""_blank"">install here</a>)
* npm 9+ (<a href=""https://docs.npmjs.com/downloading-and-installing-node-js-and-npm/"" target=""_blank"">install here</a>)
* Gradio 4.0+ (`pip install --upgrade gradio`)

## The Workflow

The Custom Components workflow consists of 4 steps: create, dev, build, and publish.
### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)

By default, this uses HuggingFace‚Äôs hosted Inference API (you can supply your own API key or use the public access without an API key), or you can also run `pip install transformers` and run the model computations locally if you‚Äôd like.

Do you want to customize the demo? You can override any of the default parameters of the [Interface class](https://gradio.app/docs) by passing in your own parameters:


![GIF of Gradio 2.0](./assets/22_gradio/recording-21.gif)




**_But wait, there‚Äôs more!_** With 10,000 models already on Model Hub, we see models not just as standalone pieces of code, but as lego pieces that can be **composed and mixed** to create more sophisticated applications and demos. 

For example, Gradio lets you load multiple models in _parallel_ (imagine you want to compare 4 different text generation models from Hugging Face to see which one is the best for your use case):
Running a Gradio App on your Web Server with Nginx

Tags: DEPLOYMENT, WEB SERVER, NGINX

## Introduction

Gradio is a Python library that allows you to quickly create customizable web apps for your machine learning models and data processing pipelines. Gradio apps can be deployed on [Hugging Face Spaces](https://hf.space) for free.

In some cases though, you might want to deploy a Gradio app on your own web server. You might already be using [Nginx](https://www.nginx.com/), a highly performant web server, to serve your website (say `https://www.example.com`), and you want to attach Gradio to a specific subpath on your website (e.g. `https://www.example.com/gradio-demo`).

In this Guide, we will guide you through the process of running a Gradio app behind Nginx on your own web server to achieve this.

**Prerequisites**
### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


### The Gradio Python & JavaScript Ecosystem
#### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


#### The Gradio Python & JavaScript Ecosystem
Using virtual environments ensures that you can work on multiple Python projects on the same machine without any conflicts. This is particularly useful when different projects require different versions of the same library. It also simplifies dependency management and enhances reproducibility, as you can easily share the requirements of your project with others.


## Installing Gradio on Windows

To install Gradio on a Windows system in a virtual environment, follow these steps:

1. **Install Python**: Ensure you have Python 3.8 or higher installed. You can download it from [python.org](https://www.python.org/). You can verify the installation by running `python --version` or `python3 --version` in Command Prompt.


2. **Create a Virtual Environment**:
   Open Command Prompt and navigate to your project directory. Then create a virtual environment using the following command:

   ```bash
   python -m venv gradio-env
This is an advanced guide, so make sure you know the basics of Gradio already, such as [how to create and launch a Gradio Interface](https://gradio.app/guides/quickstart/). Most of the information in this Guide is relevant whether you are hosting your demo on [Hugging Face Spaces](https://hf.space) or on your own server.

## Overview of Gradio's Queueing System

By default, every Gradio demo includes a built-in queuing system that scales to thousands of requests. When a user of your app submits a request (i.e. submits an input to your function), Gradio adds the request to the queue, and requests are processed in order, generally speaking (this is not exactly true, as discussed below). When the user's request has finished processing, the Gradio server returns the result back to the user using server-side events (SSE). The SSE protocol has several advantages over simply using HTTP POST requests:
Quickstart

Gradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif"" style=""padding-bottom: 10px"">

It just takes a few lines of Python to create a beautiful demo like the one above, so let's get started üí´

## Installation

**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)


We recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:

```bash
pip install gradio
```

## 5. Create Gradio Chatbot backed by Amazon SageMaker

We can also create a gradio application to chat with our model. Gradio is a python library that allows you to quickly create customizable UI components around your machine learning models. You can find more about gradio [here](https://gradio.app/).

```python
!pip install gradio  --upgrade
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features

- [#5216](https://github.com/gradio-app/gradio/pull/5216) [`4b58ea6d`](https://github.com/gradio-app/gradio/commit/4b58ea6d98e7a43b3f30d8a4cb6f379bc2eca6a8) - Update i18n tokens and locale files. Thanks [@hannahblair](https://github.com/hannahblair)!

### Fixes
Gradio-Lite: Serverless Gradio Running Entirely in Your Browser

Tags: SERVERLESS, BROWSER, PYODIDE

Gradio is a popular Python library for creating interactive machine learning apps. Traditionally, Gradio applications have relied on server-side infrastructure to run, which can be a hurdle for developers who need to host their applications. 

Enter Gradio-lite (`@gradio/lite`): a library that leverages [Pyodide](https://pyodide.org/en/stable/) to bring Gradio directly to your browser. In this blog post, we'll explore what `@gradio/lite` is, go over example code, and discuss the benefits it offers for running Gradio applications.

## What is `@gradio/lite`?
```

**How Requests are Processed from the Queue**

When a Gradio server is launched, a pool of threads is used to execute requests from the queue. By default, the maximum size of this thread pool is `40` (which is the default inherited from FastAPI, on which the Gradio server is based). However, this does *not* mean that 40 requests are always processed in parallel from the queue. 

Instead, Gradio uses a **single-function-single-worker** model by default. This means that each worker thread is only assigned a single function from among all of the functions that could be part of your Gradio app. This ensures that you do not see, for example, out-of-memory errors, due to multiple workers calling a machine learning model at the same time. Suppose you have 3 functions in your Gradio app: A, B, and C. And you see the following sequence of 7 requests come in from users using your app:

```
1 2 3 4 5 6 7
-------------
A B A A C B A
## Introduction

Tabular data science is the most widely used domain of machine learning, with problems ranging from customer segmentation to churn prediction. Throughout various stages of the tabular data science workflow, communicating your work to stakeholders or clients can be cumbersome; which prevents data scientists from focusing on what matters, such as data analysis and model building. Data scientists can end up spending hours building a dashboard that takes in dataframe and returning plots, or returning a prediction or plot of clusters in a dataset. In this guide, we'll go through how to use `gradio` to improve your data science workflows. We will also talk about how to use `gradio` and [skops](https://skops.readthedocs.io/en/stable/) to build interfaces with only one line of code!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started).

## Let's Create a Simple Interface!
```

When you run this code, a public URL will be generated for your demo in a matter of seconds, something like:

üëâ &nbsp; `https://a23dsf231adb.gradio.live`

Now, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.

To learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).


### An Overview of Gradio

So far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?

#### Chatbots with `gr.ChatInterface`
Gradio, check![[gradio-check]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

This wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:

- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities.
- Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces).
- How to integrate Gradio demos with models and Spaces on the Hugging Face Hub.
- Advanced features like storing state in a demo or providing authentication.
- How to have full control of the data flow and layout of your demo with Gradio Blocks.

If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section!

## Where to next?[[where-to-next]]

If you want to learn more about Gradio you can
üöÄ Creating Discord Bots from Gradio Apps üöÄ

Tags: NLP, TEXT, CHAT

We're excited to announce that Gradio can now automatically create a discord bot from a deployed app! ü§ñ

Discord is a popular communication platform that allows users to chat and interact with each other in real-time. By turning your Gradio app into a Discord bot, you can bring cutting edge AI to your discord server and give your community a whole new way to interact.

## üíª How does it work? üíª

With `gradio_client` version `0.3.0`, any gradio `ChatInterface` app on the internet can automatically be deployed as a discord bot via the `deploy_discord` method of the `Client` class.

Technically, any gradio app that exposes an api route that takes in a single string and outputs a single string can be deployed to discord. In this guide, we will focus on `gr.ChatInterface` as those apps naturally lend themselves to discord's chat functionality.

## üõ†Ô∏è Requirements üõ†Ô∏è
Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API

Hugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load(""Helsinki-NLP/opus-mt-en-es"", src=""models"")

demo.launch()

Here is the question: How do Gradio's setup and local development processes support scalable AI workflows?
Here is answer 1:By automating deployment pipelines
Here is answer 2: By enabling modular component management
Here is answer3: By reducing the need for local dependencies
Here is answer 4: By integrating dataset curation tools","The correct answer option number should be number two because according upon given details:""...enabling modular component management..."""
What is the first step in debugging errors in a fine-tuned Transformer model when working with external datasets?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (4629, '65a68f3d-7555-49cf-a137-b0000b75f9d0'), (1560, '9c881267-9303-4686-81be-1ac7f7fd886c'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (13892, '18f9d2dd-b24d-40df-bf4b-70633772beaa'), (30278, '9a49a14a-c17e-40eb-b72a-5621d602f5b0'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (26202, 'a3e63e78-75c5-4e73-a5b3-110285882551'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (20071, '163a9ba3-cb9e-4a4f-bdd6-07ceaa145535'), (23145, '093af302-133e-498d-a722-88a8b6199cee'), (12912, '426a098c-d467-4adb-a3a9-bb78cb34af6b'), (18549, 'd84bf0de-2759-4c1e-9a94-cc27a2136a9b'), (1150, 'b91271f2-c6c3-489b-b0cd-5c23b6b7df5e'), (3715, 'c4518fec-8276-4e52-964a-6148955da040'), (3724, '1c0e1b75-0bd9-45a1-af24-e9cf468d539c'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (7314, 'ee1d43d7-35a0-4c1a-9504-651aacab5ee8'), (20117, '344e2a72-849d-4598-a5f5-e5a7a605e227'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (23723, '2de4e811-42ea-47bb-b611-e241662b0083'), (1195, '3e7433ba-fae2-4e13-8802-9ff9da0b8900'), (6317, '56ffe729-9ef7-4575-b631-fdfdf483ee5f'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (6316, '979e27ae-ecb6-421a-9259-ab1fcb97b027'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (23729, '5d899127-2a50-48fa-9d69-557ec6ed9ded'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (702, '43197162-b6f7-4689-ab00-064a4b19af86'), (1215, '5739b2fb-8f4c-41a1-b697-0e0fd5f0a76f'), (1216, '485aadcf-fe5c-4160-a137-0b1dc3b07685'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (30918, '5636518a-1eb1-4077-baf0-ae21e3344396'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (19668, '55b764e4-abdd-42d3-aae5-2fbaedf7cc68'), (19669, 'aee04d72-b4ef-45e8-8857-bc8147c6672b'), (1236, '9c2610a6-e2df-499f-a97e-cad2b46d1edd'), (1240, 'ae168361-4065-4c53-9046-4d69f34c3e19'), (1241, '906de20e-0472-4937-8882-c423749564ec'), (17628, '216e7cec-f307-4f22-873e-a4c5479c0a41'), (1246, 'f1c4bb4f-9bb9-4e34-a9a3-2f65bf45e38a'), (24287, '0f309be3-9d92-421d-bae1-8bfa12a4840d'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (30945, '1f492b07-5014-4107-a135-6821e6248736'), (30944, '650f5a5a-ee1b-47bc-83ff-f6cf7a368a11'), (19682, '68246be8-8fe2-4942-800d-96fc8ca804d3'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (1257, 'e17a5fc9-f0ae-4120-af12-4c50cfd1be95'), (31467, 'd5b8cb99-b5ce-4df9-8ab2-5c8ebaf7db26'), (11500, 'defee3f2-8e67-4389-bb9e-ddd62fbd2657'), (7920, 'b0ec8a3b-aacb-4851-a377-64d834457caf'), (30961, '3948329f-86ce-4dd0-9d40-bade197ac0eb'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (17154, '2fb79b0d-c61b-4500-9151-440dcfada262'), (28938, 'c79cdb2e-622b-41c8-bde6-21c9ca8d958a'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (28952, '79800c43-9871-4516-b042-0c6ed54ed114'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (20252, 'b9e133f5-90ab-47a2-9af2-55d084a98c15'), (15142, '10c62a15-e400-4c35-93a8-017a7ab8b3d6'), (5416, 'ae0beed4-07b2-4cd5-9638-323b957185ef'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (20298, 'cf07b1e2-6cae-4114-989b-151be05e8ba7'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (7534, '4354b918-223f-4fd0-92ea-e59bd0b20f6b'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (7570, '30cef6d0-64c6-47bd-8d5c-6aeb6012e5a8'), (20887, 'e4c88d19-b6b7-484a-b21a-646c0f2984c3'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (20905, '7f194331-ef4b-4aff-9ae8-6a824fa631f5'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (20908, 'ee5a5b82-d78b-40de-b0b2-ed026ae576d3'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (20933, '4c0a1879-65bd-4145-81d4-aa0b9c5d6d0d'), (20934, 'fca222e3-d4bf-4031-863b-50a1512af5b9'), (17863, '80f6eaf0-456d-4fa7-a935-720bef0dc3f6'), (11209, '3b5ff375-2e1c-4a92-8077-43f3d30f62ae'), (22476, '6a8f51fd-9677-457f-8034-63e0566b51c3'), (20948, '7445ee23-4b44-4cf9-93da-c4aa8ae24afa'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (4067, 'db389377-70fa-4887-84a7-4ea544f7efea'), (3047, '613dfbaa-b77d-455c-a74c-88e0a04c24de'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (24057, 'd292faa1-de56-402c-878a-e2d243e5db03'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(""glue"", ""mnli"")

model_checkpoint = ""distilbert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples[""premise""], examples[""hypothesis""], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)
n this video, we will see how to debug an error you encounter when running trainer.train(). As an example, we will use this script that finetunes a bert model on the GLUE MNLI dataset. Checkout the videos linked below to see how we came to such a script, here we want to learn how to debug the problems in it. Running the script gives us an error pretty fast. It happens at the line where we feed the inputs to the model, according to the traceback. That tells us there is a problem there, but the problem could come from many different causes. To debug an error in a training, you need to make sure each step of the training pipeline works as intended. This means checking that the inputs of your dataset are correct, you can batch them together, feed them through the model to get a loss, then compute the gradients of that loss before performing an optimizer step. So let's start by looking at the training dataset this Trainer is using. There is definitely a problem there as we see texts and not
What to do when you get an error[[what-to-do-when-you-get-an-error]]

<CourseFloatingBanner chapter={8}
  classNames=""absolute z-10 right-0 top-0""
  notebooks={[
    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb""},
    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb""},
]} />

In this section we'll look at some common errors that can occur when you're trying to generate predictions from your freshly tuned Transformer model. This will prepare you for [section 4](/course/chapter8/section4), where we'll explore how to debug the training phase itself.

<Youtube id=""DQ-CpJn6Rc4""/>
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""N9kO52itd0Q""/>

The problem when you encounter an error in `model.fit()` is that it could come from multiple sources, as training usually brings together a lot of things that you've been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `model.fit()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""L-WSwUWde1U""/>

The problem when you encounter an error in `trainer.train()` is that it could come from multiple sources, as the `Trainer` usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.
<Question
	choices={[
		{
			text: ""The optimization step where we compute gradients and perform backpropagation"",
			explain: ""Although there may be bugs in your optimizer, this is usually several steps into the training pipeline, so there are other things to check first. Try again!""
		},
		{
			text: ""The evaluation step where we compute metrics"",
			explain: ""Evaluation is usually what you do after training for a full epoch, so you should first check somewhere earlier in the training pipeline."",
		},
		{
			text: ""The datasets"",
			explain: ""Correct! Looking at your data is almost always the first thing you should do, to make sure the text is encoded appropriately, has the expected features, and so on."",
			correct: true
		},
		{
			text: ""The dataloaders"",
			explain: ""Try again -- this is very close to the first thing you should check. Do you remember what object we pass to the dataloaders?""
		}
	]}
/>

### 5. What is the best way to debug a CUDA error?
```

In this instance, there are no more problems, and our script will fine-tune a model that should give reasonable results. But what can we do when the training proceeds without any error, and the model trained does not perform well at all? That's the hardest part of machine learning, and we'll show you a few techniques that can help.

<Tip>

üí° If you're using a manual training loop, the same steps apply to debug your training pipeline, but it's easier to separate them. Make sure you have not forgotten the `model.eval()` or `model.train()` at the right places, or the `zero_grad()` at each step, however!

</Tip>

## Debugging silent errors during training[[debugging-silent-errors-during-training]]

What can we do to debug a training that completes without error but doesn't get good results? We'll give you some pointers here, but be aware that this kind of debugging is the hardest part of machine learning, and there is no magical answer.
he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
```

The most likely outcome is that you'll see a bunch of errors. Don't worry, this is expected! Debugging ML models is
notoriously hard, and the key ingredient to success is patience (and `breakpoint()`). In our experience, the hardest
problems arise from subtle mismatches between ML frameworks, for which we have a few pointers at the end of this guide.
In other cases, a general test might not be directly applicable to your model, in which case we suggest an override
at the model test class level. Regardless of the issue, don't hesitate to ask for help in your draft pull request if
you're stuck.

When all tests pass, congratulations, your model is nearly ready to be added to the ü§ó Transformers library! üéâ

### 6.-7. Ensure everyone can use your model

**6. Submit the pull request**
<Youtube id=""_PAli-V4wj0""/>

When you are sure you have a bug in your hand, the first step is to build a minimal reproducible example.

## Creating a minimal reproducible example[[creating-a-minimal-reproducible-example]]

It's very important to isolate the piece of code that produces the bug, as no one in the Hugging Face team is a magician (yet), and they can't fix what they can't see. A minimal reproducible example should, as the name indicates, be reproducible. This means that it should not rely on any external files or data you may have. Try to replace the data you are using with some dummy values that look like your real ones and still produce the same error.

<Tip>

üö® Many issues in the ü§ó Transformers repository are unsolved because the data used to reproduce them is not accessible.

</Tip>
```

We expect that every model added to ü§ó Transformers passes a couple of
integration tests, meaning that the original model and the reimplemented
version in ü§ó Transformers have to give the exact same output up to a
precision of 0.001! Since it is normal that the exact same model written
in different libraries can give a slightly different output depending on
the library framework, we accept an error tolerance of 1e-3 (0.001). It
is not enough if the model gives nearly the same output, they have to be
the almost identical. Therefore, you will certainly compare the
intermediate outputs of the ü§ó Transformers version multiple times
against the intermediate outputs of the original implementation of
*[camelcase name of model]* in which case an **efficient** debugging environment
of the original repository is absolutely important. Here is some advice
to make your debugging environment as efficient as possible.
```

We expect that every model added to ü§ó Transformers passes a couple of
integration tests, meaning that the original model and the reimplemented
version in ü§ó Transformers have to give the exact same output up to a
precision of 0.001! Since it is normal that the exact same model written
in different libraries can give a slightly different output depending on
the library framework, we accept an error tolerance of 1e-3 (0.001). It
is not enough if the model gives nearly the same output, they have to be
the almost identical. Therefore, you will certainly compare the
intermediate outputs of the ü§ó Transformers version multiple times
against the intermediate outputs of the original implementation of
*BigBird* in which case an **efficient** debugging environment
of the original repository is absolutely important. Here is some advice
to make your debugging environment as efficient as possible.
At the time of writing this blog post, the best performing models are very large models containing more than 10 billion parameters most of which are not open-sourced, *e.g.* *ST-MoE-32B*, *Turing NLR v5*, or
*ERNIE 3.0*. One of the top-ranking models that is easily accessible is [DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta). Therefore, let's try out DeBERTa's newest base version - *i.e.* [`microsoft/deberta-v3-base`](https://huggingface.co/microsoft/deberta-v3-base).


## Training / Fine-tuning a model with ü§ó Transformers and ü§ó Datasets

In this section, we will jump into the technical details of how to
fine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.

Cool! Let's start by installing all necessary pip packages and setting up our code environment, then look into preprocessing the dataset, and finally start training the model.
```

It is very likely that the ü§ó Transformers implementation and the
original model implementation don't give the exact same output the very
first time or that the forward pass throws an error. Don't be
disappointed - it's expected! First, you should make sure that the
forward pass doesn't throw any errors. It often happens that the wrong
dimensions are used leading to a `""Dimensionality mismatch""`
error or that the wrong data type object is used, *e.g.*, `torch.long`
instead of `torch.float32`. Don't hesitate to ask [name of mentor]
for help, if you don't manage to solve certain errors.
```

We expect that every model added to ü§ó Transformers passes a couple of integration tests, meaning that the original
model and the reimplemented version in ü§ó Transformers have to give the exact same output up to a precision of 0.001!
Since it is normal that the exact same model written in different libraries can give a slightly different output
depending on the library framework, we accept an error tolerance of 1e-3 (0.001). It is not enough if the model gives
nearly the same output, they have to be almost identical. Therefore, you will certainly compare the intermediate
outputs of the ü§ó Transformers version multiple times against the intermediate outputs of the original implementation of
*brand_new_bert* in which case an **efficient** debugging environment of the original repository is absolutely
important. Here is some advice to make your debugging environment as efficient as possible.
```

It is very likely that the ü§ó Transformers implementation and the
original model implementation don't give the exact same output the very
first time or that the forward pass throws an error. Don't be
disappointed - it's expected! First, you should make sure that the
forward pass doesn't throw any errors. It often happens that the wrong
dimensions are used leading to a `""Dimensionality mismatch""`
error or that the wrong data type object is used, *e.g.*, `torch.long`
instead of `torch.float32`. Don't hesitate to ask Patrick
for help, if you don't manage to solve certain errors.
```

It is very likely that the ü§ó Transformers implementation and the original model implementation don't give the exact
same output the very first time or that the forward pass throws an error. Don't be disappointed - it's expected! First,
you should make sure that the forward pass doesn't throw any errors. It often happens that the wrong dimensions are
used leading to a *Dimensionality mismatch* error or that the wrong data type object is used, *e.g.* `torch.long`
instead of `torch.float32`. Don't hesitate to ask the Hugging Face team for help, if you don't manage to solve
certain errors.
I highly recommend reading the blog post [Sequence Modeling with CTC
(2017)](https://distill.pub/2017/ctc/) very well-written blog post by
Awni Hannun.

Before we start, let\'s install both `datasets` and `transformers` from
master. Also, we need the `soundfile` package to load audio files and
the `jiwer` to evaluate our fine-tuned model using the [word error rate
(WER)](https://huggingface.co/metrics/wer) metric \\({}^1\\).

```bash
!pip install datasets>=1.18.3
!pip install transformers==4.11.3
!pip install librosa
!pip install jiwer
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model -- one as close as possible to the task you have at hand -- and fine-tune it.

## General architecture[[general-architecture]]

In this section, we'll go over the general architecture of the Transformer model. Don't worry if you don't understand some of the concepts; there are detailed sections later covering each of the components.

<Youtube id=""H39Z_720T5s"" />

## Introduction[[introduction]]

The model is primarily composed of two blocks:

* **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
* **Decoder (right)**: The decoder uses the encoder's representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.
The final part to make sure the ü§ó Transformers implementation works
correctly is to ensure that the outputs are equivalent to a precision of
`1e-3`. First, you should ensure that the output shapes are identical,
*i.e.* `outputs.shape` should yield the same value for the script of the
ü§ó Transformers implementation and the original implementation. Next, you
should make sure that the output values are identical as well. This one
of the most difficult parts of adding a new model. Common mistakes why
the outputs are not identical are:

Here is the question: What is the first step in debugging errors in a fine-tuned Transformer model when working with external datasets?
Here is answer 1:Validate dataset format against the model's pre-trained requirements
Here is answer 2: Directly modify the model's architecture to match input data
Here is answer3: Check the pipeline compatibility without inspecting the dataset
Here is answer 4: Retry model training using the default configuration",The correct answer is: The datasets
How does the PatchTST model improve computational efficiency for long-term time series forecasting?,"[(7682, '69400042-ec99-4c8a-bf23-7e37c59fe21f'), (517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (520, 'b101edb2-f02a-4bd8-b69e-0e3ba6a11f4d'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (12298, '75d1fbe7-d1d3-4868-b940-d25e740f5bdd'), (24592, '1bc68948-b978-48cf-aee4-932675f8db96'), (6209, 'dc759306-d7ce-4e78-b89d-4681bc9dd25d'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (6212, '9d2539d2-4c9d-4ea5-8dad-8105ad0d656e'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29771, '54c867d1-dd3a-4a9e-97f5-dac70011ef4b'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29786, '945fd8e4-77cb-409e-af9d-4d7171e7ea93'), (20570, '5dd4252e-a29c-4f30-ae23-7b7de46bb5a0'), (7266, 'f5c4435e-d192-4727-9e6d-35acf4404166'), (30306, '22273990-7b23-4785-accd-84c550db4f53'), (29818, '02cc33d8-0c79-466e-ba0c-bdadb265c65f'), (29820, '49d6a04e-36ec-4a38-adbf-50f525d423b1'), (29821, '84d9d326-f3f5-4a05-b9fc-f14b56bf8280'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (17031, '3f233fb1-a6a0-4c1c-8465-4333aaaee216'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (22657, '9268b17f-2fc5-4ae4-8106-8039de92a69b'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (9877, '02d00698-e099-4595-9b5b-915a102c8c31'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (749, 'aadabaa8-059f-4e53-bb4a-defc70cc04cc'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (765, '65b17d57-f43a-42e2-a6d4-b96ae347324c'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5379, '287c1ed6-130b-4784-8132-3f5287c3471c'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (20229, 'cecdc71b-2ef4-4100-9920-876a3adacde0'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (779, 'c84b16d0-6361-44a8-b2c9-cc03a7a33748'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5396, '18d8ed78-6549-4720-9c12-5ae48fb89fc8'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (5923, 'd508a7d3-3d71-4f96-9bcd-951254088aab'), (805, '45825ed8-8f85-40ed-a174-19eeebd97cf7'), (807, 'ce4fda9f-c07a-4ca5-8538-9f8ededb01e2'), (808, '2232e5cb-cb93-4ea7-8c40-d51528355f6c'), (5419, 'ca377909-1f4d-48de-af71-01684b43fc59'), (812, 'ecabe7c7-05c4-4cfa-9e6e-c24df8d129b4'), (5420, '19d2ca4f-3e1d-4789-848e-5363e52ea2e9'), (5422, 'c12a80a5-4e72-4a35-b1ea-7113b01b5c00'), (815, '00347210-f522-4fa2-a819-98958034885b'), (814, '3c86feb6-9865-4995-82a9-c9e1bafa680f'), (5426, '5052dd1e-66fa-4818-9c3d-a9efed8343c7'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (27532, '2ee290a6-5920-46c1-894f-e17c3af8e39f'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (29586, '67cb4809-8361-4ee5-9dd4-cb31eef9b1bf'), (23442, '35ca6462-1413-46ab-b1be-a5db65e32709'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (23993, '1ab46630-fc89-4db9-8991-e991994175e0'), (20415, '0ff6dba8-cfe6-4157-a337-93d21a8a4a9e'), (17857, '640d96b2-19b8-40ad-a0f5-1a0a7b13b9d1'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (16364, 'd30aeb8c-78e2-4444-b089-e1fd3fe37afe'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
1. **Quadratic computation of canonical self-attention:** The vanilla Transformer has a computational complexity of \\(O(T^2 D)\\) where \\(T\\) is the time series length and \\(D\\) is the dimension of the hidden states. For long sequence time-series forecasting (also known as the _LSTF problem_), this might be really computationally expensive. To solve this problem, Informer employs a new self-attention mechanism called _ProbSparse_ attention, which has \\(O(T \log T)\\) time and space complexity.
1. **Memory bottleneck when stacking layers:** When stacking \\(N\\) encoder/decoder layers, the vanilla Transformer has a memory usage of \\(O(N T^2)\\), which limits the model's capacity for long sequences. Informer uses a _Distilling_ operation, for reducing the input size between layers into its half slice. By doing so, it reduces the whole memory usage to be \\(O(N\cdot T \log T)\\).
*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research ‡§∏‡•á) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ‡§∏‡•á) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM „Åã„Çâ) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google „Åã„Çâ) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google „Åã„Çâ) Jason Phang, Yao Zhao, and Peter J. Liu „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.Ïùò [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google ÏóêÏÑú) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu Ïùò [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google ÏóêÏÑú) Jason Phang, Yao Zhao, Peter J. Liu Ïùò [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Informer

## Overview

The Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting ](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.

This method introduces a Probabilistic Attention mechanism to select the ""active"" queries rather than the ""lazy"" queries and provides a sparse Transformer thus mitigating the quadratic compute and memory requirements of vanilla attention.

The abstract from the paper is the following:
and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*
*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (Êù•Ëá™ Google AI) ‰º¥ÈöèËÆ∫Êñá [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) Áî± Matthias Minderer, Alexey Gritsenko, Neil Houlsby ÂèëÂ∏É„ÄÇ
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (Êù•Ëá™  IBM Research) ‰º¥ÈöèËÆ∫Êñá [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) Áî± Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam ÂèëÂ∏É„ÄÇ
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (Êù•Ëá™ IBM) ‰º¥ÈöèËÆ∫Êñá [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) Áî± Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam ÂèëÂ∏É„ÄÇ
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PatchTSMixer

## Overview

The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.
*TSMixer is a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules designed for multivariate forecasting and representation learning on patched time series. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components,
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy.*
At a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:

![model](https://github.com/namctin/transformers/assets/8100/150af169-29de-419a-8d98-eb78251c21fa)

The abstract from the paper is the following:
- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teacher (which, in the
  DeiT paper, is a ResNet like-model). The distillation token is learned through backpropagation, by interacting with
  the class ([CLS]) and patch tokens through the self-attention layers.
- There are 2 ways to fine-tune distilled models, either (1) in a classic way, by only placing a prediction head on top
  of the final hidden state of the class token and not using the distillation signal, or (2) by placing both a
  prediction head on top of the class token and on top of the distillation token. In that case, the [CLS] prediction
  head is trained using regular cross-entropy between the prediction of the head and the ground-truth label, while the
  distillation prediction head is trained using hard distillation (cross-entropy between the prediction of the
  distillation head and the label predicted by the teacher). At inference time, one takes the average prediction

Here is the question: How does the PatchTST model improve computational efficiency for long-term time series forecasting?
Here is answer 1:By reducing the size of attention maps through patch-based segmentation
Here is answer 2: By introducing multivariate embeddings for all channels
Here is answer3: By implementing dynamic memory allocation for prediction tasks
Here is answer 4: By scaling transformer layers based on dataset size",The correct answer is: By reducing the size of attention maps through patch-based segmentation
"What is a common pitfall when loading external datasets for NLP pipelines, and how can it be mitigated?","[(13825, 'f1446f89-52a4-4aaf-ab1a-9b92f5010ce7'), (9, 'ee938445-5e5d-4e33-b4a3-e17998c36b16'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (11792, 'e3cb7b34-455d-4c33-be90-a3547abf420d'), (26641, '972af779-bf9e-4865-bebd-97def3d5dd6d'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (26651, 'f1b12a30-cfeb-4c01-a3a4-7f8d4640303f'), (3611, '5acb0ab4-672d-47da-9ea1-6cdc6cb1d8db'), (8223, '2a8810cd-a76d-40d2-acfb-a7c4dede20d4'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (8737, 'ea87eaf8-31e3-409d-a5cc-58342498b4af'), (28719, '348f9b74-e568-4574-9dab-2e6621e37239'), (24640, '815d70f7-2c5b-4af2-8a4e-fa377374ffad'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (28739, '50948ec6-1039-4799-9d5c-f657b72f8d39'), (5707, 'f1652f24-131f-48b0-a83d-0e7049f1189e'), (25175, '1e8094f9-39e6-4247-b6d2-5b55744f798d'), (25176, '754d1c50-6a81-4ccd-99cf-0c9628db876b'), (28767, '553d5e61-e486-48b6-b9ad-9a9ef6b3e3af'), (19040, 'fd7fbc60-e179-4a50-ad6d-74b9dbce7e55'), (19041, 'e0aa2a3f-538a-4293-a270-06da8e763a95'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (7272, '90527d40-f42d-47ed-a5d3-255fa1425973'), (20073, 'ae4fa9d8-d08d-4725-b3c4-0f8a67b679c8'), (24682, '6246272f-fdbc-42c1-80f5-adc29d46040f'), (13422, '7fcc282e-51e4-4714-887d-b6075a5d4b55'), (24691, 'e2088a54-3bfc-40df-b1bd-8d4fb59815b9'), (4212, '12f87a03-1269-4e9f-938f-0f142e08b79a'), (23159, '8df7603d-0d12-428d-8ffd-3b4c55cd0c5f'), (9337, 'e15f8374-58a2-41f8-ace2-17f6d19efe49'), (18046, '4c76e605-c6d5-4738-ae77-d6e0407ebc5b'), (24704, 'c6d8c610-53c9-47f6-821c-69ce50caa756'), (4741, '210088c4-7fef-45fb-b3bd-ba5759a63329'), (23687, 'a74310e1-5e6b-478b-a97a-505cac3cf243'), (4757, 'f13d0378-7292-42fa-881e-3186d8096beb'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (24727, '710d9250-ca94-45f1-a999-cbc01b7e8ff7'), (13465, 'cbcbfaf3-ee38-4ff2-8f58-74eca846f9a2'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (9381, '1695844a-c3fc-40f7-8e86-4f2d6eea7efa'), (9383, '5987ba42-1ed1-474f-b447-872e7f6c8277'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (28844, '2ac701fb-f40a-465d-b95a-8ecf70258ad1'), (9390, '8ca789f9-6a64-4dfd-a067-8a38701e9237'), (9903, '584836fd-d59e-43e0-93a4-4436c108b340'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (5815, '7a89c479-9823-4c0f-a0de-e3ecf304e920'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (21195, 'faa2b5eb-cd0a-4c23-a187-337ef7e890bd'), (5328, '7e28df2f-1656-43a9-b36b-adab211aeb45'), (19664, '2872ed56-b851-405f-8601-3748f47a00e1'), (21205, '6bda238b-e50a-4ace-95e5-877b1ae72029'), (5335, 'f1e2bdbb-8a30-4d40-9a37-04b75cc51d54'), (20186, 'e4800925-3dd9-4604-b5d2-755f623ef3b9'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (20703, '185d614c-743f-4b53-9078-c44a17e5c1ae'), (7393, '69b3bd6a-2eb1-4f94-84eb-2ecd7239847c'), (31458, '772a8786-3e64-4959-84fa-931ce6254e05'), (7395, 'e6cbeb63-c998-433d-b0b3-afb65bf27b67'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (20717, 'ceb57f22-5344-4bc1-8c33-80214f9440a7'), (26885, 'f2bb8a8e-0f3c-4545-9014-c725c069e776'), (7942, '14990c0d-4b9f-4a48-b72f-ad5c83124c6f'), (23303, 'c8340867-9c9b-4274-adb6-873aeaef3797'), (23302, '55b911a2-3a70-4664-a3f7-305e13907838'), (23306, 'ceda3f28-4ba3-4ca6-84a7-a4b914fd8d4f'), (3339, '6c368a7d-8543-44b3-9993-4674ebdec341'), (16141, 'aa084d5d-8025-4004-9e13-9e10abb5e17c'), (3342, '40de6e0e-cd3f-4ac4-8b93-e0a427a98e4d'), (27407, 'cc6b271a-4d91-452a-b62b-5123e6bd61d7'), (28948, '35cfc8f5-5fbd-4311-bc92-d1f421c75890'), (7957, 'e79d5025-a9c4-432e-b94c-57adc02cbeba'), (21788, '76f89282-5e29-480a-9fb9-107b45af559d'), (21789, 'f348a103-0156-4f06-a4aa-5eb608e899a5'), (13091, 'fd87ced8-d15d-4eb8-98f3-447a6c638420'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (25898, '73ac5dcb-df39-4bca-8d7e-42637669ea82'), (25900, '22302b1e-0111-4e79-a205-ad225b9df0ac'), (24890, 'afcbcb7c-ee42-4823-ad2c-9b6d06c0f04b'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (15178, '98813955-fdd5-49a0-9a60-14d3ae8b1b65'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (4438, 'c433e5f5-9651-489c-8a76-f04bf6884777'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (25442, 'c84e5d7a-448b-458c-8c4e-d72920abe394'), (11619, '82602400-9a2e-4fb0-9c55-464106efdee1'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (31086, '7c58f058-2d25-456b-b666-b121481ab6d4'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (11650, 'e316dc70-9979-4cbe-b6b2-414868e8e8f2'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (31110, '9726dad8-10ae-460c-94fa-9e76f693eb05'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (17302, '10950558-fd37-474e-868f-4f3fca1d098a'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (25519, 'a69222a9-5a13-4e58-b18b-59551968d308'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (26036, '0e8e3f85-e18c-46e7-97ab-595f1fda1603'), (21952, '138ee00c-29cc-4a4a-b56c-0e17a9529580'), (31170, '694c9ec9-5c6b-4ea4-92ba-b75bc909f395'), (11716, '5aa7f3a8-b022-4936-88ea-c22c3c501a94'), (7623, 'ebcb37fc-acf3-4241-9938-95c52730e549'), (16850, '8d4c8a3e-04e1-47df-82a1-1b7d7af6968c'), (16853, '3cb9a538-dae1-4898-8b6a-a7940cd75253'), (30171, 'c3402588-8a4f-4dcd-81b8-f2315cfaefc7'), (16871, 'e66a5e45-2cf0-4d15-bcfc-0485a92e992f'), (1513, '710901b9-382c-447e-b95f-bb910a1fdce4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

[[autodoc]] pipeline

## Pipeline batching

All pipelines can use batching. This will work
whenever the pipeline uses its streaming ability (so when passing lists or `Dataset` or `generator`).

```python
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
import datasets

dataset = datasets.load_dataset(""imdb"", name=""plain_text"", split=""unsupervised"")
pipe = pipeline(""text-classification"", device=0)
for out in pipe(KeyDataset(dataset, ""text""), batch_size=8, truncation=""only_first""):
    print(out)
    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]
    # Exactly the same output as before, but the content are passed
    # as batches to the model
```

If `args.dataset` contains the `gs://` identifier, TensorFlow will understand that it needs to look into a GCS bucket. Loading locally is as easy as removing the `gs://` identifier. For the rest of the data pipeline-related code, you can refer to [this section](https://github.com/huggingface/transformers/blob/474bf508dfe0d46fc38585a1bb793e5ba74fddfd/examples/tensorflow/language-modeling-tpu/run_mlm.py#L186-#L201) in the training script.
Beam Datasets

Some datasets are too large to be processed on a single machine. Instead, you can process them with [Apache Beam](https://beam.apache.org/), a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as [Apache Flink](https://flink.apache.org/), [Apache Spark](https://spark.apache.org/), or [Google Cloud Dataflow](https://cloud.google.com/dataflow).

We have already created Beam pipelines for some of the larger datasets like [wikipedia](https://huggingface.co/datasets/wikipedia), and [wiki40b](https://huggingface.co/datasets/wiki40b). You can load these normally with [`load_dataset`]. But if you want to run your own Beam pipeline with Dataflow, here is how:

1. Specify the dataset and configuration you want to process:

```
DATASET_NAME=your_dataset_name  # ex: wikipedia
CONFIG_NAME=your_config_name    # ex: 20220301.en
```

2. Input your Google Cloud Platform information:
#### Philosophy #4: You shouldn‚Äôt have to write your own data pipeline, especially for common tasks

In addition to `transformers`, a huge open repository of pre-trained models, there is also ü§ó `datasets`, a huge open repository of datasets - text, vision, audio and more. These datasets convert easily to TensorFlow Tensors and Numpy arrays, making it easy to use them as training data. Here‚Äôs a quick example showing us tokenizing a dataset and converting it to Numpy. As always, make sure your tokenizer matches the model you want to train with, or things will get very weird!

```py
from datasets import load_dataset
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

dataset = load_dataset(""glue"", ""cola"")  # Simple text classification dataset
dataset = dataset[""train""]  # Just take the training split for now
```

The [`pipeline`] can also iterate over an entire dataset for any task you like. For this example, let's choose automatic speech recognition as our task:

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline(""automatic-speech-recognition"", model=""facebook/wav2vec2-base-960h"")
```

Load an audio dataset (see the ü§ó Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio) for more details) you'd like to iterate over. For example, load the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset:

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset(""PolyAI/minds14"", name=""en-US"", split=""train"")  # doctest: +IGNORE_RESULT
```

For larger datasets where the inputs are big (like in speech or vision), you'll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the [pipeline API reference](./main_classes/pipelines) for more information.

### Use another model and tokenizer in the pipeline

The [`pipeline`] can accommodate any model from the [Hub](https://huggingface.co/models), making it easy to adapt the [`pipeline`] for other use-cases. For example, if you'd like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) finetuned for sentiment analysis you can use for French text:

```py
>>> model_name = ""nlptown/bert-base-multilingual-uncased-sentiment""
### Memory usage and data loading

One thing to note is that all data is loaded into memory in this script. Most question answering datasets are small
enough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle
data streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data
required to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and 
README, but for more information you can see the 'Input Datasets' section of 
[this document](https://www.tensorflow.org/guide/tpu).

### Example command
```

As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png"" alt=""The task selector on the web interface."" width=""80%""/>
</div>

You can also instantiate the checkpoint using the model architecture directly:
Build and load

Nearly every deep learning workflow begins with loading a dataset, which makes it one of the most important steps. With ü§ó Datasets, there are more than 900 datasets available to help you get started with your NLP task. All you have to do is call: [`load_dataset`] to take your first step. This function is a true workhorse in every sense because it builds and loads every dataset you use.

## ELI5: `load_dataset`

Let's begin with a basic Explain Like I'm Five.

A dataset is a directory that contains:

- Some data files in generic formats (JSON, CSV, Parquet, text, etc.)
- A dataset card named `README.md` that contains documentation about the dataset as well as a YAML header to define the datasets tags and configurations
- An optional dataset script if it requires some code to read the data files. This is sometimes used to load files of specific formats and structures.
<Tip>
‚ö†Ô∏è The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! <a href=""https://huggingface.co/join"">Create a huggingface.co</a> account to benefit from all available features!
</Tip>

Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems.

## Working with pipelines[[working-with-pipelines]]

<Youtube id=""tiZFewofSLM"" />

The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:

```python
from transformers import pipeline

classifier = pipeline(""sentiment-analysis"")
classifier(""I've been waiting for a HuggingFace course my whole life."")
```

```python out
{'answer': '\nü§ó Transformers: State of the Art NLP', 'start': 0, 'end': 37, 'score': 0.33867}
{'answer': 'Jax, PyTorch and TensorFlow', 'start': 1892, 'end': 1919, 'score': 0.97149}
```

If we ignore the first result, we get the same result as our pipeline for this long context -- yay!

<Tip>

‚úèÔ∏è **Try it out!** Use the best scores you computed before to show the five most likely answers (for the whole context, not each chunk). To check your results, go back to the first pipeline and pass in `top_k=5` when calling it.

</Tip>

This concludes our deep dive into the tokenizer's capabilities. We will put all of this in practice again in the next chapter, when we show you how to fine-tune a model on a range of common NLP tasks.
### Memory usage and data loading

One thing to note is that all data is loaded into memory in this script. Most text classification datasets are small
enough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle
data streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data
required to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and 
README, but for more information you can see the 'Input Datasets' section of 
[this document](https://www.tensorflow.org/guide/tpu).

### Example command
```

Pipelines are great for experimentation as switching from one model to another is trivial; however, there are some ways to optimize them for larger workloads than experimentation. See the following guides that dive into iterating over whole datasets or using pipelines in a webserver:
of the docs:
* [Using pipelines on a dataset](#using-pipelines-on-a-dataset)
* [Using pipelines for a webserver](./pipeline_webserver)

## Parameters

[`pipeline`] supports many parameters; some are task specific, and some are general to all pipelines.
In general, you can specify parameters anywhere you want:

```py
transcriber = pipeline(model=""openai/whisper-large-v2"", my_parameter=1)

out = transcriber(...)  # This will use `my_parameter=1`.
out = transcriber(..., my_parameter=2)  # This will override and use `my_parameter=2`.
out = transcriber(...)  # This will go back to using `my_parameter=1`.
- Our [paper](https://arxiv.org/abs/2109.02846) ""[Datasets: A Community Library for Natural Language Processing](https://arxiv.org/abs/2109.02846)"" documents the Hugging Face Datasets project that has over 300 contributors. This community project gives easy access to hundreds of datasets to researchers. It has facilitated new use cases of cross-dataset NLP, and has advanced features for tasks like indexing and streaming large datasets.
- Our collaboration with researchers from TU Darmstadt lead to another paper accepted at the conference ([""Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning""](https://arxiv.org/abs/2109.04144)). In this paper, we show that prompt-based fine-tuned language models (which achieve strong performance in few-shot setups) still suffer from learning surface heuristics (sometimes called *dataset biases*), a pitfall that zero-shot models don't exhibit.
### Loading data as a tf.data.Dataset

If you want to avoid slowing down training, you can load your data as a `tf.data.Dataset` instead. Although you can write your own
`tf.data` pipeline if you want, we have two convenience methods for doing this:

- [`~TFPreTrainedModel.prepare_tf_dataset`]: This is the method we recommend in most cases. Because it is a method
on your model, it can inspect the model to automatically figure out which columns are usable as model inputs, and
discard the others to make a simpler, more performant dataset.
- [`~datasets.Dataset.to_tf_dataset`]: This method is more low-level, and is useful when you want to exactly control how
your dataset is created, by specifying exactly which `columns` and `label_cols` to include.

Before you can use [`~TFPreTrainedModel.prepare_tf_dataset`], you will need to add the tokenizer outputs to your dataset as columns, as shown in
the following code sample:
Load a dataset from the Hub

Finding high-quality datasets that are reproducible and accessible can be difficult. One of ü§ó Datasets main goals is to provide a simple way to load a dataset of any format or type. The easiest way to get started is to discover an existing dataset on the [Hugging Face Hub](https://huggingface.co/datasets) - a community-driven collection of datasets for tasks in NLP, computer vision, and audio - and use ü§ó Datasets to download and generate the dataset.

This tutorial uses the [rotten_tomatoes](https://huggingface.co/datasets/rotten_tomatoes) and [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) datasets, but feel free to load any dataset you want and follow along. Head over to the Hub now and find a dataset for your task!

## Load a dataset
```

This is exactly what we wanted. Now, we can apply various preprocessing techniques to clean up the data, tokenize the reviews, and so on.

<Tip>

The `data_files` argument of the `load_dataset()` function is quite flexible and can be either a single file path, a list of file paths, or a dictionary that maps split names to file paths. You can also glob files that match a specified pattern according to the rules used by the Unix shell (e.g., you can glob all the JSON files in a directory as a single split by setting `data_files=""*.json""`). See the ü§ó Datasets [documentation](https://huggingface.co/docs/datasets/loading.html#local-and-remote-files) for more details.

</Tip>

The loading scripts in ü§ó Datasets actually support automatic decompression of the input files, so we could have skipped the use of `gzip` by pointing the `data_files` argument directly to the compressed files:
```

<Tip warning={true}>

`pct1_dropremainder` rounding may truncate the last examples in a dataset if the number of examples in your dataset don't divide evenly by 100.

</Tip>

<a id='troubleshoot'></a>

## Troubleshooting

Sometimes, you may get unexpected results when you load a dataset. Two of the most common issues you may encounter are manually downloading a dataset and specifying features of a dataset.

### Manual download

Certain datasets require you to manually download the dataset files due to licensing incompatibility or if the files are hidden behind a login page. This causes [`load_dataset`] to throw an `AssertionError`. But ü§ó Datasets provides detailed instructions for downloading the missing files. After you've downloaded the files, use the `data_dir` argument to specify the path to the files you just downloaded.

For example, if you try to download a configuration from the [MATINF](https://huggingface.co/datasets/matinf) dataset:
```

- Speech Translation via Pipelines

  The automatic speech recognition pipeline can also be used to translate speech in just a couple lines of code

```python
>>> from datasets import load_dataset
>>> from transformers import pipeline

>>> librispeech_en = load_dataset(""hf-internal-testing/librispeech_asr_dummy"", ""clean"", split=""validation"")
>>> asr = pipeline(
...     ""automatic-speech-recognition"",
...     model=""facebook/s2t-wav2vec2-large-en-de"",
...     feature_extractor=""facebook/s2t-wav2vec2-large-en-de"",
... )

>>> translation_de = asr(librispeech_en[0][""file""])
Additionally, the `LOAD_DATASETS` will let you configure the sample datasets that will be pre-loaded. The default value is `single` and the supported values for this variable are:
    1. `single`: Load single datasets for TextClassification task.
    2. `full`: Load all the sample datasets for NLP tasks (TokenClassification, TextClassification, Text2Text)
    3. `none`: No datasets being loaded.


## How to upload data

Once your Argilla Space is running:

1. You need to find the **Space Direct URL under the ""Embed this Space""** option (top right, see screenshot below).
2. This URL gives you access to a full-screen Argilla UI for data labelling. The **Direct URL is the api_url parameter** for connecting the argilla Python client in order to read and write data programmatically.
3. You are now ready to **upload your first dataset into Argilla**.

Here is the question: What is a common pitfall when loading external datasets for NLP pipelines, and how can it be mitigated?
Here is answer 1:Incompatible file formats; resolve by converting to JSON
Here is answer 2: Unlabeled data; resolve by adding manual labels
Here is answer3: File encoding mismatches; resolve by specifying encoding formats
Here is answer 4: Duplicate records; resolve by filtering datasets beforehand","Accordingly based upon given information I would say correct answer would be 

File encoding mismatches; resolve by specifying encoding formats"
"What feature of the Transformers library supports its broad applicability to different frameworks (e.g., PyTorch, TensorFlow)?","[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (17412, '1d9b136f-d20c-4989-8975-80625fa8da45'), (17413, '5c272bf9-ff61-4ced-9930-7c174b92dc98'), (17415, '0542b335-d341-4e8d-8d7c-cbf9eeebae8d'), (17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (27146, 'cbb71c05-7152-43db-b473-37078bba89e2'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (3599, '9cf1589b-a7c4-4c31-a0b5-c8ee2365a3b5'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (5155, '86625bb7-792a-426a-9ade-ea98317da2e0'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (12906, '39ba7aa1-526f-44ed-814c-779d9283deaf'), (24691, 'e2088a54-3bfc-40df-b1bd-8d4fb59815b9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (20667, 'b0b180ee-2478-48b8-9446-bf6a9feabb3b'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (9415, '78661659-8f11-4005-800b-e1d23d747852'), (214, 'ced565fd-8f3c-4e7b-a0f9-9e46e7ed2922'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (17116, 'df0e2cb6-a368-4e77-894f-ce91ac16ed6c'), (6380, '6aed28bc-1ba7-45a0-9248-1d8f31471c81'), (22770, '978ad2c3-695e-4a95-82c8-b6fb9af51394'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (15608, '334237b9-d851-4edf-85f1-c4c4f52047ad'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (22786, 'dc8223e8-f89b-4a2a-b807-5215be7a03da'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (15625, 'abc82f6a-5afa-43a2-8de3-f016f34e7055'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (28429, '9c6fb59e-1d9f-48bd-be33-bd99c86d369d'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (3363, 'c8cd6d58-bf3c-4c0f-8cd0-6d6d28ea07b9'), (16678, '1e0fefb6-23b9-4fea-8cfe-628c3ca26e11'), (16679, '4002f15e-8a33-4cad-927c-232fe4ea7a37'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (17739, '540c412e-c6fc-4761-a366-165b00c59e31'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (340, 'f260b640-4e93-47fd-9cf4-b3ad7c90bd35'), (341, '5237d4cb-369e-42c3-9609-725e481f1c38'), (9051, '09b3a3e8-2734-459b-9dcf-d885565ee6dd'), (26975, 'a77435a6-b248-4c25-88bb-c6f596b9ed0c'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (14704, 'd152c193-d62e-412a-9a73-fad192c92a19'), (7546, '125265e4-f906-4180-bf64-53f1be1bd941'), (28539, '15ca206b-e242-4240-9c56-b957d70a1add'), (25980, '44d9eb20-ac45-46e9-b4ea-310683db7670'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (20871, 'edfcf78d-61e7-443d-8514-6db1ea1b0bca'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (20872, '21fadc48-4612-45a6-a09a-f093a8fb038d'), (24972, '9084f511-5580-4546-944c-05245a37f48b'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (22434, '8bea8922-5c12-40fa-87bf-aa092348b13f'), (17828, 'e0e24538-49e6-418a-8fb8-b10e6a8d00f1'), (5044, '3347ade6-a88b-4653-97f6-3b4beff0fa0c'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (5048, '4fd5ee84-be31-4b12-bfc2-332bc69dfa07'), (28601, 'eb9de433-f226-428a-9cca-06ee8c6b08ce'), (28602, '5b1d4ee5-f2ec-46e3-801a-8d10081798fd'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (15808, '528599cb-95fc-450e-b657-89b44f74fb95'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (15810, '26a1f21c-ebc4-40c0-8a1a-34e9a46f1698'), (6083, '34942899-e37f-425b-ae2a-278f328948fc'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (6087, '5ae43b09-0e79-4166-8a50-8540cdbdf06a'), (9163, 'd7206e44-29b4-499b-92c4-0008be204752'), (13775, '081f194c-20d7-476c-a3bf-56c80cdcc4e5'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (12257, '0dae409c-84f2-422a-ad8c-f8832627d8a9'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (7154, 'a06b9d90-8d8a-4fb6-b53c-619bed721fbb'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem.

In a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code.

In this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware.

And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you.
- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.
- **Flexibility**: At their core, all models are simple PyTorch `nn.Module` or TensorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) frameworks.
- **Simplicity**: Hardly any abstractions are made across the library. The ""All in one file"" is a core concept: a model's forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.

This last feature makes ü§ó Transformers quite different from other ML libraries. The models are not built on modules 
that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.
```
</tf>
</frameworkcontent>

One particularly cool ü§ó Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The `from_pt` or `from_tf` parameter can convert the model from one framework to the other:

<frameworkcontent>
<pt>

```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>

```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
## The technical philosophy

In this section, we wanted to share our philosophy behind adding support for Computer Vision in ü§ó¬†Transformers so that the community is aware of the design choices specific to this area.

Even though Transformers started with NLP, we support multiple modalities today, for example ‚Äì vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:

- Easy model download with a single line of code with `from_pretrained()`
- Easy model upload with `push_to_hub()`
- Support for loading huge checkpoints with efficient checkpoint sharding techniques
- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))
- Initialization from model configurations
- Support for both PyTorch and TensorFlow (non-exhaustive)
- and many more
Using ü§ó `transformers` at Hugging Face

ü§ó `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. We are a bit biased, but we really like ü§ó `transformers`!

## Exploring ü§ó transformers in the Hub

There are over 25,000 `transformers` models in the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=transformers&sort=downloads). 

You can find models for many different tasks:
and models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint.
  - On top of those three base classes, the library provides two APIs: [`pipeline`] for quickly
    using a model for inference on a given task and [`Trainer`] to quickly train or fine-tune a PyTorch model (all TensorFlow models are compatible with `Keras.fit`).
  - As a consequence, this library is NOT a modular toolbox of building blocks for neural nets. If you want to
    extend or build upon the library, just use regular Python, PyTorch, TensorFlow, Keras modules and inherit from the base
    classes of the library to reuse functionalities like model loading and saving. If you'd like to learn more about our coding philosophy for models, check out our [Repeat Yourself](https://huggingface.co/blog/transformers-design-philosophy) blog post.
4. Easily customize a model or an example to your needs:
  - We provide examples for each architecture to reproduce the results published by its original authors.
  - Model internals are exposed as consistently as possible.
  - Model files can be used independently of the library for quick experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
""""""
question_answerer(question=question, context=long_context)
## Supported model architectures and frameworks

We intend to support state-of-the-art transformer architectures for natural language processing, computer vision, and speech, such as BERT, DistilBERT, ROBERTA, Vision Transformer, CLIP, and Wav2Vec2.  Of course, generative AI models will be available too (e.g., GPT2, GPT-NeoX, T5, OPT, LLaMA), including our own BLOOM and StarCoder models. Lastly, we will also support more traditional computer vision models, like ResNet and ResNext, and deep learning recommendation models, a first for us.

We'll do our best to test and validate these models for PyTorch, TensorFlow, and ONNX Runtime for the above platforms. Please remember that not all models may be available for training and inference for all frameworks or all hardware platforms.

## The road ahead
üìù **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.<br>
üñºÔ∏è **Computer Vision**: image classification, object detection, and segmentation.<br>
üó£Ô∏è **Audio**: automatic speech recognition and audio classification.<br>
üêô **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model's life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.
# Replace this with your own checkpoint
model_checkpoint = ""huggingface-course/bert-finetuned-squad""
question_answerer = pipeline(""question-answering"", model=model_checkpoint)

context = """"""
ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch and TensorFlow ‚Äî with a seamless integration
between them. It's straightforward to train your models with one before loading them for inference with the other.
""""""
question = ""Which deep learning libraries back ü§ó Transformers?""
question_answerer(question=question, context=context)
## Open Source

### New in Transformers

Summer has been an exciting time for ü§ó Transformers! The library reached 50,000 stars, 30 million total downloads, and almost 1000 contributors! ü§©

So what's new? JAX/Flax is now the 3rd supported framework with over [5000](https://huggingface.co/models?library=jax&sort=downloads) models in the Hub! You can find actively maintained [examples](https://github.com/huggingface/transformers/tree/master/examples/flax) for different tasks such as text classification. We're also working hard on improving our TensorFlow support: all our [examples](https://github.com/huggingface/transformers/tree/master/examples/tensorflow) have been reworked to be more robust, TensorFlow idiomatic, and clearer. This includes examples such as summarization, translation, and named entity recognition.
#### Suraj Patil & Patrick von Platen, Machine Learning Engineers at Hugging Face
- Talk: How to use JAX/Flax with Transformers
- Abstract: Transformers is one of the most popular open-source ML libraries and supports PyTorch, Tensorflow, and JAX/Flax. In this talk, we will explain how JAX/Flax models should be used in Transformers and compare their design in Transformers with the design of PyTorch models in Transformers. In the second part, we will give you a hands-on presentation of how a model can be trained end-to-end with the official JAX/Flax example scripts using Transformers & Datasets. Along the way, we want to give you some tips and tricks on how to best realize your project.
- Speaker info: Suraj and Patrick are part of Hugging Face‚Äôs open source team and lead the integration of JAX/Flax into Transformers.
- GitHub: https://github.com/patil-suraj & https://github.com/patrickvonplaten
On the 2021 version, we kept the ability to run inference workloads through PyTorch and Tensorflow as in the 
previous blog [(1)](https://medium.com/huggingface/benchmarking-transformers-pytorch-and-tensorflow-e2917fb891c2) along with their traced counterpart
[TorchScript (6)](https://pytorch.org/docs/stable/jit.html), [Google Accelerated Linear Algebra (XLA) (7)](https://www.tensorflow.org/xla).  
Also, we decided to include support for [ONNX Runtime (8)](https://www.onnxruntime.ai/) as it provides many optimizations 
specifically targeting transformers based models which makes it a strong candidate to consider when discussing 
performance.
```

## BetterTransformer

<Tip warning={true}>

Some BetterTransformer features are being upstreamed to Transformers with default support for native `torch.nn.scaled_dot_product_attention`. BetterTransformer still has a wider coverage than the Transformers SDPA integration, but you can expect more and more architectures to natively support SDPA in Transformers.

</Tip>

<Tip>

Check out our benchmarks with BetterTransformer and scaled dot product attention in the [Out of the box acceleration and memory savings of ü§ó decoder models with PyTorch 2.0](https://pytorch.org/blog/out-of-the-box-acceleration/) and learn more about the fastpath execution in the [BetterTransformer](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) blog post.

</Tip>
-->

# How to add a model to ü§ó Transformers?

The ü§ó Transformers library is often able to offer new models thanks to community contributors. But this can be a challenging project and requires an in-depth knowledge of the ü§ó Transformers library and the model to implement. At Hugging Face, we're trying to empower more of the community to actively add models and we've put together this guide to walk you through the process of adding a PyTorch model (make sure you have [PyTorch installed](https://pytorch.org/get-started/locally/)).

<Tip>

If you're interested in implementing a TensorFlow model, take a look at the [How to convert a ü§ó Transformers model to TensorFlow](add_tensorflow_model) guide!

</Tip>

Along the way, you'll:
- A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning.
  - Simple ways to mask and prune Transformer heads.

- Easily switch between PyTorch, TensorFlow 2.0 and Flax, allowing training with one framework and inference with another.

## Main concepts

The library is built around three types of classes for each model:
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
Ë¶ÅÊ™¢Êü•ÊüêÂÄãÊ®°ÂûãÊòØÂê¶Â∑≤Êúâ Flax„ÄÅPyTorch Êàñ TensorFlow ÁöÑÂØ¶‰ΩúÔºåÊàñÂÖ∂ÊòØÂê¶Âú®ü§ó Tokenizers ÂáΩÂºèÂ∫´‰∏≠ÊúâÂ∞çÊáâÁöÑ tokenizerÔºåÊï¨Ë´ãÂèÉÈñ±[Ê≠§Ë°®](https://huggingface.co/docs/transformers/index#supported-frameworks)„ÄÇ

ÈÄô‰∫õÂØ¶‰ΩúÂùáÂ∑≤ÊñºÂ§öÂÄãË≥áÊñôÈõÜÊ∏¨Ë©¶ÔºàË´ãÂèÉÈñ±ÁØÑ‰æãËÖ≥Êú¨Ôºâ‰∏¶ÊáâËàáÂéüÁâàÂØ¶‰ΩúË°®ÁèæÁõ∏Áï∂„ÄÇ‰Ω†ÂèØ‰ª•Âú®ÁØÑ‰æãÊñá‰ª∂ÁöÑ[Ê≠§ÁØÄ](https://huggingface.co/docs/transformers/examples)‰∏≠‰∫ÜËß£ÂØ¶‰ΩúÁöÑÁ¥∞ÁØÄ„ÄÇ


## ‰∫ÜËß£Êõ¥Â§ö
# ü§ó Transformers

State-of-the-art Machine Learning for [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [JAX](https://jax.readthedocs.io/en/latest/).

ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:
Ë¶ÅÊ£ÄÊü•Êüê‰∏™Ê®°ÂûãÊòØÂê¶Â∑≤Êúâ Flax„ÄÅPyTorch Êàñ TensorFlow ÁöÑÂÆûÁé∞ÔºåÊàñÂÖ∂ÊòØÂê¶Âú® ü§ó Tokenizers Â∫ì‰∏≠ÊúâÂØπÂ∫îËØçÁ¨¶ÂåñÂô®ÔºàtokenizerÔºâÔºåÊï¨ËØ∑ÂèÇÈòÖ[Ê≠§Ë°®](https://huggingface.co/docs/transformers/index#supported-frameworks)„ÄÇ

Ëøô‰∫õÂÆûÁé∞ÂùáÂ∑≤‰∫éÂ§ö‰∏™Êï∞ÊçÆÈõÜÊµãËØïÔºàËØ∑ÂèÇÁúãÁî®‰æãËÑöÊú¨ÔºâÂπ∂Â∫î‰∫éÂéüÁâàÂÆûÁé∞Ë°®Áé∞Áõ∏ÂΩì„ÄÇ‰Ω†ÂèØ‰ª•Âú®Áî®‰æãÊñáÊ°£ÁöÑ[Ê≠§ËäÇ](https://huggingface.co/docs/transformers/examples)‰∏≠‰∫ÜËß£Ë°®Áé∞ÁöÑÁªÜËäÇ„ÄÇ


## ‰∫ÜËß£Êõ¥Â§ö

Here is the question: What feature of the Transformers library supports its broad applicability to different frameworks (e.g., PyTorch, TensorFlow)?
Here is answer 1:Unified pre-trained model formats
Here is answer 2: Custom pipeline creation for specific tasks
Here is answer3: Interoperability with JAX, PyTorch, and TensorFlow
Here is answer 4: Built-in visualization tools for debugging","The correct answer should be:

Here Is Answer3 : Interoperability With JAx , pytroch , And tensorflow"
Why is licensing compliance critical when sharing models on the Hugging Face Hub?,"[(19982, '4cfecf26-a0c7-4482-83fd-9d15cf4f3a8d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (22547, '51e87e51-6544-440b-9057-4201251fc27d'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (19488, 'e5ffa4ca-1cb8-46ea-b311-723f83f32707'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (29235, '28f83b83-b564-4757-a1e4-3c0a127d21bd'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (12348, '57a3c79a-af62-43be-bebc-4ef70c6c1495'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (22588, 'a59c4df1-bdaa-4faa-b0f4-f3b5aeff16a6'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (28741, '50733951-b5db-4e9b-83a4-b2911af5c989'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (25685, 'e7551698-0040-4251-b71c-50be7bc42a95'), (24664, 'e88147dc-45fe-448b-b483-b5a91f408802'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (14944, 'cd075ce4-d02e-47be-ad35-21b38e850692'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (1127, '61bce4e3-ba06-42fd-82da-5f4b1ae2562a'), (3183, '05adffa9-2646-489c-b3ad-2357cb108a8e'), (13937, 'edfad0ce-4e2e-4e9b-ac33-041d619a5911'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (13457, '3eab0a1f-ba7f-4162-a614-4f6915347e14'), (19603, '561460ee-7e32-4394-b0d3-d59f4b7790dc'), (27284, '344b1139-9e69-4a8a-a5b7-593e7b089268'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (30380, 'b4858a62-bfab-4374-b4e5-c287a54d33be'), (14509, '64200989-d1d6-4b32-8613-3c0ad1c88f4f'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (17596, 'be398fa1-fe61-4e41-ac13-876c1fb3040e'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (7873, '6685e912-4253-46db-a9e7-b7dfb8fc6141'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (29896, 'b72ba5ef-9507-48a0-91d0-2fe3e2cc33f3'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (11975, 'c977182b-d101-4944-95e4-2c832d9bf6fd'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (14535, 'b9835dda-ff2d-4d55-a497-5553a72948a6'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (10461, '791d16b9-ac09-45b6-a426-0eea3d3b4063'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (24804, '23cde3d9-30d2-4fd8-84f5-8504a15e3a42'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (5873, '9d5439bb-61db-43f2-ba27-d82d1ce267c5'), (14577, '03a094b5-b29c-4840-a743-d2ceb896c494'), (23284, '59c5d501-a716-4200-a252-695f07ef83dc'), (21243, '4c853a7f-56ab-4489-93ba-acedd42e9107'), (11526, '994250e0-f714-40fe-b8ff-52c2610db830'), (7434, '5d226047-dfd9-4632-a2fe-7a48cd58fe3e'), (8970, 'd5af4c95-97c8-4196-bab1-0d74644248f4'), (4366, 'fa03c662-5522-4732-a0ba-6b673062d9af'), (1301, '5137aed7-d723-4ecf-8874-26a7c9ecb926'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (10530, '04d72a8e-2ec5-4e32-86ed-6aad0064f400'), (26919, '5ebb5465-6f4c-4e1c-8326-5f71516e66a4'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (347, 'd614507c-0464-48b7-bad7-c25df4c56372'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (4449, '98291195-a6fd-4eeb-8e55-4bc54d2a1fb3'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (8549, '7ee9801f-9160-4b67-ab5e-ccacabf36b60'), (17767, 'a7af65e5-6000-4846-b469-ecc521b89dfa'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (24436, '62eee91c-545a-404b-97af-1d99deab920c'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (19335, 'e7a25af1-ba2a-4e2e-af04-f55deda54356'), (13196, '5f4d0098-0f25-4cd7-9d59-4037a77c492b'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (22421, 'bdae70d4-1482-4a7a-aa11-6e2aa1cd81c2'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (17301, '9e130d4e-12df-4941-91c1-6e5fd07be093'), (11163, '395c4c3d-8c49-42dc-9fda-2e3de70b64bc'), (10655, 'b65f54ab-b114-433c-b9d8-c631d2973538'), (5027, 'd4bbbeb5-0f59-45d8-b525-8c4180353a26'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (7096, '3ba57845-c986-4965-b8a2-53c99388658b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (30155, 'aa59f78f-c4f3-4a02-8d4f-f1432be5136b'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (25050, '4ea83d3e-bb4a-4ab5-b25a-c79f5c52c46a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (9690, 'c221a30d-3ba1-4731-af02-978855235bc7'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (25065, '0e262c4e-5cc0-4b85-9743-25d55e76332e'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (5625, 'c79981e0-e8ac-454a-a1ec-1e4b2c4b1ba2'), (22012, '5c55b2ec-3284-45a4-9f86-cf514fba02e0')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
## Why might Galleries, Libraries, Archives and Museums want to use the Hugging Face hub?

There are many different reasons why institutions want to contribute to
the Hugging Face Hub:

- **Exposure to a new audience**: the Hub has become a central destination for people working in machine learning, AI and related fields. Sharing on the Hub will help expose your collections and work to this audience. This also opens up the opportunity for further collaboration with this audience.

- **Community:** The Hub has many community-oriented features, allowing users and potential users of your material to ask questions and engage with materials you share via the Hub. Sharing trained models and machine learning datasets also allows people to build on each other's work and lowers the barrier to using machine learning in the sector.
- **On-prem Private Hub**: on-premise deployment of the Hugging Face Hub on your own infrastructure. For customers with strict compliance rules and/or workloads where they don't want or are not allowed to run on a public cloud.

Now that we have covered the basics of what the Private Hub is, let's go over how companies are using it to accelerate their ML development. 

## 3. How Are Companies Using the Private Hub to Accelerate Their ML Roadmap?

[ü§ó Transformers](https://github.com/huggingface/transformers) is one of the [fastest growing open source projects of all time](https://star-history.com/#tensorflow/tensorflow&nodejs/node&kubernetes/kubernetes&pytorch/pytorch&huggingface/transformers&Timeline). We now offer [25+ open source libraries](https://github.com/huggingface) and over 10,000 companies are now using Hugging Face to build technology with machine learning.
Each of these models is hosted as a Git repository, which allows versioning and reproducibility. Sharing a model on the Hub means opening it up to the community and making it accessible to anyone looking to easily use it, in turn eliminating their need to train a model on their own and simplifying sharing and usage. 

Additionally, sharing a model on the Hub automatically deploys a hosted Inference API for that model. Anyone in the community is free to test it out directly on the model's page, with custom inputs and appropriate widgets.

The best part is that sharing and using any public model on the Hub is completely free! [Paid plans](https://huggingface.co/pricing) also exist if you wish to share models privately.

The video below shows how to navigate the Hub.

<Youtube id=""XvSGPZFEjDY""/>

Having a huggingface.co account is required to follow along this part, as we'll be creating and managing repositories on the Hugging Face Hub: [create an account](https://huggingface.co/join)
To upload models to the Hub, or download models and integrate them into your work, explore the [**Models documentation**](./models). You can also choose from [**over a dozen libraries**](./models-libraries) such as ü§ó Transformers, Asteroid, and ESPnet that support the Hub.

## Datasets

The Hub is home to over 5,000 datasets in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. The Hub makes it simple to find, download, and upload datasets. Datasets are accompanied by extensive documentation in the form of [**Dataset Cards**](./model-cards) and [**Dataset Preview**](./datasets-overview#datasets-on-the-hub) to let you explore the data directly in your browser. While many datasets are public, [**organizations**](./organizations) and individuals can create private datasets to comply with licensing or privacy issues. You can learn more about [**Datasets here on Hugging Face Hub documentation**](./datasets-overview).
!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# How to contribute to huggingface_hub, the GitHub repository?

Everyone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community.
Answering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.
We engage directly with contributors and have addressed pressing issues. To bring this to the next level, we are building community-based processes. This approach empowers both Hugging Face contributors, and those affected by contributions, to inform the limitations, sharing, and additional mechanisms necessary for models and data made available on our platform. The three main aspects we will pay attention to are: the origin of the artifact, how the artifact is handled by its developers, and how the artifact has been used. In that respect we:
- launched a [flagging feature](https://twitter.com/GiadaPistilli/status/1571865167092396033) for our community to determine whether ML artifacts or community content (model, dataset, space, or discussion) violate our [content guidelines](https://huggingface.co/content-guidelines),
- monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
From research to production, your data never leaves your servers. The Private Hub runs in your own compliant server. It provides enterprise security features like security scans, audit trail, SSO, and control access to keep your models and data secure.

We provide flexible options for deploying your Private Hub in your private, compliant environment, including:

- **Managed Private Hub (SaaS)**: runs in segregated virtual private servers (VPCs) owned by Hugging Face. You can enjoy the full Hugging Face experience on your own private Hub without having to manage any infrastructure.

- **On-cloud Private Hub**: runs in a cloud account on AWS, Azure or GCP owned by the customer. This deployment option gives you full administrative control of the underlying cloud infrastructure and lets you achieve stronger security and compliance.
reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable information, including a `license` tag.
reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable information, including a `license` tag.
Domain-specific datasets are vital for evaluating and training machine learning models, helping to overcome the limitations of existing models. Creating these datasets, however, is challenging, requiring significant time, resources, and domain expertise, particularly for annotating data. Maximizing the impact of this data is crucial for the benefit of both the researchers involved and their respective fields.

The Hugging Face Hub can help achieve this maximum impact. 

## What is the Hugging Face Hub?

The [Hugging Face Hub](https://huggingface.co/) has become the central hub for sharing open machine learning models, datasets and demos, hosting over 360,000 models and 70,000 datasets. The Hub enables people ‚Äì including researchers ‚Äì to access state-of-the-art machine learning models and datasets in a few lines of code.
This post outlines:

- Why researchers should openly share their data (feel free to skip this section if you are already convinced about this!)
- What the Hugging Face Hub offers for researchers who want to share their datasets.
- Resources for getting started with sharing your datasets on the Hugging Face Hub.

## Why share your data?

Machine learning is increasingly utilized across various disciplines, enhancing research efficiency in tackling diverse problems. Data remains crucial for training and evaluating models, especially when developing new machine-learning methods for specific tasks or domains. Large Language Models may not perform well on specialized tasks like bio-medical entity extraction, and computer vision models might struggle with classifying domain specific images.
Most current model developers seem to think so, as the majority of openly released models have an open source license (e.g., Apache 2.0). See for instance the Hugging Face [Model Hub](https://huggingface.co/models?license=license:apache-2.0&sort=downloads) and [Mu√±oz Ferrandis & Duque Lizarralde (2022)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4018413).
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# ü§ó Benchmark results

Here, you can find a list of the different benchmark results created by the community.

If you would like to list benchmark results on your favorite models of the [model hub](https://huggingface.co/models) here, please open a Pull Request and add it below.
!---
Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License"");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an ""AS IS"" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# ü§ó Benchmark results

Here, you can find a list of the different benchmark results created by the community.

If you would like to list benchmark results on your favorite models of the [model hub](https://huggingface.co/models) here, please open a Pull Request and add it below.
of Conduct](https://huggingface.co/code-of-conduct) and [content guidelines](https://huggingface.co/content-guidelines) to help guide discussions along dimensions we believe to be important for an inclusive community space. We have developed a [Private Hub](https://huggingface.co/blog/introducing-private-hub) for secure ML development, a [library for evaluation](https://huggingface.co/blog/eval-on-the-hub) to make it easier for developers to evaluate their models rigorously, [code for analyzing data for skews and biases](https://github.com/huggingface/data-measurements-tool), and [tools for tracking carbon emissions when training a model](https://huggingface.co/blog/carbon-emissions-on-the-hub). We are also developing [new open and responsible AI licensing](https://huggingface.co/blog/open_rail), a modern form of licensing that directly addresses the harms that AI systems can create. And this week, we made it possible to [‚Äúflag‚Äù model and Spaces

Here is the question: Why is licensing compliance critical when sharing models on the Hugging Face Hub?
Here is answer 1:To meet academic standards for reproducibility
Here is answer 2: To ensure ethical use and avoid legal disputes
Here is answer3: To maintain compatibility with proprietary datasets
Here is answer 4: To streamline the fine-tuning process",Accordingly based upon analysis I conclude Answer : Here Is Answer :To Ensure Ethical Use And Avoid Legal Disputes
What debugging technique is recommended for resolving 'AttributeError' during model forward passes?,"[(3077, '45070278-b6c4-4645-8c0b-f2a7a699a326'), (1545, '2d8b8a39-c24d-481c-bb1e-f25dc29bc460'), (24083, '03ad03d2-915e-4319-b7cb-4c8afd73232b'), (1560, '9c881267-9303-4686-81be-1ac7f7fd886c'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (4127, '8b77edf9-5fbe-4e5a-84df-0360075e8182'), (6688, 'b9aef38c-a854-4a94-a9c4-fb3e1f95ef5e'), (1593, '6034732e-8f8a-475e-b5ff-361ff5bb2324'), (19515, 'e49b8303-c291-4b3c-9e0b-8536793c3176'), (8771, '6e71f559-b4cc-4d5f-85d1-281effc2bb38'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (25700, '23e83777-b949-4f93-bc54-d45092bb6976'), (20069, 'c53fb95e-58c9-41b3-aa8c-8ba29cb514f0'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (23142, '8fd73c82-896f-4e49-9763-6a97ee39a709'), (22640, '53ddd74d-fbd4-4faa-922b-0591f35c527b'), (4721, 'b91d43c8-99fe-47b8-8bad-c85882d3a9ff'), (20081, '4c4666b9-a4e3-4a78-ac8a-016853b43c6e'), (29812, '37b531a9-377d-43be-a980-a2de1bc9c32f'), (20085, '7bcc1692-9fd3-479f-84b4-4b930e6caa83'), (1150, 'b91271f2-c6c3-489b-b0cd-5c23b6b7df5e'), (20099, '5e7fe10f-0024-4628-aa58-73dc11da2265'), (1160, 'f7757a01-ad04-440b-8307-00fc7e0285de'), (1161, '9d7a9c13-b785-4334-b67a-324ae65947d6'), (10888, '11531df8-1ac7-4a86-906d-d2dee9e00e2a'), (1168, '0e3b941c-35b9-4bc5-b4dc-9c3309e9f562'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (1171, '542581c9-1c35-4102-be0d-d89e6a2225c6'), (1175, '140bfebc-c10e-415e-bfb0-9cbc492c2a7b'), (1176, 'bb9a042a-a3fb-46a4-831f-51906924decb'), (1178, '50ff656a-d70d-4407-8f69-09dc714a3b66'), (1188, '84476d43-a102-4354-b00a-2b9e71636e89'), (25254, '281f8259-9493-40e9-869f-f4066fb8bef7'), (30888, 'cd591269-a920-4614-b6b3-88241f6bfb33'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (1203, 'fa5804fc-2e57-4cfe-8041-e03bba3c0d2f'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (1205, '19799104-79e5-4b3b-bcc1-036b3fd80da5'), (1206, '1d985b81-a7e0-4313-b060-7cfdd799be25'), (1207, 'ba67be42-b645-475b-a449-454ae493e9ef'), (30906, 'd0acfe0e-7c52-44b9-a03b-f99643195c36'), (14523, '7eaad718-02e7-45b8-a664-b3c1b35d6c39'), (30908, 'f813e9a8-eab4-4c6c-b738-b58567335fa5'), (1213, 'bec86d0f-9e27-4271-bd19-fa2a88278612'), (1214, 'c6152586-1c48-41e4-975f-13183809eb4b'), (30910, '562b2c34-aa31-45cf-a703-d544e0641ad7'), (1215, '5739b2fb-8f4c-41a1-b697-0e0fd5f0a76f'), (1216, '485aadcf-fe5c-4160-a137-0b1dc3b07685'), (14524, '9edaacf8-e6a9-45da-9168-efea104fecf4'), (30909, '7990fb83-8871-47ad-b63e-08b50ef15d14'), (30916, '83870699-d55d-4da0-9257-d46016695c8c'), (30917, '963d0459-77e4-4c23-ae6f-2ef4da277f61'), (30918, '5636518a-1eb1-4077-baf0-ae21e3344396'), (19652, 'a23c3788-b782-425c-b713-61263ca4301b'), (19647, '7128f827-4d66-4eef-a604-8dfb92874def'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (19668, '55b764e4-abdd-42d3-aae5-2fbaedf7cc68'), (19669, 'aee04d72-b4ef-45e8-8857-bc8147c6672b'), (1238, '7a8ecc14-0e3a-4b2e-856a-9df768f253cf'), (1239, 'cbdef099-57bf-4d75-bd28-c97758db1411'), (1246, 'f1c4bb4f-9bb9-4e34-a9a3-2f65bf45e38a'), (30943, '1d6f7d97-f6d0-42cc-a25b-b2ba69a79f65'), (1248, '5162b3cb-da7d-411e-92b2-275490ed5442'), (30950, '8c43e960-5196-430d-a443-73c88cf050e0'), (22254, 'e27fface-fa4d-4917-ac05-c99b3208e99f'), (12022, 'ca590bdd-6993-45bf-bc5c-6216ff9e670d'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (7936, 'a5dbf1a5-4346-46c1-ac10-c332139056bc'), (7937, '72e80afc-d1e0-4a0f-85ab-e5658b60f5d1'), (20250, '06ee8af0-01dd-4c97-aae3-5ddee389cb4b'), (16155, '88f1863a-42df-40eb-8137-81a4119fa560'), (20252, 'b9e133f5-90ab-47a2-9af2-55d084a98c15'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (798, 'e25738d0-cf95-4ef5-a393-52b324ec4219'), (22815, '74db38e2-08ba-488a-9ea5-7bc39c8ea15d'), (3354, '0fd52be9-eb8d-4b15-b173-bfaf47f79b4f'), (16167, '2178036f-632c-47f1-bd2c-49882a635406'), (23849, '4484ca3c-0f23-423b-b0f2-bbe12951cc16'), (16178, 'c8a23931-5f9c-441f-a223-3dbfd115498c'), (20278, '20a49944-5093-4845-b2bb-d6f91d60ee65'), (11575, '28589867-4683-4d14-8fbc-3442efb0da38'), (14649, 'a79aeb51-b658-462d-841e-93d3ef673fb7'), (16186, 'd28a9bb2-1d44-4685-89a2-827e11eeb7f4'), (24379, '70fe8edf-d85c-4e28-b69c-0095888bb75c'), (20287, 'b9745a11-37a1-41f2-8092-7aad294368b2'), (24384, '2eeae0e2-f19b-42a7-a1d5-2130b027be53'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (4426, '8099d9dc-e246-4cba-ab73-f704c0327af2'), (2912, '8d27ca69-54a9-4680-b4bf-12d0172d7a83'), (14185, '5fbf3e82-047c-476d-b8e6-ef4f1870121a'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (17793, '8e853aa7-59db-4aaf-b04c-fe7fa8b49fcd'), (11662, '928cd162-1e95-4a12-9fd8-d4d850d43a48'), (11663, '5ef12e91-f3a0-4bd0-a8e9-bccb08282751'), (11664, 'df5f018c-98db-46c8-9e1c-59df03701ef0'), (20881, '33e801e1-f227-425d-a8d8-54759c599993'), (16275, '74585de3-90e9-44d0-973d-2d2fd8ced55b'), (20896, '54b807b5-1a34-48ec-80e9-686c9508556d'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (20899, 'e77570b8-6051-4cdf-b2d5-bce5f86ecf00'), (20900, '1482385c-4de4-4a6a-abb9-eccea1d3fb5e'), (20906, '4c0585c7-5b68-4b5f-820b-740cca4e0187'), (20907, '789c7a86-81f9-481b-a930-b5e65873ccf1'), (20908, 'ee5a5b82-d78b-40de-b0b2-ed026ae576d3'), (28590, '0882d2dc-76a2-4d07-be9a-e555af910496'), (3502, '18a2ff4c-5b6b-451c-ad35-a358c425930b'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (25536, '218027de-680d-422f-b703-435d44a91ce5'), (25537, 'c395a1e1-9e7b-45e2-9950-903e9a31b1f8'), (25026, '9e3a9873-ca79-49a5-a14e-a0cfd408d99d'), (20932, '79a5d60b-5251-49a9-a1d1-cc6eebb75e0d'), (25541, 'c23dff23-c24b-406b-aff6-016be08df943'), (20939, '8ede0b48-613d-4eb4-ace4-a4fa4749ce80'), (21979, '16fe4c5a-614e-4bf2-8c5d-dd6054dea3f9'), (4067, 'db389377-70fa-4887-84a7-4ea544f7efea'), (30181, 'cb4670d6-2bec-4922-9873-7da6829bc6df'), (31210, '99578f19-a57b-424c-958f-b852e6b00a5b')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

Next, regarding the debugging strategy, there are generally a few from which to choose from:

- Decompose the original model into many small testable components and run a forward pass on each of those for
  verification
- Decompose the original model only into the original *tokenizer* and the original *model*, run a forward pass on
  those, and use intermediate print statements or breakpoints for verification

Again, it is up to you which strategy to choose. Often, one or the other is advantageous depending on the original code
base.

If the original code-base allows you to decompose the model into smaller sub-components, *e.g.* if the original
code-base can easily be run in eager mode, it is usually worth the effort to do so. There are some important advantages
to taking the more difficult road in the beginning:
```

Next, regarding the debugging strategy, there are generally a few from
which to choose from:

-   Decompose the original model into many small testable components and
    run a forward pass on each of those for verification
-   Decompose the original model only into the original *tokenizer* and
    the original *model*, run a forward pass on those, and use
    intermediate print statements or breakpoints for verification

Again, it is up to you which strategy to choose. Often, one or the other
is advantageous depending on the original code base.

If the original code-base allows you to decompose the model into smaller
sub-components, *e.g.*, if the original code-base can easily be run in
eager mode, it is usually worth the effort to do so. There are some
important advantages to taking the more difficult road in the beginning:
```

Next, regarding the debugging strategy, there are generally a few from
which to choose from:

-   Decompose the original model into many small testable components and
    run a forward pass on each of those for verification
-   Decompose the original model only into the original *tokenizer* and
    the original *model*, run a forward pass on those, and use
    intermediate print statements or breakpoints for verification

Again, it is up to you which strategy to choose. Often, one or the other
is advantageous depending on the original code base.

If the original code-base allows you to decompose the model into smaller
sub-components, *e.g.*, if the original code-base can easily be run in
eager mode, it is usually worth the effort to do so. There are some
important advantages to taking the more difficult road in the beginning:
## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]

Although the `pipeline` is great for most applications where you need to quickly generate predictions, sometimes you'll need to access the model's logits (say, if you have some custom post-processing that you'd like to apply). To see what can go wrong in this case, let's first grab the model and tokenizer from our `pipeline`:

```python
tokenizer = reader.tokenizer
model = reader.model
```

Woohoo, it worked! Let's recap what you've just learned:

- The error messages in Python are known as _tracebacks_ and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem.
- If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs.
- If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue.
- The `huggingface_hub` 
// ü§ó Hub?
library provides a suite of tools that you can use to interact with and debug repositories on the Hub.

Now that you know how to debug a pipeline, let's take a look at a trickier example in the forward pass of the model itself.

## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]
checkpoint, the faster your debug cycle becomes. It is not efficient
    if your pretrained model is so big that your forward pass takes more
    than 10 seconds. In case only very large checkpoints are available,
    it might make more sense to create a dummy model in the new
    environment with randomly initialized weights and save those weights
    for comparison with the ü§ó Transformers version of your model
-   Make sure you are using the easiest way of calling a forward pass in
    the original repository. Ideally, you want to find the function in
    the original repository that **only** calls a single forward pass,
    *i.e.* that is often called `predict`, `evaluate`, `forward` or
    `__call__`. You don't want to debug a function that calls `forward`
    multiple times, *e.g.*, to generate text, like
    `autoregressive_sample`, `generate`.
-   Try to separate the tokenization from the model's
    forward pass. If the original repository shows
checkpoint, the faster your debug cycle becomes. It is not efficient
    if your pretrained model is so big that your forward pass takes more
    than 10 seconds. In case only very large checkpoints are available,
    it might make more sense to create a dummy model in the new
    environment with randomly initialized weights and save those weights
    for comparison with the ü§ó Transformers version of your model
-   Make sure you are using the easiest way of calling a forward pass in
    the original repository. Ideally, you want to find the function in
    the original repository that **only** calls a single forward pass,
    *i.e.* that is often called `predict`, `evaluate`, `forward` or
    `__call__`. You don't want to debug a function that calls `forward`
    multiple times, *e.g.*, to generate text, like
    `autoregressive_sample`, `generate`.
-   Try to separate the tokenization from the model's
    forward pass. If the original repository shows
becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds.
  In case only very large checkpoints are available, it might make more sense to create a dummy model in the new
  environment with randomly initialized weights and save those weights for comparison with the ü§ó Transformers version
  of your model
- Make sure you are using the easiest way of calling a forward pass in the original repository. Ideally, you want to
  find the function in the original repository that **only** calls a single forward pass, *i.e.* that is often called
  `predict`, `evaluate`, `forward` or `__call__`. You don't want to debug a function that calls `forward`
  multiple times, *e.g.* to generate text, like `autoregressive_sample`, `generate`.
- Try to separate the tokenization from the model's *forward* pass. If the original repository shows examples where
- Try to separate the tokenization from the model's *forward* pass. If the original repository shows examples where
  you have to input a string, then try to find out where in the forward call the string input is changed to input ids
  and start from this point. This might mean that you have to possibly write a small script yourself or change the
  original code so that you can directly input the ids instead of an input string.
- Make sure that the model in your debugging setup is **not** in training mode, which often causes the model to yield
  random outputs due to multiple dropout layers in the model. Make sure that the forward pass in your debugging
  environment is **deterministic** so that the dropout layers are not used. Or use *transformers.utils.set_seed*
  if the old and new implementations are in the same framework.
> Source of IndexError in the AutoModel forward pass?

This title tells the reader _where_ you think the bug is coming from, and if they've encountered an `IndexError` before, there's a good chance they'll know how to debug it. Of course, the title can be anything you want, and other variations like:

> Why does my model produce an IndexError?

could also be fine. Now that we've got a descriptive title, let's take a look at improving the body.

### Formatting your code snippets[[formatting-your-code-snippets]]

Reading source code is hard enough in an IDE, but it's even harder when the code is copied and pasted as plain text! Fortunately, the Hugging Face forums support the use of Markdown, so you should always enclose your code blocks with three backticks (```) so it's more easily readable. Let's do this to prettify the error message -- and while we're at it, let's make the body a bit more polite than our original version:
- Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should
  probably take the time to write a longer script that decomposes the original model into smaller sub-components to
  retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on
  TensorFlow print operations like [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to output
  intermediate values. Is the original repository written in Jax? Then make sure that the model is **not jitted** when
  running the forward pass, *e.g.* check-out [this link](https://github.com/google/jax/issues/196).
- Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle
  becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds.
-   Find the best way of debugging intermediate results. Is the original
    repository written in PyTorch? Then you should probably take the
    time to write a longer script that decomposes the original model
    into smaller sub-components to retrieve intermediate values. Is the
    original repository written in Tensorflow 1? Then you might have to
    rely on TensorFlow print operations like
    [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to
    output intermediate values. Is the original repository written in
    Jax? Then make sure that the model is **not jitted** when running
    the forward pass, *e.g.*, check-out [this
    link](https://github.com/google/jax/issues/196).
-   Use the smallest pretrained checkpoint you can find. The smaller the
    checkpoint, the faster your debug cycle becomes. It is not efficient
    if your pretrained model is so big that your forward pass takes more
-   Find the best way of debugging intermediate results. Is the original
    repository written in PyTorch? Then you should probably take the
    time to write a longer script that decomposes the original model
    into smaller sub-components to retrieve intermediate values. Is the
    original repository written in Tensorflow 1? Then you might have to
    rely on TensorFlow print operations like
    [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to
    output intermediate values. Is the original repository written in
    Jax? Then make sure that the model is **not jitted** when running
    the forward pass, *e.g.*, check-out [this
    link](https://github.com/google/jax/issues/196).
-   Use the smallest pretrained checkpoint you can find. The smaller the
    checkpoint, the faster your debug cycle becomes. It is not efficient
    if your pretrained model is so big that your forward pass takes more
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'
""""""
In general, there are two possible debugging environments for running
the original model

-   [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)
-   Local python scripts.

Jupyter notebooks have the advantage that they allow for cell-by-cell
execution which can be helpful to better split logical components from
one another and to have faster debugging cycles as intermediate results
can be stored. Also, notebooks are often easier to share with other
contributors, which might be very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupyter notebooks, we
strongly recommend you to work with them.

The obvious disadvantage of Jupyter notebooks is that if you are not
used to working with them you will have to spend some time adjusting to
the new programming environment and that you might not be able to use
your known debugging tools anymore, like `ipdb`.

**4. Successfully run forward pass**
In general, there are two possible debugging environments for running
the original model

-   [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)
-   Local python scripts.

Jupyter notebooks have the advantage that they allow for cell-by-cell
execution which can be helpful to better split logical components from
one another and to have faster debugging cycles as intermediate results
can be stored. Also, notebooks are often easier to share with other
contributors, which might be very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupyter notebooks, we
strongly recommend you to work with them.

The obvious disadvantage of Jupyter notebooks is that if you are not
used to working with them you will have to spend some time adjusting to
the new programming environment and that you might not be able to use
your known debugging tools anymore, like `ipdb`.

**4. Successfully run forward pass**
multiple times, *e.g.*, to generate text, like
    `autoregressive_sample`, `generate`.
-   Try to separate the tokenization from the model's
    forward pass. If the original repository shows
    examples where you have to input a string, then try to find out
    where in the forward call the string input is changed to input ids
    and start from this point. This might mean that you have to possibly
    write a small script yourself or change the original code so that
    you can directly input the ids instead of an input string.
-   Make sure that the model in your debugging setup is **not** in
    training mode, which often causes the model to yield random outputs
    due to multiple dropout layers in the model. Make sure that the
    forward pass in your debugging environment is **deterministic** so
    that the dropout layers are not used. Or use
    `transformers.utils.set_seed` if the old and new
    implementations are in the same framework.
multiple times, *e.g.*, to generate text, like
    `autoregressive_sample`, `generate`.
-   Try to separate the tokenization from the model's
    forward pass. If the original repository shows
    examples where you have to input a string, then try to find out
    where in the forward call the string input is changed to input ids
    and start from this point. This might mean that you have to possibly
    write a small script yourself or change the original code so that
    you can directly input the ids instead of an input string.
-   Make sure that the model in your debugging setup is **not** in
    training mode, which often causes the model to yield random outputs
    due to multiple dropout layers in the model. Make sure that the
    forward pass in your debugging environment is **deterministic** so
    that the dropout layers are not used. Or use
    `transformers.utils.set_seed` if the old and new
    implementations are in the same framework.
model = FlaxBertModel(config)

- The `__call__` method defines forward pass. It takes all necessary model inputs and parameters (and any other arguments required for the forward pass). The parameters are optional; when no parameters are passed, it uses the previously initialized or loaded parameters which can be accessed using `model.params`. It then calls the `module.apply` method, passing it the parameters and inputs to do the actual forward pass. So we can do a forward pass using

      output = model(inputs, params=params)


Let's look at an example to see how this works. We will write a simple two-layer MLP model.

First, write a Flax module that will declare the layers and computation.

```python
import flax.linen as nn
import jax.numpy as jnp

class MLPModule(nn.Module):
   config: MLPConfig
   dtype: jnp.dtype = jnp.float32
```
~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)
    471             raise ValueError(""You cannot specify both input_ids and inputs_embeds at the same time"")
    472         elif input_ids is not None:
--> 473             input_shape = input_ids.size()
    474         elif inputs_embeds is not None:
    475             input_shape = inputs_embeds.size()[:-1]

AttributeError: 'list' object has no attribute 'size'

Here is the question: What debugging technique is recommended for resolving 'AttributeError' during model forward passes?
Here is answer 1:Switch to a different pre-trained checkpoint
Here is answer 2: Ensure tensor conversion during tokenization
Here is answer3: Disable attention masking in the model
Here is answer 4: Update pipeline configuration files","The correct answer choice would depend upon understanding what exactly caused this error - i.e., `'list' object has no attribute 'size'`. 

However based upon given context content ""When creating models within Hugging Face Transformers"", I would recommend option2 - ""Ensure tensor conversion during tokenization""."
How does the Datasets library handle data stored remotely?,"[(2561, 'a241eea3-9d38-4b32-9dc5-392dd9f97ef0'), (24583, '3e20db8d-779d-4bed-9130-2436441e7181'), (523, '57f16a54-3425-4cc8-b057-14b8199a9197'), (15376, '18b4fd51-c4a0-4806-9a8c-66bd3fb7b887'), (26641, '972af779-bf9e-4865-bebd-97def3d5dd6d'), (23575, '3cedbab4-4057-4eca-9e31-44fc411ca9a2'), (23065, 'd98cc50f-6b46-43db-a019-c4f0aea3d979'), (8224, '44baca0d-26b7-42a0-9b85-bb757038c40e'), (20531, '58733ce5-d6e4-4037-be5a-f82e81ea407f'), (23605, 'cb4e48ed-d022-4aac-ae79-8303f065e1e8'), (18488, 'd75be24a-7b2f-45c2-b82a-35db1d9e237d'), (20543, 'b5986ba7-30ec-4955-ba6c-72d1afd89b5b'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (25673, 'eea35395-f70f-4040-b8c6-591cadd77b52'), (5707, 'f1652f24-131f-48b0-a83d-0e7049f1189e'), (27735, '2e00db33-c75a-4cb0-b29e-68bc4acce90a'), (24156, 'cfb1685e-60cf-4fb0-a3ef-a2470dbb5881'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (11877, '6269ad60-72d2-4e18-81da-010b5682cff4'), (6766, 'e35d1260-f910-44b2-a82d-648bc59cb455'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (4212, '12f87a03-1269-4e9f-938f-0f142e08b79a'), (11393, '9930410b-3008-41e2-9bc0-7dec7e568ed0'), (130, '426de6ec-a6b0-421b-8201-d87a7c67d0be'), (4741, '210088c4-7fef-45fb-b3bd-ba5759a63329'), (11398, 'c896d600-5e2c-467c-87a5-e393c3572337'), (3734, '7bbadbb5-638d-46a5-981d-f293cc3e6d0a'), (21657, 'caaddc26-338c-4b5f-89ab-bc044b94afe4'), (24734, 'f3f60e0e-fe06-4732-8b9a-79db264b56f1'), (24736, 'dbfea546-1d21-471b-a468-ca263a9274fc'), (5801, 'e6df3fe7-f709-49e1-88a7-c4beb3a53bc9'), (15020, '37820be1-34ef-4e05-8b4f-880db34fb455'), (22713, '31320a45-af6a-4672-b522-8677e668387b'), (5313, 'e3673df9-36cc-476d-b755-3bf1fb595ebf'), (5327, '7bb56183-8dbd-4678-987c-44526c61ed91'), (5328, '7e28df2f-1656-43a9-b36b-adab211aeb45'), (5329, '08880c56-ac91-49cf-9b42-a59277ab2e37'), (9936, '9c48fcdd-c609-463f-ad7e-442feef8c8d5'), (31446, 'c4e8f3c8-49c2-4215-8868-959cf5b4ecbd'), (5337, '6ac0cb69-eb3f-4938-b193-a28613826b62'), (7386, '1e75d6ef-eb2a-47b6-8bf4-a1960ff737f8'), (7389, 'b481fda8-b5de-4d04-8ec6-e36a9f1f4039'), (24798, 'e0f48390-3fe1-4f09-a5e1-dd6f48593b5a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (20703, '185d614c-743f-4b53-9078-c44a17e5c1ae'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (20704, 'c152cb8b-269f-4a6c-abcf-20254a409de0'), (7395, 'e6cbeb63-c998-433d-b0b3-afb65bf27b67'), (13028, '916bc188-accd-498f-9b24-b64e40ed6c6a'), (17128, 'b8f3678a-4561-4c83-b4eb-b6617ffedb76'), (23274, '20ba3cf8-d2ff-4e5f-8b8b-323410a96fc3'), (17130, '881bf517-ab0a-4a1f-a757-ecf13e903b3c'), (3312, '287caacd-a3a6-4b79-9882-1292d43e4c5b'), (6384, '690dfc1e-5a31-46cf-bdce-a46b5a2b71a7'), (13043, '29a53aa4-5747-4d42-81cc-a011a8d9e69b'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (23306, 'ceda3f28-4ba3-4ca6-84a7-a4b914fd8d4f'), (30989, '8ab696e1-efbe-45bd-8dab-02c487fa3171'), (8986, '4d8d186e-ecd5-4250-be5f-bba02de56f71'), (24347, '10d68075-9bb2-41df-a0e5-357873db4b4e'), (21789, 'f348a103-0156-4f06-a4aa-5eb608e899a5'), (26398, '2e0951bf-7007-4f4e-88f6-750d405238ad'), (21791, '4f692aeb-3d3d-4422-9f32-e5a2985e2585'), (22304, '3897446b-d474-4a3d-9db5-5b91924cce5e'), (26399, 'ec38dc22-9378-4870-8e6b-47fe916cdfb0'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (21790, 'ad9580c8-18dd-4498-9dc7-b2dd03de689c'), (21797, '70fd8a17-39ef-492b-9662-a96111502aab'), (24364, '268352af-6a3c-483f-8481-c7fecd64ff92'), (22317, '0daa3d28-92e3-492b-965c-e4ad098bc076'), (15669, '0256580b-34ce-4e4a-88aa-42403eb7a050'), (3385, 'd1c40df7-f6d3-4137-bd79-ca1352f9aa57'), (9538, 'cd1c44cf-83e6-42b7-b1cd-753b168bf055'), (835, '90be4731-3d87-4691-9bc4-3013cc2baba5'), (834, 'a18cfcba-27e0-41eb-b126-fe98d29828f6'), (1354, '63a9f049-ffdf-469e-a0ec-63eb7a813f54'), (15181, 'fb7c5366-41d1-41cb-9f21-6d3b57cf1e94'), (14165, 'f8e81797-b080-4a41-9f82-86eea16eabe8'), (9559, 'a8284fc7-f6b2-4800-baa1-e5b290b17b91'), (5465, '3e501bca-2cbc-46cd-bcd9-55ef632f4e61'), (7010, 'a1650e86-e351-4bde-bd80-a0933a6b7661'), (15217, '77662209-3d2c-443f-8e8c-3085e7ecc6bf'), (7029, '00a05e41-17b7-4145-9888-ba014f3c3a94'), (7031, '9f199d53-0a08-420c-924c-971488bd01db'), (29564, '68c48bc2-1c96-4e9d-ad94-26a1ae6dab76'), (11649, 'bbd17498-b764-479c-b665-85c70a5ac394'), (31110, '9726dad8-10ae-460c-94fa-9e76f693eb05'), (31111, '15a82996-f78b-40ac-9919-f6f0307c152b'), (31112, 'ca7ab14c-5b6d-4313-b660-ca64236d08bf'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (31114, '8b46ee59-f8a3-45ee-a0cf-e6a579991e29'), (8587, '5dfac67b-30fa-4e8d-b899-f2fb32fac46c'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (24975, '84ec31e0-6e0d-40d2-aeed-7c845e768cb2'), (8591, 'da53a557-9d2c-49b1-976b-aaa341d7ad09'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (27557, '7dffbab4-38e8-4f8b-a105-0944ff6fb066'), (26031, 'f7a849c8-18fd-4d0a-8b63-6ae8789656a4'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (12742, 'f43f21cc-4dab-4dab-ac40-694072cdbeb9'), (7625, '68f9e1d3-8747-4f48-ab13-1d6a4a4baffb'), (5578, 'a60914f3-4328-43c1-b670-4b7d01e2fd74'), (12748, '7e2f7e69-f15b-4724-b46e-41d6bb2a1491'), (16850, '8d4c8a3e-04e1-47df-82a1-1b7d7af6968c'), (16851, '0edad950-cf69-4d0c-8d21-bba90d48a71f'), (12754, 'e947be7a-2841-43d5-b05e-0b28c40d9058'), (16854, '2f5366a7-967e-48fa-983b-e0965294b7f4'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (16855, '566c4392-0a1c-4d30-aee7-cd3c019e10ea'), (16861, '214b51ca-08fc-4df6-918d-c7e539d37ce9'), (16865, '8458e40d-8dcc-4644-85d2-3e073e283055'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (20465, 'a8ced92a-342e-466e-8c4b-8769775c1b58'), (20467, 'd0074f05-bf62-4e31-acfd-177faf27318a'), (13814, 'd8be2885-e7f0-4d64-973b-c7c6e554c60d'), (13820, 'ec3167c6-611c-42f7-9420-1d51ba771e0b'), (12798, '3d5604e9-6c18-43b1-8c57-99abd2a1973c')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: aving and reloading a dataset. In this video we'll take a look saving a dataset in various formats, and explore the ways to reload the saved data. When you download a dataset, the processing scripts and data are stored locally on your computer. The cache allows the Datasets library to avoid re-downloading or processing the entire dataset every time you use it. The data is stored in the form of Arrow tables whose location can be found by accessing the dataset's cache_files attribute. In this example, we've downloaded the allocine dataset from the Hugging Face Hub and you can see there are three Arrow files stored in the cache, one for each split. But in many cases, you'll want to save your dataset in a different location or format. As shown in the table, the Datasets library provides four main functions to achieve this. You're probably familiar with the CSV and JSON formats, both of which are great if you want to save small to medium-sized datasets. But if your dataset is huge, you'll
To handle these large datasets, the Datasets library is built on two core features: the Apache Arrow format and a streaming API. Arrow is designed for high-performance data processing and represents each table-like dataset with an in-memory columnar format. As you can see in this example, columnar formats group the elements of a table in consecutive blocks of RAM and this unlocks fast access and processing. Arrow is great at processing data at any scale, but some datasets are so large that you can't even fit them on your hard disk. For these cases, the Datasets library provides a streaming API that allows you to progressively download the raw data one element at a time. The result is a special object called an IterableDataset that we'll see in more detail soon. Let's start by looking at why Arrow is so powerful. The first feature is that it treat every dataset as a memory-mapped file. Memory mapping is a mechanism that maps a portion of a file or an entire file on disk to a chunk of
<Youtube id=""HyQgpJTkRdE""/>

## Working with local and remote datasets[[working-with-local-and-remote-datasets]]

ü§ó Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:

|    Data format     | Loading script |                         Example                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset(""csv"", data_files=""my_file.csv"")`     |
|     Text files     |     `text`     |    `load_dataset(""text"", data_files=""my_file.txt"")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset(""json"", data_files=""my_file.jsonl"")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset(""pandas"", data_files=""my_dataframe.pkl"")` |
oading a custom dataset. Although the Hugging Face Hub hosts over a thousand public datasets, you'll often need to work with data that is stored on your laptop or some remote server. In this video we'll explore how the Datasets library can be used to load datasets that aren‚Äôt available on the Hugging Face Hub. As you can see in this table, the Datasets library provides several in-built scripts to load datasets in several formats. To load a dataset in one of these formats, you just need to provide the name of the format to the load_dataset function, along with a data_files argument that points to one or more filepaths or URLs. To see this in action, let's start by loading a local CSV file. In this example, we first download a dataset about wine quality from the UCI machine learning repository. Since this is a CSV file, we then specify the csv loading script. This script needs to know where our data is located, so we provide the filename as part of the data_files argument. The CSV
The dataset is generated with a Python generator, which doesn't load all the data in memory. As a result, the generator can handle large datasets. However, before the generated samples are flushed to the dataset file on disk, they are stored in an `ArrowWriter` buffer. This means the generated samples are written by batch. If your dataset samples consumes a lot of memory (images or videos), then make sure to specify a low value for the `DEFAULT_WRITER_BATCH_SIZE` attribute in [`DatasetBuilder`]. We recommend not exceeding a size of 200 MB.

## Maintaining integrity

To ensure a dataset is complete, [`load_dataset`] will perform a series of tests on the downloaded files to make sure everything is there. This way, you don't encounter any surprises when your requested dataset doesn't get generated as expected. [`load_dataset`] verifies:
```

## Remote code

Certain datasets repositories contain a loading script with the Python code used to generate the dataset.
Those datasets are generally exported to Parquet by Hugging Face, so that ü§ó Datasets can load the dataset fast and without running a loading script.

Even if a Parquet export is not available, you can still use any dataset with Python code in its repository with `load_dataset`.
All files and code uploaded to the Hub are scanned for malware (refer to the Hub security documentation for more information), but you should still review the dataset loading scripts and authors to avoid executing malicious code on your machine. You should set `trust_remote_code=True` to use a dataset with a loading script, or you will get a warning:

```py
>>> from datasets import get_dataset_config_names, get_dataset_split_names, load_dataset
Before moving on, though, let's test what you learned in this chapter.

### 1. The `load_dataset()` function in ü§ó Datasets allows you to load a dataset from which of the following locations? 

<Question
	choices={[
		{
			text: ""Locally, e.g. on your laptop"",
			explain: ""Correct! You can pass the paths of local files to the <code>data_files</code> argument of <code>load_dataset()</code> to load local datasets."",
			correct: true
		},
		{
			text: ""The Hugging Face Hub"",
			explain: ""Correct! You can load datasets on the Hub by providing the dataset ID, e.g. <code>load_dataset('emotion')</code>."",
			correct: true
		},
		{
			text: ""A remote server"",
			explain: ""Correct! You can pass URLs to the <code>data_files</code> argument of <code>load_dataset()</code> to load remote files."",
			correct: true
		},
	]}
/>

### 2. Suppose you load one of the GLUE tasks as follows:

```py
from datasets import load_dataset

dataset = load_dataset(""glue"", ""mrpc"", split=""train"")
```

This can be useful if you don't want to manually decompress many GZIP files. The automatic decompression also applies to other common formats like ZIP and TAR, so you just need to point `data_files` to the compressed files and you're good to go!

Now that you know how to load local files on your laptop or desktop, let's take a look at loading remote files.

## Loading a remote dataset[[loading-a-remote-dataset]]

If you're working as a data scientist or coder in a company, there's a good chance the datasets you want to analyze are stored on some remote server. Fortunately, loading remote files is just as simple as loading local ones! Instead of providing a path to local files, we point the `data_files` argument of `load_dataset()` to one or more URLs where the remote files are stored. For example, for the SQuAD-it dataset hosted on GitHub, we can just point `data_files` to the _SQuAD_it-*.json.gz_ URLs as follows:
Cache management

When you download a dataset, the processing scripts and data are stored locally on your computer. The cache allows ü§ó Datasets to avoid re-downloading or processing the entire dataset every time you use it. 

This guide will show you how to:

- Change the cache directory.
- Control how a dataset is loaded from the cache.
- Clean up cache files in the directory.
- Enable or disable caching.

## Cache directory

The default cache directory is `~/.cache/huggingface/datasets`. Change the cache location by setting the shell environment variable, `HF_DATASETS_CACHE` to another directory:

```
$ export HF_DATASETS_CACHE=""/path/to/another/directory""
```

When you load a dataset, you also have the option to change where the data is cached. Change the `cache_dir` parameter to the path you want:

```py
>>> from datasets import load_dataset
>>> dataset = load_dataset('LOADING_SCRIPT', cache_dir=""PATH/TO/MY/CACHE/DIR"")
The [`load_dataset`] function fetches the requested dataset locally or from the Hugging Face Hub.
The Hub is a central repository where all the Hugging Face datasets and models are stored.

If the dataset only contains data files, then [`load_dataset`] automatically infers how to load the data files from their extensions (json, csv, parquet, txt, etc.).
Under the hood, ü§ó Datasets will use an appropriate [`DatasetBuilder`] based on the data files format. There exist one builder per data file format in ü§ó Datasets:
```

Next, we can load the dataset using the method for remote files that we learned in [section 2](/course/chapter5/2):

```py
from datasets import load_dataset

# This takes a few minutes to run, so go grab a tea or coffee while you wait :)
data_files = ""https://the-eye.eu/public/AI/pile_preliminary_components/PUBMED_title_abstracts_2019_baseline.jsonl.zst""
pubmed_dataset = load_dataset(""json"", data_files=data_files, split=""train"")
pubmed_dataset
```

```python out
Dataset({
    features: ['meta', 'text'],
    num_rows: 15518009
})
The ü§ó Datasets library provides a very simple command to download and cache a dataset on the Hub. We can download the MRPC dataset like this:

```py
from datasets import load_dataset

raw_datasets = load_dataset(""glue"", ""mrpc"")
raw_datasets
### Support for large datasets

The Hub can host large datasets; it currently hosts datasets with multiple TBs of data.The datasets library, which users can use to download and process datasets from the Hub, supports streaming, making it possible to work with large datasets without downloading the entire dataset upfront. This can be invaluable for allowing researchers with less computational resources to work with your datasets, or to select small portions of a huge dataset for testing, development or prototyping.


<p align=""center""> 
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/researcher-dataset-sharing/filesize.png"" alt=""Screenshot of the file size information for a dataset""><br> 
<em>The Hugging Face Hub can host the large datasets often created for machine learning research.</em> 
 </p> 


## API and client library interaction with the Hub
```

### Stream data

Stream a dataset by loading it as an [`IterableDataset`]. This allows you to progressively iterate over a remote dataset without downloading it on disk and or over local data files.
Learn more about which type of dataset is best for your use case in the [choosing between a regular dataset or an iterable dataset](./about_mapstyle_vs_iterable) guide.


An iterable dataset from `datasets` inherits from `torch.utils.data.IterableDataset` so you can pass it to a `torch.utils.data.DataLoader`:

```py
>>> import numpy as np
>>> from datasets import Dataset, load_dataset
>>> from torch.utils.data import DataLoader
>>> data = np.random.rand(10_000)
>>> Dataset.from_dict({""data"": data}).push_to_hub(""<username>/my_dataset"")  # Upload to the Hugging Face Hub
>>> my_iterable_dataset = load_dataset(""<username>/my_dataset"", streaming=True, split=""train"")
>>> dataloader = DataLoader(my_iterable_dataset, batch_size=32)
Libraries

The Datasets Hub has support for several libraries in the Open Source ecosystem.
Thanks to the [huggingface_hub Python library](../huggingface_hub), it's easy to enable sharing your datasets on the Hub.
We're happy to welcome to the Hub a set of Open Source libraries that are pushing Machine Learning forward.

The table below summarizes the supported libraries and their level of integration.
ü§ó Datasets, check![[datasets-check]]

<CourseFloatingBanner
    chapter={5}
    classNames=""absolute z-10 right-0 top-0""
/>

Well, that was quite a tour through the ü§ó Datasets library -- congratulations on making it this far! With the knowledge that you've gained from this chapter, you should be able to:

- Load datasets from anywhere, be it the Hugging Face Hub, your laptop, or a remote server at your company.
- Wrangle your data using a mix of the `Dataset.map()` and `Dataset.filter()` functions.
- Quickly switch between data formats like Pandas and NumPy using `Dataset.set_format()`.
- Create your very own dataset and push it to the Hugging Face Hub.
- Embed your documents using a Transformer model and build a semantic search engine using FAISS.
```

## Load and Save your datasets using your cloud storage FileSystem

### Download and prepare a dataset into a cloud storage

You can download and prepare a dataset into your cloud storage by specifying a remote `output_dir` in `download_and_prepare`.
Don't forget to use the previously defined `storage_options` containing your credentials to write into a private cloud storage.

The `download_and_prepare` method works in two steps:
1. it first downloads the raw data files (if any) in your local cache. You can set your cache directory by passing `cache_dir` to [`load_dataset_builder`]
2. then it generates the dataset in Arrow or Parquet format in your cloud storage by iterating over the raw data files.

Load a dataset builder from the Hugging Face Hub (see [how to load from the Hugging Face Hub](./loading#hugging-face-hub)):
> @mandubian's second bullet point suggests that there's a workaround allowing you to use your offline (custom?) dataset with `datasets`. Could you please elaborate on how that should look like?

Indeed `load_dataset` allows to load remote dataset script (squad, glue, etc.) but also you own local ones.
For example if you have a dataset script at `./my_dataset/my_dataset.py` then you can do
\`\`\`python
load_dataset(""./my_dataset"")
\`\`\`
and the dataset script will generate your dataset once and for all.

----------

About I'm looking into having `csv`, `json`, `text`, `pandas` dataset builders already included in the `datasets` package, so that they are available offline by default, as opposed to the other datasets that require the script to be downloaded.
cf #1724
SCORE: 24.14896583557129
TITLE: Discussion using datasets in offline mode
URL: https://github.com/huggingface/datasets/issues/824
==================================================
```

However, for some of the datasets on the Hub, we might be keen not to download the whole dataset. We could instead try to load a sample of the dataset. However, depending on how the dataset was created, we might still end up downloading more data than we‚Äôd need onto the machine we‚Äôre working on. 

Luckily, many datasets on the Hub are available via the [datasets server](https://huggingface.co/docs/datasets-server/index). The datasets server is an API that allows us to access datasets hosted on the Hub without downloading the dataset locally. The Datasets Server powers the Datasets Viewer preview you will see for many datasets hosted on the Hub.
```

However, for some of the datasets on the Hub, we might be keen not to download the whole dataset. We could instead try to load a sample of the dataset. However, depending on how the dataset was created, we might still end up downloading more data than we‚Äôd need onto the machine we‚Äôre working on. 

Luckily, many datasets on the Hub are available via the [datasets server](https://huggingface.co/docs/datasets-server/index). The datasets server is an API that allows us to access datasets hosted on the Hub without downloading the dataset locally. The Datasets Server powers the Datasets Viewer preview you will see for many datasets hosted on the Hub.

Here is the question: How does the Datasets library handle data stored remotely?
Here is answer 1:Downloads and preprocesses files automatically based on file extension
Here is answer 2: Requires manual preprocessing scripts for remote files
Here is answer3: Only supports specific hosting platforms like GitHub
Here is answer 4: Uses pre-trained embeddings to structure remote datasets","Based solely off provided text content/, I'd pick **answer1**: Downloads **and preprocesses** **files** automatically based **on** **file extension**

Contextual clue stems mainly *from* `[backward]...[""If ]the[dataset ]only ]contains[ )(*Data )files[* armourRXtension -so ""**""](then[* ],!.inferring[*likedyou])]""how[with][automatically,[use.theŸàÿ±€å appropriate''`."
What is a unique benefit of using PatchTST for self-supervised time series pretraining?,"[(5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (5135, '8bb2ea1c-369d-4db0-ab7a-127366db9831'), (24600, 'd44ed870-3d04-435e-a54a-4e35c0e1d296'), (22553, '01c10e23-6757-48f1-b431-86c93a02358f'), (21027, 'ae9699a9-7d18-4dd8-91fe-c31dd77b8e6f'), (22063, 'b6a4b9c4-c683-4973-b991-8c2cb24c7519'), (19002, 'b4b151aa-98d0-40e4-847b-3faecb435a73'), (25147, 'a2669d4a-4881-4334-8052-9102cf1a1249'), (25150, 'c125fa87-ac9a-4bb7-9926-8dc62c5fb983'), (19014, '28a449dd-bfb5-4851-90ff-3b2cd97264cc'), (25162, '946c9ec3-717f-4307-aad1-37aac459e5a1'), (29776, 'e3203540-4467-4975-8a4a-3a14658f4f60'), (25171, 'da15400c-495a-4c04-a4f7-bc0e717aa6a8'), (19540, 'b1b16778-0b99-4188-bc83-514da8f56532'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (29800, '21cb357b-9e31-4411-852e-3c3ce9bdb3e7'), (29805, '02c5425b-9b35-42ac-93c9-203e9cd34734'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (21123, '2e262a5c-5f46-42e7-980c-481e0e6534c6'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (21128, '1b0739a8-867b-4a25-bf9e-966b397ff990'), (12937, '292ea5f5-80e1-456b-86e0-34464796bb4b'), (21129, '30b7dc88-1489-428b-b8cf-8404a7481499'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (15004, '5d426919-bf3f-42f7-b626-5dd6e9e94acc'), (20649, 'b99dfbb1-3bca-4177-9b1e-d01ea82d9060'), (15028, '86c91e1c-d5df-4053-b7f8-f475c47edcf1'), (13505, 'f66bf4af-bf2d-468b-bc25-83e2c5c904ba'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (11465, '2352d31d-4cb6-43cc-a668-77747f9ff8db'), (16079, '526ce690-4958-4079-a0ce-055d3e006d6a'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (18138, 'bc713553-0249-4017-81c9-8b7d7b301725'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (9445, 'b6bd1183-24ee-43fb-9575-54230c8c4579'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (13556, 'f047eb0d-f7be-4bf3-925a-566417d8ed33'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (254, 'eb6fe67a-123d-43fd-9e73-96693204d7dc'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (26379, '1ccd1f17-c6b7-48d1-9592-f50598ebe298'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (26383, 'ae72b8dd-bf79-46fb-a35d-9e01211070c3'), (12049, '9ebd4171-a0ce-40d1-9883-c37ead9490d1'), (787, '17f95d66-cbff-4264-9f2e-0d77da353822'), (792, '999bbe02-98fa-4c04-8bcd-c94434420a04'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (5407, '083fe81d-625c-4b43-86c3-15a5b61f9c71'), (28960, '1040c198-e0d5-401e-a9b4-fc013110dcd0'), (5411, '4fafe081-7327-4280-bc98-64f9f75d9966'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (23869, '26cd5348-6284-4c3b-96a3-3840b630697b'), (4421, '95a1272e-1f19-494f-96d5-f36f9a919a0a'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15696, 'f5c82a25-56d1-4160-a926-0d697826a6bb'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (8021, '2eb3eb4c-195e-4237-b854-70714e8cde11'), (6997, '35d0066b-d1f2-45f1-a295-d792e0dda90d'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (8022, '542d128d-72d2-46c5-a25e-8c55ddf820ec'), (22877, '79c5fcb1-22aa-4ce1-be25-33a02d2b8e03'), (4966, '42e714b3-cfee-4118-a976-588eded30493'), (4967, '43778b66-3e71-4fd3-a9bf-f2cf850f713d'), (1898, 'd0cd0bd7-ad19-4c35-a562-ddc9c9499a18'), (1899, 'f888ee25-af78-4996-8147-194908c2b4b0'), (1900, '9ac81dc8-63e8-4bb5-b1a2-66f999fdf9c9'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (25457, 'bfcd5d3f-52bb-4950-9929-e559851e2d65'), (18803, 'e7da8b1d-636a-4b36-887e-45035013d1e5'), (23941, '4552e7c6-32c1-47d7-9c48-1cd66703f9a7'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (2964, '2e5d286b-3b13-48e8-895f-ec256ffed18c'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (11684, '9cfcfbdb-6441-4284-8ef3-d14780da146f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (4530, '006e7cf7-47b3-4211-9fbb-c2a986881355'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (22456, '55e20e3e-6f8f-413c-8a0f-663f7d1df98d'), (9144, '3be21646-2d02-41ae-b3a2-d0cf74a4654a'), (29143, 'bc13a857-206c-4b91-b966-9c4913882054'), (29146, '2c98ac16-543f-4c54-b75d-62f161c19dd7'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (14320, '18b2a378-fe08-4e21-bc4a-6b7b9c5ad71d'), (31218, '453bb283-ba88-4589-860e-7e2e4fa22e63'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (9205, '813fcccf-2923-465b-ab73-2cfe6cc521fd'), (12280, '040615d3-6913-4b79-a530-25472aa800e4'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
2D interpolation of the pre-trained position embeddings, according to their location in the original image.
- The best results are obtained with supervised pre-training, which is not the case in NLP. The authors also performed
  an experiment with a self-supervised pre-training objective, namely masked patched prediction (inspired by masked
  language modeling). With this approach, the smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a significant
  improvement of 2% to training from scratch, but still 4% behind supervised pre-training.
*The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et
is usually self-supervised, which means it doesn‚Äôt require humans annotations for the labels. A very common pretraining objective is to guess the next word in a sentence, which only requires lots and lots of text. GPT-2 for instance, was pretrained this way using the content of 45 millions links posted by users on Reddit. Another example of self-supervised pretraining objective is to predict the value of randomly masked words, which is similar to fill-in-the-blank tests you may have done in school. BERT was pretrained this way using the English Wikipedia and 11,000 unpublished books. In practice, transfer learning is applied on a given model by throwing away its head, that is, its last layers focused on the pretraining objective, and replacing it with a new, randomly initialized, head suitable for the task at hand. For instance, when we fine-tuned a BERT model earlier, we removed the head that classified mask words and replaced it with a classifier with 2 outputs, since our task had
```

## Usage tips

The model can also be used for time series classification and time series regression. See the respective [`PatchTSMixerForTimeSeriesClassification`] and [`PatchTSMixerForRegression`] classes.

## PatchTSMixerConfig

[[autodoc]] PatchTSMixerConfig


## PatchTSMixerModel

[[autodoc]] PatchTSMixerModel
    - forward


## PatchTSMixerForPrediction

[[autodoc]] PatchTSMixerForPrediction
    - forward


## PatchTSMixerForTimeSeriesClassification

[[autodoc]] PatchTSMixerForTimeSeriesClassification
    - forward


## PatchTSMixerForPretraining

[[autodoc]] PatchTSMixerForPretraining
    - forward


## PatchTSMixerForRegression

[[autodoc]] PatchTSMixerForRegression
    - forward
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>
long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy.*
- T5 is an encoder-decoder model pre-trained on a multi-task mixture of unsupervised and supervised tasks and for which
each task is converted into a text-to-text format. T5 works well on a variety of tasks out-of-the-box by prepending a
different prefix to the input corresponding to each task, e.g., for translation: *translate English to German: ...*,
for summarization: *summarize: ...*.
- The pretraining includes both supervised and self-supervised training. Supervised training is conducted on downstream tasks provided by the GLUE and SuperGLUE benchmarks (converting them into text-to-text tasks as explained above).
* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.
contrast with the original ViT model, which used external data like the JFT-300M dataset/Imagenet-21k for
  pre-training.
- The authors of DeiT also released more efficiently trained ViT models, which you can directly plug into
  [`ViTModel`] or [`ViTForImageClassification`]. Techniques like data
  augmentation, optimization, and regularization were used in order to simulate training on a much larger dataset
  (while only using ImageNet-1k for pre-training). There are 4 variants available (in 3 different sizes):
  *facebook/deit-tiny-patch16-224*, *facebook/deit-small-patch16-224*, *facebook/deit-base-patch16-224* and
  *facebook/deit-base-patch16-384*. Note that one should use [`DeiTImageProcessor`] in order to
  prepare images for the model.
This model was contributed by [namctin](https://huggingface.co/namctin), [gsinthong](https://huggingface.co/gsinthong), [diepi](https://huggingface.co/diepi), [vijaye12](https://huggingface.co/vijaye12), [wmgifford](https://huggingface.co/wmgifford), and [kashif](https://huggingface.co/kashif). The original code can be found [here](https://github.com/yuqinie98/PatchTST).

## Usage tips

The model can also be used for time series classification and time series regression. See the respective [`PatchTSTForClassification`] and [`PatchTSTForRegression`] classes.


## PatchTSTConfig

[[autodoc]] PatchTSTConfig

## PatchTSTModel

[[autodoc]] PatchTSTModel
    - forward

## PatchTSTForPrediction

[[autodoc]] PatchTSTForPrediction
    - forward

## PatchTSTForClassification

[[autodoc]] PatchTSTForClassification
    - forward

## PatchTSTForPretraining

[[autodoc]] PatchTSTForPretraining
    - forward

## PatchTSTForRegression

[[autodoc]] PatchTSTForRegression
    - forward
3. The last thing to add to the patch and learnable embeddings are the *position embeddings* because the model doesn't know how the image patches are ordered. The position embeddings are also learnable and have the same size as the patch embeddings. Finally, all of the embeddings are passed to the Transformer encoder.

4. The output, specifically only the output with the `[CLS]` token, is passed to a multilayer perceptron head (MLP). ViT's pretraining objective is simply classification. Like other classification heads, the MLP head converts the output into logits over the class labels and calculates the cross-entropy loss to find the most likely class.

Ready to try your hand at image classification? Check out our complete [image classification guide](tasks/image_classification) to learn how to finetune ViT and use it for inference!

#### CNN

<Tip>
- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teacher (which, in the
  DeiT paper, is a ResNet like-model). The distillation token is learned through backpropagation, by interacting with
  the class ([CLS]) and patch tokens through the self-attention layers.
- There are 2 ways to fine-tune distilled models, either (1) in a classic way, by only placing a prediction head on top
  of the final hidden state of the class token and not using the distillation signal, or (2) by placing both a
  prediction head on top of the class token and on top of the distillation token. In that case, the [CLS] prediction
  head is trained using regular cross-entropy between the prediction of the head and the ground-truth label, while the
  distillation prediction head is trained using hard distillation (cross-entropy between the prediction of the
  distillation head and the label predicted by the teacher). At inference time, one takes the average prediction
learning and the pretrained model yields better results. And it doesn‚Äôt matter if we train longer, the training from scratch is capped around 70% accuracy while the pretrained model beats the 86% easily. This is because pretrained models are usually trained on large amounts of data that provide the model with a statistical understanding of the language used during pretraining. In computer vision, transfer learning has been applied successfully for almost ten years. Models are frequently pretrained on ImageNet, a dataset containing 1.2 millions of photo images. Each image is classified by one of 1000 labels. Training like this, on labeled data is called supervised learning. In Natural Language Processing, transfer learning is a bit more recent. A key difference with ImageNet is that the pretraining is usually self-supervised, which means it doesn‚Äôt require humans annotations for the labels. A very common pretraining objective is to guess the next word in a sentence, which only requires
```

**Note**: We checkpoint periodically during self-training. In case of preemptions, just re-run the above script and self-training will resume from the latest iteration.

### Hyperparameters for self-training
If you have development data, you might want to tune some hyperparameters for self-training.
Below are hyperparameters that could provide additional gains for your task.
*Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple
Therefore, the development of large-scale, generic time series datasets (like ImageNet in CV) is of the utmost importance. 
Creating such datasets will greatly facilitate further research on pre-trained models specifically designed for time series analysis,
and it will improve the applicability of pre-trained models in time series forecasting.

## Acknowledgements
We express our appreciation to [Lysandre Debut](https://github.com/LysandreJik) and [Pedro Cuenca](https://github.com/pcuenca)
their insightful comments and help during this project ‚ù§Ô∏è.
- MPT base: MPT base pre-trained models on next token prediction 
- MPT instruct: MPT base models fine-tuned on instruction based tasks
- MPT storywriter: MPT base models fine-tuned for 2500 steps on 65k-token excerpts of fiction books contained in the books3 corpus, this enables the model to handle very long sequences

The original code is available at the  [`llm-foundry`](https://github.com/mosaicml/llm-foundry/tree/main) repository.

Read more about it [in the release blogpost](https://www.mosaicml.com/blog/mpt-7b)

## Usage tips

- Learn more about some techniques behind training of the model [in this section of llm-foundry repository](https://github.com/mosaicml/llm-foundry/blob/main/TUTORIAL.md#faqs)
- If you want to use the advanced version of the model (triton kernels, direct flash attention integration), you can still use the original model implementation by adding `trust_remote_code=True` when calling `from_pretrained`.

## Resources

Here is the question: What is a unique benefit of using PatchTST for self-supervised time series pretraining?
Here is answer 1:Enhanced feature extraction via channel-specific encodings
Here is answer 2: Ability to handle real-time data streams during training
Here is answer3: Improved transfer learning performance across datasets
Here is answer 4: Dynamic patch segmentation based on data variance",The correct answer is: 3. Improved transfer learning performance across datasets
What common debugging tool can assist in interpreting stack traces effectively?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (24066, '952c921e-525d-4b47-9343-9bcee49f6f61'), (24067, '157c15d9-4820-40cb-9366-e7b656d413a7'), (24068, '70ef38f1-9271-4e67-b292-a311c28fb5c9'), (24070, '371ec714-93b3-4dad-a32b-b9909a7cb278'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (25105, '421712a8-0f69-436b-b77b-73a56f5dce9f'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (31258, '7f34d34d-e04a-4ab0-9cf6-5d23de269e84'), (28, 'e85e3119-1ad5-46de-8468-54238b531847'), (17443, 'bd891b6b-f323-49c6-b503-c3e499ebcdd1'), (27683, 'c32ad975-8cb8-4b0f-a698-353380f47db2'), (11303, '5eadded2-f72a-41da-a938-6bf03f630f7e'), (6700, '15379fc5-6b8b-43ad-b0c4-76dff124bcbe'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (16451, 'ef3d262a-39ec-48e4-9a8c-6e3feaa26040'), (6723, '01ac9d03-828c-47a2-95fc-a16de8fc474e'), (30278, '9a49a14a-c17e-40eb-b72a-5621d602f5b0'), (6731, 'a4c86bc7-662c-4b5d-9cbb-3967e69de023'), (19536, 'fbbacd1f-478e-4c12-96f1-94f962921780'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (19545, '43698561-ee00-4076-a164-8603710a5017'), (6749, '81c8a3fc-8f17-4481-92ab-f3d126518cc2'), (19549, '09c34333-c237-43a8-9e29-0dda3e054d55'), (6752, '5064067c-8a7f-4f23-ae5f-0b016bc7ca69'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (6756, '66d0932f-7334-4856-b0d9-2c287e50b5f6'), (16486, '44b89854-89ae-4b45-9b98-62b3d9edc5a7'), (9832, 'bdc6bea1-7f20-4300-aae9-3f7ea03a6783'), (6262, '1dad238d-be7e-48a4-9ff2-ad58d61f992d'), (1159, 'ed1c0f2d-3594-4151-ba80-832c5099d69b'), (1168, '0e3b941c-35b9-4bc5-b4dc-9c3309e9f562'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (10387, 'fd029e80-f142-427a-b5b0-aab172a1ebe3'), (20117, '344e2a72-849d-4598-a5f5-e5a7a605e227'), (30878, 'a2aa9838-7a0f-4e8a-903a-f685abf3448d'), (30879, '2ac848c0-3793-4511-8a97-46726b48dae5'), (7857, '7d99eade-0dc3-46de-bcb4-20542a337d6b'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (1205, '19799104-79e5-4b3b-bcc1-036b3fd80da5'), (1207, 'ba67be42-b645-475b-a449-454ae493e9ef'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (14524, '9edaacf8-e6a9-45da-9168-efea104fecf4'), (30908, 'f813e9a8-eab4-4c6c-b738-b58567335fa5'), (30910, '562b2c34-aa31-45cf-a703-d544e0641ad7'), (1213, 'bec86d0f-9e27-4271-bd19-fa2a88278612'), (14523, '7eaad718-02e7-45b8-a664-b3c1b35d6c39'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (30916, '83870699-d55d-4da0-9257-d46016695c8c'), (30919, 'ec3af9bb-c9be-4eca-813b-d91f994dd8af'), (19655, '6f5971e7-8354-4d7e-aec5-3ee6c1e98e7b'), (12999, '8c0a8acc-d1d0-4543-ae8f-69c312cd7840'), (6862, '0fb37f9d-1ebb-42b9-95ac-1cd17654ef37'), (20186, 'e4800925-3dd9-4604-b5d2-755f623ef3b9'), (14557, '419c5c32-2b03-40c2-9cf6-ed174b2b64d9'), (6369, '2cd13d66-4109-4355-a19d-7b7cb03135e6'), (21733, '1481b394-a1ba-4963-9bff-59b83ea88979'), (18675, '1da20ff6-a9fc-47bd-beda-e5b849c25412'), (22774, '7bea6ef0-2f31-426f-a550-040a93f64ded'), (12026, '0c59c0a7-bb17-41b9-ac73-98736a9b573e'), (20250, '06ee8af0-01dd-4c97-aae3-5ddee389cb4b'), (26407, '89088a07-9873-4409-8d25-de9fd83ca589'), (20273, '812f2b38-d4e9-4406-a021-ce5f9734d8c0'), (11579, 'e2b354ec-02aa-4b95-9146-bb9e9a359a73'), (22340, '2ae2fc34-8a2b-4041-853c-00202430737a'), (29510, '16398d72-8143-4b3f-9011-1330c903d4b0'), (20298, 'cf07b1e2-6cae-4114-989b-151be05e8ba7'), (4969, '277ff2a3-6bfe-49ef-9e45-1583b28de893'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (13690, '02b73842-811f-4da8-8c9c-dbb8545cab6e'), (13696, 'b5412eb0-f2c5-4077-b7f1-cce90588570b'), (7555, 'cf25e4ed-1e2e-4f4b-a661-9e86f72a8aac'), (7570, '30cef6d0-64c6-47bd-8d5c-6aeb6012e5a8'), (7571, 'a17fc819-2d0a-44f4-8d2d-62dff5461fbe'), (1435, '45fe399f-64a7-452a-ace1-15f7e7df5dd5'), (20896, '54b807b5-1a34-48ec-80e9-686c9508556d'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (20898, '52c8c849-3c8c-475d-89ee-582dfad7ac0b'), (20900, '1482385c-4de4-4a6a-abb9-eccea1d3fb5e'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (25002, '22fbe702-faab-4fce-b201-61d82588743f'), (20906, '4c0585c7-5b68-4b5f-820b-740cca4e0187'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (16301, '77400005-3388-49a3-86df-4cd1d768c650'), (12205, '70c5332b-6b84-438d-beff-bca04129bd86'), (16299, 'e76f46a8-1a2f-4f3d-a804-bdce969dbb81'), (26542, '55477efb-32ff-4ee8-9b02-dd83387411e5'), (25010, 'bc1b31d0-5d61-4c93-b5df-6b9cd91fcd26'), (25011, 'e16ec568-e896-44e7-95b7-22fd47a75a61'), (16308, 'e100c0a7-79a3-4834-8539-b23456ac7b3a'), (16309, '2135aa54-c407-499c-b2a2-0189de3220a5'), (16310, '4d0340ca-2008-4c33-93d4-bf10fff43a0f'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (6578, '3015fd28-83cd-4b3a-84a3-76782d312101'), (6587, '93e5fc34-4543-4839-b5b3-c7bf2b203c3e'), (19388, '141d4b90-0e49-4309-98b8-fb6890009430'), (6589, '3ed8fde9-70c5-4727-bffd-b1343bd706cf'), (6582, '0a309a47-8125-4d3f-918b-e93ab30c0d7c'), (25023, '69d90df0-d143-46ab-8898-0ca3e27188d8'), (25024, '24213a1a-72e6-48ed-b43d-8b482efb632d'), (25026, '9e3a9873-ca79-49a5-a14e-a0cfd408d99d'), (25539, '19301fff-8b06-445d-b54f-970e21aa7bc1'), (28610, 'c9183302-f179-4942-a14e-52f43f7b99da'), (25541, 'c23dff23-c24b-406b-aff6-016be08df943'), (30151, '29bf41ee-4f85-409e-bbad-6c5d179808d7'), (30152, '5ade3db8-c692-48c8-b807-fb6a9c7027cd'), (30153, 'e761ff90-a3a2-4d6a-bc33-92f64c872625'), (30154, '5e9431bd-c8d9-4fb9-ba40-fb86a51de0bc'), (30667, '5ab35ed2-c28e-4c60-be97-eb6e05d3cce4'), (10701, 'bbd5a305-164e-4ed1-afba-5cdf2aeca8ce'), (10702, '12c8b32e-bc12-4269-9240-fea9ee3c4b99'), (26061, '4bc5a19a-6b14-4c8e-8ac0-3885c5980724'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (1516, '18d0dd47-15ad-4d34-865a-9277bc2db97c'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (15861, 'cdbf3fc8-ee9f-4a6a-bfd8-e1fe30982ba1'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

Here you will get a huge number of frames dumped - as many as there were forward calls in your model, so it may or may
not what you want, but sometimes it can be easier to use for debugging purposes than a normal debugger. For example, if
a problem starts happening at batch number 150. So you can dump traces for batches 149 and 150 and compare where
numbers started to diverge.

You can also specify the batch number after which to stop the training, with:

```python
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)
```
```

Woohoo, it worked! Let's recap what you've just learned:

- The error messages in Python are known as _tracebacks_ and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem.
- If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs.
- If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue.
- The `huggingface_hub` 
// ü§ó Hub?
library provides a suite of tools that you can use to interact with and debug repositories on the Hub.

Now that you know how to debug a pipeline, let's take a look at a trickier example in the forward pass of the model itself.

## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]
to go a little deeper to debug the problem. Fortunately, you can use the Python debugger at any time you get an error in a Jupyter Notebook by typing %debug in any cell. When executing that cell, you go to the very bottom of the traceback where you can type commands and you can type commands. The first two commands you should learn are u and d (for up and down), which allow you to go up in the Traceback or down. Going up twice, we get to the point the error was reached. The third command to learn is p, for print. It allows you to print any value you want. For instance here, we can see the value of return_tensors or batch_outputs to try to understand what triggered the error. The batch outputs dictionary is a bit hard to see, so let's dive into smaller pieces of it. Inside the debugger you can not only print any variable but also evaluate any expression, so we can look independently at the inputs or labels. Those labels are definitely weird: they are of various size, which we can
problem. Fortunately, you can use the Python debugger quite easily in a terminal by launching your script with python -m pdb instead of just python. When executing that command, you are sent to the first instruction of your script. You can run just the next instruction by typing n, or continue to the error by directly typing c. Once there, you go to the very bottom of the traceback, and you can type commands. The first two commands you should learn are u and d (for up and down), which allow you to go up in the Traceback or down. Going up twice, we get to the point the error was reached. The third command to learn is p, for print. It allows you to print any value you want. For instance here, we can see the value of return_tensors or batch_outputs to try to understand what triggered the error. The batch outputs dictionary is a bit hard to see, so let's dive into smaller pieces of it. Inside the debugger you can not only print any variable but also evaluate any expression, so we can look
```

### Specific batch absolute min and max value tracing

The same debugging class can be used for per-batch tracing with the underflow/overflow detection feature turned off.

Let's say you want to watch the absolute min and max values for all the ingredients of each `forward` call of a given
batch, and only do that for batches 1 and 3. Then you instantiate this class as:

```python
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])
```

And now full batches 1 and 3 will be traced using the same format as the underflow/overflow detector does.

Batches are 0-indexed.

This is helpful if you know that the program starts misbehaving after a certain batch number, so you can fast-forward
right to that area. Here is a sample truncated output for such configuration:
```

Then you can inspect the profile at [http://localhost:6006/#profile](http://localhost:6006/#profile).

<Tip warning={true}>

If you run into version conflicts with the plugin, try uninstalling and reinstalling all versions of TensorFlow and Tensorboard. The debugging functionality of the profile plugin is still experimental, and not all views are fully functional. The `trace_viewer` cuts off events after 1M, which can result in all your device traces getting lost if for example, you profile the compilation step by accident.

</Tip>
```

This will dump a lot of NCCL-related debug information, which you can then search online if you find that some problems are reported. Or if you're not sure how to interpret the output you can share the log file in an Issue.



## Underflow and Overflow Detection

<Tip>

This feature is currently available for PyTorch-only.

</Tip>

<Tip>

For multi-GPU training it requires DDP (`torch.distributed.launch`).

</Tip>

<Tip>

This feature can be used with any `nn.Module`-based model.

</Tip>

If you start getting `loss=NaN` or the model inhibits some other abnormal behavior due to `inf` or `nan` in
activations or weights one needs to discover where the first underflow or overflow happens and what led to it. Luckily
you can accomplish that easily by activating a special module that will do the detection automatically.

If you're using [`Trainer`], you just need to add:

```bash
--debug underflow_overflow
It will interrupt the execution and launch the Python debugger at this place, and we can inspect all the variables before the next instruction is executed. Typing n executes the next instruction, which takes us back inside the traceback. One way to fix the error is to manually pad all labels to the longest, or we can use the data collator designed for this.
### Including the full traceback[[including-the-full-traceback]]

Since the last line of the traceback is often enough to debug your own code, it can be tempting to just provide that in your topic to ""save space."" Although well intentioned, this actually makes it _harder_ for others to debug the problem since the information that's higher up in the traceback can be really useful too. So, a good practice is to copy and paste the _whole_ traceback, while making sure that it's nicely formatted. Since these tracebacks can get rather long, some people prefer to show them after they've explained the source code. Let's do this. Now, our forum topic looks like the following:

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter8/forum-topic04.png"" alt=""Our example forum topic, with the complete traceback."" width=""100%""/>
</div>
In general, there are two possible debugging environments for running
the original model

-   [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)
-   Local python scripts.

Jupyter notebooks have the advantage that they allow for cell-by-cell
execution which can be helpful to better split logical components from
one another and to have faster debugging cycles as intermediate results
can be stored. Also, notebooks are often easier to share with other
contributors, which might be very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupyter notebooks, we
strongly recommend you to work with them.

The obvious disadvantage of Jupyter notebooks is that if you are not
used to working with them you will have to spend some time adjusting to
the new programming environment and that you might not be able to use
your known debugging tools anymore, like `ipdb`.

**4. Successfully run forward pass**
In general, there are two possible debugging environments for running
the original model

-   [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)
-   Local python scripts.

Jupyter notebooks have the advantage that they allow for cell-by-cell
execution which can be helpful to better split logical components from
one another and to have faster debugging cycles as intermediate results
can be stored. Also, notebooks are often easier to share with other
contributors, which might be very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupyter notebooks, we
strongly recommend you to work with them.

The obvious disadvantage of Jupyter notebooks is that if you are not
used to working with them you will have to spend some time adjusting to
the new programming environment and that you might not be able to use
your known debugging tools anymore, like `ipdb`.

**4. Successfully run forward pass**
Choose which backend to use by specifying it via `torch_compile_backend` in the [`TrainingArguments`].  Some of the most commonly used backends are:

**Debugging backends**:
* `dynamo.optimize(""eager"")` - Uses PyTorch to run the extracted GraphModule. This is quite useful in debugging TorchDynamo issues.
* `dynamo.optimize(""aot_eager"")` - Uses AotAutograd with no compiler, i.e, just using PyTorch eager for the AotAutograd's extracted forward and backward graphs. This is useful for debugging, and unlikely to give speedups.
In general, there are two possible debugging environments for running the original model

-  [Jupyter notebooks](https://jupyter.org/) / [google colab](https://colab.research.google.com/notebooks/intro.ipynb)
-  Local python scripts.

Jupyter notebooks have the advantage that they allow for cell-by-cell execution which can be helpful to better split
logical components from one another and to have faster debugging cycles as intermediate results can be stored. Also,
notebooks are often easier to share with other contributors, which might be very helpful if you want to ask the Hugging
Face team for help. If you are familiar with Jupyter notebooks, we strongly recommend you work with them.

The obvious disadvantage of Jupyter notebooks is that if you are not used to working with them you will have to spend
some time adjusting to the new programming environment and you might not be able to use your known debugging tools
anymore, like `ipdb`.
The batch outputs dictionary is a bit hard to see, so let's dive into smaller pieces of it. Inside the debugger you can not only print any variable but also evaluate any expression, so we can look independently at the inputs or labels. Those labels are definitely weird: they are of various size, which we can actually confirm by printing the sizes. No wonder the tokenizer wasn't able to create a tensor with them! This is because the pad method only takes care of the tokenizer outputs: input IDs, attention mask and token type IDs, so we have to pad the labels ourselves before trying to create a tensor with them. Once you are ready to exit the Python debugger, you can press q for quit. Another way we can access the Python debugger is to set a ""set_trace"" instruction where we want in the script. It will interrupt the execution and launch the Python debugger at this place, and we can inspect all the variables before the next instruction is executed. Typing n executes the next instruction,
Using TensorBoard

TensorBoard provides tooling for tracking and visualizing metrics as well as visualizing models. All repositories that contain TensorBoard traces have an automatic tab with a hosted TensorBoard instance for anyone to check it out without any additional effort!

## Exploring TensorBoard models on the Hub

Over 52k repositories have TensorBoard traces on the Hub. You can find them by filtering at the left of the [models page](https://huggingface.co/models?filter=tensorboard). As an example, if you go to the [aubmindlab/bert-base-arabertv02](https://huggingface.co/aubmindlab/bert-base-arabertv02) repository, there is a **Metrics** tab. If you select it, you'll view a TensorBoard instance.
The first step was to take a sentence, encode it and then feed to the `generate` function - for `fairseq` and for `transformers`. 

After a few very failing attempts to get somewhere (*) - I quickly realized that with the current level of complexity using `print` as debugging method will get me nowhere, and neither will the basic `pdb` debugger. In order to be efficient and to be able to watch multiple variables and have watches that are code-evaluations I needed a serious visual debugger. I spent a day trying all kinds of python debuggers and only when I tried `pycharm` I realized that it was the tool that I needed. It was my first time using `pycharm`, but I quickly figured out how to use it, as it was quite intuitive.

* footnote: the model was generating 'nononono' in Russian - that was fair and hilarious!
Why is that? It has to do with the way GPUs work. They are extremely efficient at executing a lot of operations in parallel, but the drawback is that when one of those instructions results in an error, you don't know it instantly. It's only when the program calls a synchronization of the multiple processes on the GPU that it will realize something went wrong, so the error is actually raised at a place that has nothing to do with what created it. For instance, if we look at our previous traceback, the error was raised during the backward pass, but we will see in a minute that it actually stems from something in the forward pass.

So how do we debug those errors? The answer is easy: we don't. Unless your CUDA error is an out-of-memory error (which means there is not enough memory in your GPU), you should always go back to the CPU to debug it.
However, we can make things even easier for them by providing the actual code that triggered the error. Let's do that now.

### Providing a reproducible example[[providing-a-reproducible-example]]

If you've ever tried to debug someone else's code, you've probably first tried to recreate the problem they've reported so you can start working your way through the traceback to pinpoint the error. It's no different when it comes to getting (or giving) assistance on the forums, so it really helps if you can provide a small example that reproduces the error. Half the time, simply walking through this exercise will help you figure out what's going wrong. In any case, the missing piece of our example is to show the _inputs_ that we provided to the model. Doing that gives us something like the following completed example:
## Running quick tests

Most examples are equipped with a mechanism to truncate the number of dataset samples to the desired length. This is useful for debugging purposes, for example to quickly check that all stages of the programs can complete, before running the same setup on the full dataset which may take hours to complete.

For example here is how to truncate all three splits to just 50 samples each:
```

   and it typically includes a traceback, so that we can see the full stack of calls the program made before it fails. This gives us the context to know why the program failed.

   Going back to the above example. If you received this error search, look at the very last line of the error which is:

   ```python
    ModuleNotFoundError: No module named 'tqdm.auto'

Here is the question: What common debugging tool can assist in interpreting stack traces effectively?
Here is answer 1:Custom Python error handlers
Here is answer 2: Interactive Python debugger (pdb)
Here is answer3: Static code analysis tools
Here is answer 4: Integrated environment linters","Based on context provided , Answer2 :Interactive Python Debugger(pdb)"
How does using JSON Lines benefit dataset preprocessing in NLP pipelines?,"[(13825, 'f1446f89-52a4-4aaf-ab1a-9b92f5010ce7'), (19459, 'cb31efe8-a229-4e7b-9ab4-f850276a68c2'), (1540, '6087e6c1-790b-400f-af65-2510f2275487'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (11792, 'e3cb7b34-455d-4c33-be90-a3547abf420d'), (3606, '8e998062-3240-49e5-b850-8e190f1edf71'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (21540, '5ff9aa91-38f7-47d2-abed-478d0a8dd9f9'), (6186, '7d25cfc6-c134-45b5-b7ac-649d42e1558c'), (8235, 'e0e09663-4604-4e22-b4bb-bb1496379185'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (8239, '601c3e59-2039-4a07-a2ff-f1e97ecadabd'), (6192, 'ce855971-8e34-465c-bc05-66c10ee3ec5a'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (6193, '64c18491-4a9d-47a6-9c49-3936a488647c'), (25145, '20079c45-5e6f-4adf-8f78-0c23c277da08'), (24126, 'f6d0b6d4-13c2-4d59-a59d-6b341a5a1508'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (17476, '622f9bf0-744a-4fe6-b4e7-eeebdc82d918'), (3679, '8e31b4d7-de14-44b3-be8a-7236eb150661'), (13422, '7fcc282e-51e4-4714-887d-b6075a5d4b55'), (7280, 'f968b214-cb48-463f-882e-be4af04c57b3'), (7282, 'b97fe108-9df7-46b1-a671-c7446a0ce2cc'), (8819, '2703836a-9d2a-4d76-90f6-34224a5780b1'), (29313, '876d81a8-eebe-4c5a-ac96-6f076997f9ba'), (4743, 'b64a40eb-e329-46ff-b747-342002c50922'), (24204, '24972dc6-4053-46d8-93be-df0faad47b39'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (13979, 'de950627-bfa4-44a7-aded-c6a93c8ee195'), (13980, 'a1b5a2e5-b7b6-4914-b2db-b07eb318ca48'), (18590, 'ac6b59ce-1adb-4f5d-b5cc-890731055b35'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (9381, '1695844a-c3fc-40f7-8e86-4f2d6eea7efa'), (11435, '35ee7661-d4c6-4384-bb77-e40ec77986b8'), (9390, '8ca789f9-6a64-4dfd-a067-8a38701e9237'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (24763, '81ad470e-e933-4a27-9979-d29627c04cf1'), (5309, '1063e5a8-266b-4339-bd57-f04a35f278f7'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (5320, 'b954f493-8fa4-4de0-b1f8-801bcad1c1a4'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (19664, '2872ed56-b851-405f-8601-3748f47a00e1'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (21205, '6bda238b-e50a-4ace-95e5-877b1ae72029'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (15581, '8a808209-2556-4ec5-bd15-34e03c26c682'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (31458, '772a8786-3e64-4959-84fa-931ce6254e05'), (24293, 'ee8be06f-b07e-40de-81fc-6549ce8b8cdc'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (15592, '481a6c32-3e99-4c07-a468-dbe480fef1b4'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (7942, '14990c0d-4b9f-4a48-b72f-ad5c83124c6f'), (3339, '6c368a7d-8543-44b3-9993-4674ebdec341'), (3342, '40de6e0e-cd3f-4ac4-8b93-e0a427a98e4d'), (3346, '032420bb-66aa-4f3e-b636-fe7b4d17188b'), (21788, '76f89282-5e29-480a-9fb9-107b45af559d'), (8989, '28551de5-f06d-4b91-b2c9-f1fdcc62c353'), (13089, '18e00bee-8e73-4033-ba07-1cc173d28da5'), (13091, 'fd87ced8-d15d-4eb8-98f3-447a6c638420'), (25898, '73ac5dcb-df39-4bca-8d7e-42637669ea82'), (25900, '22302b1e-0111-4e79-a205-ad225b9df0ac'), (26931, '3bc791e3-879f-48fa-a4a0-c5c426d6b558'), (28982, '50351ec7-ad8b-43e7-a669-2087f3d54e21'), (15672, '3c536d00-c1fc-410b-a634-96a25e8778ee'), (24890, 'afcbcb7c-ee42-4823-ad2c-9b6d06c0f04b'), (25914, 'd343a79e-38fb-4fce-9fcc-0d1e23c73b40'), (24892, '87b31d2d-afa8-4e15-8262-9ef33723c313'), (24891, '4d510e18-9e28-413e-a0cf-1a455cdee512'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (3925, '12548579-f219-484d-b120-fbfedb7bd71f'), (4438, 'c433e5f5-9651-489c-8a76-f04bf6884777'), (18265, 'f0ca8e07-29ec-4f8d-a2a7-1484fd1b7959'), (26969, '9feb9ebe-24da-4312-bfc6-1ea3e3d83468'), (26970, '56e38e53-1af9-4cec-beb0-886be736d495'), (3932, '40509c62-9846-48d2-8d6f-a3635dd9263f'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (31586, '4351b921-b4f2-4a0a-a126-07b9ddb21e7f'), (18276, '3c255798-f281-4027-9f8e-004acb7710ef'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (18286, '0aa69d94-c6a8-4e6a-a558-be1d6abd7fdb'), (31598, 'd3e6a577-47b5-4ed6-83a7-e22fb2d3c6c5'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (24961, '3bcca0f3-29da-49e7-be84-4b41c54e57d4'), (24962, '5276ff68-1983-46e4-8f9b-e33eb6dc153d'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (9610, 'b094bdc0-3783-4e56-aa01-2bb05d7808ef'), (400, 'd8436d7a-0504-4d3e-8007-c5431d2d3b62'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (19870, '359a7e1c-0599-4279-9610-c730b74ad737'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (26018, 'f342cf73-c386-4093-99cd-e639ba2efb08'), (31141, '246a8167-3468-46a7-97ea-8eba392ffd1c'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (13219, 'f4a481fd-9e87-4669-93a5-4a2aedc5d286'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (5039, '52341562-e420-4ebb-baa7-503d3b27ab6e'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (17327, '99cc7628-65c7-4491-ae85-b9a882fc399d'), (26034, '8be7f373-dafd-44e1-9781-709da1732757'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (26036, '0e8e3f85-e18c-46e7-97ab-595f1fda1603'), (31154, 'e49097a1-9b93-492c-9e9c-343a02a27e52'), (6102, '3165e349-5926-4468-ac0c-1f9d8959ed1d'), (16856, 'c092e317-6d17-4c0f-a54c-70332d7e4094'), (18912, '0d70a2a9-cf2c-4100-a0c2-aa272e08a67d'), (25575, 'd5dda4e8-ea7f-463a-984e-f76ed535f6bf'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (8186, '66d4be7e-e0f6-4443-b832-97821546eb78'), (25598, '8c2cf4ad-64ee-430a-8e85-335f69c1caf8')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: `run_translation.py` is a lightweight examples of how to download and preprocess a dataset from the [ü§ó Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.

For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets#json-files
and you also will find examples of these below.


## With Trainer

Here is an example of a translation fine-tuning with a MarianMT model:

```bash
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path Helsinki-NLP/opus-mt-en-ro \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

### Data preprocessing
In the rest of this tutorial we will be using [CodeParrot](https://huggingface.co/codeparrot/codeparrot-small) model and data as an example.

The training data requires some preprocessing. First, you need to convert it into a loose json format, with one json containing a text sample per line. If you're using ü§ó [Datasets](https://huggingface.co/docs/datasets/index), here is an example on how to do that (always inside Megatron-LM folder):
```python
from datasets import load_dataset

train_data = load_dataset('codeparrot/codeparrot-clean-train', split='train')
train_data.to_json(""codeparrot_data.json"", lines=True)
```

The data is then tokenized, shuffled and processed into a binary format for training using the following command:
```bash
pip install nltk
cd Megatron-LM
python tools/preprocess_data.py \
       --input codeparrot_data.json \
       --output-prefix codeparrot \
       --vocab vocab.json \
       --dataset-impl mmap \
       --tokenizer-type GPT2BPETokenizer \
       --merge-file merges.txt \
       --json-keys content \
       --workers 32 \
       --chunk-size 25 \
       --append-eod
```

The data is then tokenized, shuffled and processed into a binary format for training using the following command:
```bash
#if nltk isn't installed
pip install nltk
python tools/preprocess_data.py \
       --input codeparrot_data.json \
       --output-prefix codeparrot \
       --vocab vocab.json \
       --dataset-impl mmap \
       --tokenizer-type GPT2BPETokenizer \
       --merge-file merges.txt \
       --json-keys content \
       --workers 32 \
       --chunk-size 25 \
       --append-eod
`run_summarization.py` is a lightweight example of how to download and preprocess a dataset from the [ü§ó Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.

For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets#json-files
and you also will find examples of these below.

## With Trainer

Here is an example on a summarization task:
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config ""3.0.0"" \
    --source_prefix ""summarize: "" \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

As we saw in [Chapter 1](/course/chapter1), this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg"" alt=""The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg"" alt=""The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.""/>
</div>

Let's quickly go over each of these.

## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]
```

For improved performance on reading and writing large datasets stored in the cloud, this script uses the [WebDataset](https://github.com/webdataset/webdataset) format to create a preprocessing pipeline to apply transforms and create a dataset and dataloader for training. Images are processed and fed to the training loop without having to download the full dataset first.

```py
processing_pipeline = [
    wds.decode(""pil"", handler=wds.ignore_and_continue),
    wds.rename(image=""jpg;png;jpeg;webp"", text=""text;txt;caption"", handler=wds.warn_and_continue),
    wds.map(filter_keys({""image"", ""text""})),
    wds.map(transform),
    wds.to_tuple(""image"", ""text""),
]
### Highlights of this release
- We can now encode both pre-tokenized inputs, and raw strings. This is especially usefull when
processing datasets that are already pre-tokenized like for NER (Name Entity Recognition), and helps
while applying labels to each word.
- Full tokenizer serialization. It is now easy to save a tokenizer to a single JSON file, to later
load it back with just one line of code. That's what sharing a Tokenizer means now: 1 line of code.
- With the serialization comes the compatibility with `Pickle`! The Tokenizer, all of its components,
Encodings, everything can be pickled!
- Training a tokenizer is now even faster (up to 5-10x) than before!
- Compatibility with `multiprocessing`, even when using the `fork` start method. Since this library
makes heavy use of the multithreading capacities of our computers to allows a very fast tokenization,
this led to problems (deadlocks) when used with `multiprocessing`. This version now allows to
Preprocess

In addition to loading datasets, ü§ó Datasets other main goal is to offer a diverse set of preprocessing functions to get a dataset into an appropriate format for training with your machine learning framework. 

There are many possible ways to preprocess a dataset, and it all depends on your specific dataset. Sometimes you may need to rename a column, and other times you might need to unflatten nested fields. ü§ó Datasets provides a way to do most of these things. But in nearly all preprocessing cases, depending on your dataset modality, you'll need to:

- Tokenize a text dataset.
- Resample an audio dataset.
- Apply transforms to an image dataset.

The last preprocessing step is usually setting your dataset format to be compatible with your machine learning framework's expected input format.

In this tutorial, you'll also need to install the ü§ó Transformers library:

```bash
pip install transformers
```

To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:

```py
>>> tokenized_billsum = billsum.map(preprocess_function, batched=True)
```

Now create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.

<frameworkcontent>
<pt>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```
</pt>
<tf>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=""tf"")
```

We can then use the techniques from [section 2](/course/chapter5/2) to load the JSON files as follows:

```py
data_files = {
    ""train"": ""drug-reviews-train.jsonl"",
    ""validation"": ""drug-reviews-validation.jsonl"",
    ""test"": ""drug-reviews-test.jsonl"",
}
drug_dataset_reloaded = load_dataset(""json"", data_files=data_files)
```

And that's it for our excursion into data wrangling with ü§ó Datasets! Now that we have a cleaned dataset for training a model on, here are a few ideas that you could try out:

1. Use the techniques from [Chapter 3](/course/chapter3) to train a classifier that can predict the patient condition based on the drug review.
2. Use the `summarization` pipeline from [Chapter 1](/course/chapter1) to generate summaries of the reviews.

Next, we'll take a look at how ü§ó Datasets can enable you to work with huge datasets without blowing up your laptop!
```

We get the same number of training features as before, but here we've kept all the old fields. If you need them for some post-processing after applying your model, you might want to use this approach.

You've now seen how ü§ó Datasets can be used to preprocess a dataset in various ways. Although the processing functions of ü§ó Datasets will cover most of your model training needs,
there may be times when you'll need to switch to Pandas to access more powerful features, like `DataFrame.groupby()` or high-level APIs for visualization. Fortunately, ü§ó Datasets is designed to be interoperable with libraries such as Pandas, NumPy, PyTorch, TensorFlow, and JAX. Let's take a look at how this works.

## From `Dataset`s to `DataFrame`s and back[[from-datasets-to-dataframes-and-back]]

<Youtube id=""tfcY1067A5Q""/>
of our dataset, we need to tokenize them. Have a look at the video ""Preprocess sentence pairs"" for a refresher, but you just have to send the two sentences to the tokenizer with some additional keyword arguments. Here we indicate a maximum length of 128 and pad inputs shorter than this length, truncate inputs that are longer. We put all of this in a tokenize_function that we can directly apply to all the splits in our dataset with the map method. As long as the function returns a dictionary-like object, the map method will add new columns as needed or update existing ones. To speed up preprocessing and take advantage of the fact our tokenizer is backed by Rust thanks to the Hugging Face Tokenizers library, we can process several elements at the same time to our tokenize function, using the batched=True argument. Since the tokenizer can handle list of first/second sentences, the tokenize_function does not need to change for this. You can also use multiprocessing with the map method,
of our dataset, we need to tokenize them. Have a look at the video ""Preprocess sentence pairs"" for a refresher, but you just have to send the two sentences to the tokenizer with some additional keyword arguments. Here we indicate a maximum length of 128 and pad inputs shorter than this length, truncate inputs that are longer. We put all of this in a tokenize_function that we can directly apply to all the splits in our dataset with the map method. As long as the function returns a dictionary-like object, the map method will add new columns as needed or update existing ones. To speed up preprocessing and take advantage of the fact our tokenizer is backed by Rust thanks to the Hugging Face Tokenizers library, we can process several elements at the same time to our tokenize function, using the batched=True argument. Since the tokenizer can handle list of first/second sentences, the tokenize_function does not need to change for this. You can also use multiprocessing with the map method,
</Tip>

To preprocess our whole dataset, we need to tokenize all the inputs and apply `align_labels_with_tokens()` on all the labels. To take advantage of the speed of our fast tokenizer, it's best to tokenize lots of texts at the same time, so we'll write a function that processes a list of examples and use the `Dataset.map()` method with the option `batched=True`. The only thing that is different from our previous example is that the `word_ids()` function needs to get the index of the example we want the word IDs of when the inputs to the tokenizer are lists of texts (or in our case, list of lists of words), so we add that too:
```

To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:

```py
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

Now create a batch of examples using [`DataCollatorWithPadding`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.

<frameworkcontent>
<pt>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```
</pt>
<tf>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=""tf"")
Build and load

Nearly every deep learning workflow begins with loading a dataset, which makes it one of the most important steps. With ü§ó Datasets, there are more than 900 datasets available to help you get started with your NLP task. All you have to do is call: [`load_dataset`] to take your first step. This function is a true workhorse in every sense because it builds and loads every dataset you use.

## ELI5: `load_dataset`

Let's begin with a basic Explain Like I'm Five.

A dataset is a directory that contains:

- Some data files in generic formats (JSON, CSV, Parquet, text, etc.)
- A dataset card named `README.md` that contains documentation about the dataset as well as a YAML header to define the datasets tags and configurations
- An optional dataset script if it requires some code to read the data files. This is sometimes used to load files of specific formats and structures.
```

The way the ü§ó Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function:

```python out
DatasetDict({
    train: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 3668
    })
    validation: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 408
    })
    test: Dataset({
        features: ['attention_mask', 'idx', 'input_ids', 'label', 'sentence1', 'sentence2', 'token_type_ids'],
        num_rows: 1725
    })
})
Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the [previous chapter](/course/chapter2), we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in [Chapter 2](/course/chapter2). So, one way to preprocess the training dataset is:

```py
tokenized_dataset = tokenizer(
    raw_datasets[""train""][""sentence1""],
    raw_datasets[""train""][""sentence2""],
    padding=True,
    truncation=True,
)
```

This saves each split in [JSON Lines format](https://jsonlines.org), where each row in the dataset is stored as a single line of JSON. Here's what the first example looks like:

```py
!head -n 1 drug-reviews-train.jsonl

Here is the question: How does using JSON Lines benefit dataset preprocessing in NLP pipelines?
Here is answer 1:Simplifies parsing by processing one JSON object per line
Here is answer 2: Reduces data size for faster preprocessing
Here is answer3: Allows embedding metadata directly into rows
Here is answer 4: Improves compatibility with GPU pipelines","Based on context provided which seems related specifically related coding/Coding tasks(bytes,data handling/conversion,text analysis/tokenisationColorBrushendantsofNLP(ETFseq tkn/tokeconversion(Dataformerhtableflattenthe proach SDataset managerial mhtooperatointing handed ptokenspdatactmormaticstryintegratiobustrggesudasewhen padingopadddataset #+#converuffersdatdaeraseitvergthprevarietymotmakingDICevsinal structur"
What is the best approach to debug an ML pipeline while ensuring compliance with licensing requirements for datasets?,"[(29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (29201, 'cdbabce2-0910-47ac-93ec-2dc35c8b6b91'), (29202, 'a6666314-edd1-47cc-8628-335a50fe419d'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (30739, 'fda68c03-224d-4b2c-8385-15822bc392ef'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (19986, 'a48956b7-e680-4f75-8fd7-9bd7d7bf8e21'), (19988, '082a0597-a1f2-44ad-84e9-05168e283d72'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (13339, 'f046b9a1-daad-440b-8fa6-1a8de946d278'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (13857, 'eee70593-20c9-48e9-8ff9-15a701fce582'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (14382, 'f0a3cc11-b2fc-4b21-a76e-c0d46a55834a'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (28723, 'c23b045d-e37d-4922-ab87-54a26f4802b3'), (7735, '73b16f91-3f71-40bd-9699-6896aa1dd3b5'), (12354, 'fd8028df-5620-45c4-b917-7cb54c1b6133'), (9282, '91edce5b-dc46-47e2-85e4-33cf1d17dab7'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (19053, 'ae1e94a1-3807-46ed-85c6-ccce72cc33aa'), (1159, 'ed1c0f2d-3594-4151-ba80-832c5099d69b'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (1168, '0e3b941c-35b9-4bc5-b4dc-9c3309e9f562'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (20117, '344e2a72-849d-4598-a5f5-e5a7a605e227'), (10923, '48a8b0e4-0e9b-4f01-b296-83212b9301c1'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (24752, '3798bf75-5a2c-4b7a-9800-c75a034bfae1'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (20153, 'dcf1b3ac-5649-402f-a8af-0e99997d1e7b'), (697, 'a79771f8-9f63-4615-8faf-cca019cc8939'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (20671, '817cbafe-ab8a-4498-88c1-de98146869d4'), (20672, 'e610e274-c761-4856-9c20-1c74ba007cdf'), (11457, 'b6ea4cc8-fd8c-4df2-b3a6-eaa7852e8372'), (21187, 'd9dcef43-8125-461e-9c4f-131925ad7523'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16582, 'a5a53226-6dca-425c-afa6-94c7b4adf300'), (29896, 'b72ba5ef-9507-48a0-91d0-2fe3e2cc33f3'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (17609, '4cdc2094-1a54-40b7-9a6e-05ea32af5ee7'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (28880, 'c5576ba8-d702-4f10-b06a-87380bcb7a9f'), (24277, 'ed65358e-23d7-460f-9fcd-87b89b8b726b'), (17623, '9ffafb20-3956-436d-ae73-eb7132a07981'), (28889, 'a03452b3-d0fc-4ca5-a961-53dc3439cfee'), (732, 'd42b2bda-6ad0-4591-845a-4dd23901ca1a'), (28893, 'fc3d92d3-71f0-4842-97ff-435385c1ab3f'), (28894, '62d52bf8-8829-4280-8d8e-044fe915ed0b'), (12022, 'ca590bdd-6993-45bf-bc5c-6216ff9e670d'), (17658, '9d5fe55e-4bcc-4604-991c-a59dd47f9d33'), (7942, '14990c0d-4b9f-4a48-b72f-ad5c83124c6f'), (3337, '86cf17f6-cc33-43f4-8fc6-628d03c4520f'), (3339, '6c368a7d-8543-44b3-9993-4674ebdec341'), (16146, 'e100c4ec-db20-4e52-93d6-f9535f7f90b8'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (20252, 'b9e133f5-90ab-47a2-9af2-55d084a98c15'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (21799, 'eef8b3e0-a59f-4579-9fbe-b2fff100f9c0'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (11575, '28589867-4683-4d14-8fbc-3442efb0da38'), (2873, 'b3811c6d-da6f-4c9f-8885-cc9b2dbf1bc7'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (20298, 'cf07b1e2-6cae-4114-989b-151be05e8ba7'), (4442, '274884d8-70a8-4a04-900c-0d6dc0e00410'), (5978, '46c8cc0b-9403-4474-9d60-2b2578a424c6'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (31586, '4351b921-b4f2-4a0a-a126-07b9ddb21e7f'), (18276, '3c255798-f281-4027-9f8e-004acb7710ef'), (28517, '83d1dded-982b-449c-9af4-e5fecc3aac5b'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (28522, 'd5bd37ca-d3b3-41a0-b14f-2a82a1f0daa1'), (31601, 'f5490662-af6a-4c95-883f-c30838703a4e'), (29689, 'deb041f0-74a4-4cfd-8792-9dd1e899718d'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (14725, 'f93b2fc4-f0b6-42bb-9b39-5f3d69f3047b'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (19857, '894d909b-b9a8-4c18-8789-710e8a2fd282'), (19867, '88ae508e-cedb-469a-b846-e494beb34b6b'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (2469, '24b5a6c3-9526-4bd4-b85f-cf7733f0a28b'), (20906, '4c0585c7-5b68-4b5f-820b-740cca4e0187'), (17326, '2891f89d-3318-435c-816d-b7537b0b5338'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (25049, 'cf8bc526-5eeb-4029-aba2-8ec814ad0167'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (1516, '18d0dd47-15ad-4d34-865a-9277bc2db97c'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (24560, 'ae4f8022-24f0-469e-9cb5-358e58b5773d'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (5625, 'c79981e0-e8ac-454a-a1ec-1e4b2c4b1ba2'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (5627, '79dad456-91d9-4b8a-8282-4c0a0cf743eb')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(""glue"", ""mnli"")

model_checkpoint = ""distilbert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples[""premise""], examples[""hypothesis""], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""L-WSwUWde1U""/>

The problem when you encounter an error in `trainer.train()` is that it could come from multiple sources, as the `Trainer` usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.
!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Pipelines

Pipelines provide a simple way to run state-of-the-art diffusion models in inference by bundling all of the necessary components (multiple independently-trained models, schedulers, and processors) into a single end-to-end class. Pipelines are flexible and they can be adapted to use different schedulers or even model components.
- **Open:** these licenses allow royalty free access and flexible downstream use and re-distribution of the licensed material, and distribution of any derivatives of it.

- **Responsible:** OpenRAIL licenses embed a specific set of restrictions for the use of the licensed AI artifact in identified critical scenarios. Use-based restrictions are informed by an evidence-based approach to ML development and use limitations which forces to draw a line between promoting wide access and use of ML against potential social costs stemming from harmful uses of the openly licensed AI artifact. Therefore, while benefiting from an open access to the ML model, the user will not be able to use the model for the specified restricted scenarios.
If indeed ML models and software/source code are different artifacts, why is the former released under open source licenses? The answer is easy, open source licenses have become the de facto standard in software-related markets for the open sharing of code among software communities. This ""open source"" approach to collaborative software development has permeated and influenced AI development and licensing practices and has brought huge benefits. Both open source and Open & Responsible AI licenses (""OpenRAIL"") might well be complementary initiatives.

**Why don't we design a set of licensing mechanisms inspired by movements such as open source and led by an evidence-based approach from the ML field?** In fact, there is a new set of licensing frameworks which are going to be the vehicle towards open and responsible ML development, use and access: Open & Responsible AI Licenses ([OpenRAIL](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses)).
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""N9kO52itd0Q""/>

The problem when you encounter an error in `model.fit()` is that it could come from multiple sources, as training usually brings together a lot of things that you've been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `model.fit()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):
<Question
	choices={[
		{
			text: ""The optimization step where we compute gradients and perform backpropagation"",
			explain: ""Although there may be bugs in your optimizer, this is usually several steps into the training pipeline, so there are other things to check first. Try again!""
		},
		{
			text: ""The evaluation step where we compute metrics"",
			explain: ""Evaluation is usually what you do after training for a full epoch, so you should first check somewhere earlier in the training pipeline."",
		},
		{
			text: ""The datasets"",
			explain: ""Correct! Looking at your data is almost always the first thing you should do, to make sure the text is encoded appropriately, has the expected features, and so on."",
			correct: true
		},
		{
			text: ""The dataloaders"",
			explain: ""Try again -- this is very close to the first thing you should check. Do you remember what object we pass to the dataloaders?""
		}
	]}
/>

### 5. What is the best way to debug a CUDA error?
!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Utilities for pipelines

This page lists all the utility functions the library provides for pipelines.

Most of those are only useful if you are studying the code of the models in the library.


## Argument handling
Same concerns are rising in commercial and government ML licensing practices. In the words of [Bowe & Martin (2022)](https://www.gmu.edu/news/2022-04/no-10-implementing-responsible-ai-proposed-framework-data-licensing): ""_Babak Siavoshy, general counsel at Anduril Industries, asked what type of license terms should apply to an AI algorithm privately developed for computer-vision object detection and adapt it for military targeting or threat-evaluation? Neither commercial software licenses nor standard DFARS data rights clauses adequately answer this question as neither appropriately protects the developer's interest or enable the government to gain the insight into the system to deploy it responsibly_"".
!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# How to create a custom pipeline?

In this guide, we will see how to create a custom pipeline and share it on the [Hub](hf.co/models) or add it to the
ü§ó Transformers library.
A new wave of research in AI has called for a fundamental paradigm shift in how the field approaches ML datasets ([Paullada et al., 2020](https://arxiv.org/abs/2012.05345), [Denton et al., 2021](https://journals.sagepub.com/doi/full/10.1177/20539517211035955)). This includes defining fine-grained requirements for dataset creation from the start ([Hutchinson et al., 2021](https://dl.acm.org/doi/pdf/10.1145/3442188.3445918)), curating datasets in light of problematic content and bias concerns ([Yang et al., 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3375709), [Prabhu and Birhane, 2020](https://arxiv.org/abs/2006.16923)), and making explicit the values inherent in dataset construction and maintenance ([Scheuerman et al., 2021](https://dl.acm.org/doi/pdf/10.1145/3476058), [Birhane et al., 2021](https://arxiv.org/abs/2110.01963)).  Although there is general agreement that dataset development is a task that people from many different disciplines should be able to inform, in practice
```

The most likely outcome is that you'll see a bunch of errors. Don't worry, this is expected! Debugging ML models is
notoriously hard, and the key ingredient to success is patience (and `breakpoint()`). In our experience, the hardest
problems arise from subtle mismatches between ML frameworks, for which we have a few pointers at the end of this guide.
In other cases, a general test might not be directly applicable to your model, in which case we suggest an override
at the model test class level. Regardless of the issue, don't hesitate to ask for help in your draft pull request if
you're stuck.

When all tests pass, congratulations, your model is nearly ready to be added to the ü§ó Transformers library! üéâ

### 6.-7. Ensure everyone can use your model

**6. Submit the pull request**
## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]

Although the `pipeline` is great for most applications where you need to quickly generate predictions, sometimes you'll need to access the model's logits (say, if you have some custom post-processing that you'd like to apply). To see what can go wrong in this case, let's first grab the model and tokenizer from our `pipeline`:

```python
tokenizer = reader.tokenizer
model = reader.model
## Addressing Bias throughout the ML Development Cycle

Ready for some practical advice yet? Here we go ü§ó

There is no one single way to develop ML systems; which steps happen in what order depends on a number of factors including the development setting (university, large company, startup, grassroots organization, etc‚Ä¶), the modality (text, tabular data, images, etc‚Ä¶), and the preeminence or scarcity of publicly available ML resources. However, we can identify three common stages of particular interest in addressing bias. These are the task definition, the data curation, and the model training. Let‚Äôs have a look at how bias handling may differ across these various stages.

<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_pipeline.png"" alt=""The Bias ML Pipeline by Meg"" width=""500"" />
    <em>The Bias ML Pipeline by <a href=""https://huggingface.co/meg"">Meg</a></em>
</p>
Fetch‚Äôs ML pipeline is powered by several Amazon SageMaker features, particularly [Amazon SageMaker Model Training](https://aws.amazon.com/sagemaker/train/), which reduces the time and cost to train and tune ML models at scale, and [Amazon SageMaker Processing](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html), a simplified, managed experience to run data-processing workloads. The company runs its custom ML models using multi-GPU instances for fast performance. ‚ÄúThe GPU instances on Amazon SageMaker are simple to use,‚Äù says Ellen Light, backend engineer at Fetch. Fetch trains these models to identify and extract key information on receipts that the company can use to generate valuable insights and reward users. And on Amazon SageMaker, Fetch‚Äôs custom ML system is seamlessly scalable. ‚ÄúBy using Amazon SageMaker, we have a simple way to scale up our systems, especially for inference and runtime,‚Äù says Sam Corzine, ML engineer at Fetch. Meanwhile, standardized model
Keywords: Model interpretation, Visualization

## [mlrun](https://github.com/mlrun/mlrun)

MLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications, significantly reducing engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.

Keywords: MLOps

## [FederatedScope](https://github.com/alibaba/FederatedScope)
1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and
2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).

In the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.


## Addressing Bias throughout the ML Development Cycle

Ready for some practical advice yet? Here we go ü§ó
A pipeline is a quick and easy way to run a model for inference, requiring no more than four lines of code to generate an image:

```py
>>> from diffusers import DDPMPipeline

>>> ddpm = DDPMPipeline.from_pretrained(""google/ddpm-cat-256"", use_safetensors=True).to(""cuda"")
>>> image = ddpm(num_inference_steps=25).images[0]
>>> image
With a model performing well in offline testing and a small team of ML engineers, we were faced with a new challenge: seamless integration of that model into our production pipeline. The existing regex system processed more than 100 million transactions per month with a very bursty load, so it was crucial to have a high-availability system that could scale dynamically to load and maintain a low overall latency within the pipeline coupled with a system that was compute-optimized for the models we were serving. As a small startup at the time, we chose to buy rather than build the model serving solution. At the time, we didn‚Äôt have in-house model ops expertise and we needed to focus the energy of our ML engineers on enhancing the performance of the models within the product. With this in mind, we set out in search of the solution.
## **Open source licenses do not fit all**

Access, development and use of ML models is highly influenced by open source licensing schemes. For instance, ML developers might colloquially refer to ""open sourcing a model"" when they make its weights available by attaching an official open source license, or any other open software or content license such as Creative Commons. This begs the question: why do they do it? Are ML artifacts and source code really that similar? Do they share enough from a technical perspective that private governance mechanisms (e.g. open source licenses) designed for source code should also govern the development and use of ML models?

Here is the question: What is the best approach to debug an ML pipeline while ensuring compliance with licensing requirements for datasets?
Here is answer 1:Focus only on fixing errors in the pipeline without considering licensing
Here is answer 2: Ensure compliance with dataset licenses while ignoring pipeline errors
Here is answer3: Integrate debugging techniques, dataset licensing checks, and optimized workflows
Here is answer 4: Use predefined pipeline templates to avoid the need for debugging","Accordingly based On context provided I think correct Answer Is:

Answer3:Integrate debugging techniques ,dataset licensing checks,and optimized workflows"
Which combination of techniques ensures optimal time-series forecasting with PatchTST?,"[(517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (520, 'b101edb2-f02a-4bd8-b69e-0e3ba6a11f4d'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (12298, '75d1fbe7-d1d3-4868-b940-d25e740f5bdd'), (6209, 'dc759306-d7ce-4e78-b89d-4681bc9dd25d'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (6212, '9d2539d2-4c9d-4ea5-8dad-8105ad0d656e'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29771, '54c867d1-dd3a-4a9e-97f5-dac70011ef4b'), (29772, 'e9f0dc72-4849-4954-ab57-e595cbc0c79d'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (20570, '5dd4252e-a29c-4f30-ae23-7b7de46bb5a0'), (29818, '02cc33d8-0c79-466e-ba0c-bdadb265c65f'), (22651, 'ead7a63c-a6d5-446d-8807-99d33418b4ba'), (29820, '49d6a04e-36ec-4a38-adbf-50f525d423b1'), (29821, '84d9d326-f3f5-4a05-b9fc-f14b56bf8280'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (22657, '9268b17f-2fc5-4ae4-8106-8039de92a69b'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (17031, '3f233fb1-a6a0-4c1c-8465-4333aaaee216'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (9877, '02d00698-e099-4595-9b5b-915a102c8c31'), (20632, 'bc02c575-f28b-4824-b171-9d79cdbbccb2'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (4765, '289b3e37-708b-4282-af84-e4f1c3501749'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11467, '99d6488b-6726-4a42-866c-9569f978641d'), (1739, 'daa73787-386c-4e77-9c96-580d96b227fd'), (18126, '7975e5be-9bee-4c7f-a5d1-dbbcdbdbc120'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (13544, '1da8af21-fa53-45bb-b392-40172dafefba'), (749, 'aadabaa8-059f-4e53-bb4a-defc70cc04cc'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (28911, '179dfbeb-a989-4498-b4fb-6fac44b1aa4a'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (242, '2eca8361-c9f3-4bbe-8cd0-710465ac0d1e'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (765, '65b17d57-f43a-42e2-a6d4-b96ae347324c'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5379, '287c1ed6-130b-4784-8132-3f5287c3471c'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (17157, 'b9540860-ae17-4e26-b1f5-6ea2ad1ca46e'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5396, '18d8ed78-6549-4720-9c12-5ae48fb89fc8'), (14105, '30d56fe4-77e8-4044-a195-afe51b4ed84a'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (805, '45825ed8-8f85-40ed-a174-19eeebd97cf7'), (807, 'ce4fda9f-c07a-4ca5-8538-9f8ededb01e2'), (808, '2232e5cb-cb93-4ea7-8c40-d51528355f6c'), (5419, 'ca377909-1f4d-48de-af71-01684b43fc59'), (812, 'ecabe7c7-05c4-4cfa-9e6e-c24df8d129b4'), (814, '3c86feb6-9865-4995-82a9-c9e1bafa680f'), (815, '00347210-f522-4fa2-a819-98958034885b'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (9062, '51850a4f-1794-457b-acdc-6dda09458247'), (10101, '93f0483d-8fce-49ba-8617-9adcebcc9265'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (29586, '67cb4809-8361-4ee5-9dd4-cb31eef9b1bf'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (18356, '31d586a8-3709-4998-8ff5-6d7466228a8f'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (23993, '1ab46630-fc89-4db9-8991-e991994175e0'), (5053, '276ed589-b825-4d36-bdc1-a6568aa8af45'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PatchTSMixer

## Overview

The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
*TSMixer is a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules designed for multivariate forecasting and representation learning on patched time series. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components,
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research ‡§∏‡•á) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ‡§∏‡•á) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (Êù•Ëá™ Google AI) ‰º¥ÈöèËÆ∫Êñá [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) Áî± Matthias Minderer, Alexey Gritsenko, Neil Houlsby ÂèëÂ∏É„ÄÇ
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (Êù•Ëá™  IBM Research) ‰º¥ÈöèËÆ∫Êñá [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) Áî± Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam ÂèëÂ∏É„ÄÇ
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (Êù•Ëá™ IBM) ‰º¥ÈöèËÆ∫Êñá [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) Áî± Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam ÂèëÂ∏É„ÄÇ
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
At a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:

![model](https://github.com/namctin/transformers/assets/8100/150af169-29de-419a-8d98-eb78251c21fa)

The abstract from the paper is the following:
## Introduction

Time series forecasting is an essential scientific and business problem and as such has also seen a lot of innovation recently with the use of [deep learning based](https://dl.acm.org/doi/abs/10.1145/3533382) models in addition to the [classical methods](https://otexts.com/fpp3/). An important difference between classical methods like ARIMA and novel deep learning methods is the following.

##  Probabilistic Forecasting

Typically, classical methods are fitted on each time series in a dataset individually. These are often referred to as  ""single"" or ""local"" methods. However, when dealing with a large amount of time series for some applications, it is beneficial to train a ""global"" model on all available time series, which enables the model to learn latent representations from many different sources.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM „Åã„Çâ) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google „Åã„Çâ) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google „Åã„Çâ) Jason Phang, Yao Zhao, and Peter J. Liu „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.Ïùò [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google ÏóêÏÑú) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu Ïùò [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google ÏóêÏÑú) Jason Phang, Yao Zhao, Peter J. Liu Ïùò [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (Google AI „Åã„Çâ) Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230)
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (Google AI „Åã„Çâ) Matthias Minderer, Alexey Gritsenko, Neil Houlsby. „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683)
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research „Åã„Çâ) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf)
1. **[OWL-ViT](https://huggingface.co/docs/transformers/model_doc/owlvit)** (Google AI ÏóêÏÑú) Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran Shen, Xiao Wang, Xiaohua Zhai, Thomas Kipf, and Neil Houlsby Ïùò [Simple Open-Vocabulary Object Detection with Vision Transformers](https://arxiv.org/abs/2205.06230) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (Google AI ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Matthias Minderer, Alexey Gritsenko, Neil Houlsby.Ïùò [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683)ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.Ïùò [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf)ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
- Compared to ViT, DeiT models use a so-called distillation token to effectively learn from a teacher (which, in the
  DeiT paper, is a ResNet like-model). The distillation token is learned through backpropagation, by interacting with
  the class ([CLS]) and patch tokens through the self-attention layers.
- There are 2 ways to fine-tune distilled models, either (1) in a classic way, by only placing a prediction head on top
  of the final hidden state of the class token and not using the distillation signal, or (2) by placing both a
  prediction head on top of the class token and on top of the distillation token. In that case, the [CLS] prediction
  head is trained using regular cross-entropy between the prediction of the head and the ground-truth label, while the
  distillation prediction head is trained using hard distillation (cross-entropy between the prediction of the
  distillation head and the label predicted by the teacher). At inference time, one takes the average prediction
```

## Usage tips

The model can also be used for time series classification and time series regression. See the respective [`PatchTSMixerForTimeSeriesClassification`] and [`PatchTSMixerForRegression`] classes.

## PatchTSMixerConfig

[[autodoc]] PatchTSMixerConfig


## PatchTSMixerModel

[[autodoc]] PatchTSMixerModel
    - forward


## PatchTSMixerForPrediction

[[autodoc]] PatchTSMixerForPrediction
    - forward


## PatchTSMixerForTimeSeriesClassification

[[autodoc]] PatchTSMixerForTimeSeriesClassification
    - forward


## PatchTSMixerForPretraining

[[autodoc]] PatchTSMixerForPretraining
    - forward


## PatchTSMixerForRegression

[[autodoc]] PatchTSMixerForRegression
    - forward
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Informer

## Overview

The Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting ](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.

This method introduces a Probabilistic Attention mechanism to select the ""active"" queries rather than the ""lazy"" queries and provides a sparse Transformer thus mitigating the quadratic compute and memory requirements of vanilla attention.

The abstract from the paper is the following:
!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PatchTST

## Overview

The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.

Here is the question: Which combination of techniques ensures optimal time-series forecasting with PatchTST?
Here is answer 1:Fine-tune PatchTST without preprocessing data
Here is answer 2: Use default Transformer workflows for forecasting
Here is answer3: Prepare time-series data, fine-tune PatchTST, and leverage its patching mechanism
Here is answer 4: Skip fine-tuning and rely on pretrained models","Accordingly my understanding based purely off textual information provided here(IN THIS CASE), I will go ahead & respond : 

Answer : Here Is Answer3 : Prepare_time_series_data,fine_tune_Patch_T_ST & leverage_its_patching_mechanism"
What is the most effective way to integrate datasets into Transformer workflows while maintaining ethical standards?,"[(4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (19481, 'cf242eeb-0067-4382-864e-8b376d0da920'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (9762, '06dd74a0-6842-41d5-b4c7-e706b0ae0c23'), (29224, 'a1b49036-83a8-4545-96ac-718a57c76b4b'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (24636, '34c928bf-700f-4de6-b158-b590ff679457'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (1601, '06cbfead-420b-465d-a07c-566ed263f552'), (13892, '18f9d2dd-b24d-40df-bf4b-70633772beaa'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (26202, 'a3e63e78-75c5-4e73-a5b3-110285882551'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (26207, 'dd7cd3b1-658f-4b98-9830-c5742411682f'), (26212, '8d5de3f2-1ac9-40c8-b7e9-607000e02cd5'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (12912, '426a098c-d467-4adb-a3a9-bb78cb34af6b'), (6774, '9e32e928-42a2-4f8a-8751-39ce7e917839'), (1183, '83219e89-2f3e-4e38-8e76-34ba4626dc2d'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (28846, 'a95944ca-cb3f-47e9-a1ca-670e4f9f9fea'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (17088, '17e59b2c-7682-45e9-abb8-a88720517b2c'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (19187, '3903562f-59a1-4286-92d6-9666ba3e9bd1'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (13064, '589f6f88-a4f1-45d4-863a-1d5eee88fa12'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28424, '118d80b7-d822-4e36-87a3-b04479f5676e'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (781, '12fec9ca-f24a-44df-9ea5-9cd201ac15a5'), (16142, 'c771e38b-cfa7-4d59-96c0-b3151c87a39d'), (5401, 'fabf08d1-3eb0-4c54-abc0-8267df388968'), (10015, '39ee57c9-083a-4eae-9edd-a5123097c744'), (10016, 'd9fba1b8-db8b-428f-a932-69993e26d4f7'), (22304, '3897446b-d474-4a3d-9db5-5b91924cce5e'), (15142, '10c62a15-e400-4c35-93a8-017a7ab8b3d6'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (17720, '944ded38-82f0-4442-8aed-7cb4d8402fe8'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (7534, '4354b918-223f-4fd0-92ea-e59bd0b20f6b'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (22434, '8bea8922-5c12-40fa-87bf-aa092348b13f'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (28601, 'eb9de433-f226-428a-9cca-06ee8c6b08ce'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (22476, '6a8f51fd-9677-457f-8034-63e0566b51c3'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (1499, '1faf2c53-b46d-4aec-a233-dd712314dc97'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (4065, '40ca2d84-7561-4754-bbb3-490ed78468fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Transformers have proven to be extremely efficient for a wide range of functions, including feature extraction, text generation, sentiment analysis, translation and many more. Models like BERT are widely used by Graphcore customers in a huge array of applications including cybersecurity, voice call automation, drug discovery, and translation.

Optimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. In providing an open-source library of transformer models, Hugging Face has directly addressed these issues. Integrating IPUs with HuggingFace also allows developers to leverage not just the models, but also datasets available in the HuggingFace Hub.
Given a dataset, most dataloaders follow the following recipe:

1. Fetch data (e.g. where your JPEG images are stored on disk)
2. The CPU reads encoded images
3. The CPU decodes images
4. The CPU applies image transformations to augment images
5. Finally, images are sent to devices (although this is usually not done by the dataloader itself)

Instead of doing the whole process on CPU and send ready-to-train data to devices, a more efficient workflow would be to send encoded images to devices first and then perform image decoding and augmentations:

1. Same as before
2. Same as before
3. Encoded images are sent to devices
4. Devices decode images
5. Devices apply image transformations to augment images
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
As a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.

In addition, the Hugging Face Hub offers:

-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)
-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)
-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)
-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)
-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)


### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.
In [Chapter 7](/course/chapter7), we'll put all of this to good use as we take a deep dive into the core NLP tasks that Transformer models are great for. Before jumping ahead, though, put your knowledge of ü§ó Datasets to the test with a quick quiz!
```

While most datasets on the hub are ready to use out of the box, sometimes we wish to perform some additional processing or modification of the dataset. In this case [we wish to match the author's implementation](https://github.com/kzl/decision-transformer), that is we need to:

- Normalize each feature by subtracting the mean and dividing by the standard deviation.
- Pre-compute discounted returns for each trajectory.
- Scale the rewards and returns by a factor of 1000.
- Augment the dataset sampling distribution so it takes into account the length of the expert agent‚Äôs trajectories.

In order to perform this dataset preprocessing, we will use a custom ü§ó [Data Collator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator). 

Now let‚Äôs get started on the Custom Data Collator for Offline Reinforcement Learning.
Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune‚Äôs integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.

And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!
There's no denying that Machine Learning-powered applications are reaching into every corner of IT. But what does that mean for companies and organizations? How do we build rock-solid Machine Learning workflows? Should we all hire 100 Data Scientists¬†? Or 100 DevOps engineers?

> ""Transformers have emerged as a general purpose architecture for ML. Not just for Natural Language Processing, but also Speech, Computer Vision or even protein structure prediction.""

Old timers have learned the hard way that there is [no silver bullet](https://en.wikipedia.org/wiki/No_Silver_Bullet) in IT. Yet, the [Transformer](https://arxiv.org/abs/1706.03762) architecture is indeed very efficient on a wide variety of Machine Learning tasks. But how can we all keep up with the frantic pace of innovation in Machine Learning? Do we really need expert skills to leverage these state of the art models? Or is there a shorter path to creating business value in less time?

Well, here's what I think.
#### Philosophy #4: You shouldn‚Äôt have to write your own data pipeline, especially for common tasks

In addition to `transformers`, a huge open repository of pre-trained models, there is also ü§ó `datasets`, a huge open repository of datasets - text, vision, audio and more. These datasets convert easily to TensorFlow Tensors and Numpy arrays, making it easy to use them as training data. Here‚Äôs a quick example showing us tokenizing a dataset and converting it to Numpy. As always, make sure your tokenizer matches the model you want to train with, or things will get very weird!

```py
from datasets import load_dataset
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

dataset = load_dataset(""glue"", ""cola"")  # Simple text classification dataset
dataset = dataset[""train""]  # Just take the training split for now
Today, [Habana¬Æ Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they‚Äôre joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana‚Äôs [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost.
# Examples

This folder contains actively maintained examples of the use of ü§ó Transformers organized into different ML tasks. All examples in this folder are **TensorFlow** examples and are written using native Keras rather than classes like `TFTrainer`, which we now consider deprecated. If you've previously only used ü§ó Transformers via `TFTrainer`, we highly recommend taking a look at the new style - we think it's a big improvement!

In addition, all scripts here now support the [ü§ó Datasets](https://github.com/huggingface/datasets) library - you can grab entire datasets just by changing one command-line argument!

## A note on code folding

Most of these examples have been formatted with #region blocks. In IDEs such as PyCharm and VSCode, these blocks mark
named regions of code that can be folded for easier viewing. If you find any of these scripts overwhelming or difficult
to follow, we highly recommend beginning with all regions folded and then examining regions one at a time!
to analyze [ML datasets](https://huggingface.co/spaces/huggingface/data-measurements-tool) and [models](https://huggingface.co/spaces/society-ethics/StableBias). We believe this helps everyone interested to better understand [the limitations of ML systems](https://huggingface.co/blog/ethics-soc-2) and how they can safely be leveraged to best serve users and those affected by these systems. These approaches have already proven their utility in promoting accountability, especially in the larger multidisciplinary research endeavors we‚Äôve helped organize, including [BigScience](https://huggingface.co/bigscience) (see our blog series [on the social stakes of the project](https://montrealethics.ai/category/columns/social-context-in-llm-research/)), and the more recent [BigCode project](https://huggingface.co/bigcode) (whose governance is [described in more details here](https://huggingface.co/datasets/bigcode/governance-card)).
```

Finally, combine everything in two functions that you'll use to transform training and validation data. The two functions 
are similar except data augmentation is applied only to the training data.  

```python
from PIL import Image


def train_transforms(example_batch):
    images = [jitter(handle_grayscale_image(x)) for x in example_batch[""image""]]
    labels = [x for x in example_batch[""annotation""]]
    inputs = image_processor(images, labels)
    return inputs


def val_transforms(example_batch):
    images = [handle_grayscale_image(x) for x in example_batch[""image""]]
    labels = [x for x in example_batch[""annotation""]]
    inputs = image_processor(images, labels)
    return inputs
```

To apply the preprocessing functions over the entire dataset, use the ü§ó Datasets `set_transform` function:

```python 
train_ds.set_transform(train_transforms)
test_ds.set_transform(val_transforms)
-   get insights into open-source best practices
-   understand the design principles of one of the most popular NLP
    libraries
-   learn how to do efficiently test large NLP models
-   learn how to integrate Python utilities like `black`, `ruff`,
    `make fix-copies` into a library to always ensure clean and readable
    code

To start, let's try to get a general overview of the Transformers
library.

General overview of ü§ó Transformers
----------------------------------

First, you should get a general overview of ü§ó Transformers. Transformers 
is a very opinionated library, so there is a chance that
you don't agree with some of the library's philosophies or design
choices. From our experience, however, we found that the fundamental
design choices and philosophies of the library are crucial to
efficiently scale Transformers while keeping maintenance costs at a
reasonable level.
he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
```

The same sequence of workflow can be applied to the validation and evaluation sets: 

```py 
>>> val_transform = Compose(
...     [
...         ApplyTransformToKey(
...             key=""video"",
...             transform=Compose(
...                 [
...                     UniformTemporalSubsample(num_frames_to_sample),
...                     Lambda(lambda x: x / 255.0),
...                     Normalize(mean, std),
...                     Resize(resize_to),
...                 ]
...             ),
...         ),
...     ]
... )

>>> val_dataset = pytorchvideo.data.Ucf101(
...     data_path=os.path.join(dataset_root_path, ""val""),
...     clip_sampler=pytorchvideo.data.make_clip_sampler(""uniform"", clip_duration),
...     decode_audio=False,
...     transform=val_transform,
... )
## Scope

The Diffusers community will apply the following ethical guidelines to the project‚Äôs development and help coordinate how the community will integrate the contributions, especially concerning sensitive topics related to ethical concerns.


## Ethical guidelines

The following ethical guidelines apply generally, but we will primarily implement them when dealing with ethically sensitive issues while making a technical choice. Furthermore, we commit to adapting those ethical principles over time following emerging harms related to the state of the art of the technology in question.

- **Transparency**: we are committed to being transparent in managing PRs, explaining our choices to users, and making technical decisions.

- **Consistency**: we are committed to guaranteeing our users the same level of attention in project management, keeping it technically stable and consistent.
#### Suraj Patil & Patrick von Platen, Machine Learning Engineers at Hugging Face
- Talk: How to use JAX/Flax with Transformers
- Abstract: Transformers is one of the most popular open-source ML libraries and supports PyTorch, Tensorflow, and JAX/Flax. In this talk, we will explain how JAX/Flax models should be used in Transformers and compare their design in Transformers with the design of PyTorch models in Transformers. In the second part, we will give you a hands-on presentation of how a model can be trained end-to-end with the official JAX/Flax example scripts using Transformers & Datasets. Along the way, we want to give you some tips and tricks on how to best realize your project.
- Speaker info: Suraj and Patrick are part of Hugging Face‚Äôs open source team and lead the integration of JAX/Flax into Transformers.
- GitHub: https://github.com/patil-suraj & https://github.com/patrickvonplaten
```

ü§ó Transformers doesn't have a data collator for ASR, so you'll need to adapt the [`DataCollatorWithPadding`] to create a batch of examples. It'll also dynamically pad your text and labels to the length of the longest element in its batch (instead of the entire dataset) so they are a uniform length. While it is possible to pad your text in the `tokenizer` function by setting `padding=True`, dynamic padding is more efficient.

Unlike other data collators, this specific data collator needs to apply a different padding method to `input_values` and `labels`:

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = ""longest""
So that really became my shift into ethical AI work.

### In what applications is data ethics most important?

**Meg:** Human-centric technology that deals with people and identity (face recognition, pedestrian recognition). In NLP this would pertain more to the privacy of individuals, how individuals are talked about, and the biases models pick up with regards to descriptors used for people.

### How can ML teams be more aware of harmful bias?

**Meg:** A primary issue is that these concepts haven't been taught and most teams simply aren‚Äôt aware. Another problem is the lack of a lexicon to contextualize and communicate what is going on.

For example:
- This is what marginalization is
- This is what a power differential is
- Here is what inclusion is
- Here is how stereotypes work

Having a better understanding of these pillars is really important.

Here is the question: What is the most effective way to integrate datasets into Transformer workflows while maintaining ethical standards?
Here is answer 1:Use public datasets without considering ethical implications
Here is answer 2: Load datasets efficiently but skip fine-tuning
Here is answer3: Follow legal guidelines, debug datasets, and structure them for Transformers
Here is answer 4: Focus on dataset preprocessing only",The correct answer choice depends entirely upon understanding both short-term goals along-side certain philosophical leanings throughout sectors included within traditional areas approach where corporate obligations may vary depending upon available operational conditions relative expense measurements weighing profit margins authority consolidations compliance petitions tend towards market optimum utilities rather than ensuring quality control implementation processeslep≈°√≠ values fashion third-party vendors based hazardous situation cultural sensitivity religious impartiality diverse workforce agendas concentrated institutional interactions associated human experiences sustaining growth enforcement realistic results loss aggregated methods meticulous trouv√© jurisdictional regulatory hinderances supplanted accountable vulnerabilities
How can time-series datasets be prepared for use in Transformer-based models like PatchTST?,"[(517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (18438, '46782e4c-a0e6-4646-81b0-50890671322d'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (5135, '8bb2ea1c-369d-4db0-ab7a-127366db9831'), (5136, 'd9840db6-0933-4364-81bb-b7ab396b1651'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29776, 'e3203540-4467-4975-8a4a-3a14658f4f60'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (29785, '0648b88f-f28f-478b-a1f2-87949481cff2'), (29786, '945fd8e4-77cb-409e-af9d-4d7171e7ea93'), (29791, 'daadbbdc-5083-4b37-be0d-583734c74458'), (29792, '68aa8771-8ebc-47c4-96a6-3071c24003d1'), (29795, 'e8acfe88-7634-4f4d-a180-dbea58d5a556'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (29828, '4a689bdc-aef9-4849-9b09-6a5b97979b6c'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (17029, '48fc79d9-063a-4c64-b9a0-98a6773c1208'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11463, '20088ef2-37e8-4def-98fa-8c9ab021a5f1'), (11465, '2352d31d-4cb6-43cc-a668-77747f9ff8db'), (11467, '99d6488b-6726-4a42-866c-9569f978641d'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (23784, 'ed5ad04f-6d3c-413f-a4b9-4419c92834de'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (4845, '940b1611-e095-4f8e-8f1c-183c7c7752cd'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (764, '803a240a-32a6-4cfb-885a-8813241a5488'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5380, 'c3402ba2-db82-486f-b86b-a5f292b965d1'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (779, 'c84b16d0-6361-44a8-b2c9-cc03a7a33748'), (780, '6344f2d1-6815-49e5-bc6e-11d86a1bb2cf'), (781, '12fec9ca-f24a-44df-9ea5-9cd201ac15a5'), (782, '6279502b-8279-4967-bca2-1610f71071a5'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (5401, 'fabf08d1-3eb0-4c54-abc0-8267df388968'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (1817, '1c818452-3fbe-49d9-a499-ac6ab04ee510'), (18206, '8db1e991-a927-49a1-bf19-bfdc54622518'), (5422, 'c12a80a5-4e72-4a35-b1ea-7113b01b5c00'), (5423, 'ab3c7b43-3e08-4256-bd61-500e538c8f31'), (5426, '5052dd1e-66fa-4818-9c3d-a9efed8343c7'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (13623, '61a8c7a1-9ad3-4d9a-aa84-06e57daeda3a'), (17720, '944ded38-82f0-4442-8aed-7cb4d8402fe8'), (21825, '9da38f4b-3f9a-466a-b4b8-577288ffae78'), (322, 'f79309af-2846-48e5-a187-020a8da015fe'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15696, 'f5c82a25-56d1-4160-a926-0d697826a6bb'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (9144, '3be21646-2d02-41ae-b3a2-d0cf74a4654a'), (9145, 'a9a8d501-4863-4964-b90e-1c9b79a97a29'), (10185, '7824c364-4a2f-4032-a5e8-10723b216cbe'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example. 

The current model assumes the presence of a date-time together with the time series values, which might not be the case for every time series in the wild. See for instance neuroscience datasets like the one from [WOODS](https://woods-benchmarks.github.io/). Thus, one would need to generalize the current model to make some inputs optional in the whole pipeline.

Finally, the NLP/Vision domain has benefitted tremendously from [large pre-trained models](https://arxiv.org/abs/1810.04805), while this is not the case as far as we are aware for the time series domain. Transformer based models seem like the obvious choice in pursuing this avenue of research and we cannot wait to see what researchers and practitioners come up with!
- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer without any head on top, and [`TimeSeriesTransformerForPrediction`]
adds a distribution head on top of the former, which can be used for time-series forecasting. Note that this is a so-called probabilistic forecasting model, not a
point forecasting model. This means that the model learns a distribution, from which one can sample. The model doesn't directly output values.
- [`TimeSeriesTransformerForPrediction`] consists of 2 blocks: an encoder, which takes a `context_length` of time series values as input (called `past_values`),
and a decoder, which predicts a `prediction_length` of time series values into the future (called `future_values`). During training, one needs to provide
pairs of (`past_values` and `future_values`) to the model.
- In addition to the raw (`past_values` and `future_values`), one typically provides additional features to the model. These can be the following:
As time series researchers will know, there has been a lot of interest in applying Transformer based models to the time series problem. The vanilla Transformer is just one of many attention-based models and so there is a need to add more models to the library.

At the moment nothing is stopping us from modeling multivariate time series, however for that one would need to instantiate the model with a multivariate distribution head. Currently, diagonal independent distributions are supported, and other multivariate distributions will be added. Stay tuned for a future blog post that will include a tutorial.

Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example.
Secondly, a Transformer helps us to train on time series data which might contain thousands of time points. It might not be feasible to input *all* the history of a time series at once to the model, due to the time- and memory constraints of the attention mechanism. Thus, one can consider some appropriate context window and sample this window and the subsequent prediction length sized window from the training data when constructing batches for stochastic gradient descent (SGD). The context sized window can be passed to the encoder and the prediction window to a *causal-masked* decoder. This means that the decoder can only look at previous time steps when learning the next value. This is equivalent to how one would train a vanilla Transformer for machine translation, referred to as ""teacher forcing"".
At a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:

![model](https://github.com/namctin/transformers/assets/8100/150af169-29de-419a-8d98-eb78251c21fa)

The abstract from the paper is the following:
The ü§ó Transformers library comes with a vanilla probabilistic time series Transformer model, simply called the [Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer). In the sections below, we'll show how to train such a model on a custom dataset.


## Set-up Environment

First, let's install the necessary libraries: ü§ó Transformers, ü§ó Datasets, ü§ó Evaluate,  ü§ó Accelerate and [GluonTS](https://github.com/awslabs/gluonts).

As we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.


```python
!pip install -q transformers

!pip install -q datasets

!pip install -q evaluate

!pip install -q accelerate

!pip install -q gluonts ujson
To summarize, Transformers are definitely far from being outdated when it comes to time-series forcasting! 
Yet the availability of large-scale datasets is crucial for maximizing their potential. 
Unlike in CV and NLP, the field of time series lacks publicly accessible large-scale datasets. 
Most existing pre-trained models for time series are trained on small sample sizes from archives like [UCR and UEA](https://www.timeseriesclassification.com/), 
which contain only a few thousands or even hundreds of samples. 
Although these benchmark datasets have been instrumental in the progress of the time series community, 
their limited sample sizes and lack of generality pose challenges for pre-training deep learning models.
It is common in the probabilistic setting to learn the future parameters of some chosen parametric distribution, like Gaussian or Student-T; or learn the conditional quantile function; or use the framework of Conformal Prediction adapted to the time series setting. The choice of method does not affect the modeling aspect and thus can be typically thought of as yet another hyperparameter. One can always turn a probabilistic model into a point-forecasting model, by taking empirical means or medians.

## The Time Series Transformer

In terms of modeling time series data which are sequential in nature, as one can imagine, researchers have come up with models which use Recurrent Neural Networks (RNN) like LSTM or GRU, or Convolutional Networks (CNN), and more recently Transformer based methods which fit naturally to the time series forecasting setting.
```
    
By reducing the input of each layer by two, we get a memory usage of \\(O(N\cdot T \log T)\\) instead of \\(O(N\cdot T^2)\\) where \\(N\\) is the number of encoder/decoder layers. This is what we wanted!
    
The Informer model in [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in the ü§ó Transformers library, and simply called `InformerModel`. In the sections below, we will show how to train this model on a custom multivariate time-series dataset.


## Set-up Environment

First, let's install the necessary libraries: ü§ó Transformers, ü§ó Datasets, ü§ó Evaluate, ü§ó Accelerate and [GluonTS](https://github.com/awslabs/gluonts).

As we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.


```python
!pip install -q transformers datasets evaluate accelerate gluonts ujson
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
##  Multivariate Probabilistic Time Series Forecasting

As far as the modeling aspect of probabilistic forecasting is concerned, the Transformer/Informer will require no change when dealing with multivariate time series. In both the univariate and multivariate setting, the model will receive a sequence of vectors and thus the only change is on the output or emission side.

Modeling the full joint conditional distribution of high dimensional data can get computationally expensive and thus methods resort to some approximation of the distribution, the easiest being to model the data as an independent distribution from the same family, or some low-rank approximation to the full covariance, etc. Here we will just resort to the independent (or diagonal) emissions which are supported for the families of distributions we have implemented [here](https://huggingface.co/docs/transformers/main/en/internal/time_series_utils).

## Informer - Under The Hood
*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling
An example here is the image representation of the product for which you have the time-series values (like the [ResNet](resnet) embedding of a ""shoe"" picture,
    if your time-series is about the sales of shoes).
    Note that these features need to be known for ALL data points (also those in the future).
- The model is trained using ""teacher-forcing"", similar to how a Transformer is trained for machine translation. This means that, during training, one shifts the
`future_values` one position to the right as input to the decoder, prepended by the last value of `past_values`. At each time step, the model needs to predict the
next target. So the set-up of training is similar to a GPT model for language, except that there's no notion of `decoder_start_token_id` (we just use the last value
of the context as initial input for the decoder).
*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic
```

## Usage tips

The model can also be used for time series classification and time series regression. See the respective [`PatchTSMixerForTimeSeriesClassification`] and [`PatchTSMixerForRegression`] classes.

## PatchTSMixerConfig

[[autodoc]] PatchTSMixerConfig


## PatchTSMixerModel

[[autodoc]] PatchTSMixerModel
    - forward


## PatchTSMixerForPrediction

[[autodoc]] PatchTSMixerForPrediction
    - forward


## PatchTSMixerForTimeSeriesClassification

[[autodoc]] PatchTSMixerForTimeSeriesClassification
    - forward


## PatchTSMixerForPretraining

[[autodoc]] PatchTSMixerForPretraining
    - forward


## PatchTSMixerForRegression

[[autodoc]] PatchTSMixerForRegression
    - forward
- In addition to the raw (`past_values` and `future_values`), one typically provides additional features to the model. These can be the following:
    - `past_time_features`: temporal features which the model will add to `past_values`. These serve as ""positional encodings"" for the Transformer encoder.
    Examples are ""day of the month"", ""month of the year"", etc. as scalar values (and then stacked together as a vector).
    e.g. if a given time-series value was obtained on the 11th of August, then one could have [11, 8] as time feature vector (11 being ""day of the month"", 8 being ""month of the year"").
    - `future_time_features`: temporal features which the model will add to `future_values`. These serve as ""positional encodings"" for the Transformer decoder.
    Examples are ""day of the month"", ""month of the year"", etc. as scalar values (and then stacked together as a vector).
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research ‡§∏‡•á) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ‡§∏‡•á) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PatchTSMixer

## Overview

The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.

Here is the question: How can time-series datasets be prepared for use in Transformer-based models like PatchTST?
Here is answer 1:Use raw data without transformations
Here is answer 2: Segment time-series into patches and preprocess data
Here is answer3: Focus only on model hyperparameter tuning
Here is answer 4: Skip segmentation for simplicity",Answer : Here segmenta-time-serieesinto-patches-and-preprocess-data
Which approach best resolves dataset-related errors in an ML pipeline?,"[(17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (30730, 'e011abf6-7dd0-49d9-bdb0-7971bf51aba6'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (29201, 'cdbabce2-0910-47ac-93ec-2dc35c8b6b91'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (30746, '63d7f2f1-c139-46db-85bb-2cb86c04e01c'), (13339, 'f046b9a1-daad-440b-8fa6-1a8de946d278'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (3611, '5acb0ab4-672d-47da-9ea1-6cdc6cb1d8db'), (9242, '2ba611df-f05b-4369-a8f4-78c83dbc97ee'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (13344, '8351ae83-b363-4005-b8b5-26db3adae749'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (8239, '601c3e59-2039-4a07-a2ff-f1e97ecadabd'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (7735, '73b16f91-3f71-40bd-9699-6896aa1dd3b5'), (18488, 'd75be24a-7b2f-45c2-b82a-35db1d9e237d'), (7229, '2f99d43f-c4bc-428b-bf1d-34c40438e64f'), (9282, '91edce5b-dc46-47e2-85e4-33cf1d17dab7'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (20071, '163a9ba3-cb9e-4a4f-bdd6-07ceaa145535'), (20075, '29516309-c7f6-48f9-9710-db5e67efea2f'), (19053, 'ae1e94a1-3807-46ed-85c6-ccce72cc33aa'), (1150, 'b91271f2-c6c3-489b-b0cd-5c23b6b7df5e'), (1159, 'ed1c0f2d-3594-4151-ba80-832c5099d69b'), (19079, 'f3bedfe8-7295-452a-b97a-76a688901be8'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (20109, 'c1cf2a53-efd5-4487-a825-14bd70fd3cf8'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (6317, '56ffe729-9ef7-4575-b631-fdfdf483ee5f'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (7351, '974ac7b4-c4c2-4761-8654-0816e4e2c955'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (16576, '02456d8c-7662-44df-894b-be28d7834936'), (16577, '65b23733-c969-4f5c-815e-b4d8c66dc8a9'), (16582, 'a5a53226-6dca-425c-afa6-94c7b4adf300'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (6859, '93632aec-a5b2-4214-a02d-264a1c961147'), (21205, '6bda238b-e50a-4ace-95e5-877b1ae72029'), (7387, '49e59733-bddf-4e92-be0e-5013437fa51b'), (7397, 'e1ff602a-b5f3-4ecd-be3a-6e21ca34b728'), (7398, '383d361a-826c-44cb-a553-5def932b4a1a'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (12022, 'ca590bdd-6993-45bf-bc5c-6216ff9e670d'), (7936, 'a5dbf1a5-4346-46c1-ac10-c332139056bc'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (3339, '6c368a7d-8543-44b3-9993-4674ebdec341'), (3342, '40de6e0e-cd3f-4ac4-8b93-e0a427a98e4d'), (16146, 'e100c4ec-db20-4e52-93d6-f9535f7f90b8'), (17175, 'a0d830f7-cded-4564-acf5-276d18511164'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (20252, 'b9e133f5-90ab-47a2-9af2-55d084a98c15'), (16154, '98ae0966-a859-47ea-bf2c-582ee47bd1c5'), (14110, '0460530a-535c-4c70-a78d-72af3548e3d5'), (20255, 'e4f72617-0d68-4257-a017-b9389b18e11b'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (13091, 'fd87ced8-d15d-4eb8-98f3-447a6c638420'), (16166, '20aef5a8-f234-46db-baab-75c60f505a23'), (21799, 'eef8b3e0-a59f-4579-9fbe-b2fff100f9c0'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (11575, '28589867-4683-4d14-8fbc-3442efb0da38'), (20287, 'b9745a11-37a1-41f2-8092-7aad294368b2'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (20290, 'c24ca7c6-c884-4476-add4-545fba5cd666'), (4427, '3a4777b5-58f7-4c71-994f-d317fee8f1db'), (20304, '17c77f39-33fb-44d1-9c3d-ba8f6556e075'), (28505, 'dc37ccb3-6fee-474b-a35b-6a331cfa196c'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (18276, '3c255798-f281-4027-9f8e-004acb7710ef'), (28517, '83d1dded-982b-449c-9af4-e5fecc3aac5b'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (18818, 'a938f2f4-0d19-4f1e-bd3e-858ce23a0d1d'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (26012, '030266f1-f990-44e8-9ff4-7b377a358502'), (26014, '960c2e57-dc37-4bae-9c70-0448415e6410'), (12192, 'e8178d2a-59a2-4f9a-9381-891b9fe71553'), (26018, 'f342cf73-c386-4093-99cd-e639ba2efb08'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (26028, 'f8cc1383-70d9-453f-a623-628d8cbafd47'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (21948, 'e1dd1005-94e8-4562-b9c3-6ba611211942'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (31170, '694c9ec9-5c6b-4ea4-92ba-b75bc909f395'), (11716, '5aa7f3a8-b022-4936-88ea-c22c3c501a94'), (23503, '3b5d2d8b-76f5-4f37-957a-813df4470815'), (25049, 'cf8bc526-5eeb-4029-aba2-8ec814ad0167'), (15330, '9f43616d-6dfb-4651-ae47-169eef44cbd0'), (30179, '2ce3afe5-1f88-459a-8f8e-f954df770550'), (16871, 'e66a5e45-2cf0-4d15-bcfc-0485a92e992f'), (13802, '54b46bfe-ed8a-4198-9b58-bf91ffa494d0'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (24560, 'ae4f8022-24f0-469e-9cb5-358e58b5773d'), (29689, 'deb041f0-74a4-4cfd-8792-9dd1e899718d')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: #### **3. What‚Äôs a common mistake you see people make when trying to integrate ML into Engineering?**
Using an overpowered ML technique on a small problem dataset is one common mistake I see people making in integrating ML into Engineering.  Deep Learning, for example, is moving AI and ML to heights unimagined in such a short period, but it may not be one‚Äôs best method for solving a problem, depending on your problem space.  Often more straightforward methods work just as well or better when working with small training datasets on limited hardware.
The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(""glue"", ""mnli"")

model_checkpoint = ""distilbert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples[""premise""], examples[""hypothesis""], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""L-WSwUWde1U""/>

The problem when you encounter an error in `trainer.train()` is that it could come from multiple sources, as the `Trainer` usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""N9kO52itd0Q""/>

The problem when you encounter an error in `model.fit()` is that it could come from multiple sources, as training usually brings together a lot of things that you've been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `model.fit()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):
## Addressing Bias throughout the ML Development Cycle

Ready for some practical advice yet? Here we go ü§ó

There is no one single way to develop ML systems; which steps happen in what order depends on a number of factors including the development setting (university, large company, startup, grassroots organization, etc‚Ä¶), the modality (text, tabular data, images, etc‚Ä¶), and the preeminence or scarcity of publicly available ML resources. However, we can identify three common stages of particular interest in addressing bias. These are the task definition, the data curation, and the model training. Let‚Äôs have a look at how bias handling may differ across these various stages.

<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_pipeline.png"" alt=""The Bias ML Pipeline by Meg"" width=""500"" />
    <em>The Bias ML Pipeline by <a href=""https://huggingface.co/meg"">Meg</a></em>
</p>
This can be beneficial for the model in some case, for instance, as you can see here:

![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-03.png)

In this case, the model got a +1 score for ranking the correct answer highest among the 4 options. But if we take a look at the full vocabulary it would have rather generated a word outside of our four options: the word ‚ÄúZygote‚Äù (this is more of an example than a real use case üôÇ)

How can we make sure that the model does as few as possible of these types of errors?

We can use a ‚Äú**few shots**‚Äù approach in which we provide the model with one or several examples in the prompt, with their expected answers as well. Here is how it looks:

![png](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/evaluating-mmlu-leaderboard/LLM-04.png)
1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and
2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).

In the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.


## Addressing Bias throughout the ML Development Cycle

Ready for some practical advice yet? Here we go ü§ó
#### **3. What‚Äôs a common mistake you see people make trying to integrate ML into existing products?**
There are two critical mistakes that I‚Äôve seen organizations of all sizes make when getting started with ML.  The first mistake is underestimating the importance of investing in senior leaders with substantial hands-on ML experience.  ML strategy and operations leadership benefits from a depth of technical expertise beyond what is typically found in the BI / Analytics domain or provided by educational programs that offer a limited introduction to the field.  The second mistake is waiting too long to design, test, and implement production deployment pipelines.  Effective prototype models can languish in repos for months ‚Äì even years ‚Äì while waiting on ML pipeline development.  This can impose significant opportunity costs on an organization and frustrate ML teams to the point of increasing attrition risk.
```

If `args.dataset` contains the `gs://` identifier, TensorFlow will understand that it needs to look into a GCS bucket. Loading locally is as easy as removing the `gs://` identifier. For the rest of the data pipeline-related code, you can refer to [this section](https://github.com/huggingface/transformers/blob/474bf508dfe0d46fc38585a1bb793e5ba74fddfd/examples/tensorflow/language-modeling-tpu/run_mlm.py#L186-#L201) in the training script.
```

The most likely outcome is that you'll see a bunch of errors. Don't worry, this is expected! Debugging ML models is
notoriously hard, and the key ingredient to success is patience (and `breakpoint()`). In our experience, the hardest
problems arise from subtle mismatches between ML frameworks, for which we have a few pointers at the end of this guide.
In other cases, a general test might not be directly applicable to your model, in which case we suggest an override
at the model test class level. Regardless of the issue, don't hesitate to ask for help in your draft pull request if
you're stuck.

When all tests pass, congratulations, your model is nearly ready to be added to the ü§ó Transformers library! üéâ

### 6.-7. Ensure everyone can use your model

**6. Submit the pull request**
<Question
	choices={[
		{
			text: ""The optimization step where we compute gradients and perform backpropagation"",
			explain: ""Although there may be bugs in your optimizer, this is usually several steps into the training pipeline, so there are other things to check first. Try again!""
		},
		{
			text: ""The evaluation step where we compute metrics"",
			explain: ""Evaluation is usually what you do after training for a full epoch, so you should first check somewhere earlier in the training pipeline."",
		},
		{
			text: ""The datasets"",
			explain: ""Correct! Looking at your data is almost always the first thing you should do, to make sure the text is encoded appropriately, has the expected features, and so on."",
			correct: true
		},
		{
			text: ""The dataloaders"",
			explain: ""Try again -- this is very close to the first thing you should check. Do you remember what object we pass to the dataloaders?""
		}
	]}
/>

### 5. What is the best way to debug a CUDA error?
#### **3. What‚Äôs a common mistake you see people make trying to integrate ML?**
The most common mistake that companies make while trying to integrate ML is insufficient data in their training datasets. Most ML models cannot distinguish between good data and insufficient data. Therefore, training datasets are considered relevant and used as a precedent to determine the results in most cases. This challenge isn‚Äôt limited to small- or medium-sized businesses; large enterprises have the same challenge.

No matter what the ML processes are, companies need to ensure that the training datasets are reliable and exhaustive for their desired outcome by incorporating a human element into the early stages of machine learning.

However, companies can create the required foundation for successful machine learning projects with a thorough review of accurate, comprehensive, and constant training data.
A new wave of research in AI has called for a fundamental paradigm shift in how the field approaches ML datasets ([Paullada et al., 2020](https://arxiv.org/abs/2012.05345), [Denton et al., 2021](https://journals.sagepub.com/doi/full/10.1177/20539517211035955)). This includes defining fine-grained requirements for dataset creation from the start ([Hutchinson et al., 2021](https://dl.acm.org/doi/pdf/10.1145/3442188.3445918)), curating datasets in light of problematic content and bias concerns ([Yang et al., 2020](https://dl.acm.org/doi/abs/10.1145/3351095.3375709), [Prabhu and Birhane, 2020](https://arxiv.org/abs/2006.16923)), and making explicit the values inherent in dataset construction and maintenance ([Scheuerman et al., 2021](https://dl.acm.org/doi/pdf/10.1145/3476058), [Birhane et al., 2021](https://arxiv.org/abs/2110.01963)).  Although there is general agreement that dataset development is a task that people from many different disciplines should be able to inform, in practice
#### Model selection/development: recommendations

For models just as for datasets, different tools for documentation and evaluation will provide different views of bias risks in a model which all have a part to play in helping developers choose, develop, or understand ML systems.

* Visualize
    * Generative model: visualize how the model‚Äôs outputs may reflect stereotypes
    * Classification model: visualize model errors to identify failure modes that could lead to disparate performance
* Evaluate
    * When possible, evaluate models on relevant benchmarks
* Document
    * Share your learnings from visualization and qualitative evaluation
    * Report your model‚Äôs disaggregated performance and results on applicable fairness benchmarks


## Conclusion and Overview of Bias Analysis and Documentation Tools from ü§ó
!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Using pipelines for a webserver

<Tip>
Creating an inference engine is a complex topic, and the ""best"" solution 
will most likely depend on your problem space. Are you on CPU or GPU? Do
you want the lowest latency, the highest throughput, support for
many models, or just highly optimize 1 specific model?
There are many ways to tackle this topic, so what we are going to present is a good default
to get started which may not necessarily be the most optimal solution for you.
</Tip>


The key thing to understand is that we can use an iterator, just like you would [on a
dataset](pipeline_tutorial#using-pipelines-on-a-dataset), since a webserver is basically a system that waits for requests and
treats them as they come in.
Also, not setting up an effective CI/CD (continuous integration/ continuous deployment) structure for your ML solution is another mistake I see. Very often, a once-trained model won‚Äôt suffice not only because data changes over time but resources and personnel do as well. Today‚Äôs ML practitioner needs to:
1. secure consistent flow of data as it changes and continuously retrain new models to keep it accurate and useful,  
2. ensure the structure is in place to allow for seamless replacement of older models by newly trained models while, 
3. allowing for minimal disruption to the consumer of the ML model outputs.
for the source of bugs that you encounter, let's use this example script, and I'll show it to you here in two parts. First, we do all our imports, we load a dataset, we create our tokenizer and we tokenize the dataset. Next, we convert our datasets to TensorFlow datasets, so that we can run fit() on them, and then we load our model from a pretrained checkpoint, compile it and fit it.  It seems straightforward enough, but beware! This spooky code hides many dark and mysterious secrets. What happens when we run it? Well, this isn't great. What does that mean? We tried to train on our data, but we got no gradient? This is pretty perplexing - how do we even begin to debug something like that? When the error you get doesn't immediately suggest where the problem is, the best solution is often to walk through things in sequence, making sure at each stage that things look right. And of course, the place to start is always to check your data. The best way to do that to grab a batch from the
So, who‚Äôs on the hook for machine biases in ML? These three cases illustrate one of the reasons why discussions about the responsibility of ML developers in addressing bias can get so complicated: depending on decisions made at other points in the ML system development process by other people, the biases in an ML dataset or model may land anywhere between being irrelevant to the application settings and directly leading to grievous harm. However, in all of these cases, **stronger biases in the model/dataset increase the risk of negative outcomes**. The European Union has started to develop frameworks that address this phenomenon in [recent regulatory efforts](https://ec.europa.eu/info/business-economy-euro/doing-business-eu/contract-rules/digital-contracts/liability-rules-artificial-intelligence_en): in short, a company that deploys an AI system based on a measurably biased model is liable for harm caused by the system.
#### **2. What are the biggest ML challenges within Scientific research?**
There are many challenges. One example is when using ML for Science research, we have to think carefully through the data collection and curation protocols. In some cases, the protocols we used for non-ML analysis are not appropriate or effective. The quality of the data and how representative it is of what we expect to see in the application can make a huge impact on the performance, reliability, and trustworthiness of an ML-based system.

#### **3. What‚Äôs a common mistake you see people make trying to integrate ML into Science?**
Related to the question above, one common mistake is misinterpreting results or performance to be a function of just the ML system and not also considering the data collection, curation, calibration, and normalization protocols.
Now that we know our datasets look good, it's time to check the next step of the training pipeline.

### From datasets to dataloaders[[from-datasets-to-dataloaders]]

The next thing that can go wrong in the training pipeline is when the `Trainer` tries to form batches from the training or validation set. Once you are sure the `Trainer`'s datasets are correct, you can try to manually form a batch by executing the following (replace `train` with `eval` for the validation dataloader):

```py
for batch in trainer.get_train_dataloader():
    break

Here is the question: Which approach best resolves dataset-related errors in an ML pipeline?
Here is answer 1:Focus on debugging the pipeline without analyzing datasets
Here is answer 2: Ensure licensing compliance but skip dataset integration
Here is answer3: Combine dataset loading techniques, legal checks, and debugging
Here is answer 4: Use predefined datasets to avoid errors",The correct answer is: 3. The datasets
How does PatchTST achieve efficiency in long-term time-series forecasting?,"[(517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (520, 'b101edb2-f02a-4bd8-b69e-0e3ba6a11f4d'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (12298, '75d1fbe7-d1d3-4868-b940-d25e740f5bdd'), (24592, '1bc68948-b978-48cf-aee4-932675f8db96'), (6209, 'dc759306-d7ce-4e78-b89d-4681bc9dd25d'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (6211, '829e40da-b92f-45c7-9ba7-fc66be14b35f'), (6212, '9d2539d2-4c9d-4ea5-8dad-8105ad0d656e'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29771, '54c867d1-dd3a-4a9e-97f5-dac70011ef4b'), (29772, 'e9f0dc72-4849-4954-ab57-e595cbc0c79d'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29786, '945fd8e4-77cb-409e-af9d-4d7171e7ea93'), (20570, '5dd4252e-a29c-4f30-ae23-7b7de46bb5a0'), (29818, '02cc33d8-0c79-466e-ba0c-bdadb265c65f'), (22651, 'ead7a63c-a6d5-446d-8807-99d33418b4ba'), (29820, '49d6a04e-36ec-4a38-adbf-50f525d423b1'), (29821, '84d9d326-f3f5-4a05-b9fc-f14b56bf8280'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (17031, '3f233fb1-a6a0-4c1c-8465-4333aaaee216'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (22657, '9268b17f-2fc5-4ae4-8106-8039de92a69b'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29828, '4a689bdc-aef9-4849-9b09-6a5b97979b6c'), (9877, '02d00698-e099-4595-9b5b-915a102c8c31'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (749, 'aadabaa8-059f-4e53-bb4a-defc70cc04cc'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (765, '65b17d57-f43a-42e2-a6d4-b96ae347324c'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5378, '45c18aac-df8d-468a-a6eb-a02ad673236f'), (5379, '287c1ed6-130b-4784-8132-3f5287c3471c'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (5382, 'df27d4fd-9541-4ebc-a46e-52e46fdf5ee9'), (774, 'e2833191-1067-4180-a284-3e7092c768cb'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (5383, '496f6221-9c05-4fdb-b452-3aa7c5a8b963'), (5387, 'f4e3df67-9c81-416c-94fa-53706d07085e'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5396, '18d8ed78-6549-4720-9c12-5ae48fb89fc8'), (14105, '30d56fe4-77e8-4044-a195-afe51b4ed84a'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (5923, 'd508a7d3-3d71-4f96-9bcd-951254088aab'), (805, '45825ed8-8f85-40ed-a174-19eeebd97cf7'), (807, 'ce4fda9f-c07a-4ca5-8538-9f8ededb01e2'), (808, '2232e5cb-cb93-4ea7-8c40-d51528355f6c'), (5419, 'ca377909-1f4d-48de-af71-01684b43fc59'), (812, 'ecabe7c7-05c4-4cfa-9e6e-c24df8d129b4'), (5420, '19d2ca4f-3e1d-4789-848e-5363e52ea2e9'), (814, '3c86feb6-9865-4995-82a9-c9e1bafa680f'), (815, '00347210-f522-4fa2-a819-98958034885b'), (5422, 'c12a80a5-4e72-4a35-b1ea-7113b01b5c00'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (29586, '67cb4809-8361-4ee5-9dd4-cb31eef9b1bf'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (23993, '1ab46630-fc89-4db9-8991-e991994175e0'), (17857, '640d96b2-19b8-40ad-a0f5-1a0a7b13b9d1'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM „Åã„Çâ) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google „Åã„Çâ) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google „Åã„Çâ) Jason Phang, Yao Zhao, and Peter J. Liu „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ÏóêÏÑú Ï†úÍ≥µ)ÏùÄ Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.Ïùò [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google ÏóêÏÑú) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu Ïùò [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google ÏóêÏÑú) Jason Phang, Yao Zhao, Peter J. Liu Ïùò [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research ‡§∏‡•á) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM ‡§∏‡•á) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (Êù•Ëá™ Google AI) ‰º¥ÈöèËÆ∫Êñá [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) Áî± Matthias Minderer, Alexey Gritsenko, Neil Houlsby ÂèëÂ∏É„ÄÇ
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (Êù•Ëá™  IBM Research) ‰º¥ÈöèËÆ∫Êñá [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) Áî± Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam ÂèëÂ∏É„ÄÇ
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (Êù•Ëá™ IBM) ‰º¥ÈöèËÆ∫Êñá [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) Áî± Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam ÂèëÂ∏É„ÄÇ
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[OWLv2](https://huggingface.co/docs/transformers/model_doc/owlv2)** (from Google AI) released with the paper [Scaling Open-Vocabulary Object Detection](https://arxiv.org/abs/2306.09683) by Matthias Minderer, Alexey Gritsenko, Neil Houlsby.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** (from  IBM Research) released with the paper [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (from IBM) released with the paper [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.
*Extending the forecasting time is a critical demand for real applications, such as extreme weather early warning and long-term energy consumption planning. This paper studies the long-term forecasting problem of time series. Prior Transformer-based models adopt various self-attention mechanisms to discover the long-range dependencies. However, intricate temporal patterns of the long-term future prohibit the model from finding reliable dependencies. Also, Transformers have to adopt the sparse versions of point-wise self-attentions for long series efficiency, resulting in the information utilization bottleneck. Going beyond Transformers, we design Autoformer as a novel decomposition architecture with an Auto-Correlation mechanism. We break with the pre-processing convention of series decomposition and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic
*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PatchTSMixer

## Overview

The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.
!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PatchTST

## Overview

The PatchTST model was proposed in [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/abs/2211.14730) by Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
and renovate it as a basic inner block of deep models. This design empowers Autoformer with progressive decomposition capacities for complex time series. Further, inspired by the stochastic process theory, we design the Auto-Correlation mechanism based on the series periodicity, which conducts the dependencies discovery and representation aggregation at the sub-series level. Auto-Correlation outperforms self-attention in both efficiency and accuracy. In long-term forecasting, Autoformer yields state-of-the-art accuracy, with a 38% relative improvement on six benchmarks, covering five practical applications: energy, traffic, economics, weather and disease.*
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Informer

## Overview

The Informer model was proposed in [Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting ](https://arxiv.org/abs/2012.07436) by Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.

This method introduces a Probabilistic Attention mechanism to select the ""active"" queries rather than the ""lazy"" queries and provides a sparse Transformer thus mitigating the quadratic compute and memory requirements of vanilla attention.

The abstract from the paper is the following:
*TSMixer is a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules designed for multivariate forecasting and representation learning on patched time series. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components,
long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy.*
self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling highlights dominating attention by halving cascading layer input, and efficiently handles extreme long input sequences. (iii) the generative style decoder, while conceptually simple, predicts the long time-series sequences at one forward operation rather than a step-by-step way, which drastically improves the inference speed of long-sequence predictions. Extensive experiments on four large-scale datasets demonstrate that Informer significantly outperforms existing methods and provides a new solution to the LSTF problem.*
At a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:

![model](https://github.com/namctin/transformers/assets/8100/150af169-29de-419a-8d98-eb78251c21fa)

The abstract from the paper is the following:

Here is the question: How does PatchTST achieve efficiency in long-term time-series forecasting?
Here is answer 1:By eliminating preprocessing steps
Here is answer 2: By using unstructured datasets
Here is answer3: By segmenting time-series data into patches and reducing attention map size
Here is answer 4: By relying solely on pretrained weights",Answer number three looks like it aligns well - By segmentIng
What role does dataset preprocessing play in successful Transformer model fine-tuning?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (5155, '86625bb7-792a-426a-9ade-ea98317da2e0'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (15420, '8ac3193e-579b-4ffe-9257-4fbbd1a77881'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (2647, 'a9b33316-4844-4d79-ace2-5d36fae71281'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (26207, 'dd7cd3b1-658f-4b98-9830-c5742411682f'), (26212, '8d5de3f2-1ac9-40c8-b7e9-607000e02cd5'), (15972, '0b384a9e-35c6-4a62-b202-426be4dd17da'), (23145, '093af302-133e-498d-a722-88a8b6199cee'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (3724, '1c0e1b75-0bd9-45a1-af24-e9cf468d539c'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (6355, '9cf0e1e3-ca85-4c73-afc6-a71c99755175'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (19187, '3903562f-59a1-4286-92d6-9666ba3e9bd1'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (5876, '6c565e09-ac75-4392-891f-2a1bbddd3dd9'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (17166, 'f8edb48c-3037-4068-b3cc-1bc5a12bd5d3'), (7443, '9f6d74aa-75e8-4d1e-b0f7-7135867538b0'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (25884, '585808ef-bcbd-4bda-9dc1-376342a66a99'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (15659, '54679008-9ef1-4235-9d99-b2a9666e3d13'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (4916, 'eef334ce-c96c-41d6-8320-5bb2ac306f38'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (17740, '278dd567-d102-4857-990f-de19046c1e3c'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (7534, '4354b918-223f-4fd0-92ea-e59bd0b20f6b'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (22399, '8c31bbea-69a2-4f83-afb3-796762dba928'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (5537, '98a3798c-4c95-4db2-aad9-77037f52f460'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (6083, '34942899-e37f-425b-ae2a-278f328948fc'), (11721, '1aead5c5-7fcb-464e-b7ab-40f535514f4c'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (22485, '13852224-673a-4353-9524-8a0123f2dcc8'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (24057, 'd292faa1-de56-402c-878a-e2d243e5db03')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
{/if}

For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results. 

However, there are a few cases where you'll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!
### Preprocessing
Graph transformer frameworks usually apply specific preprocessing to their datasets to generate added features and properties which help the underlying learning task (classification in our case).
Here, we use Graphormer's default preprocessing, which generates in/out degree information, the shortest path between node matrices, and other properties of interest for the model. 
 
```python
from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator

dataset_processed = dataset.map(preprocess_item, batched=False)
```

It is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (by setting `on_the_fly_processing` to True): not all datasets are as small as `ogbg-molhiv`, and for large graphs, it might be too costly to store all the preprocessed data beforehand. 

## Model

### Loading
Here, we load an existing pretrained model/checkpoint and fine-tune it on our downstream task, which is a binary classification task (hence `num_classes = 2`). We could also fine-tune our model on regression tasks (`num_classes = 1`) or on multi-task classification.
```python
from transformers import GraphormerForGraphClassification

model = GraphormerForGraphClassification.from_pretrained(
    ""clefourrier/pcqm4mv2_graphormer_base"",
    num_classes=2, # num_classes for the downstream task 
    ignore_mismatched_sizes=True,
)
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these
findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the
number of parameters at the fine-tuning stage.*
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>
At the time of writing this blog post, the best performing models are very large models containing more than 10 billion parameters most of which are not open-sourced, *e.g.* *ST-MoE-32B*, *Turing NLR v5*, or
*ERNIE 3.0*. One of the top-ranking models that is easily accessible is [DeBERTa](https://huggingface.co/docs/transformers/model_doc/deberta). Therefore, let's try out DeBERTa's newest base version - *i.e.* [`microsoft/deberta-v3-base`](https://huggingface.co/microsoft/deberta-v3-base).


## Training / Fine-tuning a model with ü§ó Transformers and ü§ó Datasets

In this section, we will jump into the technical details of how to
fine-tune a model end-to-end to be able to automatically filter out very unsatisfied customer feedback messages.

Cool! Let's start by installing all necessary pip packages and setting up our code environment, then look into preprocessing the dataset, and finally start training the model.
2. Provide state-of-the-art models with performances as close as possible to the original models:

  - We provide at least one example for each architecture which reproduces a result provided by the official authors
    of said architecture.
  - The code is usually as close to the original code base as possible which means some PyTorch code may be not as
    *pytorchic* as it could be as a result of being converted TensorFlow code and vice versa.

A few other goals:

- Expose the models' internals as consistently as possible:

  - We give access, using a single API, to the full hidden-states and attention weights.
  - The preprocessing classes and base model APIs are standardized to easily switch between models.

- Incorporate a subjective selection of promising tools for fine-tuning and investigating these models:

  - A simple and consistent way to add new tokens to the vocabulary and embeddings for fine-tuning.
  - Simple ways to mask and prune Transformer heads.
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model -- one as close as possible to the task you have at hand -- and fine-tune it.

## General architecture[[general-architecture]]

In this section, we'll go over the general architecture of the Transformer model. Don't worry if you don't understand some of the concepts; there are detailed sections later covering each of the components.

<Youtube id=""H39Z_720T5s"" />

## Introduction[[introduction]]

The model is primarily composed of two blocks:

* **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
* **Decoder (right)**: The decoder uses the encoder's representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.
```

While most datasets on the hub are ready to use out of the box, sometimes we wish to perform some additional processing or modification of the dataset. In this case [we wish to match the author's implementation](https://github.com/kzl/decision-transformer), that is we need to:

- Normalize each feature by subtracting the mean and dividing by the standard deviation.
- Pre-compute discounted returns for each trajectory.
- Scale the rewards and returns by a factor of 1000.
- Augment the dataset sampling distribution so it takes into account the length of the expert agent‚Äôs trajectories.

In order to perform this dataset preprocessing, we will use a custom ü§ó [Data Collator](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator). 

Now let‚Äôs get started on the Custom Data Collator for Offline Reinforcement Learning.
```

As usual with image datasets, we need to apply the same image transformations that were used at training time. We use the preprocessing defined in the processor. We also define a data collation function to feed the model batches of properly formatted tensors.
‚Äã
- Chapters 1 to 4 provide an introduction to the main concepts of the ü§ó Transformers library. By the end of this part of the course, you will be familiar with how Transformer models work and will know how to use a model from the [Hugging Face Hub](https://huggingface.co/models), fine-tune it on a dataset, and share your results on the Hub!
- Chapters 5 to 8 teach the basics of ü§ó Datasets and ü§ó Tokenizers before diving into classic NLP tasks. By the end of this part, you will be able to tackle the most common NLP problems by yourself.
- Chapters 9 to 12 go beyond NLP, and explore how Transformer models can be used to tackle tasks in speech processing and computer vision. Along the way, you'll learn how to build and share demos of your models, and optimize them for production environments. By the end of this part, you will be ready to apply ü§ó Transformers to (almost) any machine learning problem!

This course:
it's perfectly normal to see it adapted its suggestions like this. Notice how it keeps the same predictions as the pretrained model afterward. Even if the fine-tuned model adapts to the new dataset, it's not forgetting what it was pretrained on. This is another example on a translation task. On top we use a pretrained French/English model and at the bottom, the version we fine-tuned in chapter 7. The top model is pretrained on lots of texts, and leaves technical English terms like plugin and email unchanged in the translation (both are perfectly understood by French people). The dataset picked for the fine-tuning is a dataset of technical texts where special attention was picked to translate everything in French. As a result, the fine-tuned model picked that habit and translated both plugin and email.
```

```python out
""""""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?
```

Use ü§ó Datasets [`~datasets.Dataset.set_transform`] to apply the transformations on the fly:

```py
food[""train""].set_transform(preprocess_train)
food[""test""].set_transform(preprocess_val)
```

As a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike other data collators in ü§ó Transformers, the
`DefaultDataCollator` does not apply additional preprocessing, such as padding.

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors=""tf"")
```

With two basic classes - configuration and model - and an additional preprocessing class (tokenizer, image processor, feature extractor, or processor), you can create any of the models supported by ü§ó Transformers. Each of these base classes are configurable, allowing you to use the specific attributes you want. You can easily setup a model for training or modify an existing pretrained model to fine-tune.
Preprocess

In addition to loading datasets, ü§ó Datasets other main goal is to offer a diverse set of preprocessing functions to get a dataset into an appropriate format for training with your machine learning framework. 

There are many possible ways to preprocess a dataset, and it all depends on your specific dataset. Sometimes you may need to rename a column, and other times you might need to unflatten nested fields. ü§ó Datasets provides a way to do most of these things. But in nearly all preprocessing cases, depending on your dataset modality, you'll need to:

- Tokenize a text dataset.
- Resample an audio dataset.
- Apply transforms to an image dataset.

The last preprocessing step is usually setting your dataset format to be compatible with your machine learning framework's expected input format.

In this tutorial, you'll also need to install the ü§ó Transformers library:

```bash
pip install transformers
```

To apply the preprocessing transformations over the entire dataset, use the ü§ó Datasets [`~datasets.Dataset.set_transform`] function.
The transform is applied on the fly which is faster and consumes less disk space:

```py
>>> train_ds.set_transform(train_transforms)
>>> test_ds.set_transform(val_transforms)
```
</tf>
</frameworkcontent>

### Evaluate

Including a metric during training is often helpful for evaluating your model's performance. You can quickly load an evaluation method with the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) metric (see the ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):

```py
>>> import evaluate

>>> metric = evaluate.load(""mean_iou"")
Let's quickly go over each of these.

## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]

Like other neural networks, Transformer models can't process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:

- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*
- Mapping each token to an integer
- Adding additional inputs that may be useful to the model

Here is the question: What role does dataset preprocessing play in successful Transformer model fine-tuning?
Here is answer 1:Preprocessing is optional for most workflows
Here is answer 2: It enhances model accuracy and supports Transformer workflows
Here is answer3: It replaces the need for model fine-tuning
Here is answer 4: It simplifies licensing requirements",Answer: Here Is Answer2 :It enhances.model accuracy.and.supports.Transformer.workflows
Which method ensures ethical compliance when using custom datasets in NLP workflows?,"[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (21510, 'c46191e4-71b9-4418-af1a-b816bb6ad8a0'), (13321, '10328f23-c6d0-4eca-a172-090f97ccf6a3'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (12314, 'e385ad95-96ab-4d32-aa88-01c826234363'), (13339, 'f046b9a1-daad-440b-8fa6-1a8de946d278'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (29223, 'b3eb4452-489b-46d5-8643-ec033c281140'), (29224, 'a1b49036-83a8-4545-96ac-718a57c76b4b'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (8239, '601c3e59-2039-4a07-a2ff-f1e97ecadabd'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5682, 'fb18c66c-cdd7-47e6-a4bf-022dc4f0f8af'), (1601, '06cbfead-420b-465d-a07c-566ed263f552'), (23114, '7ed38831-876e-4eca-806b-567443739839'), (29261, '1715bea8-d32b-4d78-b7b9-eff4094c9c6d'), (29262, '5dfbb43a-ae42-4a9d-a676-5a72a99fe9c8'), (29265, '47595a18-5b82-4980-91ef-b2cc34ee15b8'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (7398, '383d361a-826c-44cb-a553-5def932b4a1a'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (4361, '69a44467-ab86-4ff5-bd68-f4fcb4287814'), (4362, 'a3614fd5-f69b-44a3-90b0-01f9eb078f1a'), (4363, '3a3e4a17-7092-49fb-a163-1e382dffbc6d'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (7957, 'e79d5025-a9c4-432e-b94c-57adc02cbeba'), (15127, '58cfa5d1-aa1f-4671-909f-1abfdb9b7292'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (2870, '8630c5af-b3e7-4d83-b67d-8cca87440b11'), (15672, '3c536d00-c1fc-410b-a634-96a25e8778ee'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29503, '8d8f1ab2-d503-4300-a5cb-65e057381f71'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (7501, '7bfa1032-a0a7-45b6-89e8-bf4aa1e10568'), (1359, '1ee49ddb-9952-4cfc-b4ed-2acfce41628f'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (11610, '7abc171e-bc54-496c-a7d6-c25500c846c1'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (3440, '51f99fe7-7751-4e98-914f-d073dd0afcf5'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (8618, 'b72c1623-10fb-4fdc-99e7-31e030260a85'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (27566, '228f8e78-7d1b-4627-8417-c05cbd941d34'), (8624, '339598bc-e183-4af9-9283-072e1969ff78'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (8626, 'b2adf91d-3907-4c74-a7a7-bbcc199cea59'), (21948, 'e1dd1005-94e8-4562-b9c3-6ba611211942'), (29629, '6616af81-5a7a-45be-b96a-4aefa6ba382a'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (21952, '138ee00c-29cc-4a4a-b56c-0e17a9529580'), (29633, '470b577b-aa9a-4eb1-8a92-21446ac23695'), (29635, '1703fceb-c153-43cb-9e28-9f71223b471c'), (7108, '7e8be3a4-3d9e-4572-b072-eaacb2c1a9a1'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (7110, 'ace1a408-647b-4149-a9c1-b6a9a032d552'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (1499, '1faf2c53-b46d-4aec-a233-dd712314dc97'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27101, 'a0b5c992-ca35-43fb-8604-666943c50f3c'), (1502, 'be3626fb-0c72-415e-99b7-8b512a97ad59'), (1500, '958de4d0-417a-4556-b8cb-9391406533c5'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (1510, 'a7df3897-3c02-4187-b2c6-88844cd91c0a'), (1513, '710901b9-382c-447e-b95f-bb910a1fdce4'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (25072, '756040d5-e13a-41c7-b379-2fee2efdc6d4'), (24560, 'ae4f8022-24f0-469e-9cb5-358e58b5773d'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: So that really became my shift into ethical AI work.

### In what applications is data ethics most important?

**Meg:** Human-centric technology that deals with people and identity (face recognition, pedestrian recognition). In NLP this would pertain more to the privacy of individuals, how individuals are talked about, and the biases models pick up with regards to descriptors used for people.

### How can ML teams be more aware of harmful bias?

**Meg:** A primary issue is that these concepts haven't been taught and most teams simply aren‚Äôt aware. Another problem is the lack of a lexicon to contextualize and communicate what is going on.

For example:
- This is what marginalization is
- This is what a power differential is
- Here is what inclusion is
- Here is how stereotypes work

Having a better understanding of these pillars is really important.
As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.

Data sources, data governance, management of copyrighted data are just a few of the most important compliance areas to consider. BigScience, the older cousin and inspiration for BigCode, addressed these areas in working groups before they were broadly recognized by the draft AI EU Act, and as a result was [graded as most compliant among Foundational Model Providers in a Stanford CRFM study](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html).
Now that we have a great pre-trained model for financial data, the next step is to fine-tune it using our own data for doing sentiment analysis!  

So, we first upload a [custom dataset for sentiment analysis](https://huggingface.co/datasets/FinanceInc/auditor_sentiment) that we built internally with the team to our Private Hub. This dataset has several thousand sentences from financial news in English and proprietary financial data manually categorized by our team according to their sentiment. This data contains sensitive information, so our compliance team only allows us to upload this data on our own servers. Luckily, this is not an issue as we run the Private Hub on our own AWS instance.

Then, we use [AutoTrain](https://huggingface.co/autotrain) to quickly fine-tune the FinBert model with our custom sentiment analysis dataset. We can do this straight from the datasets page on our Private Hub:
In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.

In the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.

## Compliance as a Core Principle

As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.
to analyze [ML datasets](https://huggingface.co/spaces/huggingface/data-measurements-tool) and [models](https://huggingface.co/spaces/society-ethics/StableBias). We believe this helps everyone interested to better understand [the limitations of ML systems](https://huggingface.co/blog/ethics-soc-2) and how they can safely be leveraged to best serve users and those affected by these systems. These approaches have already proven their utility in promoting accountability, especially in the larger multidisciplinary research endeavors we‚Äôve helped organize, including [BigScience](https://huggingface.co/bigscience) (see our blog series [on the social stakes of the project](https://montrealethics.ai/category/columns/social-context-in-llm-research/)), and the more recent [BigCode project](https://huggingface.co/bigcode) (whose governance is [described in more details here](https://huggingface.co/datasets/bigcode/governance-card)).
However, relying on closed-source Code LLMs to create internal code assistants exposes companies to compliance and security issues. First during training, as fine-tuning a closed-source Code LLM on an internal codebase requires exposing this codebase to a third party. And then during inference, as fine-tuned Code LLMs are likely to ‚Äúleak‚Äù code from their training dataset during inference. To meet compliance requirements, enterprises need to deploy fine-tuned Code LLMs within their own infrastructure - which is not possible with closed source LLMs.
Process text data

This guide shows specific methods for processing text datasets. Learn how to:

- Tokenize a dataset with [`~Dataset.map`].
- Align dataset labels with label ids for NLI datasets.

For a guide on how to process any type of dataset, take a look at the <a class=""underline decoration-sky-400 decoration-2 font-semibold"" href=""./process"">general process guide</a>.

## Map

The [`~Dataset.map`] function supports processing batches of examples at once which speeds up tokenization.

Load a tokenizer from ü§ó [Transformers](https://huggingface.co/transformers/):

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")
On one end is the purely model-generated IFT dataset such as Unnatural Instructions ([Honovich et al., ‚Äò22](https://arxiv.org/abs/2212.09689)) and on the other is a large community effort of hand-crafted instructions as in Super-natural instructions ([Wang et al., ‚Äò22](https://arxiv.org/abs/2204.07705)). In between these two are works on using a small set of high quality seed dataset followed by bootstrapping such as Self-instruct ([Wang et al., 22](https://arxiv.org/pdf/2212.10560.pdf)). Yet another way of collating a dataset for IFT is to take the existing high-quality crowdsourced NLP datasets on various tasks (including prompting) and cast those as instructions using a unified schema or diverse templates. This line of work includes the T0 ([Sanh et al., ‚Äò22](https://arxiv.org/pdf/2110.08207.pdf)), Natural instructions dataset ([Mishra et al., ‚Äò22](https://arxiv.org/pdf/2104.08773.pdf)), the FLAN LM ([Wei et al., ‚Äò22](https://arxiv.org/pdf/2109.01652.pdf)), and the OPT-IML ([Iyer
Finally, if you want more information on the how and why of dataset cards, we strongly recommend reading the foundational works [Datasheets for Datasets](https://arxiv.org/abs/1803.09010) and [Data Statements for NLP](https://www.aclweb.org/anthology/Q18-1041/).

Thank you for your contribution!

## Code of conduct

This project adheres to the HuggingFace [code of conduct](CODE_OF_CONDUCT.md).
By participating, you are expected to abide by this code.
Since we consider the hypothetical use case of *filtering out unsatisfied messages*, let's look into what datasets are available.

For your real-world use case, it is **very likely** that you have internal data that best represents the actual data your NLP system is supposed to handle. Therefore, you should use such internal data to train your NLP system.
It can nevertheless be helpful to also include publicly available data to improve the generalizability of your model.
* [Holland et al. (2018)](https://huggingface.co/papers/1805.03677) propose data nutrition labels, akin to nutrition facts for foodstuffs and nutrition labels for privacy disclosures, as a tool for analyzing and making decisions about datasets. The Data Nutrition Label team released an updated design of and interface for the label in 2020  ([Chmielinski et al., 2020)](https://huggingface.co/papers/2201.03954)).
* [McMillan-Major et al. (2021)](https://huggingface.co/papers/2108.07374) describe the development process and resulting templates for **data cards for NLP** in the form of data cards on the Hugging Face Hub[^3] and data cards for datasets that are part of the NLP benchmark for Generation and its Evaluation Metrics (GEM) environment[^4].
The details of IDEFICS' architecture, training methodology, and evaluations, as well as information about the dataset, are available in the [model card](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct) and our [research paper](https://huggingface.co/papers/2306.16527). Additionally, we have documented [technical insights and learnings](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) from the model's training, offering valuable perspective on IDEFICS' development.

## Ethical evaluation

At the outset of this project, through a set of discussions, we developed an [ethical charter](https://huggingface.co/blog/ethical-charter-multimodal) that would help steer decisions made during the project. This charter sets out values, including being self-critical, transparent, and fair which we have sought to pursue in how we approached the project and the release of the models.
```

Data inspection is a very important task in almost all ML development stages, but it can also be very time consuming.

> ‚ÄúManual inspection of data has probably the highest value-to-prestige ratio of any activity in machine learning.‚Äù‚Ää‚Äî‚ÄäGreg Brockman
> 

[Spotlight](https://renumics.com/docs) helps you to **make data inspection more scalable** along two dimensions: Setting up and maintaining custom data inspection workflows and finding relevant data samples and clusters to inspect. In the following sections we show some examples based on Hugging Face datasets.

## Spotlight ü§ù Hugging Face datasets

The *datasets* library has several features that makes it an ideal tool for working with ML datasets: It stores tabular data (e.g. metadata, labels) along with unstructured data (e.g. images, audio) in a common Arrows table. *Datasets* also describes important data semantics through features (e.g. images, audio) and additional task-specific metadata.
_This blog post from the [Ethics and Society regulars @ü§ó](https://huggingface.co/blog/ethics-soc-1) shares some of the lessons we have learned along with tools we have developed to support ourselves and others in our community‚Äôs efforts to better address bias in Machine Learning. The first part is a broader reflection on bias and its context. If you‚Äôve already read it and are coming back specifically for the tools, feel free to jump to the [datasets](#i-am-curatingpicking-a-dataset-for-my-ml-system-how-can-i-address-bias) or [models](#i-am-trainingselecting-a-model-for-my-ml-system-how-can-i-address-bias)
section!_

<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img1.jpg"" alt=""Selection of tools developed by HF team members to address bias in ML"" />
    <em>Selection of tools developed by ü§ó team members to address bias in ML</em>
</p>
<p align=""center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/121_model-cards/upload_a_mc.gif"" width=""600""/>
</p>

The writing tool encourages those who have yet to write model cards to create them more easily. For those who have previously written model cards, this approach invites them to add to the prompted information -- while centering the ethical components of model documentation.

As ML continues to be more intertwined with different domains, collaborative and open-source ML processes that center accessibility, ethics and inclusion are a critical part of the machine learning lifecycle and a stepping stone in ML documentation.
publisher = ""Association for Computational Linguistics"",
    url = ""https://aclanthology.org/2021.emnlp-demo.21"",
    pages = ""175--184"",
    abstract = ""The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets."",
Note: While StarCoder is the inspiration and model powering the initial version of SafeCoder, an important benefit of building a LLM solution upon open source models is that it can adapt to the latest and greatest open source models available. In the future, SafeCoder may offer other similarly commercially permissible open source models built upon ethically sourced and transparent datasets as the base LLM available for fine-tuning.

## Privacy and Security as a Core Principle

For any company, the internal codebase is some of its most important and valuable intellectual property. A core principle of SafeCoder is that the customer internal codebase will never be accessible to any third party (including Hugging Face) during training or inference.
## When can I use the ü§ó Data Measurements Tool?
The ü§ó Data Measurements Tool can be used iteratively for exploring one or more existing NLP datasets, and will soon support iterative development of datasets from scratch. It provides actionable insights informed by research on datasets and responsible dataset development, allowing users to hone in on both high-level information and specific items.

## What can I learn using the ü§ó Data Measurements Tool?
### Dataset Basics
**For a high-level overview of the dataset**

*This begins to answer questions like ‚ÄúWhat is this dataset?  Does it have missing items?‚Äù.  You can use this as ‚Äúsanity checks‚Äù that the dataset you‚Äôre working with is as you expect it to be.*

- A description of the dataset (from the Hugging Face Hub)

- Number of missing values or NaNs

### Descriptive Statistics
**To look at the surface characteristics of the dataset**
As a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.

In addition, the Hugging Face Hub offers:

-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)
-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)
-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)
-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)
-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)


### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.
<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img5.png"" alt=""Data Measurements tool by Meg, Sasha, Bibi, and the Gradio team"" />
    <em><a href=""https://huggingface.co/spaces/huggingface/data-measurements-tool"">Data Measurements tool</a> by <a href=""https://huggingface.co/meg"">Meg</a>, <a href=""https://huggingface.co/sasha"">Sasha</a>, <a href=""https://huggingface.co/Bibss"">Bibi</a>, and the <a href=""https://gradio.app/"">Gradio team</a></em>
</p>


#### Dataset selection/curation: recommendations

These tools aren‚Äôt full solutions by themselves, rather, they are designed to support critical examination and improvement of datasets through several lenses, including the lens of bias and bias-related risks. In general, we encourage you to keep the following steps in mind when leveraging these and other tools to mitigate bias risks at the dataset curation/selection stage:

Here is the question: Which method ensures ethical compliance when using custom datasets in NLP workflows?
Here is answer 1:Avoid using public datasets
Here is answer 2: Adhere to legal standards, preprocess data, and debug pipelines
Here is answer3: Use pretrained models without modifications
Here is answer 4: Focus on hyperparameter tuning instead of datasets","Based solely off context within given passage above; 
Answer : 
Here Is Answer2 : Adhere To Legal Standards preprocess Data And Debug Pipelines"
What is the significance of licensing considerations when sharing Transformer models trained on custom datasets?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (17412, '1d9b136f-d20c-4989-8975-80625fa8da45'), (24074, 'c7902235-64ce-4d97-b746-5682c76f72ec'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (16405, '35bf25f4-bda1-4a53-8c87-7c4c58181fa4'), (16406, '57321316-4910-41b2-9525-559cf4584035'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (12351, '3507bacd-33e0-47cf-a2a6-154bda1acd90'), (26177, 'f49670a1-d2e0-4dbe-9b78-0b59ee600cda'), (12356, 'd426880d-8fc8-4f06-9e8d-f072e54790f4'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (13937, 'edfad0ce-4e2e-4e9b-ac33-041d619a5911'), (13455, 'f0b6cad7-53a6-427e-8bec-1909b4d63763'), (12947, '8890d1af-9df8-4229-a9ce-37479fb8c9d3'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (1186, '50598bfb-1eff-44c7-a14f-8886c016a045'), (30886, '54b36145-f8cd-4fc5-98d8-547220fe27b3'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (29896, 'b72ba5ef-9507-48a0-91d0-2fe3e2cc33f3'), (9416, '28859eb3-1961-4028-8c62-23786a3bd60d'), (29899, '22859170-9617-4bf1-86c1-757ba4d4cc59'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (6889, 'f9703e3f-789b-4d40-bb13-0dcb8de3863a'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (28424, '118d80b7-d822-4e36-87a3-b04479f5676e'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (23844, 'c14d0dd8-b543-44ba-b5a9-290396c2a5d2'), (8488, 'b63dba89-ad6e-4edd-a90b-3c654aac9306'), (5928, '46517678-0861-4d86-b92b-eb731ae44060'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (29487, '2321da63-59f7-4fbb-b56d-00810837d618'), (6959, 'f5be3be7-71a6-4eaf-9d09-683a8afd900c'), (28979, '89c0d9c3-1678-4fa1-be87-cd2090709597'), (28980, '2b0c07ed-129d-48ce-a54a-c5be8281aba6'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (19783, 'b8e43816-1431-4c21-afbb-e2941ff6391e'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (5978, '46c8cc0b-9403-4474-9d60-2b2578a424c6'), (4442, '274884d8-70a8-4a04-900c-0d6dc0e00410'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (26975, 'a77435a6-b248-4c25-88bb-c6f596b9ed0c'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (17798, '7ae25397-da2a-4397-9fa8-858e74f29d9b'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (24972, '9084f511-5580-4546-944c-05245a37f48b'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (6083, '34942899-e37f-425b-ae2a-278f328948fc'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (2517, 'a33f44cd-0ffd-441b-8e41-7b24cefe6980'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (31712, '1ab53c8b-a333-4593-afb6-7d635831de43'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
All these releases a) included model weights (under varyingly open licenses) and b) had good performance for models on the smaller side (between 3B and 70B parameters), and therefore, they were instantly adopted by the community. Almost all of these models use the decoder transformer architecture, with various tweaks (ALiBi or RoPE, RMS pre-normalization, SwiGLU), as well as some changes to the attention functions (Flash-Attention, GQA, sliding windows) and different code base implementations to optimize for training or inference speed. These tweaks are likely to affect the performance and training speed to some extent; however, as all the architectures have been released publicly with the weights, the core differences that remain are the training data and the licensing of the models.
The ü§ó Transformers library comes with a vanilla probabilistic time series Transformer model, simply called the [Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer). In the sections below, we'll show how to train such a model on a custom dataset.


## Set-up Environment

First, let's install the necessary libraries: ü§ó Transformers, ü§ó Datasets, ü§ó Evaluate,  ü§ó Accelerate and [GluonTS](https://github.com/awslabs/gluonts).

As we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.


```python
!pip install -q transformers

!pip install -q datasets

!pip install -q evaluate

!pip install -q accelerate

!pip install -q gluonts ujson
Industry and academia make enormous contributions to advancing Document AI. There are a wide assortment of models and datasets available for data scientists to use. However, licensing can be a non-starter for building an enterprise solution.¬†Some well-known models have restrictive licenses that forbid the model from being used for commercial purposes. Most notably, Microsoft's [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv2) and [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) checkpoints cannot be used commercially. When you start a project, we advise carefully evaluating the license of prospective models. Knowing which models you want to use is essential at the outset, since that may affect data collection and annotation. A table of the popular models with their licensing license information is at the end of this post. 
</div>
    </div>
        </div>
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>
he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
```

Now that we explained the theory behind Decision Transformer, the Trainer, and how to train it. **You're ready to train your first offline Decision Transformer model from scratch to make a half-cheetah run** üëâ https://github.com/huggingface/blog/blob/main/notebooks/101_train-decision-transformers.ipynb
The Colab includes visualizations of the trained model, as well as how to save your model on the ü§ó hub.


## Conclusion

This post has demonstrated how to train the Decision Transformer on an offline RL dataset, hosted on [ü§ó datasets](https://huggingface.co/docs/datasets/index). We have used a ü§ó transformers [Trainer](https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/decision_transformer#overview) and a custom data collator.
**Important**: This doesn't mean that our model cannot handle longer input sequences, it just means that we use a maximum length of 128 for training since it covers 99% of our training and we don't want to waste memory. Transformer models have shown to be very good at generalizing to longer sequences after training.
Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Transformer Temporal

A Transformer model for video-like data.

## TransformerTemporalModel

[[autodoc]] models.transformer_temporal.TransformerTemporalModel

## TransformerTemporalModelOutput

[[autodoc]] models.transformer_temporal.TransformerTemporalModelOutput
If you want to run the examples locally, we recommend taking a look at the <a href=""/course/chapter0"">setup</a>.
</Tip>

## Transformers are everywhere![[transformers-are-everywhere]]

Transformer models are used to solve all kinds of NLP tasks, like the ones mentioned in the previous section. Here are some of the companies and organizations using Hugging Face and Transformer models, who also contribute back to the community by sharing their models:

<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/companies.PNG"" alt=""Companies using Hugging Face"" width=""100%"">

The [ü§ó Transformers library](https://github.com/huggingface/transformers) provides the functionality to create and use those shared models. The [Model Hub](https://huggingface.co/models) contains thousands of pretrained models that anyone can download and use. You can also upload your own models to the Hub!
```

```python out
""""""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?
In [Chapter 7](/course/chapter7), we'll put all of this to good use as we take a deep dive into the core NLP tasks that Transformer models are great for. Before jumping ahead, though, put your knowledge of ü§ó Datasets to the test with a quick quiz!
## Why should I use transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding & generation, computer vision, and audio tasks.
    - Low barrier to entry for educators and practitioners.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.

1. Lower compute costs, smaller carbon footprint:
    - Researchers can share trained models instead of always retraining.
    - Practitioners can reduce compute time and production costs.
    - Dozens of architectures with over 60,000 pretrained models across all modalities.

1. Choose the right framework for every part of a model's lifetime:
    - Train state-of-the-art models in 3 lines of code.
    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.
    - Seamlessly pick the right framework for training, evaluation, and production.
Imagine if each time a research team, a student organization, or a company wanted to train a model, it did so from scratch. This would lead to huge, unnecessary global costs!

This is why sharing language models is paramount: sharing the trained weights and building on top of already trained weights reduces the overall compute cost and carbon footprint of the community.

By the way, you can evaluate the carbon footprint of your models' training through several tools. For example [ML CO2 Impact](https://mlco2.github.io/impact/) or [Code Carbon]( https://codecarbon.io/) which is integrated in ü§ó Transformers. To learn more about this, you can read this [blog post](https://huggingface.co/blog/carbon-emissions-on-the-hub) which will show you how to generate an `emissions.csv` file with an estimate of the footprint of your training, as well as the [documentation](https://huggingface.co/docs/hub/model-cards-co2) of ü§ó Transformers addressing this topic.
Once the contract was signed, we began the migration of moving off our regex based system to direct an increasing amount of critical path traffic to the transformer model. Internally, we had to build some new telemetry for both model and production data monitoring. Given that this system is positioned so early in the product experience, any inaccuracies in model outcomes could significantly impact business metrics. We ran an extensive experiment where new users were split equally between the old system and the new model. We assessed model performance in conjunction with broader business metrics, such as paid user retention and engagement. The ML model clearly outperformed in terms of retention, leading us to confidently make the decision to scale the system - first to new users and then to existing users - ramping to 100% over a span of two months.
With this in mind, let's go a bit deeper into the general library
design.

### Overview of models

To successfully add a model, it is important to understand the
interaction between your model and its config,
`PreTrainedModel`, and `PretrainedConfig`. For
exemplary purposes, we will call the PyTorch model to be added to ü§ó Transformers
`BrandNewBert`.

Let's take a look:

![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png)
With this in mind, let's go a bit deeper into the general library
design.

### Overview of models

To successfully add a model, it is important to understand the
interaction between your model and its config,
`PreTrainedModel`, and `PretrainedConfig`. For
exemplary purposes, we will call the PyTorch model to be added to ü§ó Transformers
`BrandNewBert`.

Let's take a look:

![image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers_overview.png)
!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Building custom models

The ü§ó Transformers library is designed to be easily extensible. Every model is fully coded in a given subfolder
of the repository with no abstraction, so you can easily copy a modeling file and tweak it to your needs.

Here is the question: What is the significance of licensing considerations when sharing Transformer models trained on custom datasets?
Here is answer 1:Licenses ensure models remain proprietary
Here is answer 2: Licenses allow broader model sharing while maintaining legal standards
Here is answer3: Licenses simplify dataset preprocessing
Here is answer 4: Licenses replace the need for model fine-tuning",Here lies answer number two.Licenses allow broader
How can Transformer workflows be debugged for scalability in time-series applications?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (1601, '06cbfead-420b-465d-a07c-566ed263f552'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29776, 'e3203540-4467-4975-8a4a-3a14658f4f60'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (29786, '945fd8e4-77cb-409e-af9d-4d7171e7ea93'), (29795, 'e8acfe88-7634-4f4d-a180-dbea58d5a556'), (4223, '386573d6-bd83-4bd5-824c-7fbdd3122bad'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29828, '4a689bdc-aef9-4849-9b09-6a5b97979b6c'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (12948, 'de99c19f-cfde-4076-a099-a2a5d6202e05'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (30895, 'd9909539-e9b8-4ba7-a169-17d2b6a76435'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (20160, 'c0c6fd6f-69be-4907-9b87-052041804e6c'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11463, '20088ef2-37e8-4def-98fa-8c9ab021a5f1'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (23784, 'ed5ad04f-6d3c-413f-a4b9-4419c92834de'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (7932, 'b99512aa-f8fc-45ad-ae42-7b543455a8d7'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (5380, 'c3402ba2-db82-486f-b86b-a5f292b965d1'), (779, 'c84b16d0-6361-44a8-b2c9-cc03a7a33748'), (782, '6279502b-8279-4967-bca2-1610f71071a5'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (5401, 'fabf08d1-3eb0-4c54-abc0-8267df388968'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (6083, '34942899-e37f-425b-ae2a-278f328948fc'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Secondly, a Transformer helps us to train on time series data which might contain thousands of time points. It might not be feasible to input *all* the history of a time series at once to the model, due to the time- and memory constraints of the attention mechanism. Thus, one can consider some appropriate context window and sample this window and the subsequent prediction length sized window from the training data when constructing batches for stochastic gradient descent (SGD). The context sized window can be passed to the encoder and the prediction window to a *causal-masked* decoder. This means that the decoder can only look at previous time steps when learning the next value. This is equivalent to how one would train a vanilla Transformer for machine translation, referred to as ""teacher forcing"".
As time series researchers will know, there has been a lot of interest in applying Transformer based models to the time series problem. The vanilla Transformer is just one of many attention-based models and so there is a need to add more models to the library.

At the moment nothing is stopping us from modeling multivariate time series, however for that one would need to instantiate the model with a multivariate distribution head. Currently, diagonal independent distributions are supported, and other multivariate distributions will be added. Stay tuned for a future blog post that will include a tutorial.

Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example.
*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling
Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series Forecasting**. Our comparison shows that the simple linear model, known as _DLinear_, is not better than Transformers as claimed. When compared against equivalent sized models in the same setting as the linear models, the Transformer-based models perform better on the test set metrics we consider.
Another benefit of Transformers over the other architectures is that we can incorporate missing values (which are common in the time series setting) as an additional mask to the encoder or decoder and still train without resorting to in-filling or imputation. This is equivalent to the `attention_mask` of models like BERT and GPT-2 in the Transformers library, to not include padding tokens in the computation of the attention matrix.

A drawback of the Transformer architecture is the limit to the sizes of the context and prediction windows because of the quadratic compute and memory requirements of the vanilla Transformer, see [Tay et al., 2020](https://arxiv.org/abs/2009.06732). Additionally, since the Transformer is a powerful architecture, it might overfit or learn spurious correlations much more easily compared to other [methods](https://openreview.net/pdf?id=D7YBmfX_VQy).
--
title: ""Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)""
thumbnail: /blog/assets/150_autoformer/thumbnail.png
authors:
- user: elisim
  guest: true
- user: kashif
- user: nielsr
---

# Yes, Transformers are Effective for Time Series Forecasting (+ Autoformer)


<script async defer src=""https://unpkg.com/medium-zoom-element@0/dist/medium-zoom-element.min.js""></script>

<a target=""_blank"" href=""https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/autoformer-transformers-are-effective.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>

## Introduction
- In addition to the raw (`past_values` and `future_values`), one typically provides additional features to the model. These can be the following:
    - `past_time_features`: temporal features which the model will add to `past_values`. These serve as ""positional encodings"" for the Transformer encoder.
    Examples are ""day of the month"", ""month of the year"", etc. as scalar values (and then stacked together as a vector).
    e.g. if a given time-series value was obtained on the 11th of August, then one could have [11, 8] as time feature vector (11 being ""day of the month"", 8 being ""month of the year"").
    - `future_time_features`: temporal features which the model will add to `future_values`. These serve as ""positional encodings"" for the Transformer decoder.
    Examples are ""day of the month"", ""month of the year"", etc. as scalar values (and then stacked together as a vector).
- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer without any head on top, and [`TimeSeriesTransformerForPrediction`]
adds a distribution head on top of the former, which can be used for time-series forecasting. Note that this is a so-called probabilistic forecasting model, not a
point forecasting model. This means that the model learns a distribution, from which one can sample. The model doesn't directly output values.
- [`TimeSeriesTransformerForPrediction`] consists of 2 blocks: an encoder, which takes a `context_length` of time series values as input (called `past_values`),
and a decoder, which predicts a `prediction_length` of time series values into the future (called `future_values`). During training, one needs to provide
pairs of (`past_values` and `future_values`) to the model.
- In addition to the raw (`past_values` and `future_values`), one typically provides additional features to the model. These can be the following:
!--Copyright 2022 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Time Series Transformer

## Overview

The Time Series Transformer model is a vanilla encoder-decoder Transformer for time series forecasting.
This model was contributed by [kashif](https://huggingface.co/kashif).

## Usage tips
Two main types of decomposition are additive and multiplicative decomposition, which are implemented in the [great statsmodels library](https://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html). By decomposing a time series into these components, we can better understand and model the underlying patterns in the data. 

But how can we incorporate decomposition into the Transformer architecture? Let's see how Autoformer does it.

#### Decomposition in Autoformer

| ![autoformer_architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/148_autoformer/autoformer_architecture.png) |
|:--:|
| Autoformer architecture from [the paper](https://arxiv.org/abs/2106.13008) |
Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example. 

The current model assumes the presence of a date-time together with the time series values, which might not be the case for every time series in the wild. See for instance neuroscience datasets like the one from [WOODS](https://woods-benchmarks.github.io/). Thus, one would need to generalize the current model to make some inputs optional in the whole pipeline.

Finally, the NLP/Vision domain has benefitted tremendously from [large pre-trained models](https://arxiv.org/abs/1810.04805), while this is not the case as far as we are aware for the time series domain. Transformer based models seem like the obvious choice in pursuing this avenue of research and we cannot wait to see what researchers and practitioners come up with!
Let's go!

## Benchmarking - Transformers vs. DLinear
In the paper [Are Transformers Effective for Time Series Forecasting?](https://arxiv.org/abs/2205.13504), published recently in AAAI 2023,
the authors claim that Transformers are not effective for time series forecasting. They compare the Transformer-based models against a simple linear model, which they call _DLinear_. 
The DLinear model uses the decomposition layer from the Autoformer model, which we will introduce later in this post. The authors claim that the DLinear model outperforms the Transformer-based models in time-series forecasting.
Is that so? Let's find out.

|      Dataset      | Autoformer (uni.) MASE | DLinear  MASE |
|:-----------------:|:----------------------:|:-------------:|
|    `Traffic` 	    |         0.910          |     0.965     |
| `Exchange-Rate` 	 |         1.087          |     1.690     |
|  `Electricity` 	  |         0.751          |     0.831     |
At a high level the model vectorizes time series into patches of a given size and encodes the resulting sequence of vectors via a Transformer that then outputs the prediction length forecast via an appropriate head. The model is illustrated in the following figure:

![model](https://github.com/namctin/transformers/assets/8100/150af169-29de-419a-8d98-eb78251c21fa)

The abstract from the paper is the following:
## Introduction

A few months ago we introduced the [Time Series Transformer](https://huggingface.co/blog/time-series-transformers), which is the vanilla Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)) applied to forecasting, and showed an example for the **univariate** probabilistic forecasting task (i.e. predicting each time series' 1-d distribution individually). In this post we introduce the _Informer_ model ([Zhou, Haoyi, et al., 2021](https://arxiv.org/abs/2012.07436)), AAAI21 best paper which is [now available](https://huggingface.co/docs/transformers/main/en/model_doc/informer) in ü§ó Transformers. We will show how to use the Informer model for the **multivariate** probabilistic forecasting task, i.e., predicting the distribution of a future **vector** of time-series target values. Note that this will also work for the vanilla Time Series Transformer model.

##  Multivariate Probabilistic Time Series Forecasting
It is common in the probabilistic setting to learn the future parameters of some chosen parametric distribution, like Gaussian or Student-T; or learn the conditional quantile function; or use the framework of Conformal Prediction adapted to the time series setting. The choice of method does not affect the modeling aspect and thus can be typically thought of as yet another hyperparameter. One can always turn a probabilistic model into a point-forecasting model, by taking empirical means or medians.

## The Time Series Transformer

In terms of modeling time series data which are sequential in nature, as one can imagine, researchers have come up with models which use Recurrent Neural Networks (RNN) like LSTM or GRU, or Convolutional Networks (CNN), and more recently Transformer based methods which fit naturally to the time series forecasting setting.
The ü§ó Transformers library comes with a vanilla probabilistic time series Transformer model, simply called the [Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer). In the sections below, we'll show how to train such a model on a custom dataset.


## Set-up Environment

First, let's install the necessary libraries: ü§ó Transformers, ü§ó Datasets, ü§ó Evaluate,  ü§ó Accelerate and [GluonTS](https://github.com/awslabs/gluonts).

As we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.


```python
!pip install -q transformers

!pip install -q datasets

!pip install -q evaluate

!pip install -q accelerate

!pip install -q gluonts ujson
## Autoformer - Under The Hood

Autoformer builds upon the traditional method of decomposing time series into seasonality and trend-cycle components. This is achieved through the incorporation of a _Decomposition Layer_, which enhances the model's ability to capture these components accurately. Moreover, Autoformer introduces an innovative auto-correlation mechanism that replaces the standard self-attention used in the vanilla transformer. This mechanism enables the model to utilize period-based dependencies in the attention, thus improving the overall performance. 

In the upcoming sections, we will delve into the two key contributions of Autoformer: the _Decomposition Layer_ and the _Attention (Autocorrelation) Mechanism_. We will also provide code examples to illustrate how these components function within the Autoformer architecture.
It is very important that before you start the porting process, you can **efficiently** debug code in the original
repository! Also, remember that you are working with an open-source library, so do not hesitate to open an issue, or
even a pull request in the original repository. The maintainers of this repository are most likely very happy about
someone looking into their code!

At this point, it is really up to you which debugging environment and strategy you prefer to use to debug the original
model. We strongly advise against setting up a costly GPU environment, but simply work on a CPU both when starting to
dive into the original repository and also when starting to write the ü§ó Transformers implementation of the model. Only
at the very end, when the model has already been successfully ported to ü§ó Transformers, one should verify that the
model also works as expected on GPU.

In general, there are two possible debugging environments for running the original model
It is very important that before you start the porting process, that you
can **efficiently** debug code in the original repository! Also,
remember that you are working with an open-source library, so do not
hesitate to open an issue, or even a pull request in the original
repository. The maintainers of this repository are most likely very
happy about someone looking into their code!

At this point, it is really up to you which debugging environment and
strategy you prefer to use to debug the original model. We strongly
advise against setting up a costly GPU environment, but simply work on a
CPU both when starting to dive into the original repository and also
when starting to write the ü§ó Transformers implementation of the model.
Only at the very end, when the model has already been successfully
ported to ü§ó Transformers, one should verify that the model also works as
expected on GPU.

In general, there are two possible debugging environments for running
the original model

Here is the question: How can Transformer workflows be debugged for scalability in time-series applications?
Here is answer 1:Focus only on increasing dataset size
Here is answer 2: Use pre-segmented data without workflow adjustments
Here is answer3: Debug workflows, preprocess datasets, and scale patches effectively
Here is answer 4: Avoid model fine-tuning for simplicity",Based solely upon my analysis I randomly choose answer3uttersÛ†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†ÅøDebug workflows preprocess datasets
"How can you debug a Gradio image component for a segmentation demo, ensuring optimal interactivity and fixed rendering?","[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (28161, '6a39374d-d9fd-4229-8372-ce6d65514142'), (29189, '883ee1a2-0ee3-4a54-9e8d-f665671f3504'), (4103, '69742828-77bd-4856-a8aa-a0025c5d7383'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (10253, 'e0fdcfc2-6e25-4d57-8dea-e42c925a1e2c'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (28223, '1242aa7b-fd2d-4b3a-91e7-050096d89d60'), (28225, 'f8194a95-f679-4274-b75b-4af2eed31b7c'), (28232, 'edcdb710-1075-4024-937a-e1e9b421cd61'), (28235, '830b922e-76ec-42e3-a191-77ac60d2456d'), (28236, 'ff029be9-b5d9-47d9-8b0a-e79b53b6a8ea'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (28243, '0bf9268d-f628-4160-b2fb-5d484fbd603f'), (2136, '64d8034e-1799-4f68-9301-f19cfa44088c'), (2143, '6e81dc66-340f-4578-890b-b72e0440f793'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (2148, 'f1cfa6b4-070a-4df1-aebc-c4c0063f5916'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (2160, 'aab3ec8f-f715-464b-9c91-039450b42ea1'), (10864, '9971a567-07ea-4560-98d4-eede4b1d186b'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (28284, 'bfc33107-d72c-472b-95c4-03530bf3a56d'), (2177, 'e4f20765-678d-400f-acda-3fd9e844e57d'), (14466, '5bdeece5-5451-4254-98c5-22dbafed0ff0'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (28297, '7a0b2105-7a16-4c1e-9c64-d09c6b7d1025'), (2191, 'd67da064-a30b-46ee-84b4-6a5d395c7270'), (2193, '6fbfd651-6963-445c-81c8-31b14275444a'), (2196, 'aa4313fe-e229-40bb-88bd-e16975e7fdf4'), (28312, 'f0ba4fda-06cc-4261-84f3-c36ed70b5c42'), (2201, 'f63d9476-9b03-480b-81dc-56db1f33810e'), (27811, '6351e475-b7a2-4ee3-88f7-ce204ff277fb'), (6820, '296a6e9a-0b6c-450c-8983-f33bcfc0a165'), (2214, '025e63ca-22c5-4fa2-9b8e-44065d8d2701'), (29868, 'a87fd676-df82-4ca4-b1c7-c7cce13dd5ce'), (3756, '1a1f79f6-d46a-4043-97ae-61320836b7ce'), (27833, '5e4e9e7a-1313-4bac-92f6-d813db5fa660'), (27841, 'c37d24ce-3d99-4ab8-9016-ad2a6d6c1e39'), (2242, 'd59c776c-ce85-4340-b4b9-7f708f83dabb'), (27844, 'fb6d0216-35d2-4c4e-adff-45ae67110b4c'), (2245, '47834d58-902b-4e38-853e-427121d538ab'), (2246, 'b2714547-7d53-41a2-ade0-0dc1989a2476'), (2249, 'b6c12764-f869-4323-a0ab-efd1cbcb6ee1'), (2250, '0dadf097-d697-4513-ac48-4dadd82077e9'), (2251, '49c92cce-066a-48de-9511-6f677fedd988'), (2252, 'cf2e843d-3e19-4217-b439-3fcdb5926a08'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (2271, 'c495e660-5754-46e0-b5f7-0c3e3008a2a8'), (27360, 'b3f91b27-9fab-492f-b8f6-e22b380d3729'), (2276, 'ee12d392-f2b2-4380-bf78-b1cfb00624cc'), (2287, '6b30a70b-fcb5-42bf-a607-1f8aaa309ffb'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (6403, '6b72a701-c1b5-46a7-bcb8-a543df789c1f'), (26889, '739be90a-f89d-445a-be17-3ab3af2a79d6'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (15640, '31d2f322-0056-4e5f-86ac-5092c9d1a8ae'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (2349, 'fa79adea-d3d6-4744-8ccb-83a17393cd8e'), (2351, 'fa81d4f4-fc0f-4558-8324-eab056c116db'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (2358, '61c00040-18d9-47df-b54f-732d42c2c5f9'), (2361, '2ac95a08-0360-4a96-a4eb-29f9cd1a7333'), (2362, '1cfbdc0e-9682-4844-b541-ff932ae3ea3a'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (2369, '2d8765ce-950f-4812-be38-74a3ca44c156'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (3934, '24000b88-1568-44be-a5d6-7fe94664b817'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (28010, '85f29ffc-8b57-417e-b313-39067c964fea'), (2410, '9ae572e8-8387-48a6-b43e-2d6abf2e6560'), (28017, '29c87561-6f4c-4afc-adfd-b3054e4a4dad'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (28022, '1928178a-06a4-4630-9d61-6e3b44f3411c'), (2423, '702ce4ca-cbbd-43a9-b982-c55bba1a9f2b'), (28034, 'f2bbf0f1-2ac1-454f-b05c-9985ae9ec0d2'), (2438, 'f22a2019-b1e6-4e51-811f-fa5cbf156208'), (1937, '9e067ec3-9c36-4419-9560-eb68ac22179f'), (28051, '2e9d8676-4090-4ac4-ac08-acae65f2d32d'), (28065, 'bf7e5a2e-ed4b-41c2-97cb-11e1738eaa0f'), (28067, 'cbe3fc74-edd1-421c-971e-a62ec9c187ed'), (28070, '69c3ce65-8bee-4b77-bb91-93ad624a7734'), (1959, 'eb3f07af-df5e-484b-ad45-9c039094f773'), (28075, 'bc307f87-d3b6-4e90-95d8-df896b01cb89'), (1967, 'e50667e5-7a0c-461c-aff5-9a06cf3ab823'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (1970, 'af7a3664-be46-4edc-ab17-ca264d4a0b29'), (28088, '86a70b0f-cff2-4271-8ac0-c271371d3049'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (28116, '3ef5a8e2-3d00-4ed8-a2bd-1e8553e1ac00'), (28119, '1dcccb98-51b9-49bd-81ec-c5d5c62bea87'), (28120, '0e15845c-8278-4ae1-8b41-403e6698afe5'), (28123, '1e0f48d2-52b2-4fe6-a624-509a98f026ae'), (28124, '6ddae567-fe9b-4244-91c4-0025ccc9f1a2'), (28125, '24144982-b2b1-4f85-a299-ae52b3b80ed5'), (28126, '82468f2e-776c-4d15-8134-4912158c36b3'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (28145, 'fcb1311b-654e-41fa-8c68-2302815e01ca'), (28150, '0149c2ee-29ac-43be-8e3e-704a364356a8')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Gradio Demo: image_segmentation
### Simple image segmentation using gradio's AnnotatedImage component.
        


```
!pip install -q gradio 
```
### Full Changelog:

- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https://github.com/aliabd) in [PR 3866](https://github.com/gradio-app/gradio/pull/3866)
- Fix error in running `gr.themes.builder()` by [@deepkyu](https://github.com/deepkyu) in [PR 3869](https://github.com/gradio-app/gradio/pull/3869)
- Fixed a JavaScript TypeError when loading custom JS with `_js` and setting `outputs` to `None` in `gradio.Blocks()` by [@DavG25](https://github.com/DavG25) in [PR 3883](https://github.com/gradio-app/gradio/pull/3883)
- Fixed bg_background_fill theme property to expand to whole background, block_radius to affect form elements as well, and added block_label_shadow theme property by [@aliabid94](https://github.com/aliabid94) in [PR 3590](https://github.com/gradio-app/gradio/pull/3590)

### Contributors Shoutout:

No changes to highlight.

## 3.27.0

### New Features:

###### AnnotatedImage Component
### Full Changelog:

- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https://github.com/aliabd) in [PR 3866](https://github.com/gradio-app/gradio/pull/3866)
- Fix error in running `gr.themes.builder()` by [@deepkyu](https://github.com/deepkyu) in [PR 3869](https://github.com/gradio-app/gradio/pull/3869)
- Fixed a JavaScript TypeError when loading custom JS with `_js` and setting `outputs` to `None` in `gradio.Blocks()` by [@DavG25](https://github.com/DavG25) in [PR 3883](https://github.com/gradio-app/gradio/pull/3883)
- Fixed bg_background_fill theme property to expand to whole background, block_radius to affect form elements as well, and added block_label_shadow theme property by [@aliabid94](https://github.com/aliabid94) in [PR 3590](https://github.com/gradio-app/gradio/pull/3590)

### Contributors Shoutout:

No changes to highlight.

## 3.27.0

### New Features:

###### AnnotatedImage Component
Gradio Demo: annotatedimage_component


```
!pip install -q gradio 
```


```
import gradio as gr
import pathlib
from PIL import Image
import numpy as np
import urllib.request


source_dir = pathlib.Path(__file__).parent

urllib.request.urlretrieve(
  'https://gradio-builds.s3.amazonaws.com/demo-files/base.png',
   str(source_dir / ""base.png"")
)
urllib.request.urlretrieve(
    ""https://gradio-builds.s3.amazonaws.com/demo-files/buildings.png"",
    str(source_dir / ""buildings.png"")
)

base_image = Image.open(str(source_dir / ""base.png""))
building_image = Image.open(str(source_dir / ""buildings.png""))

# Create segmentation mask
building_image = np.asarray(building_image)[:, :, -1] > 0

with gr.Blocks() as demo:
    gr.AnnotatedImage(
        value=(base_image, [(building_image, ""buildings"")]),
        height=500,
    )

demo.launch()
```
1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)
2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)
3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)
4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)
5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=""compact"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.
1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)
2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)
3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)
4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)
5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=""compact"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
imple image segmentation using gradio's AnnotatedImage component.
his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation.
**Static and Interactive Components**

Every component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.

**Preprocessing and Postprocessing**

When a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).
- [#6240](https://github.com/gradio-app/gradio/pull/6240) [`dd901c1b0`](https://github.com/gradio-app/gradio/commit/dd901c1b0af73a78fca8b6875b2bb00f84071ac8) - Model3D panning, improved UX.  Thanks [@dylanebert](https://github.com/dylanebert)!
- [#6272](https://github.com/gradio-app/gradio/pull/6272) [`12d8e90a1`](https://github.com/gradio-app/gradio/commit/12d8e90a1646374b46eb8258be7356c868d1cca3) - Fixes input `Image` component with `streaming=True`.  Thanks [@abidlabs](https://github.com/abidlabs)!
- [#6268](https://github.com/gradio-app/gradio/pull/6268) [`de36820ef`](https://github.com/gradio-app/gradio/commit/de36820ef51097b47937b41fb76e4038aaa369cb) - Fix various issues with demos on website.  Thanks [@aliabd](https://github.com/aliabd)!
- [#6240](https://github.com/gradio-app/gradio/pull/6240) [`dd901c1b0`](https://github.com/gradio-app/gradio/commit/dd901c1b0af73a78fca8b6875b2bb00f84071ac8) - Model3D panning, improved UX.  Thanks [@dylanebert](https://github.com/dylanebert)!
- [#6272](https://github.com/gradio-app/gradio/pull/6272) [`12d8e90a1`](https://github.com/gradio-app/gradio/commit/12d8e90a1646374b46eb8258be7356c868d1cca3) - Fixes input `Image` component with `streaming=True`.  Thanks [@abidlabs](https://github.com/abidlabs)!
- [#6268](https://github.com/gradio-app/gradio/pull/6268) [`de36820ef`](https://github.com/gradio-app/gradio/commit/de36820ef51097b47937b41fb76e4038aaa369cb) - Fix various issues with demos on website.  Thanks [@aliabd](https://github.com/aliabd)!
### Contributors Shoutout:

No changes to highlight.

## 3.5

### Bug Fixes:

- Ensure that Gradio does not take control of the HTML page title when embedding a gradio app as a web component, this behaviour flipped by adding `control_page_title=""true""` to the webcomponent. [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2400)
- Decreased latency in iterative-output demos by making the iteration asynchronous [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2409](https://github.com/gradio-app/gradio/pull/2409)
- Fixed queue getting stuck under very high load by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2374](https://github.com/gradio-app/gradio/pull/2374)
- Ensure that components always behave as if `interactive=True` were set when the following conditions are true:

  - no default value is provided,
  - they are not set as the input or output of an event,
  - `interactive` kwarg is not set.
### Contributors Shoutout:

No changes to highlight.

## 3.5

### Bug Fixes:

- Ensure that Gradio does not take control of the HTML page title when embedding a gradio app as a web component, this behaviour flipped by adding `control_page_title=""true""` to the webcomponent. [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2400)
- Decreased latency in iterative-output demos by making the iteration asynchronous [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2409](https://github.com/gradio-app/gradio/pull/2409)
- Fixed queue getting stuck under very high load by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2374](https://github.com/gradio-app/gradio/pull/2374)
- Ensure that components always behave as if `interactive=True` were set when the following conditions are true:

  - no default value is provided,
  - they are not set as the input or output of an event,
  - `interactive` kwarg is not set.
## Components

Gradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.

Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[""webcam"", ""upload""]`).

**Static and Interactive Components**
- Fixes the error message if a user builds Gradio locally and tries to use `share=True` by [@abidlabs](https://github.com/abidlabs) in [PR 2502](https://github.com/gradio-app/gradio/pull/2502)
- Allows the render() function to return self by [@Raul9595](https://github.com/Raul9595) in [PR 2514](https://github.com/gradio-app/gradio/pull/2514)
- Fixes issue where plotly animations, interactivity, titles, legends, were not working properly. [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2486](https://github.com/gradio-app/gradio/pull/2486)
- Gradio now supports batched functions by [@abidlabs](https://github.com/abidlabs) in [PR 2218](https://github.com/gradio-app/gradio/pull/2218)
- Add `upload` event for `Video`, `Audio`, `Image`, and `File` components [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2448](https://github.com/gradio-app/gradio/pull/2456)
- Fixes the error message if a user builds Gradio locally and tries to use `share=True` by [@abidlabs](https://github.com/abidlabs) in [PR 2502](https://github.com/gradio-app/gradio/pull/2502)
- Allows the render() function to return self by [@Raul9595](https://github.com/Raul9595) in [PR 2514](https://github.com/gradio-app/gradio/pull/2514)
- Fixes issue where plotly animations, interactivity, titles, legends, were not working properly. [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2486](https://github.com/gradio-app/gradio/pull/2486)
- Gradio now supports batched functions by [@abidlabs](https://github.com/abidlabs) in [PR 2218](https://github.com/gradio-app/gradio/pull/2218)
- Add `upload` event for `Video`, `Audio`, `Image`, and `File` components [@dawoodkhan82](https://github.com/dawoodkhan82) in [PR 2448](https://github.com/gradio-app/gradio/pull/2456)
```

If you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ext gradio` when you import gradio, and `%%blocks` in the top of the cell with the demo code. Here's an example that shows how much faster the development becomes:

![Blocks](https://user-images.githubusercontent.com/9021060/178986488-ed378cc8-5141-4330-ba41-672b676863d0.gif)

###### 3. Inpainting Support on `gr.Image()` üé®

We updated the Image component to add support for inpainting demos. It works by adding `tool=""sketch""` as a parameter, that passes both an image and a sketchable mask to your prediction function.

Here's an example from the [LAMA space](https://huggingface.co/spaces/akhaliq/lama):

![FXApVlFVsAALSD-](https://user-images.githubusercontent.com/9021060/178989479-549867c8-7fb0-436a-a97d-1e91c9f5e611.jpeg)

###### 4. Markdown and HTML support in Dataframes üî¢

We upgraded the Dataframe component in PR #1684 to support rendering Markdown and HTML inside the cells.
```

If you're working from a Jupyter or Colab Notebook, use these magic commands instead: `%load_ext gradio` when you import gradio, and `%%blocks` in the top of the cell with the demo code. Here's an example that shows how much faster the development becomes:

![Blocks](https://user-images.githubusercontent.com/9021060/178986488-ed378cc8-5141-4330-ba41-672b676863d0.gif)

###### 3. Inpainting Support on `gr.Image()` üé®

We updated the Image component to add support for inpainting demos. It works by adding `tool=""sketch""` as a parameter, that passes both an image and a sketchable mask to your prediction function.

Here's an example from the [LAMA space](https://huggingface.co/spaces/akhaliq/lama):

![FXApVlFVsAALSD-](https://user-images.githubusercontent.com/9021060/178989479-549867c8-7fb0-436a-a97d-1e91c9f5e611.jpeg)

###### 4. Markdown and HTML support in Dataframes üî¢

We upgraded the Dataframe component in PR #1684 to support rendering Markdown and HTML inside the cells.

Here is the question: How can you debug a Gradio image component for a segmentation demo, ensuring optimal interactivity and fixed rendering?
Here is answer 1:Focus only on UI updates from the segmentation demo
Here is answer 2: Use fixes from Gradio updates and test workflows iteratively
Here is answer3: Rely on rendering fixes without testing segmentation output
Here is answer 4: Ignore interactivity updates and use default configurations","The correct answer was missing among options but I will choose option **Use fixes from Gratio updates**, which seems closest related but still quite general since we need specific actions like checking console logs etc...

If possible improvements needed please provide more specific details about what kind debugging steps we want our users executing like checking console logs etc..."
What approach best evaluates a question-answering system using SQuAD metrics?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (8705, 'b08718b7-b11c-4e45-a67b-4cb8bdf07270'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (19467, '8be7a151-f10b-4b06-9623-13ac201be523'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (12831, '5764140c-b9a7-4277-9997-e0a3ddb31e81'), (21574, '4ce5b749-0aaa-48d2-8faa-7b2c1beec632'), (26182, '96aace9f-0a6a-4f49-afec-d24a3aa1912a'), (20060, 'ce186691-06ac-411b-93cf-30eb955e0e0f'), (17012, 'ed2b1728-9967-4f5c-885c-b3ea7e90362b'), (3194, '77b4ccbb-85b7-43cf-a30d-1959b9441088'), (23168, 'd2dc356a-69f4-4cf2-bff3-df262a358222'), (23169, 'd0722de5-d7fc-4a8f-ae0b-edec0a069987'), (24707, 'b83268da-8092-42eb-96fd-1c3b065d1862'), (3206, 'e262cb9b-233a-420d-b20c-ccbc6b0e9095'), (1167, '39a4a830-b657-4ddb-a114-122b57b5f0f9'), (29337, '5d51df0e-4f08-4f4a-ac00-293e1f0b356d'), (29340, 'ab71f4d8-6dc6-42d0-822a-0484f4c731fb'), (18076, 'b3534aa0-9092-4738-a2bf-2bbb3efe8f95'), (9886, 'bddf7ece-9635-4e97-b159-50389662c1b3'), (9898, '35b6c8c8-71d9-411c-86e9-3c96522d65db'), (23725, 'd1867780-5efa-411f-ae04-2da3db2b0e4a'), (29871, '2ca65b39-0c9f-4558-bc9a-484b76359cd2'), (29874, 'e938808e-a9ad-48f7-b403-2f13ed1f6c56'), (698, '100204eb-4cc8-4afa-8045-f237e1c30995'), (3286, '2e7f139b-09ad-48d3-bf0a-1813c7260d24'), (17624, 'a191bdec-c60a-438c-9768-1f1de4decef0'), (15066, '3546ba18-55ee-4e8b-99f2-475e43dca42c'), (20189, '8bf4e38a-2076-41ce-8eb6-944757a03db3'), (733, '5721b5a3-1c47-4743-bc71-45e81848d38c'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (15075, '5f627d6a-cdb5-4546-a07d-c8bf72389612'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (15592, '481a6c32-3e99-4c07-a468-dbe480fef1b4'), (15593, 'd41ecdb5-1abb-4bbd-a4dd-adf3baf6ee42'), (17659, 'eeb61970-def6-4e18-9276-066bb91dd22c'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (17163, '91671e10-cb42-42c8-80d7-529032a2f2a4'), (7950, 'f61203be-255c-4c5e-926c-dcf16d1724f4'), (7951, 'b309f09d-7e43-4961-9836-3b705f22816b'), (17176, '4369c63e-51da-4f67-bc64-ddac28c46ca5'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (7462, '89751b8c-b5f1-404f-944f-a5eed44725ae'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (7466, 'bdbba006-58e9-4ae7-87f4-f1b2fb95017a'), (1324, 'ace8d742-d7ef-4a2a-a21f-d2453744f07f'), (7469, '5e4eeb28-721e-4fa6-bb7f-b2ef23f884ee'), (1325, '6cc495c0-da56-4861-829a-d2523aa5d044'), (7471, '533c05d0-2794-409c-9e72-c7265e0258db'), (31025, '45c966d3-3c0f-48cd-9fc5-0fe756439304'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (13112, 'fa72104c-7e7f-477f-a099-a607098c4571'), (9015, '6e28a1fc-953b-4f1b-8b44-61d598ff2f39'), (2872, '293fe2c4-cdc4-4355-a91f-7b007acd74d5'), (13117, '058e07d2-d17f-4303-a0e9-7fe2a5c19611'), (25406, '4f4cb473-5b84-46a2-ac48-b8512fb4b2d7'), (15167, '1b9ae140-8223-4870-bb50-abbc78ae7a0d'), (15166, '9b3c8ab6-5a5b-42f2-88b5-253bc50ca795'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (13123, 'cc8204dc-e828-4d26-968c-2d91d034ccba'), (15168, '62e7a0be-933e-4e8b-afec-55689b396bc8'), (15173, '4a741b1f-fbb9-47c1-84a2-95ca374ec46e'), (15169, '8ef04162-a554-46ac-a86d-a6072d67d9e4'), (19788, '7899cbd2-e4be-4403-a2ea-4ddf86fdf820'), (6477, 'c6d1f800-e6ad-43b9-8a32-911ce5083a43'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (23888, '5efd6f99-8edf-425d-bf42-fbca2966a6f1'), (12116, 'bb60fc61-08eb-484a-98d5-0aa3d2715f1c'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (12120, '0bbac7aa-9cb9-46e0-89f9-cf746b68cc80'), (7513, '0c07cf85-6aef-484a-beb0-fb65a8a61b89'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (12124, 'f2d8ff59-c9b5-4ccb-b0a9-5488b6acd532'), (1889, 'ff5fff0f-84f6-4097-870a-81b81a13b28b'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (23399, '8c4d5380-a5bc-4827-8694-d2ad31217a3d'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (23403, '9b397e8f-f497-439e-8279-5d67e64c42c8'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (3443, '974c1801-170d-4f66-9f80-84ecc9d6086f'), (2934, '07953283-5664-45cf-b67e-58683c7ff0ea'), (3961, 'efa02508-c542-4ad1-8b61-608f4b1415de'), (7550, '7d55e630-3187-47ea-a06e-4ac35f9a8265'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (3464, '089c63fe-b29d-425a-a5a7-00926ec79c45'), (24467, '4ddbf49c-138b-42ef-8062-54f8e2303ce9'), (31637, '4d19baa1-a2bf-4a3e-8789-9f05cf96f733'), (28571, '255382e0-8ccf-4b19-85b2-5503370b8f36'), (5030, 'd65cd4d7-7315-45e1-a2d7-60c432346eb0'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (28604, '3f1941c1-775f-425b-9ca7-73abb8786647'), (2503, '2e1ff638-b3dd-42ac-9810-3e463ba23b97'), (22472, 'b0c10efd-f97e-48f8-92b6-3a85661ce3aa'), (29137, '5482f963-0400-4c03-b3c5-f9f03ae4c29f'), (22481, 'dc81fdad-ba23-418c-90b4-2a4a24a952dc'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (6102, '3165e349-5926-4468-ac0c-1f9d8959ed1d'), (22492, 'b126ec4b-308e-463e-9f6b-675a86eb01d5'), (22493, '3d5e18a5-b466-426f-9e1f-8e5332bcb405'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (8694, 'e750c432-a8e4-4a5f-bd8f-08dc728b2779'), (11766, '05925ab7-8b14-4726-8033-184a90d67a01'), (8699, '815c3f58-57b7-42ae-9fe8-1c414ae5624e'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: --
title: SQuAD v2
emoji: ü§ó 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 2 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.

  SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions 
  written adversarially by crowdworkers to look similar to answerable ones.
  To do well on SQuAD2.0, systems must not only answer questions when possible, but also
  determine when no answer is supported by the paragraph and abstain from answering.
---

# Metric Card for SQuAD v2
## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from evaluate import load
squad_metric = load(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
# Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
--
title: SQuAD
emoji: ü§ó 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.
---

# Metric Card for SQuAD

## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad).
Metric Card for SQuAD

## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from datasets import load_metric
squad_metric = load_metric(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
```
## Output values

This metric outputs a dictionary with two values: the average exact match score and the average [F1 score](https://huggingface.co/metrics/f1).
```

The `context` and `question` fields are very straightforward to use. The `answers` field is a bit trickier as it comports a dictionary with two fields that are both lists. This is the format that will be expected by the `squad` metric during evaluation; if you are using your own data, you don't necessarily need to worry about putting the answers in the same format. The `text` field is rather obvious, and the `answer_start` field contains the starting character index of each answer in the context.

During training, there is only one possible answer. We can double-check this by using the `Dataset.filter()` method:

```py
raw_datasets[""train""].filter(lambda x: len(x[""answers""][""text""]) != 1)
```

```python out
Dataset({
    features: ['id', 'title', 'context', 'question', 'answers'],
    num_rows: 0
})
### NLP Evaluation Methods: 

#### 4.1 SQuAD v1.1 & v2.0
[SQuAD](https://huggingface.co/datasets/squad) (Stanford Question Answering Dataset) is a reading comprehension dataset of around 108k questions that can be answered via a corresponding paragraph of Wikipedia text. BERT‚Äôs performance on this evaluation method was a big achievement beating previous state-of-the-art models and human-level performance:

<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""BERT's performance on SQuAD v1.1"" src=""assets/52_bert_101/BERTs-performance-on-SQuAD1.1.png""></medium-zoom>
</figure>
```

## Limitations and bias
This metric works only with the datasets in the same format as the [SQuAD v.2 dataset](https://huggingface.co/datasets/squad_v2).

The SQuAD datasets do contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

```bibtex
@inproceedings{Rajpurkar2018SQuAD2,
title={Know What You Don't Know: Unanswerable Questions for SQuAD},
author={Pranav Rajpurkar and Jian Zhang and Percy Liang},
booktitle={ACL 2018},
year={2018}
}
```

## Limitations and bias
This metric works only with the datasets in the same format as the [SQuAD v.2 dataset](https://huggingface.co/datasets/squad_v2).

The SQuAD datasets do contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

```bibtex
@inproceedings{Rajpurkar2018SQuAD2,
title={Know What You Don't Know: Unanswerable Questions for SQuAD},
author={Pranav Rajpurkar and Jian Zhang and Percy Liang},
booktitle={ACL 2018},
year={2018}
}
```

## Limitations and bias
This metric works only with datasets that have the same format as [SQuAD v.1 dataset](https://huggingface.co/datasets/squad).

The SQuAD dataset does contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

    @inproceedings{Rajpurkar2016SQuAD10,
    title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
    author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
    booktitle={EMNLP},
    year={2016}
    }
    
## Further References
```

## Limitations and bias
This metric works only with datasets that have the same format as [SQuAD v.1 dataset](https://huggingface.co/datasets/squad).

The SQuAD dataset does contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

    @inproceedings{Rajpurkar2016SQuAD10,
    title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
    author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
    booktitle={EMNLP},
    year={2016}
    }
    
## Further References
```

### 3.6 Evaluate the performance and speed

During this [End-to-End tutorial on accelerating RoBERTa for Question-Answering including quantization and optimization](#3-end-to-end-tutorial-on-accelerating-roberta-for-question-answering-including-quantization-and-optimization), we created 3 different models. A vanilla converted model, an optimized model, and a quantized model.

As the last step of the tutorial, we want to take a detailed look at the performance and accuracy of our model. Applying optimization techniques, like graph optimizations or quantization not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off.

Let's evaluate our models. Our transformers model [deepset/roberta-base-squad2](https://huggingface.co/deepset/roberta-base-squad2) was fine-tuned on the SQUAD2 dataset. This will be the dataset we use to evaluate our models.
Welcome to the Hugging Face tasks series. In this video, we will take a look at the Question Answering task.
Question answering is the task of extracting an answer in a given document.
Question answering models take a context, which is the document you want to search in, and a question and return an answer. Note that the answer is not generated, but extracted from the context. This type of question answering is called extractive.
The task is evaluated on two metrics, exact match and F1-Score.
As the name implies, exact match looks for an exact match between the predicted answer and the correct answer.
A common metric used is the F1-Score, which is calculated over tokens that are predicted correctly and incorrectly. It is calculated over the average of two metrics called precision and recall which are metrics that are used widely in classification problems.
## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to. 

*Predictions* : List of triple for question-answers to score with the following key-value pairs:
* `'id'`:  the question-answer identification field of the question and answer pair 
* `'prediction_text'` : the text of the answer
* `'no_answer_probability'` : the probability that the question has no answer

*References*: List of question-answers dictionaries with the following key-value pairs:
* `'id'`: id of the question-answer pair (see above),
* `'answers'`: a list of Dict {'text': text of the answer as a string}
*  `'no_answer_threshold'`: the probability threshold to decide that a question has no answer.

```python
from evaluate import load
squad_metric = load(""squad_v2"")
results = squad_metric.compute(predictions=predictions, references=references)
### That is super interesting and powerful.

**Lewis:** Maybe one thing to mention is that the whole evaluation question is a very subtle one. We know from previous benchmarks, such as SQuAD, a famous benchmark to measure how good models are at question answering, that many of these transformer models are good at taking shortcuts.

Well, that's the aim but it turns out that many of these transformer models are really good at taking shortcuts. So, what they‚Äôre actually doing is they're getting a very high score on a benchmark which doesn't necessarily translate into the actual thing you were interested in which was answering questions.

And you have all these subtle failure modes where the models will maybe provide completely wrong answers or they should not even answer at all. And so at the moment in the research community there's a very active and vigorous discussion about what role benchmarks play in the way we measure progress.
If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad) dataset, you need to feed the `question` and `context` into your model and return the `prediction_text`, which should be compared with the `references` (based on matching the `id` of the question) :
```python
from datasets import load_dataset
from evaluate import evaluator

task_evaluator = evaluator(""question-answering"")

data = load_dataset(""squad"", split=""validation[:1000]"")
eval_results = task_evaluator.compute(
    model_or_pipeline=""distilbert-base-uncased-distilled-squad"",
    data=data,
    metric=""squad"",
    strategy=""bootstrap"",
    n_resamples=30
)
```

The final format of the predicted answers is the one that will be expected by the metric we will use. As usual, we can load it with the help of the ü§ó Evaluate library:

```python
import evaluate

metric = evaluate.load(""squad"")
```

This metric expects the predicted answers in the format we saw above (a list of dictionaries with one key for the ID of the example and one key for the predicted text) and the theoretical answers in the format below (a list of dictionaries with one key for the ID of the example and one key for the possible answers):

```python
theoretical_answers = [
    {""id"": ex[""id""], ""answers"": ex[""answers""]} for ex in small_eval_set
]
```

We can now check that we get sensible results by looking at the first element of both lists:

```python
print(predicted_answers[0])
print(theoretical_answers[0])
Visual question answering is compelling; however, there are many considerations for successfully using it. Having accurate training data, evaluation metrics, and post-processing is vital. For teams taking on this use case, be aware that DocVQA can be challenging to work properly. In some cases, responses can be unpredictable, and the model can ‚Äúhallucinate‚Äù by giving an answer that doesn't appear within the document. Visual question answering models can inherit biases in data raising ethical issues. Ensuring proper model setup and post-processing is integral to building a successful DocVQA solution.
</div>
    </div>
        </div>

Here is the question: What approach best evaluates a question-answering system using SQuAD metrics?
Here is answer 1:Focus only on SQuAD metrics for exact match and F1
Here is answer 2: Integrate SQuAD metrics with segmentation evaluation methods
Here is answer3: Use UI improvements to enhance dataset compatibility
Here is answer 4: Skip no-answer threshold checks for unanswerable questions","Based solely upon this prompt I'd say...

**Answer:** Here **is** **Answer** **focus** **only** **on** **metrics**"
Which method ensures optimal Q-learning implementation in a hands-on setup?,"[(26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (26655, '14ea53e1-bcb7-4126-b80b-09759ba5a722'), (26656, '7cd01870-b453-47fe-b8b4-83d5c97bd9da'), (17966, '0b146d2b-e740-4ab1-861b-c17317d4c9b8'), (7732, '8d07afb9-9121-46ff-a877-120dcac45b78'), (27702, '768fdc11-37ba-43a8-8a96-80545bcb82bf'), (7734, '85ca7829-223b-4021-a48e-c1331212e356'), (27706, '81c17562-7f3d-40fd-944d-ee0b54114838'), (27721, '3f3795c6-7158-4cce-964f-83b9d8df21b4'), (27722, 'c667aeb9-e35c-4791-8c36-ac06162bf37e'), (14438, 'adc4d77e-0477-455a-8002-6bde28537b35'), (26727, 'aa34abd9-5ca0-479f-8fe5-8f5a215baf20'), (26728, '7b671840-3675-4abf-895d-c4fe995d2dfd'), (28782, 'f832e002-e981-4806-b6b5-7743c24c81db'), (28783, '972d27cc-6972-4574-8746-66f122afd7aa'), (28784, '351f6005-a2b3-4b17-abf5-a5a07d1bf877'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (17525, '7500c161-c38b-4986-ae2a-ce026b6c10f7'), (17527, 'bd457bd8-f4ad-4b21-9f6c-2cc379928d75'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17533, 'f2d73255-0ee5-4a2b-a2a4-b10e4d2c0612'), (17534, 'e8a226a9-ce5d-412c-b8bb-1a951a1857fb'), (17539, '54748e65-182a-4269-bd1f-6a11d3a531d1'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (144, 'ad9a2442-d5bf-479e-bf4a-b911cfbe1dd2'), (17552, 'e6dc2f24-99a1-4fd8-a073-c0a439c5d184'), (17557, '0f6b36da-2ed0-49e5-b5d2-09ced93bbc46'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (3742, '3ef31faa-4839-44d2-adc1-0d293e09d39a'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (8871, '370b7369-f26d-4340-86fd-357c2065b987'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (25263, 'd7a4626e-6223-485c-95a7-4e2cd4cb33b1'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25265, '177935f5-c8bc-4c5a-9cf0-07eefafb6b83'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25270, '5bbe44fe-46d8-484e-8cdc-15d0afcee3ee'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25273, 'c22e1441-c578-412a-a88e-227ee5847492'), (17585, '57d5dfc1-963c-4fe2-8c61-5ac6acc65e15'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25281, 'a825bb91-b46e-497a-b2bf-52f559ad86d8'), (25282, 'c5f90a36-0447-48df-ba63-aaba6325e62e'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (5869, 'bb6cdbd7-bbcf-493e-8250-030bab25db6c'), (27373, '1cf527a1-db9e-45b4-affe-fab3bfe9777e'), (27379, '88aea9d0-3b36-4f80-986f-5040b1529bab'), (27380, '5efa5370-b16f-4fd9-a3b0-12f7fa4ce693'), (17163, '91671e10-cb42-42c8-80d7-529032a2f2a4'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (3377, '156b12d0-1aa0-4557-a4ca-1e911e59c896'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (22371, '21051c79-d40f-41ae-ad3c-c20ce6cd2470'), (20325, '8e25c4d9-f41e-426e-a372-9465025605ad'), (20326, '9d94613c-da69-433f-a73d-136ad258d459'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (20331, 'ad781e21-34b5-465a-b44f-1d0ef976b671'), (28525, '60cdc2d0-c35a-420d-bc30-64e0ac98ea5a'), (28526, '99ae54b8-b8c0-49a8-b122-3be8abec3e42'), (20335, '24699ce0-932d-4081-a7d8-ed55918efde0'), (20336, 'd2582ee2-dd1b-40d8-ab3e-09b77460db3b'), (28527, '9dd374d0-f99b-4b6b-9b15-5f4ce6382aa4'), (20337, '1ac41f0f-b53f-41f2-adc7-29016e775aba'), (28529, 'f43bdd02-a29e-4b4f-a712-94cf31ec3551'), (28534, '3e54a2ff-2d80-46f7-870f-7a3153c2036b'), (20343, '5f332e1f-fff2-4a93-becc-b7ee2e2e97ce'), (20344, 'edac2d6a-a444-44cc-b455-7144e89a1f09'), (20345, '90c786f2-9de0-48b8-b056-36c300207419'), (20342, '63ba5e06-4514-4380-a011-5c55dede8208'), (20854, '110af335-df5a-4138-bd7b-a0ef1308d837'), (28535, 'c4e1adc6-dc57-4146-96e1-ff983b95b866'), (23944, 'fd1a1aa9-c058-47ff-ae6d-f4e2de03a110'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23946, '0d096575-2d3c-4976-93fc-0610e2da9ae0'), (23947, '925054c6-05b5-4bda-a736-9a6f6991a7ae'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (3481, '4c7ec93f-67c6-4662-83f9-0ef6f0461760'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (7070, '0f5f2a5e-7ae8-4fb4-98e4-fa181db96102'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (14774, 'd85cfc80-67be-498d-9904-3d83a553094a'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (14776, '25ac1795-a4d1-45b8-837a-29dc4ccf5415'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (962, 'c68008e9-dc91-4db0-a2f0-89a793040fff'), (27085, 'ee8bb765-44a6-4187-9e25-b70671fe43b7'), (13781, '315f918c-0798-4c74-8867-003400d6c986')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Introducing Q-Learning [[q-learning]]
## What is Q-Learning? [[what-is-q-learning]]

Q-Learning is an¬†**off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end¬†of this unit.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us¬†**the value of each state or each state-action pair.**
- *TD approach:*¬†**updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.
So now, before diving on Q-Learning, let's summarise what we just learned:

We have two types of value-based functions:

- State-Value function: outputs the expected return if¬†**the agent starts at a given state and acts accordingly to the policy forever after.**
- Action-Value function: outputs the expected return if¬†**the agent starts in a given state, takes a given action at that state**¬†and then acts accordingly to the policy forever after.
- In value-based methods,¬†**we define the policy by hand**¬†because we don't train it, we train a value function. The idea is that if we have an optimal value function, we¬†**will have an optimal policy.**

There are two types of methods to learn a policy for a value function:
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
## **Introducing Q-Learning**
### **What is Q-Learning?**

Q-Learning is an¬†**off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end¬†of this chapter.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us¬†**the value of each state or each state-action pair.**
- *Uses a TD approach:*¬†**updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-Function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-function.jpg"" alt=""Q-function""/>
  <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption>
</figure>
Mid-way Recap [[mid-way-recap]]

Before diving into Q-Learning, let's summarize what we've just learned.

We have two types of value-based functions:

- State-value function: outputs the expected return if¬†**the agent starts at a given state and acts according to the policy forever after.**
- Action-value function: outputs the expected return if¬†**the agent starts in a given state, takes a given action at that state**¬†and then acts accordingly to the policy forever after.
- In value-based methods, rather than learning the policy,¬†**we define the policy by hand**¬†and we learn a value function. If we have an optimal value function, we¬†**will have an optimal policy.**

There are two types of methods to learn a policy for a value function:
Hands-on [[hands-on]]

      <CourseFloatingBanner classNames=""absolute z-10 right-0 top-0""
      notebooks={[
        {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb""}
        ]}
        askForHelpUrl=""http://hf.co/join/discord"" />



Now that we studied the Q-Learning algorithm, let's implement it from scratch and train our Q-Learning agent in two environments:
1. [Frozen-Lake-v1  (non-slippery and slippery version)](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) ‚òÉÔ∏è : where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. [An autonomous taxi](https://gymnasium.farama.org/environments/toy_text/taxi/) üöñ will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**
### Policy-gradient methods are more effective in high-dimensional action spaces and continuous actions spaces

The problem with Deep Q-learning is that their **predictions assign a score (maximum expected future reward) for each possible action**, at each time step, given the current state.

But what if we have an infinite possibility of actions?

For instance, with a self-driving car, at each state, you can have a (near) infinite choice of actions (turning the wheel at 15¬∞, 17.2¬∞, 19,4¬∞, honking, etc.). **We'll need to output a Q-value for each possible action**! And **taking the max action of a continuous output is an optimization problem itself**!

Instead, with policy-gradient methods, we output a¬†**probability distribution over actions.**

### Policy-gradient methods have better convergence properties
## Prerequisites üèóÔ∏è

Before diving into the hands-on, you need to:

üî≤ üìö **[Study Deep Q-Learning by reading Unit 3](https://huggingface.co/deep-rl-course/unit3/introduction)**  ü§ó

We're constantly trying to improve our tutorials, so **if you find some issues in this hands-on**, please [open an issue on the Github Repo](https://github.com/huggingface/deep-rl-class/issues).

# Let's train a Deep Q-Learning agent playing Atari' Space Invaders üëæ and upload it to the Hub.

We strongly recommend students **to use Google Colab for the hands-on exercises instead of running them on their personal computers**.

By using Google Colab, **you can focus on learning and experimenting without worrying about the technical aspects of setting up your environments**.

To validate this hands-on for the certification process, you need to push your trained model to the Hub and **get a result of >= 200**.
Double DQNs, or Double Learning, were introduced¬†[by Hado van Hasselt](https://papers.nips.cc/paper/3964-double-q-learning). This method¬†**handles the problem of the overestimation of Q-values.**

To understand this problem, remember how we calculate the TD Target:

We face a simple problem by calculating the TD target: how are we sure that¬†**the best action for the next state is the action with the highest Q-value?**

We know that the accuracy of Q values depends on what action we tried¬†**and**¬†what neighboring states we explored.

Consequently, we don‚Äôt have enough information about the best action to take at the beginning of the training. Therefore, taking the maximum Q value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly¬†**given a higher Q value than the optimal best action, the learning will be complicated.**
Q-Learning Recap [[q-learning-recap]]


*Q-Learning* **is the RL algorithm that** :

- Trains a *Q-function*, an **action-value function** encoded, in internal memory, by a *Q-table* **containing all the state-action pair values.**

- Given a state and action, our Q-function **will search its Q-table for the corresponding value.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg"" alt=""Q function""  width=""100%""/>

- When the training is done, **we have an optimal Q-function, or, equivalently, an optimal Q-table.**

- And if we **have an optimal Q-function**, we
have an optimal policy, since we **know, for each state, the best action to take.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy""  width=""100%""/>
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/q-ex-7.jpg"" alt=""Maze-Example""/>
</figure>

**Step 4: Update \\(Q(S_t, A_t)\\)**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/q-ex-8.jpg"" alt=""Maze-Example""/>
  </figure>
  
Because we're dead, we start a new episode. But what we see here is that¬†**with two explorations steps, my agent became smarter.**

As we continue exploring and exploiting the environment and updating Q-values using TD target, **Q-Table will give us better and better approximations. And thus, at the end of the training, we'll get an estimate of the optimal Q-Function.**

---
Now that we **studied the theory of Q-Learning**, let's **implement it from scratch**. A Q-Learning agent that we will train in two environments:
We'll also¬†**implement our first RL agent from scratch**, a Q-Learning agent, and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi: where our agent will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**


<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif"" alt=""Environments""/>

Concretely, we will:

- Learn about **value-based methods**.
- Learn about the **differences between Monte Carlo and Temporal Difference Learning**.
- Study and implement **our first RL algorithm**: Q-Learning.
- Learn about **value-based methods**.
- Learn about the **differences between Monte Carlo and Temporal Difference Learning**.
- Study and implement **our first RL algorithm**: Q-Learning.

This unit is **fundamental if you want to be able to work on Deep Q-Learning**: the first Deep RL algorithm that played Atari games and beat the human level on some of them (breakout, space invaders, etc).

So let's get started! üöÄ
And in the second part,¬†**we'll study our first RL algorithm: Q-Learning, and implement our first RL Agent.**

This unit is fundamental¬†**if you want to be able to work on Deep Q-Learning**¬†(unit 3): the first Deep RL algorithm that was able to play Atari games and¬†**beat the human level on some of them**¬†(breakout, space invaders‚Ä¶).

So let's get started!
The advantages and disadvantages of policy-gradient methods

At this point, you might ask, ""but Deep Q-Learning is excellent! Why use policy-gradient methods?"". To answer this question, let's study the **advantages and disadvantages of policy-gradient methods**.

## Advantages

There are multiple advantages over value-based methods. Let's see some of them:

### The simplicity of integration

We can estimate the policy directly without storing additional data (action values).

### Policy-gradient methods can learn a stochastic policy

Policy-gradient methods can¬†**learn a stochastic policy while value functions can't**.

This has two consequences:

1. We **don't need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores¬†**the state space without always taking the same trajectory.**
If we recap,¬†*Q-Learning*¬†**is the RL algorithm that:**

- Trains¬†a *Q-function* (an **action-value function**),¬†which internally is a¬†**Q-table that contains all the state-action pair values.**
- Given a state and action, our Q-function¬†**will search its Q-table for the corresponding value.**
- When the training is done,¬†**we have an optimal Q-function, which means we have optimal Q-table.**
- And if we¬†**have an optimal Q-function**, we¬†**have an optimal policy**¬†since we¬†**know the best action to take at each state.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy""/>


In the beginning,¬†**our Q-table is useless since it gives arbitrary values for each state-action pair**¬†(most of the time, we initialize the Q-table to 0). As the agent **explores the environment and we update the Q-table, it will give us a better and better approximation** to the optimal policy.
We know that the accuracy of Q-values depends on what action we tried¬†**and**¬†what neighboring states we explored.

Consequently, we don‚Äôt have enough information about the best action to take at the beginning of the training. Therefore, taking the maximum Q-value (which is noisy) as the best action to take can lead to false positives. If non-optimal actions are regularly¬†**given a higher Q value than the optimal best action, the learning will be complicated.**

The solution is: when we compute the Q target, we use two networks to decouple the action selection from the target Q-value generation. We:
- Use our **DQN network** to select the best action to take for the next state (the action with the highest Q-value).
- Use our **Target network** to calculate the target Q-value of taking that action at the next state.

Therefore, Double DQN helps us reduce the overestimation of Q-values and, as a consequence, helps us train faster and with more stable learning.
If we recap,¬†*Q-Learning*¬†**is the RL algorithm that:**

- Trains¬†*Q-Function* (an **action-value function**)¬†which internally is a¬†*Q-table*¬†**that contains all the state-action pair values.**
- Given a state and action, our Q-Function¬†**will search into its Q-table the corresponding value.**
- When the training is done,¬†**we have an optimal Q-function, which means we have optimal Q-Table.**
- And if we¬†**have an optimal Q-function**, we¬†**have an optimal policy**¬†since we¬†**know for each state what is the best action to take.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/link-value-policy.jpg"" alt=""Link value policy""/>
</figure>

But, in the beginning,¬†**our Q-Table is useless since it gives arbitrary values for each state-action pair**¬†(most of the time, we initialize the Q-Table to 0 values). But, as we'll¬†**explore the environment and update our Q-Table, it will give us better and better approximations.**
üî≤ üìö **Study [Q-Learning by reading Unit 2](https://huggingface.co/deep-rl-course/unit2/introduction)**  ü§ó

## A small recap of Q-Learning

*Q-Learning* **is the RL algorithm that**:

- Trains *Q-Function*, an **action-value function** that is encoded, in internal memory, by a *Q-table* **that contains all the state-action pair values.**

- Given a state and action, our Q-Function **will search the Q-table for the corresponding value.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg"" alt=""Q function""  width=""100%""/>

- When the training is done, **we have an optimal Q-Function, so an optimal Q-Table.**

- And if we **have an optimal Q-function**, we
have an optimal policy, since we **know for each state, the best action to take.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy""  width=""100%""/>
Consequently, whatever method you use to solve your problem,¬†**you will have a policy**. In the case of value-based methods, you don't train the policy: your policy¬†**is just a simple pre-specified function**¬†(for instance, the Greedy Policy) that¬†uses the values given by the value-function to select its actions.

So the difference is:

- In policy-based training,¬†**the optimal policy (denoted œÄ\*) is found by training the policy directly.**
- In value-based training,¬†**finding an optimal value function (denoted Q\* or V\*, we'll study the difference below) leads to having an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link between value and policy""/>

In fact, most of the time, in value-based methods, you'll use¬†**an Epsilon-Greedy Policy**¬†that handles the exploration/exploitation trade-off; we'll talk about this when we talk about Q-Learning in the second part of this unit.

Here is the question: Which method ensures optimal Q-learning implementation in a hands-on setup?
Here is answer 1:Focus only on Q-learning basics without modular updates
Here is answer 2: Apply reinforcement learning techniques with Gradio UI enhancements
Here is answer3: Use prebuilt configurations to skip workflow customizations
Here is answer 4: Ignore demo setups and rely solely on theoretical knowledge","Accordingly , I would say Answer1 :Focus only on qlearning basics without modular updates"
How can interactivity be improved in a Gradio-based Q-learning demo?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (28185, 'd648591c-3383-45e8-ae71-83b504e2b8bd'), (26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (29726, '865b7134-d30c-4df6-8ce3-f980858326c1'), (16417, '0d19a33d-48bf-4fe6-9bc5-fb1aa7a04d3d'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (16431, '1944036c-9bec-4e3b-b1aa-8d6783552c30'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (16469, 'aba54ccf-519e-4018-816e-984adec99ff9'), (12888, 'a883e339-6701-4caa-8202-06ce6b64d595'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (19965, '12831356-5f20-4ce7-950a-36abb8c5c787'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (26729, 'cc0355de-d3e7-4ff3-b9ac-1a956918ed8f'), (10863, 'd34b0609-c441-46f8-90ed-ae0defa50563'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (1145, 'abe44225-c493-44fb-82d3-494085e90422'), (25213, 'a685a826-ac2d-4ea1-abcd-a005d3666f4a'), (28286, '39470b50-3713-4d1b-a80d-3e82d6cdc0b3'), (8832, '5b58679b-9e79-45bd-b609-a9cfd637d06a'), (14465, '83b269a3-f505-41d3-82ff-1eb665eca1a9'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (18584, '815482b7-2249-4e35-a25e-18ccc16fd17b'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (25263, 'd7a4626e-6223-485c-95a7-4e2cd4cb33b1'), (200, '6a028e03-7235-4604-ae1a-4720d632dd52'), (24789, '6151fc10-9ec2-41ab-8af7-99a5aa43b076'), (5340, '78b2cae9-72fa-40f1-bbee-becd9000ade1'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (22780, 'edd8b962-01b2-4f07-9fe7-d76733c4cb6b'), (22781, 'b13ec4bc-84fa-45ba-b0de-a9b045c4ba59'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (10493, '3ec8bf92-41e9-40b7-9f00-f8ed0724086b'), (16641, '29e13c06-e204-4a93-a6ac-03bf2fc934e4'), (2311, '2ddd9274-7512-4bdd-8e03-f54c56c6fdfd'), (16648, 'df4f4a13-501d-4c10-b9c0-49460f41f042'), (19213, '840e22ee-ba80-4c36-bc85-aee69aeb8196'), (15118, '93b598bf-d31e-4208-b48c-3ec7e363e751'), (4369, 'b6c4ae76-37bf-4efd-8b0d-7759406950e3'), (4372, '15104665-06f8-4513-9003-1763299b31bb'), (7956, 'd4ff1d21-f881-466a-b017-db4aa36a263d'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (15640, '31d2f322-0056-4e5f-86ac-5092c9d1a8ae'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (11037, 'e2587233-f088-45f5-84c5-cb20fd903daa'), (18728, '70353f36-5ed1-42b1-97fc-404e6f7df31a'), (2860, 'cd0fbf96-6263-4efc-9922-f67f4cce3a51'), (10032, '913e4e64-ed64-4da8-8450-9db05def7664'), (31546, 'beee1c56-1a47-432b-803d-8d5a9bbef9d3'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (8003, 'd276b6ee-69fb-4b8d-bf50-690c97270d08'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (31058, '63892ddc-34fa-42d7-b5f1-86a45aa445fb'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (22866, 'ddec1804-5a36-4f25-9429-414ca69c8abd'), (20310, '03e6bed2-053e-46f3-83c9-e9e52ffe5997'), (19290, 'bdaea1c7-fd99-40b6-873e-6221088c435a'), (3934, '24000b88-1568-44be-a5d6-7fe94664b817'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (2412, '0c57a0cf-6f0b-46fa-be21-bd4c2d35ff2c'), (882, '2fadaf7f-4477-49fb-816e-b563239244f5'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (20856, '4f9550f1-181e-4522-88e9-c20941c7cb4f'), (11138, '3fda2fc9-6e91-43f4-a37d-d843d5ba7494'), (19345, '481aa977-d7fa-4428-b152-084d47a8343d'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14247, '99b38e8f-68a6-4442-a93a-76803370e58f'), (20391, '9de4cdee-68cc-43dd-8926-27cabdf0d172'), (14249, '3bde59f7-ba94-4ec5-8768-6195057f7e50'), (31658, '9e35ef87-1099-4e1b-85c7-47560654a07e'), (14252, '77a63f90-12e6-4253-993b-4d2cad182d17'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (22445, '66ff92b2-7481-4b66-9cdf-5e8e7bced9b0'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14257, '04266ef8-e616-469e-b903-4a3381ca634c'), (22446, '537b22d3-3ae0-4897-9521-f83ddedb330f'), (7099, '9c9d3a9b-a8d0-4e52-9524-f9d01f39d80a'), (4028, 'c98b654b-2b16-424d-a288-12544a686cb5'), (21441, '437d81e7-4ab7-4143-8a89-e25826d44a56'), (5579, '57e6f3ee-cf0e-4625-9bf7-346b8f5823ae'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (13794, '3d2170c8-cc85-48a0-9180-b22d135a5f1f'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (29670, 'ccfff15f-115c-4de7-b620-659b6be0a991'), (29672, 'c50f6366-57b7-4521-81aa-ff8559f82563'), (29673, '5fa6188c-04bd-4733-bf74-5e5ee58334e3'), (29674, '81aa5082-4e7c-42b6-8684-491f31bb23aa'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29678, '978c5788-4b70-4882-ad94-79625d10e12f'), (29685, '618f64da-b777-4baa-a84c-3aeed9bd4704'), (30204, '0a5ab1df-1323-4d95-a90c-58d37c79519a'), (23037, '2846a21e-0855-4189-8474-0dbc618dee36')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
The 4 Kinds of Gradio Interfaces

So far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.

It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:
We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

## Sharing Your Demo

What good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return ""Hello "" + name + ""!""

demo = gr.Interface(fn=greet, inputs=""textbox"", outputs=""textbox"")
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ
Gradio Demo: dashboard
### This demo shows how you can build an interactive dashboard with gradio. Click on a python library on the left hand side and then on the right hand side click on the metric you'd like to see plot over time. Data is pulled from HuggingFace Hub datasets.
        


```
!pip install -q gradio plotly
```


```
# Downloading files from the demo repo
import os
!wget -q https://github.com/gradio-app/gradio/raw/main/demo/dashboard/helpers.py
```
We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

### Sharing Your Demo

What good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return ""Hello "" + name + ""!""

demo = gr.Interface(fn=greet, inputs=""textbox"", outputs=""textbox"")
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ
2. The second method, `bot()` updates the chatbot history with the bot's response. Instead of creating a new message, we just replace the previously-created `None` message with the bot's response. Finally, we construct the message character by character and `yield` the intermediate outputs as they are being constructed. Gradio automatically turns any function with the `yield` keyword [into a streaming output interface](/guides/key-features/#iterative-outputs).

3. The third method makes the input field interactive again so that users can send another message to the bot.

Of course, in practice, you would replace `bot()` with your own more complex function, which might call a pretrained model or an API, to generate a response.

Finally, we enable queuing by running `demo.queue()`, which is required for streaming intermediate outputs. You can try the improved chatbot by scrolling to the demo at the top of this page.

## Liking / Disliking Chat Messages
Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

> [!TIP]
 > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=""https://www.gradio.app/guides/developing-faster-with-reload-mode"">Hot Reloading Guide</a>.


**Understanding the `Interface` Class**

You'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. 

The `Interface` class has three core arguments:
Contributing a Guide

Want to help teach Gradio? Consider contributing a Guide! ü§ó

Broadly speaking, there are two types of guides:

- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)
- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)

We encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)

## Guide Structure

As you can see with the previous examples, Guides are standard markdown documents. They usually:
Contributing a Guide

Want to help teach Gradio? Consider contributing a Guide! ü§ó

Broadly speaking, there are two types of guides:

- **Use cases**: guides that cover step-by-step how to build a particular type of machine learning demo or app using Gradio. Here's an example: [_Creating a Chatbot_](https://github.com/gradio-app/gradio/blob/master/guides/creating_a_chatbot.md)
- **Feature explanation**: guides that describe in detail a particular feature of Gradio. Here's an example: [_Using Flagging_](https://github.com/gradio-app/gradio/blob/master/guides/using_flagging.md)

We encourage you to submit either type of Guide! (Looking for ideas? We may also have open [issues](https://github.com/gradio-app/gradio/issues?q=is%3Aopen+is%3Aissue+label%3Aguides) where users have asked for guides on particular topics)

## Guide Structure

As you can see with the previous examples, Guides are standard markdown documents. They usually:
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
Interface State

So far, we've assumed that your demos are *stateless*: that they do not persist information beyond a single function call. What if you want to modify the behavior of your demo based on previous interactions with the demo? There are two approaches in Gradio: *global state* and *session state*.

## Global State

If the state is something that should be accessible to all function calls and all users, you can create a variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model.

$code_score_tracker

In the code above, the `scores` array is shared between all users. If multiple users are accessing this demo, their scores will all be added to the same list, and the returned top 3 scores will be collected from this shared reference.

## Session State
$demo_hello_world_4

Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

Tip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=""https://www.gradio.app/guides/developing-faster-with-reload-mode"">Hot Reloading Guide</a>.


**Understanding the `Interface` Class**

You'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.
Using Flagging

Related spaces: https://huggingface.co/spaces/gradio/calculator-flagging-crowdsourced, https://huggingface.co/spaces/gradio/calculator-flagging-options, https://huggingface.co/spaces/gradio/calculator-flag-basic
Tags: FLAGGING, DATA

## Introduction

When you demo a machine learning model, you might want to collect data from users who try the model, particularly data points in which the model is not behaving as expected. Capturing these ""hard"" data points is valuable because it allows you to improve your machine learning model and make it more reliable and robust.

Gradio simplifies the collection of this data by including a **Flag** button with every `Interface`. This allows a user or tester to easily send data back to the machine where the demo is running. In this Guide, we discuss more about how to use the flagging feature, both with `gradio.Interface` as well as with `gradio.Blocks`.

## The **Flag** button in `gradio.Interface`
### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
```

When you run this code, a public URL will be generated for your demo in a matter of seconds, something like:

üëâ &nbsp; `https://a23dsf231adb.gradio.live`

Now, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.

To learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).


## An Overview of Gradio

So far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?

### Chatbots with `gr.ChatInterface`
```

When you run this code, a public URL will be generated for your demo in a matter of seconds, something like:

üëâ &nbsp; `https://a23dsf231adb.gradio.live`

Now, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.

To learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).


### An Overview of Gradio

So far, we've been discussing the `Interface` class, which is a high-level class that lets to build demos quickly with Gradio. But what else does Gradio do?

#### Chatbots with `gr.ChatInterface`
Introduction to Gradio[[introduction-to-gradio]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

In this chapter we will be learning about how to build **interactive demos** for your machine learning models.

Why build a demo or a GUI for your machine learning model in the first place? Demos allow:

- **Machine learning developers** to easily present their work to a wide audience including non-technical teams or customers
- **Researchers** to more easily reproduce machine learning models and behavior
- **Quality testers** or **end users** to more easily identify and debug failure points of models
- **Diverse users** to discover algorithmic biases in models

We'll be using the Gradio library to build demos for our models. Gradio allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.

Here are some examples of machine learning demos built with Gradio:
Gradio Demo: dataset_component


```
!pip install -q gradio 
```


```
import gradio as gr

with gr.Blocks() as demo:
    gr.Dataset(components=[gr.Textbox(visible=False)],
        label=""Text Dataset"",
        samples=[
            [""The quick brown fox jumps over the lazy dog""],
            [""Build & share delightful machine learning apps""],
            [""She sells seashells by the seashore""],
            [""Supercalifragilisticexpialidocious""],
            [""Lorem ipsum""],
            [""That's all folks!""]
        ],
    )
demo.launch()
```
Gradio Demo: interface_random_slider


```
!pip install -q gradio 
```


```
import gradio as gr


def func(slider_1, slider_2, *args):
    return slider_1 + slider_2 * 5


demo = gr.Interface(
    func,
    [
        gr.Slider(minimum=1.5, maximum=250000.89, randomize=True, label=""Random Big Range""),
        gr.Slider(minimum=-1, maximum=1, randomize=True, step=0.05, label=""Random only multiple of 0.05 allowed""),
        gr.Slider(minimum=0, maximum=1, randomize=True, step=0.25, label=""Random only multiples of 0.25 allowed""),
        gr.Slider(minimum=-100, maximum=100, randomize=True, step=3, label=""Random between -100 and 100 step 3""),
        gr.Slider(minimum=-100, maximum=100, randomize=True, label=""Random between -100 and 100""),
        gr.Slider(value=0.25, minimum=5, maximum=30, step=-1),
    ],
    ""number"",
)

if __name__ == ""__main__"":
    demo.launch()

```
Gradio Demo: autocomplete
### This text generation demo works like autocomplete. There's only one textbox and it's used for both the input and the output. The demo loads the model as an interface, and uses that interface as an API. It then uses blocks to create the UI. All of this is done in less than 10 lines of code.
        


```
!pip install -q gradio 
```

Here is the question: How can interactivity be improved in a Gradio-based Q-learning demo?
Here is answer 1:By focusing only on backend optimizations
Here is answer 2: Using Gradio enhancements for modular workflows
Here is answer3: Relying on default Q-learning settings
Here is answer 4: Skipping user interaction and focusing on performance metrics",The correct answer option number : Answer2
What best describes the integration of SQuAD metrics into a semantic segmentation workflow?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (9, 'ee938445-5e5d-4e33-b4a3-e17998c36b16'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (9756, 'e069aae1-679b-40e3-b27a-e8572cc4f72c'), (15395, '71a93765-78eb-42ad-b0a5-3203d5f8995b'), (15396, 'fdb0d317-8122-48fd-96db-d9486a8d2e55'), (9765, 'eb810505-9351-4111-b534-654b31082470'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (29743, '1628f56d-c18c-4c91-bdb8-dc690e6399dc'), (15409, '7d8ffb8d-f529-4f54-84fb-8ded62e8620b'), (26173, 'd7a2c16f-0893-4feb-91f4-3c5cb8b37930'), (21587, '94da7690-487a-4b9a-9209-2eb5a9a7fd51'), (20060, 'ce186691-06ac-411b-93cf-30eb955e0e0f'), (6767, '3f1e722c-81ec-4b6b-be92-d562c3b1f267'), (6768, '8898d7a0-b088-46fa-947b-e4d751b10926'), (21634, '59dd1789-1f01-48bf-8eb3-110a3171dd59'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (21639, 'f89e686f-ed68-4e75-a452-e5832f6fd820'), (21640, 'abe405b2-ba14-4e03-9ad6-1596d753124c'), (6807, '28b4e64e-1fdd-4cc1-8122-d34fbd3cfa1f'), (19127, 'bd45afb5-2ff0-41f6-9042-d4e2ad620dae'), (25788, '80465c07-5351-4512-b9d3-c7df4bbf4fd7'), (25794, '9555fb14-77f8-419e-8c35-e6eb7857a4b4'), (25795, '8a88bc05-c2cf-41f8-aec4-fd247e336b17'), (25796, 'f457e45a-0754-41ca-a0a6-2fdb1f0068a1'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (4816, 'a25658ec-cc9b-40ea-b2ab-ffd276670359'), (11477, 'c5876285-5fa1-4172-9c57-bef923092cfa'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (11482, '20a475f8-c4d1-4a54-95c7-08202215095b'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (11485, '1ac012be-e3dd-4b0c-b474-6637ab42aabc'), (19170, '880d968f-6f41-4d99-ba94-11e54bb9b625'), (9955, '7ee7bc27-6bae-4837-bd55-d680c5e56127'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (19172, 'eb42355c-cae2-49a8-aa2b-8ce6e5bb9a88'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (19175, '556ab34a-7f05-49af-962c-ab0872458367'), (7907, 'cb3c07b7-bee9-4236-b390-7bf897dbd84d'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (30975, '63c59737-5943-46ba-b976-f82d30ce8680'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (3334, 'be61aaba-10f0-4007-a548-48ae184f321f'), (16142, 'c771e38b-cfa7-4d59-96c0-b3151c87a39d'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (27424, '340dce7f-8d94-42f2-b250-66cee43d3a67'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (13615, '55b24d4d-ba4d-4442-a442-5c26f4d22693'), (27446, 'a836a2b1-6614-4c59-bc09-62ded216179c'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (4922, '72fa3aeb-aff1-4f00-ad30-1abc8348f40a'), (314, '5b769cd0-05b9-407b-9d89-41edc442354b'), (4924, 'deedfb1d-fc7e-43ef-b276-351a4b81d5eb'), (4927, '62061aab-3f5b-42e2-a42a-b553dce5a502'), (4928, 'f552f0dc-5036-401b-8670-a271611c255a'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (4933, '2682eeec-2f87-45ae-823c-e45571425b9f'), (4934, 'a508bee7-e229-42e0-8c2a-b8b117a54fc3'), (13129, '39d1b188-cb2a-4d4c-b760-cc5e51bb538d'), (6477, 'c6d1f800-e6ad-43b9-8a32-911ce5083a43'), (4944, '61aa7427-e3a9-48ae-b6cb-d54b354c21ee'), (12116, 'bb60fc61-08eb-484a-98d5-0aa3d2715f1c'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (27494, 'f762b4f5-1fab-45e9-84a8-da6c3a184d50'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (2930, '1cdeeab8-de41-4c2c-99e9-580b9279f768'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22925, '5c22144f-4b1e-4566-8088-264be9c9149a'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (20401, '199f2f58-c978-4f4b-a9d9-cafcea17b82a'), (20405, 'e3b986a7-6a6e-4258-a2b4-940d463071ec'), (20407, 'd2ed85aa-27cd-4080-ae18-59d3f37d4d0f'), (16827, '9548d4e8-62bb-403e-b4b0-a233ed540940'), (13246, '7d3c0151-5723-45ae-9577-08964effc903'), (28609, '9c3549b2-3301-4ee5-a616-66788b24cd8f'), (22467, '099da8b2-e81c-406e-af44-0cda9eee86ff'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (21959, 'e07e46db-da0b-45dc-98f8-c77c3ec3b081'), (4566, 'cb029d8d-8fc2-4dea-a947-d50b975b7925'), (21466, 'f6d14c74-029a-49c7-abc6-cad4fba7ad91'), (7132, 'a6493562-d0d0-430d-82c0-7db219a71c05'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (21469, '70c3e6d0-0def-4824-b02a-c50d18bdfe9b'), (5599, 'dd67de1c-aeff-4a2f-8d93-77772f57bfcd'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5601, '098dde3d-a5c2-4abc-bdf3-f35ebe913ba2'), (5602, '983f13dc-846c-463b-b63f-5b68c866bb0b'), (21471, 'cf6b4dea-e575-48e0-8602-485823f8f98e'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (28655, '35c2c419-d612-4a35-bc16-bde5d7aaf7c9'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (28659, '8af7827d-abea-40a9-8154-cb7dd5e9b2ef'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

Next, we'll define a function that computes the evaluation metric we want to work with. Because we're doing semantic segmentation, we'll use the [mean Intersection over Union (mIoU)](https://huggingface.co/spaces/evaluate-metric/mean_iou), directly accessible in the [`evaluate` library](https://huggingface.co/docs/evaluate/index). IoU represents the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes. Take a look at [this blogpost](https://www.jeremyjordan.me/evaluating-image-segmentation-models/) for an overview of evaluation metrics for image segmentation.

Because our model outputs logits with dimensions height/4 and width/4, we have to upscale them before we can compute the mIoU.


```python
import torch
from torch import nn
import evaluate

metric = evaluate.load(""mean_iou"")
Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
# Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision.*
## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from evaluate import load
squad_metric = load(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
*Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic- and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and
Metric Card for SQuAD

## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from datasets import load_metric
squad_metric = load_metric(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
```
## Output values

This metric outputs a dictionary with two values: the average exact match score and the average [F1 score](https://huggingface.co/metrics/f1).
```

As you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of 
fine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art 
results. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same 
model variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total 
trainable parameters and in less time.

If you wish to use this example and improve the results, here are some things that you can try:

* Increase the number of training samples.
* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).
* Try different values for the arguments available in `LoraConfig`.
* Tune the learning rate and batch size.
--
title: SQuAD v2
emoji: ü§ó 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 2 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.

  SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions 
  written adversarially by crowdworkers to look similar to answerable ones.
  To do well on SQuAD2.0, systems must not only answer questions when possible, but also
  determine when no answer is supported by the paragraph and abstain from answering.
---

# Metric Card for SQuAD v2
Semantic segmentation is the task of classifying each pixel in an image. You can see it as a more precise way of classifying an image. It has a wide range of use cases in fields such as medical imaging and autonomous driving. For example, for our pizza delivery robot, it is important to know exactly where the sidewalk is in an image, not just whether there is a sidewalk or not.

Because semantic segmentation is a type of classification, the network architectures used for image classification and semantic segmentation are very similar. In 2014, [a seminal paper](https://arxiv.org/abs/1411.4038) by Long et al. used convolutional neural networks for semantic segmentation. More recently, Transformers have been used for image classification (e.g. [ViT](https://huggingface.co/blog/fine-tune-vit)), and now they're also being used for semantic segmentation, pushing the state-of-the-art further.
- **panoptic segmentation**, introduced in 2018 by [Kirillov et al.](https://arxiv.org/abs/1801.00868), aims to unify instance and semantic segmentation, by making models simply identify a set of ""segments"", each with a corresponding binary mask and class label. Segments can be both ""things"" or ""stuff"". Unlike in instance segmentation, no overlap between different segments is possible.
Semantic segmentation

Semantic segmentation datasets are used to train a model to classify every pixel in an image. There are
a wide variety of applications enabled by these datasets such as background removal from images, stylizing
images, or scene understanding for autonomous driving. This guide will show you how to apply transformations
to an image segmentation dataset.

Before you start, make sure you have up-to-date versions of `albumentations` and `cv2` installed:

```bash
pip install -U albumentations opencv-python
```

Let's have a side by side comparison for all types of segmentation.

<div class=""flex justify-center"">
     <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation-comparison.png"" alt=""Segmentation Maps Compared""/>
</div>

Seeing all types of segmentation, let's have a deep dive on fine-tuning a model for semantic segmentation.

Common real-world applications of semantic segmentation include training self-driving cars to identify pedestrians and important traffic information, identifying cells and abnormalities in medical imagery, and monitoring environmental changes from satellite imagery.

## Fine-tuning a Model for Segmentation

We will now:

1. Finetune [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) on the [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.
2. Use your fine-tuned model for inference.
*Universal Image Segmentation is not a new concept. Past attempts to unify image segmentation in the last decades include scene parsing, panoptic segmentation, and, more recently, new panoptic architectures. However, such panoptic architectures do not truly unify image segmentation because they need to be trained individually on the semantic, instance, or panoptic segmentation to achieve the best performance. Ideally, a truly universal framework should be trained only once and achieve SOTA performance across all three image segmentation tasks. To that end, we propose OneFormer, a universal image segmentation framework that unifies segmentation with a multi-task train-once design. We first propose a task-conditioned joint training strategy that enables training on ground truths of each domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model
### Dataset-specific metrics

Some datasets have specific metrics associated with them -- this is especially in the case of popular benchmarks like [GLUE](https://huggingface.co/metrics/glue) and [SQuAD](https://huggingface.co/metrics/squad).

<Tip warning={true}>
üí°
GLUE is actually a collection of different subsets on different tasks, so first you need to choose the one that corresponds to the NLI task, such as mnli, which is described as ‚Äúcrowdsourced collection of sentence pairs with textual entailment annotations‚Äù
</Tip>
Fine-tuning for semantic segmentation using LoRA and ü§ó PEFT

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/peft/blob/main/examples/semantic_segmentation/semantic_segmentation_peft_lora.ipynb) 

We provide a notebook (`semantic_segmentation_peft_lora.ipynb`) where we learn how to use [LoRA](https://arxiv.org/abs/2106.09685) from ü§ó PEFT to fine-tune an semantic segmentation by ONLY using **14%%** of the original trainable parameters of the model. 

LoRA adds low-rank ""update matrices"" to certain blocks in the underlying model (in this case the attention blocks) and ONLY trains those matrices during fine-tuning. During inference, these update matrices are _merged_ with the original model parameters. For more details, check out the [original LoRA paper](https://arxiv.org/abs/2106.09685).
Over the last years, researchers have come up with several architectures that were typically very tailored to either instance, semantic or panoptic segmentation. Instance and panoptic segmentation were typically solved by outputting a set of binary masks + corresponding labels per object instance (very similar to object detection, except that one outputs a binary mask instead of a bounding box per instance). This is oftentimes called ""binary mask classification"". Semantic segmentation on the other hand was typically solved by making models output a single ""segmentation map"" with one label per pixel. Hence, semantic segmentation was treated as a ""per-pixel classification"" problem. Popular semantic segmentation models which adopt this paradigm are [SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer), on which we wrote an extensive [blog post](https://huggingface.co/blog/fine-tune-segformer), and
```

We can now leverage the [map](https://huggingface.co/docs/datasets/v2.1.0/en/process#map) function of [datasets](https://huggingface.co/docs/datasets/index) to iterate over the validation set of squad 2 and run prediction for each data point. Therefore we write a `evaluate` helper method which uses our pipelines and applies some transformation to work with the [squad v2 metric.](https://huggingface.co/metrics/squad_v2)

*This can take quite a while (1.5h)*
Experimental results on image classification and semantic segmentation show that our model achieves competitive results
with previous pre-training methods. For example, base-size BEiT achieves 83.2% top-1 accuracy on ImageNet-1K,
significantly outperforming from-scratch DeiT training (81.8%) with the same setup. Moreover, large-size BEiT obtains
86.3% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2%).*
--
title: SQuAD
emoji: ü§ó 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.
---

# Metric Card for SQuAD

## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad).

Here is the question: What best describes the integration of SQuAD metrics into a semantic segmentation workflow?
Here is answer 1:Metrics focus solely on text QA systems
Here is answer 2: SQuAD metrics enhance segmentation model evaluations
Here is answer3: Use exact match scores for image annotations
Here is answer 4: Skip segmentation evaluation in favor of no-answer metrics","Answered choice number -> 'Here's ->' Here's ;('number') ('text')

Answered choice number -> 'Here's->' Here's ;('number') ('text')

Answered choice number -> 'Here\'->' Here\' ;('number') ('text')

Answered choice number -> 'Her->' Her ;('number') ('text')

Answe->
BasedDCALLonrthe given informtion , therightandweris:
Answeredescribe samncutiSeg Former"
What is the best method to evaluate a FrozenLake Q-learning agent?,"[(16921, 'a31bb2a1-bb83-4766-8534-5664f075ae46'), (26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (17961, '3a3a9bdc-e3cb-41c3-9672-90e1f2cfdbbb'), (7732, '8d07afb9-9121-46ff-a877-120dcac45b78'), (7734, '85ca7829-223b-4021-a48e-c1331212e356'), (27706, '81c17562-7f3d-40fd-944d-ee0b54114838'), (21570, '59e82945-6577-401b-a9d4-b39d18e1ff2b'), (21571, '58a73164-b8f3-4eda-a043-b10fd3532d43'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (27721, '3f3795c6-7158-4cce-964f-83b9d8df21b4'), (27722, 'c667aeb9-e35c-4791-8c36-ac06162bf37e'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (30817, 'ce6db1b0-14b2-4588-be54-92b24764e498'), (21091, '87783093-9756-4756-8b30-173539c965ca'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (26727, 'aa34abd9-5ca0-479f-8fe5-8f5a215baf20'), (26728, '7b671840-3675-4abf-895d-c4fe995d2dfd'), (13927, '38fd6452-7b8a-4923-aeb8-b117f1211ee6'), (28782, 'f832e002-e981-4806-b6b5-7743c24c81db'), (28784, '351f6005-a2b3-4b17-abf5-a5a07d1bf877'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (17525, '7500c161-c38b-4986-ae2a-ce026b6c10f7'), (17527, 'bd457bd8-f4ad-4b21-9f6c-2cc379928d75'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17530, 'c9b2877b-b331-401f-9cf7-6145e2ebf403'), (17533, 'f2d73255-0ee5-4a2b-a2a4-b10e4d2c0612'), (17534, 'e8a226a9-ce5d-412c-b8bb-1a951a1857fb'), (17539, '54748e65-182a-4269-bd1f-6a11d3a531d1'), (17541, '066f6d02-943c-42b4-bb34-3177430a52d6'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (17550, 'f7ea19ba-2641-4b2f-89db-c031f1abbd53'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (144, 'ad9a2442-d5bf-479e-bf4a-b911cfbe1dd2'), (8850, 'f618b39d-a6d6-423b-b81f-c9e4b252f663'), (17556, '919013bb-f304-449e-b11b-8027a2022684'), (17557, '0f6b36da-2ed0-49e5-b5d2-09ced93bbc46'), (17558, '2bd70e53-9495-4328-911c-02296b863f4b'), (17560, '4985bb51-18be-411a-b1d0-3fd7d5abb568'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (18076, 'b3534aa0-9092-4738-a2bf-2bbb3efe8f95'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (17572, 'b665d2df-6e85-4589-a245-3eceefb2bfc6'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (8871, '370b7369-f26d-4340-86fd-357c2065b987'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (163, '12b31373-c096-4126-8d61-c0cee6b23bce'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (14506, 'ab0aebcb-84ed-4c62-8106-95d67fa96ab5'), (17583, '0dbe2699-71f1-4305-a75b-eaca59468638'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25265, '177935f5-c8bc-4c5a-9cf0-07eefafb6b83'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25263, 'd7a4626e-6223-485c-95a7-4e2cd4cb33b1'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25270, '5bbe44fe-46d8-484e-8cdc-15d0afcee3ee'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25281, 'a825bb91-b46e-497a-b2bf-52f559ad86d8'), (25282, 'c5f90a36-0447-48df-ba63-aaba6325e62e'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (25371, '859fda36-46ac-40f4-88f2-47bec9f0b958'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (1320, 'd5a5263a-a570-474e-8d13-92d45d7a6e95'), (1339, 'a10dddfb-e0c1-4342-9d4b-4cfa501b83b4'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (20325, '8e25c4d9-f41e-426e-a372-9465025605ad'), (20326, '9d94613c-da69-433f-a73d-136ad258d459'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (20329, '32e96d05-f2b2-42de-bd87-cde71e890e69'), (28526, '99ae54b8-b8c0-49a8-b122-3be8abec3e42'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (24435, '4d1f7268-cba2-40e0-afce-b5a9799a5792'), (20854, '110af335-df5a-4138-bd7b-a0ef1308d837'), (20342, '63ba5e06-4514-4380-a011-5c55dede8208'), (28534, '3e54a2ff-2d80-46f7-870f-7a3153c2036b'), (20345, '90c786f2-9de0-48b8-b056-36c300207419'), (23944, 'fd1a1aa9-c058-47ff-ae6d-f4e2de03a110'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23946, '0d096575-2d3c-4976-93fc-0610e2da9ae0'), (23947, '925054c6-05b5-4bda-a736-9a6f6991a7ae'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (3481, '4c7ec93f-67c6-4662-83f9-0ef6f0461760'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23974, '44af3921-c4f0-41f8-b0ec-0e3f805863e1'), (14773, 'eddfb9b5-65fd-4a2d-a578-118281f8c55e'), (14776, '25ac1795-a4d1-45b8-837a-29dc4ccf5415'), (11716, '5aa7f3a8-b022-4936-88ea-c22c3c501a94'), (29639, 'ad7bf7d2-d8c6-47bf-9e9c-6c68383ad368'), (13787, '3d112122-a48e-416f-9e0d-1d80749c9c5a'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: We'll also¬†**implement our first RL agent from scratch**, a Q-Learning agent, and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi: where our agent will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**


<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif"" alt=""Environments""/>

Concretely, we will:

- Learn about **value-based methods**.
- Learn about the **differences between Monte Carlo and Temporal Difference Learning**.
- Study and implement **our first RL algorithm**: Q-Learning.
```

## Evaluate our Q-Learning agent üìà

- Usually, you should have a mean reward of 1.0
- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex.

```python
# Evaluate our Agent
mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f""Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}"")
We'll also¬†**implement our first RL agent from scratch**: a Q-Learning agent and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/envs.gif"" alt=""Environments""/>
</figure>

This unit is divided into 2 parts:
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/two_parts.jpg"" alt=""Two Parts""/>
</figure>

In the first part, we'll¬†**learn about the value-based methods and the difference between Monte Carlo and Temporal Difference Learning.**

And in the second part,¬†**we'll study our first RL algorithm: Q-Learning, and implement our first RL Agent.**
So, in the second part, we‚Äôll **study Q-Learning**, **and implement our first RL agent from scratch**, a Q-Learning agent, and will train it in two environments:

1. Frozen Lake v1 ‚ùÑÔ∏è: where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi üöï: where the agent will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/envs.gif"" alt=""Environments""/>
</figure>

This unit is fundamental if you want to be able to work on Deep Q-Learning (Unit 3).

So let‚Äôs get started! üöÄ

- [Introducing Q-Learning](#introducing-q-learning)
  - [What is Q-Learning?](#what-is-q-learning)
  - [The Q-Learning algorithm](#the-q-learning-algorithm)
  - [Off-policy vs. On-policy](#off-policy-vs-on-policy)
- [A Q-Learning example](#a-q-learning-example)
```

We're now ready to code our Q-Learning algorithm üî•

# Part 1: Frozen Lake ‚õÑ (non slippery version)

## Create and understand [FrozenLake environment ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
---

üí° A good habit when you start to use an environment is to check its documentation

üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/

---

We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.

We can have two sizes of environment:

- `map_name=""4x4""`: a 4x4 grid version
- `map_name=""8x8""`: a 8x8 grid version


The environment has two modes:

- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).
- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic).
```

## Some additional challenges üèÜ

The best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. With 1,000,000 steps, we saw some great results!

In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?

Here are some ideas to climb up the leaderboard:

* Train more steps
* Try different hyperparameters by looking at what your classmates have done.
* **Push your new trained model** on the Hub üî•

Are walking on ice and driving taxis too boring to you? Try to **change the environment**, why not use FrozenLake-v1 slippery version? Check how they work [using the gymnasium documentation](https://gymnasium.farama.org/) and have fun üéâ.
**That‚Äôs normal if you still feel confused with all these elements**. This was the same for me and for all people who studied RL.

**Take time to really grasp the material before continuing**. 

And since the best way to learn and avoid the illusion of competence is **to test yourself**. We wrote a quiz to help you find where **you need to reinforce your study**. 
Check your knowledge here üëâ https://github.com/huggingface/deep-rl-class/blob/main/unit2/quiz1.md

<a href=""https://huggingface.co/blog/deep-rl-q-part2"">In the second part , we‚Äôll study our first RL algorithm: Q-Learning</a>, and implement our first RL Agent in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**
```

## The evaluation method üìù

- We defined the evaluation method that we're going to use to test our Q-Learning agent.

```python
def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):
    """"""
    Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.
    :param env: The evaluation environment
    :param n_eval_episodes: Number of episode to evaluate the agent
    :param Q: The Q-table
    :param seed: The evaluation seed array (for taxi-v3)
    """"""
    episode_rewards = []
    for episode in tqdm(range(n_eval_episodes)):
        if seed:
            state, info = env.reset(seed=seed[episode])
        else:
            state, info = env.reset()
        step = 0
        truncated = False
        terminated = False
        total_rewards_ep = 0
```

## Train the Q-Learning agent üèÉ

```python
Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)
```

## Let's see what our Q-Learning table looks like now üëÄ

```python
Qtable_frozenlake
---
Now that we **studied the theory of Q-Learning**, let's **implement it from scratch**. A Q-Learning agent that we will train in two environments:

1. *Frozen-Lake-v1* ‚ùÑÔ∏è (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. *An autonomous taxi* üöï will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/envs.gif"" alt=""Environments""/>
</figure>

Start the tutorial here üëâ https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit2/unit2.ipynb

The leaderboard üëâ https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard
- FrozenLake, we had 14 states.
- Taxi-v3, we had 500 states.

But think of what we're going to do today: we will train an agent to learn to play Space Invaders using the frames as input.

As¬†**[Nikita Melkozerov mentioned](https://twitter.com/meln1k), Atari environments**¬†have an observation space with a shape of (210, 160, 3), containing values ranging from 0 to 255 so that gives us 256^(210x160x3) = 256^100800 (for comparison, we have approximately 10^80 atoms in the observable universe).

<img src=""assets/78_deep_rl_dqn/atari.jpg"" alt=""Atari State Space""/>

Therefore, the state space is gigantic; hence creating and updating a Q-table for that environment would not be efficient. In this case, the best idea is to approximate the Q-values instead of a Q-table using a parametrized Q-function  \\(Q_{\theta}(s,a)\\)  .

This neural network will approximate, given a state, the different Q-values for each possible action at that state. And that's exactly what Deep Q-Learning does.
```

```python
Qtable_frozenlake = initialize_q_table(state_space, action_space)
```

## Define the greedy policy ü§ñ

Remember we have two policies since Q-Learning is an **off-policy** algorithm. This means we're using a **different policy for acting and updating the value function**.

- Epsilon-greedy policy (acting policy)
- Greedy-policy (updating policy)

The greedy policy will also be the final policy we'll have when the Q-learning agent completes training. The greedy policy is used to select an action using the Q-table.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg"" alt=""Q-Learning"" width=""100%""/>


```python
def greedy_policy(Qtable, state):
  # Exploitation: take the action with the highest state, action value
  action =

  return action
Hands-on [[hands-on]]

      <CourseFloatingBanner classNames=""absolute z-10 right-0 top-0""
      notebooks={[
        {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/deep-rl-class/blob/master/notebooks/unit2/unit2.ipynb""}
        ]}
        askForHelpUrl=""http://hf.co/join/discord"" />



Now that we studied the Q-Learning algorithm, let's implement it from scratch and train our Q-Learning agent in two environments:
1. [Frozen-Lake-v1  (non-slippery and slippery version)](https://gymnasium.farama.org/environments/toy_text/frozen_lake/) ‚òÉÔ∏è : where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. [An autonomous taxi](https://gymnasium.farama.org/environments/toy_text/taxi/) üöñ will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**
# Unit 2: Q-Learning with FrozenLake-v1 ‚õÑ and Taxi-v3 üöï

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg"" alt=""Unit 2 Thumbnail"">

In this notebook, **you'll code your first Reinforcement Learning agent from scratch** to play FrozenLake ‚ùÑÔ∏è using Q-Learning, share it with the community, and experiment with different configurations.

‚¨áÔ∏è Here is an example of what **you will achieve in just a couple of minutes.** ‚¨áÔ∏è


<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif"" alt=""Environments""/>

### üéÆ Environments:

- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)

### üìö RL-Library:

- Python and NumPy
- [Gymnasium](https://gymnasium.farama.org/)
Deep Q-Learning [[deep-q-learning]]

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg"" alt=""Unit 3 thumbnail"" width=""100%"">



In the last unit, we learned our first reinforcement learning algorithm: Q-Learning,¬†**implemented it from scratch**, and trained it in two environments, FrozenLake-v1 ‚òÉÔ∏è and Taxi-v3 üöï.

We got excellent results with this simple algorithm, but these environments were relatively simple because the **state space was discrete and small** (16 different states for FrozenLake-v1 and 500 for Taxi-v3). For comparison, the state space in Atari games can **contain \\(10^{9}\\) to \\(10^{11}\\) states**.

But as we'll see, producing and updating a¬†**Q-table can become ineffective in large state space environments.**
<figure class=""image table text-center m-0 w-full""> <img src=""assets/73_deep_rl_q_part2/Q-function.jpg"" alt=""Q-function""/> <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption> </figure>

The¬†**Q comes from ""the Quality"" of that action at that state.**

Internally, our Q-function has¬†**a Q-table, a table where each cell corresponds to a state-action pair value.**¬†Think of this Q-table as¬†**the memory or cheat sheet of our Q-function.**

The problem is that Q-Learning is a¬†*tabular method*. Aka, a problem in which the state and actions spaces¬†**are small enough to approximate value functions to be represented as arrays and tables**. And this is¬†**not scalable**.

Q-Learning was working well with small state space environments like:

- FrozenLake, we had 14 states.
- Taxi-v3, we had 500 states.

But think of what we're going to do today: we will train an agent to learn to play Space Invaders using the frames as input.
Conclusion [[conclusion]]

Congrats on finishing this chapter!¬†There was a lot of information. And congrats on finishing the tutorial. You‚Äôve just trained your first Deep Q-Learning agent and shared it on the Hub ü•≥.

Take time to really grasp the material before continuing.

Don't hesitate to train your agent in other environments (Pong, Seaquest, QBert, Ms Pac Man). The¬†**best way to learn is to try things on your own!**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif"" alt=""Environments""/>


In the next unit, **we're going to learn about Optuna**. One of the most critical tasks in Deep Reinforcement Learning is to find a good set of training hyperparameters. Optuna is a library that helps you to automate the search.

Finally, we would love **to hear what you think of the course and how we can improve it**. If you have some feedback then please üëâ  [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)
---


‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit3/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*

[In the last unit](https://huggingface.co/blog/deep-rl-q-part2), we learned our first reinforcement learning algorithm: Q-Learning,¬†**implemented it from scratch**, and trained it in two environments, FrozenLake-v1 ‚òÉÔ∏è and Taxi-v3 üöï.

We got excellent results with this simple algorithm. But these environments were relatively simple because the **State Space was discrete and small** (14 different states for FrozenLake-v1 and 500 for Taxi-v3).

But as we'll see, producing and updating a¬†**Q-table can become ineffective in large state space environments.**
```

Congrats ü•≥ you've just implemented from scratch, trained, and uploaded your first Reinforcement Learning agent.
FrozenLake-v1 no_slippery is very simple environment, let's try a harder one üî•.

# Part 2: Taxi-v3 üöñ

## Create and understand [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)
---

üí° A good habit when you start to use an environment is to check its documentation

üëâ https://gymnasium.farama.org/environments/toy_text/taxi/

---

In `Taxi-v3` üöï, there are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue).

When the episode starts, **the taxi starts off at a random square** and the passenger is at a random location. The taxi drives to the passenger‚Äôs location, **picks up the passenger**, drives to the passenger‚Äôs destination (another one of the four specified locations), and then **drops off the passenger**. Once the passenger is dropped off, the episode ends.
Introducing Q-Learning [[q-learning]]
## What is Q-Learning? [[what-is-q-learning]]

Q-Learning is an¬†**off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end¬†of this unit.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us¬†**the value of each state or each state-action pair.**
- *TD approach:*¬†**updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.

Here is the question: What is the best method to evaluate a FrozenLake Q-learning agent?
Here is answer 1:Use prebuilt policies without evaluation
Here is answer 2: Combine agent evaluation with interactivity improvements
Here is answer3: Skip evaluation for deterministic environments
Here is answer 4: Rely solely on leaderboard results",None estoy seguro de la respuesta correcto>No es una de las opciones / I'm not sure about correct answer > It's none
Which combination optimizes Gradio demos for semantic segmentation?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (10253, 'e0fdcfc2-6e25-4d57-8dea-e42c925a1e2c'), (530, '283699ed-c384-40ef-bd5c-44fb732077a8'), (28185, 'd648591c-3383-45e8-ae71-83b504e2b8bd'), (5660, 'f060d5f9-0dce-4d78-8298-2524cde9ccdb'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (16430, '66815a93-d7da-4188-9272-6bbfdf32ae6b'), (7741, '404ecfe6-cf93-4fff-b68d-6c5d1f3e1871'), (11838, 'e4ab66e3-f6c3-4b1e-a385-2e87d7ec1d07'), (27724, '97cf97fc-7c5f-4d7b-9e58-02224ba44e30'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (28243, '0bf9268d-f628-4160-b2fb-5d484fbd603f'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (28255, 'bcb424b2-729f-4991-87ac-45ef5ce77853'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (10863, 'd34b0609-c441-46f8-90ed-ae0defa50563'), (10864, '9971a567-07ea-4560-98d4-eede4b1d186b'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (1144, 'a663b646-8b6a-43f0-8527-660d724e1849'), (28281, '1f26f38e-955f-43c3-b484-cf828c48fd85'), (28282, '464f484f-7d78-406d-9b75-a994dc1cba25'), (28284, 'bfc33107-d72c-472b-95c4-03530bf3a56d'), (25213, 'a685a826-ac2d-4ea1-abcd-a005d3666f4a'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (28305, '35e22005-7747-4988-a75c-23a2275641c0'), (2201, 'f63d9476-9b03-480b-81dc-56db1f33810e'), (31387, '47d6c03e-2d5b-4713-9953-0a20c8fa6bbf'), (2203, '1daeda64-b42f-49a8-913c-f245e97796d6'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (27817, 'a9c81310-0c86-4115-9626-68c493091cda'), (3250, '2210ab99-86d5-4a6e-aced-d630e74a9aac'), (27841, 'c37d24ce-3d99-4ab8-9016-ad2a6d6c1e39'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (8420, 'f005d893-d4fc-411d-96ea-a435bfa37288'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (6403, '6b72a701-c1b5-46a7-bcb8-a543df789c1f'), (2311, '2ddd9274-7512-4bdd-8e03-f54c56c6fdfd'), (24842, '0d694989-93f1-48d3-b256-e21d25ab982b'), (24331, 'e99463c9-3fff-46a2-98cd-b973d53da958'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (30999, '53998aea-d5c1-4b05-87c7-09e519c96443'), (22318, '04f5bd94-17e7-41e7-8caa-f844242aacf8'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (6453, '250cf8b6-8493-460d-8374-b98f53b24df3'), (2369, '2d8765ce-950f-4812-be38-74a3ca44c156'), (10049, '54ef31a1-53bb-423e-9653-c5f902ed05ab'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (13124, 'aea52821-30f4-464d-97eb-a126eed5e070'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18247, '74315d00-318c-4e2d-84b1-cb46200e84ce'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (13129, '39d1b188-cb2a-4d4c-b760-cc5e51bb538d'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (2381, '25022ddc-db5e-48bf-a2bf-918a00646b76'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (22866, 'ddec1804-5a36-4f25-9429-414ca69c8abd'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (20310, '03e6bed2-053e-46f3-83c9-e9e52ffe5997'), (19290, 'bdaea1c7-fd99-40b6-873e-6221088c435a'), (3934, '24000b88-1568-44be-a5d6-7fe94664b817'), (5474, '1158d6f0-fad9-4bbb-babf-d6178165afc7'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (23395, '0d90b5c9-4b7b-4a57-86dc-af8d7fcb6c9c'), (2407, '59755c96-9597-4369-8901-6b6346cf8892'), (2408, '3c930812-0a9f-46fb-92ef-42ccc975ebc5'), (2410, '9ae572e8-8387-48a6-b43e-2d6abf2e6560'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (28536, 'a61182a7-0b78-447e-b85b-2fed9d53fc77'), (20857, '1561c4d2-349a-4a35-b8b9-2422553f4829'), (2431, 'd82501d2-94be-47fd-843d-2fab52c53bb9'), (24974, 'f1c0e348-09df-4e2b-b7ca-2b4104b24b23'), (3989, '997e9fba-c98d-41fc-a421-7d9ef34b2825'), (1943, 'd18071fb-d551-4b15-bdda-1775319ff90a'), (9629, '996348a9-c581-4d4a-aa01-4f8cccfad6ae'), (15781, '6a546fd4-6a50-4e31-8789-c9e292a36bec'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (28075, 'bc307f87-d3b6-4e90-95d8-df896b01cb89'), (28077, '880f9923-936e-43dd-bb07-042c35cd6eb6'), (1967, 'e50667e5-7a0c-461c-aff5-9a06cf3ab823'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (13746, '0bb29eea-90a5-4fd6-8177-0ccbbf32e132'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (4028, 'c98b654b-2b16-424d-a288-12544a686cb5'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (13794, '3d2170c8-cc85-48a0-9180-b22d135a5f1f'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (29678, '978c5788-4b70-4882-ad94-79625d10e12f'), (10238, '170dd237-1dba-434b-a54e-3b6b725cb863')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Gradio Demo: text_analysis
### This simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation.
        


```
!pip install -q gradio spacy
```
his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation.
Gradio Demo: image_segmentation
### Simple image segmentation using gradio's AnnotatedImage component.
        


```
!pip install -q gradio 
```
Gradio Demo: Echocardiogram-Segmentation


```
!pip install -q gradio -f https://download.pytorch.org/whl/torch_stable.html numpy matplotlib wget torch torchvision   
```


```
# Downloading files from the demo repo
import os
!wget -q https://github.com/gradio-app/gradio/raw/main/demo/Echocardiogram-Segmentation/img1.jpg
!wget -q https://github.com/gradio-app/gradio/raw/main/demo/Echocardiogram-Segmentation/img2.jpg
```
imple image segmentation using gradio's AnnotatedImage component.
Gradio Demo: annotatedimage_component


```
!pip install -q gradio 
```


```
import gradio as gr
import pathlib
from PIL import Image
import numpy as np
import urllib.request


source_dir = pathlib.Path(__file__).parent

urllib.request.urlretrieve(
  'https://gradio-builds.s3.amazonaws.com/demo-files/base.png',
   str(source_dir / ""base.png"")
)
urllib.request.urlretrieve(
    ""https://gradio-builds.s3.amazonaws.com/demo-files/buildings.png"",
    str(source_dir / ""buildings.png"")
)

base_image = Image.open(str(source_dir / ""base.png""))
building_image = Image.open(str(source_dir / ""buildings.png""))

# Create segmentation mask
building_image = np.asarray(building_image)[:, :, -1] > 0

with gr.Blocks() as demo:
    gr.AnnotatedImage(
        value=(base_image, [(building_image, ""buildings"")]),
        height=500,
    )

demo.launch()
```
### Full Changelog:

- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https://github.com/aliabd) in [PR 3866](https://github.com/gradio-app/gradio/pull/3866)
- Fix error in running `gr.themes.builder()` by [@deepkyu](https://github.com/deepkyu) in [PR 3869](https://github.com/gradio-app/gradio/pull/3869)
- Fixed a JavaScript TypeError when loading custom JS with `_js` and setting `outputs` to `None` in `gradio.Blocks()` by [@DavG25](https://github.com/DavG25) in [PR 3883](https://github.com/gradio-app/gradio/pull/3883)
- Fixed bg_background_fill theme property to expand to whole background, block_radius to affect form elements as well, and added block_label_shadow theme property by [@aliabid94](https://github.com/aliabid94) in [PR 3590](https://github.com/gradio-app/gradio/pull/3590)

### Contributors Shoutout:

No changes to highlight.

## 3.27.0

### New Features:

###### AnnotatedImage Component
### Full Changelog:

- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https://github.com/aliabd) in [PR 3866](https://github.com/gradio-app/gradio/pull/3866)
- Fix error in running `gr.themes.builder()` by [@deepkyu](https://github.com/deepkyu) in [PR 3869](https://github.com/gradio-app/gradio/pull/3869)
- Fixed a JavaScript TypeError when loading custom JS with `_js` and setting `outputs` to `None` in `gradio.Blocks()` by [@DavG25](https://github.com/DavG25) in [PR 3883](https://github.com/gradio-app/gradio/pull/3883)
- Fixed bg_background_fill theme property to expand to whole background, block_radius to affect form elements as well, and added block_label_shadow theme property by [@aliabid94](https://github.com/aliabid94) in [PR 3590](https://github.com/gradio-app/gradio/pull/3590)

### Contributors Shoutout:

No changes to highlight.

## 3.27.0

### New Features:

###### AnnotatedImage Component
```

See the [image_segmentation demo](https://github.com/gradio-app/gradio/tree/main/demo/image_segmentation) for a full example. By [@aliabid94](https://github.com/aliabid94) in [PR 3836](https://github.com/gradio-app/gradio/pull/3836)

### Bug Fixes:

No changes to highlight.

### Documentation Changes:

No changes to highlight.

### Testing and Infrastructure Changes:

No changes to highlight.

### Breaking Changes:

No changes to highlight.

### Full Changelog:

No changes to highlight.

### Contributors Shoutout:

No changes to highlight.

## 3.26.0

### New Features:

###### `Video` component supports subtitles

- Allow the video component to accept subtitles as input, by [@tomchang25](https://github.com/tomchang25) in [PR 3673](https://github.com/gradio-app/gradio/pull/3673). To provide subtitles, simply return a tuple consisting of `(path_to_video, path_to_subtitles)` from your function. Both `.srt` and `.vtt` formats are supported:
```

See the [image_segmentation demo](https://github.com/gradio-app/gradio/tree/main/demo/image_segmentation) for a full example. By [@aliabid94](https://github.com/aliabid94) in [PR 3836](https://github.com/gradio-app/gradio/pull/3836)

### Bug Fixes:

No changes to highlight.

### Documentation Changes:

No changes to highlight.

### Testing and Infrastructure Changes:

No changes to highlight.

### Breaking Changes:

No changes to highlight.

### Full Changelog:

No changes to highlight.

### Contributors Shoutout:

No changes to highlight.

## 3.26.0

### New Features:

###### `Video` component supports subtitles

- Allow the video component to accept subtitles as input, by [@tomchang25](https://github.com/tomchang25) in [PR 3673](https://github.com/gradio-app/gradio/pull/3673). To provide subtitles, simply return a tuple consisting of `(path_to_video, path_to_subtitles)` from your function. Both `.srt` and `.vtt` formats are supported:
examples = [[""img1.jpg""], [""img2.jpg""]]
title = None #""Left Ventricle Segmentation""
description = ""This semantic segmentation model identifies the left ventricle in echocardiogram images.""
# videos. Accurate evaluation of the motion and size of the left ventricle is crucial for the assessment of cardiac function and ejection fraction. In this interface, the user inputs apical-4-chamber images from echocardiography videos and the model will output a prediction of the localization of the left ventricle in blue. This model was trained on the publicly released EchoNet-Dynamic dataset of 10k echocardiogram videos with 20k expert annotations of the left ventricle and published as part of ‚ÄòVideo-based AI for beat-to-beat assessment of cardiac function‚Äô by Ouyang et al. in Nature, 2020.""
thumbnail = ""https://raw.githubusercontent.com/gradio-app/hub-echonet/master/thumbnail.png""
Gradio, check![[gradio-check]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

This wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:

- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities.
- Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces).
- How to integrate Gradio demos with models and Spaces on the Hugging Face Hub.
- Advanced features like storing state in a demo or providing authentication.
- How to have full control of the data flow and layout of your demo with Gradio Blocks.

If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section!

## Where to next?[[where-to-next]]

If you want to learn more about Gradio you can
- The inference will be much faster.
- The server caches your requests.
- You get built-in automatic scaling.

## Hosting your Gradio demos on Spaces

[Hugging Face Spaces](https://hf.co/spaces) allows anyone to host their Gradio demos freely, and uploading your Gradio demos take a couple of minutes. You can head to [hf.co/new-space](https://huggingface.co/new-space), select the Gradio SDK, create an `app.py` file, and voila! You have a demo you can share with anyone else. To learn more, read [this guide how to host on Hugging Face Spaces using the website](https://huggingface.co/blog/gradio-spaces).

Alternatively, you can create a Space programmatically, making use of the [huggingface_hub client library](https://huggingface.co/docs/huggingface_hub/index) library. Here's an example:
and already surpasses Swin trained with 3\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3.*
It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:

1. **Standard demos**: which have both separate inputs and outputs (e.g. an image classifier or speech-to-text model)
2. **Output-only demos**: which don't take any input but produce on output (e.g. an unconditional image generation model)
3. **Input-only demos**: which don't produce any output but do take in some sort of input (e.g. a demo that saves images that you upload to a persistent external database)
4. **Unified demos**: which have both input and output components, but the input and output components _are the same_. This means that the output produced overrides the input (e.g. a text autocomplete model)

Depending on the kind of demo, the user interface (UI) looks slightly different:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/interfaces4.png)
Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API

Hugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load(""Helsinki-NLP/opus-mt-en-es"", src=""models"")

demo.launch()
* We've also embranced a much cleaner design that will allow Gradio demos to fit in visually in more settings (such as being <a href=""https://discuss.huggingface.co/t/gradio-iframe-embedding/13021/9?u=abidlabs"">embedded</a> in blog posts). 

<img class=""max-w-full mx-auto my-6"" style=""width: 54rem"" src=""/blog/assets/68_gradio_blocks/lion.jpg"">

* We've revamped our existing components, like `Dataframe` to be more user-friendly (try dragging-and-dropping a CSV file into a Dataframe) as well as added new components, such as the `Gallery`, to allow you to build the right UI for your model.

<img class=""max-w-full mx-auto my-6"" style=""width: 54rem"" src=""/blog/assets/68_gradio_blocks/dalle.jpg"">

* We've added a `TabbedInterface` class which allows you to group together related demos as multiple tabs in one web app

<img class=""max-w-full mx-auto my-6"" style=""width: 54rem"" src=""/blog/assets/68_gradio_blocks/tts.png"">
The 4 Kinds of Gradio Interfaces

So far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.

It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:
Introduction to Gradio[[introduction-to-gradio]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

In this chapter we will be learning about how to build **interactive demos** for your machine learning models.

Why build a demo or a GUI for your machine learning model in the first place? Demos allow:

- **Machine learning developers** to easily present their work to a wide audience including non-technical teams or customers
- **Researchers** to more easily reproduce machine learning models and behavior
- **Quality testers** or **end users** to more easily identify and debug failure points of models
- **Diverse users** to discover algorithmic biases in models

We'll be using the Gradio library to build demos for our models. Gradio allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.

Here are some examples of machine learning demos built with Gradio:
Here are some examples of machine learning demos built with Gradio:

* A **sketch recognition** model that takes in a sketch and outputs labels of what it thinks is being drawn:

<iframe src=""https://course-demos-draw2.hf.space"" frameBorder=""0"" height=""450"" title=""Gradio app"" class=""container p-0 flex-grow space-iframe"" allow=""accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"" sandbox=""allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads""></iframe>

Here is the question: Which combination optimizes Gradio demos for semantic segmentation?
Here is answer 1:Focus only on segmentation weights
Here is answer 2: Apply rendering fixes and interactivity updates
Here is answer3: Skip segmentation preprocessing and test only outputs
Here is answer 4: Rely on pretrained models without updates",The correct answer is : Apply rendering fixes
How can reinforcement learning workflows benefit from Gradio enhancements?,"[(28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (27658, 'd78d6628-059b-4669-aa0e-3ec08cbd5db3'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (4119, 'b7a0eee1-d5ca-4ce1-b308-ee2deffe6bce'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (2075, '57697ebc-669f-4aa3-8585-4c1388c93630'), (5661, 'd8772dad-7e28-4ef1-bdae-a1b7fda9c18c'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (28212, '5e9daa1d-4aac-4b33-b060-98ba4480ba10'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22584, '2b489632-adb3-4d63-b746-a4475eaa6ee6'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (28219, '53ccc20d-0caf-44e6-ba84-9d617960607b'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (11835, '8acce077-7041-4728-9cb1-9671a95f2fdc'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (28261, 'bfc07af5-45ac-4bb9-85a4-5ee400369c7a'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29292, '4214c4f3-080e-4daf-807b-01ee54650589'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (28279, '835b3d07-8075-4019-a72c-9d9253024dd9'), (25210, '2697df78-2558-4fe9-b9f1-6f730389b8e0'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7803, '6a89c97a-46b0-4481-8f91-7c30ad2193d6'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (6791, 'caea27fd-5810-4f6f-afa1-5befc362cced'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (23189, '36e981bb-328e-4d35-a16b-5facd733cb56'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (2208, 'ba593b1c-6f5d-49c6-b377-4b55c72484e4'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (27351, '472b1c8c-b7ef-4926-8092-42cba02c3d9c'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (2285, '5b7edfae-d1bb-4c5d-8d8f-83b616f05d0d'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (6397, 'a42d6ffe-a12b-4bd0-b1ee-ef817d09a213'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (23807, '9d7cdc35-737d-4b2c-9e58-0c6898cdfc31'), (24842, '0d694989-93f1-48d3-b256-e21d25ab982b'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (2338, '573a62da-e270-46be-9b10-d3f2a1466060'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2345, '3bd3225a-2064-438c-80c1-e672696ab2a2'), (2860, 'cd0fbf96-6263-4efc-9922-f67f4cce3a51'), (27949, 'f9f3c1af-030d-42eb-b727-680bb2457e6d'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (2387, '7c12efc3-2c67-4d4e-b835-f6c066184a68'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (20826, '03cab1e0-bceb-43ed-b419-ce42e8f8c4eb'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (2405, 'c11b172e-c44e-453b-802d-3a9e7e40954e'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (2464, '54179c43-bf4a-4c14-b098-b5140e0a1a4b'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (12720, '43bba8c3-1827-429c-a918-da99a6efbc71'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (28082, '8c86f9c1-60c8-40ed-8084-d82bfb3fc00a'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (27066, '580b5cc7-1b78-434c-b69f-5d96e6bd54a9'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29666, 'd518d011-4a11-4759-922a-8c4af66d1d89'), (5613, '5730d53c-cfc2-46cd-8958-07e68d2c19e5'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30192, '0dd45130-aa3c-4726-bab7-30afeed79b5b'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283'), (28159, '82652842-43af-40f0-9e66-fbd1e5adac72')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.1.0

### Features

- [#5005](https://github.com/gradio-app/gradio/pull/5005) [`f5539c76`](https://github.com/gradio-app/gradio/commit/f5539c7618e31451420bd3228754774da14dc65f) - Enhancement: Add focus event to textbox and number component. Thanks [@JodyZ0203](https://github.com/JodyZ0203)!

### Fixes
```

## More advanced: temporarily upgrade your Space !

Spaces allow for a lot of different use cases. Sometimes, you might want
to temporarily run a Space on a specific hardware, do something and then shut it down. In
this section, we will explore how to benefit from Spaces to finetune a model on demand.
This is only one way of solving this particular problem. It has to be taken as a suggestion
and adapted to your use case.

Let's assume we have a Space to finetune a model. It is a Gradio app that takes as input
a model id and a dataset id. The workflow is as follows:

0. (Prompt the user for a model and a dataset)
1. Load the model from the Hub.
2. Load the dataset from the Hub.
3. Finetune the model on the dataset.
4. Upload the new model to the Hub.
## Introduction

Tabular data science is the most widely used domain of machine learning, with problems ranging from customer segmentation to churn prediction. Throughout various stages of the tabular data science workflow, communicating your work to stakeholders or clients can be cumbersome; which prevents data scientists from focusing on what matters, such as data analysis and model building. Data scientists can end up spending hours building a dashboard that takes in dataframe and returning plots, or returning a prediction or plot of clusters in a dataset. In this guide, we'll go through how to use `gradio` to improve your data science workflows. We will also talk about how to use `gradio` and [skops](https://skops.readthedocs.io/en/stable/) to build interfaces with only one line of code!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started).

## Let's Create a Simple Interface!
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.0.2

### Patch Changes

- Updated dependencies []:
  - @gradio/utils@0.0.2
- Updated dependencies [[`8f0fed857`](https://github.com/gradio-app/gradio/commit/8f0fed857d156830626eb48b469d54d211a582d2)]:
  - @gradio/icons@0.2.0
  - @gradio/atoms@0.1.3

## 0.2.0

### Features

- [#5342](https://github.com/gradio-app/gradio/pull/5342) [`afac0006`](https://github.com/gradio-app/gradio/commit/afac0006337ce2840cf497cd65691f2f60ee5912) - significantly improve the performance of `gr.Dataframe` for large datasets. Thanks [@pngwn](https://github.com/pngwn)!

## 0.1.1

### Patch Changes

- Updated dependencies [[`abf1c57d`](https://github.com/gradio-app/gradio/commit/abf1c57d7d85de0df233ee3b38aeb38b638477db)]:
  - @gradio/icons@0.1.0
  - @gradio/atoms@0.1.1

## 0.1.0

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features

Here is the question: How can reinforcement learning workflows benefit from Gradio enhancements?
Here is answer 1:Focus only on predefined interaction components
Here is answer 2: Use modular Gradio updates for scalable RL tasks
Here is answer3: Skip interactivity in RL workflows
Here is answer 4: Apply Gradio components without debugging","Based on my analysis using Natural Language Processing methods I believe best possible answer should would simply ""None"" because there isn't any clear relationship between reinforcement learning workflows benefiting directly from grADIO enhancements within provided text since grADIO seems more focused towards means other aspects like tabular data science workflwos etc , So all options seem missleading ."
What makes Q-learning scalable for complex environments?,"[(26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (26654, '042ecf39-629a-44e3-be8d-e471b2b8640a'), (26655, '14ea53e1-bcb7-4126-b80b-09759ba5a722'), (26656, '7cd01870-b453-47fe-b8b4-83d5c97bd9da'), (17966, '0b146d2b-e740-4ab1-861b-c17317d4c9b8'), (7732, '8d07afb9-9121-46ff-a877-120dcac45b78'), (7734, '85ca7829-223b-4021-a48e-c1331212e356'), (27722, 'c667aeb9-e35c-4791-8c36-ac06162bf37e'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (21604, 'ef6f4014-2b68-45df-b66a-a23331cdcc1b'), (14437, '6f4d674a-5ac8-464d-8dde-f98b8f555e87'), (14438, 'adc4d77e-0477-455a-8002-6bde28537b35'), (26727, 'aa34abd9-5ca0-479f-8fe5-8f5a215baf20'), (26728, '7b671840-3675-4abf-895d-c4fe995d2dfd'), (28782, 'f832e002-e981-4806-b6b5-7743c24c81db'), (28784, '351f6005-a2b3-4b17-abf5-a5a07d1bf877'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (17525, '7500c161-c38b-4986-ae2a-ce026b6c10f7'), (17527, 'bd457bd8-f4ad-4b21-9f6c-2cc379928d75'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (17532, 'ae190843-2669-417f-bcc6-7b17066f1ded'), (17533, 'f2d73255-0ee5-4a2b-a2a4-b10e4d2c0612'), (17534, 'e8a226a9-ce5d-412c-b8bb-1a951a1857fb'), (17539, '54748e65-182a-4269-bd1f-6a11d3a531d1'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (144, 'ad9a2442-d5bf-479e-bf4a-b911cfbe1dd2'), (17552, 'e6dc2f24-99a1-4fd8-a073-c0a439c5d184'), (17557, '0f6b36da-2ed0-49e5-b5d2-09ced93bbc46'), (17560, '4985bb51-18be-411a-b1d0-3fd7d5abb568'), (8347, '6aa8880a-8695-4c23-b28b-def12d9eea77'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (8871, '370b7369-f26d-4340-86fd-357c2065b987'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25265, '177935f5-c8bc-4c5a-9cf0-07eefafb6b83'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (17585, '57d5dfc1-963c-4fe2-8c61-5ac6acc65e15'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25270, '5bbe44fe-46d8-484e-8cdc-15d0afcee3ee'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25281, 'a825bb91-b46e-497a-b2bf-52f559ad86d8'), (25282, 'c5f90a36-0447-48df-ba63-aaba6325e62e'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (5869, 'bb6cdbd7-bbcf-493e-8250-030bab25db6c'), (27373, '1cf527a1-db9e-45b4-affe-fab3bfe9777e'), (5872, '16f36727-d1a9-4d89-9ff9-364a903634fa'), (11507, '97835531-9aad-446d-b001-fcb29b66d609'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (21807, 'b3346506-6c4a-48ac-9d75-c1258f287641'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (20325, '8e25c4d9-f41e-426e-a372-9465025605ad'), (20326, '9d94613c-da69-433f-a73d-136ad258d459'), (20327, 'db43986d-7e00-4d86-8a36-11e8aaa8e37e'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (20329, '32e96d05-f2b2-42de-bd87-cde71e890e69'), (20330, '307f8dbe-c15b-4989-a66b-e9a437fb3480'), (20331, 'ad781e21-34b5-465a-b44f-1d0ef976b671'), (28525, '60cdc2d0-c35a-420d-bc30-64e0ac98ea5a'), (28526, '99ae54b8-b8c0-49a8-b122-3be8abec3e42'), (20334, '667dc4e1-4197-4c21-ae58-e797af402ed0'), (20336, 'd2582ee2-dd1b-40d8-ab3e-09b77460db3b'), (20335, '24699ce0-932d-4081-a7d8-ed55918efde0'), (28527, '9dd374d0-f99b-4b6b-9b15-5f4ce6382aa4'), (20337, '1ac41f0f-b53f-41f2-adc7-29016e775aba'), (28529, 'f43bdd02-a29e-4b4f-a712-94cf31ec3551'), (28534, '3e54a2ff-2d80-46f7-870f-7a3153c2036b'), (28535, 'c4e1adc6-dc57-4146-96e1-ff983b95b866'), (20343, '5f332e1f-fff2-4a93-becc-b7ee2e2e97ce'), (20345, '90c786f2-9de0-48b8-b056-36c300207419'), (20344, 'edac2d6a-a444-44cc-b455-7144e89a1f09'), (20854, '110af335-df5a-4138-bd7b-a0ef1308d837'), (20342, '63ba5e06-4514-4380-a011-5c55dede8208'), (23946, '0d096575-2d3c-4976-93fc-0610e2da9ae0'), (23947, '925054c6-05b5-4bda-a736-9a6f6991a7ae'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (3481, '4c7ec93f-67c6-4662-83f9-0ef6f0461760'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (14773, 'eddfb9b5-65fd-4a2d-a578-118281f8c55e'), (14774, 'd85cfc80-67be-498d-9904-3d83a553094a'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (14776, '25ac1795-a4d1-45b8-837a-29dc4ccf5415'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (962, 'c68008e9-dc91-4db0-a2f0-89a793040fff'), (11714, '81248f40-6e7d-4225-8017-1c87a3d5c64a'), (27085, 'ee8bb765-44a6-4187-9e25-b70671fe43b7'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13782, 'cc4cbb46-9352-4787-bd3e-b5aec0c09d4e'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: From Q-Learning to Deep Q-Learning [[from-q-to-dqn]]

We learned that¬†**Q-Learning is an algorithm we use to train our Q-Function**, an¬†**action-value function**¬†that determines the value of being at a particular state and taking a specific action at that state.

<figure>
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg"" alt=""Q-function""/>
</figure>

The¬†**Q comes from ""the Quality"" of that action at that state.**

Internally, our Q-function is encoded by¬†**a Q-table, a table where each cell corresponds to a state-action pair value.**¬†Think of this Q-table as¬†**the memory or cheat sheet of our Q-function.**

The problem is that Q-Learning is a *tabular method*. This becomes a problem if the states and actions spaces **are not small enough to be represented efficiently by arrays and tables**. In other words: it is **not scalable**.
Q-Learning worked well with small state space environments like:
```

## Evaluate our Q-Learning agent üìà

- Usually, you should have a mean reward of 1.0
- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex.

```python
# Evaluate our Agent
mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f""Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}"")
<figure class=""image table text-center m-0 w-full""> <img src=""assets/73_deep_rl_q_part2/Q-function.jpg"" alt=""Q-function""/> <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption> </figure>

The¬†**Q comes from ""the Quality"" of that action at that state.**

Internally, our Q-function has¬†**a Q-table, a table where each cell corresponds to a state-action pair value.**¬†Think of this Q-table as¬†**the memory or cheat sheet of our Q-function.**

The problem is that Q-Learning is a¬†*tabular method*. Aka, a problem in which the state and actions spaces¬†**are small enough to approximate value functions to be represented as arrays and tables**. And this is¬†**not scalable**.

Q-Learning was working well with small state space environments like:

- FrozenLake, we had 14 states.
- Taxi-v3, we had 500 states.

But think of what we're going to do today: we will train an agent to learn to play Space Invaders using the frames as input.
_____________________________________________________________________
Congrats ü•≥, you've just implemented, trained, and uploaded your first Reinforcement Learning agent.

Understanding Q-Learning is an **important step to understanding value-based methods.**

In the next Unit with Deep Q-Learning, we'll see that while creating and updating a Q-table was a good strategy ‚Äî **however, it is not scalable.**

For instance, imagine you create an agent that learns to play Doom.

<img src=""https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png"" alt=""Doom""/>

Doom is a large environment with a huge state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient.

That's why we'll study Deep Q-Learning in the next unit, an algorithm **where we use a neural network that approximates, given a state, the different Q-values for each action.**
But as we'll see, producing and updating a¬†**Q-table can become ineffective in large state space environments.**

So in this unit, **we'll study our first Deep Reinforcement Learning agent**: Deep Q-Learning. Instead of using a Q-table, Deep Q-Learning uses a Neural Network that takes a state and approximates Q-values for each action based on that state.

And **we'll train it to play Space Invaders and other Atari environments using [RL-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)**, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results, and recording videos.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif"" alt=""Environments""/>

So let‚Äôs get started! üöÄ
But as we'll see, producing and updating a¬†**Q-table can become ineffective in large state space environments.**

So today, **we'll study our first Deep Reinforcement Learning agent**: Deep Q-Learning. Instead of using a Q-table, Deep Q-Learning uses a Neural Network that takes a state and approximates Q-values for each action based on that state.

And **we'll train it to play Space Invaders and other Atari environments using [RL-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)**, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results, and recording videos.
  
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/78_deep_rl_dqn/atari-envs.gif"" alt=""Environments""/>
</figure>
  
So let‚Äôs get started! üöÄ

To be able to understand this unit, **you need to understand [Q-Learning](https://huggingface.co/blog/deep-rl-q-part2) first.**
---
Now that we **studied the theory of Q-Learning**, let's **implement it from scratch**. A Q-Learning agent that we will train in two environments:

1. *Frozen-Lake-v1* ‚ùÑÔ∏è (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. *An autonomous taxi* üöï will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/envs.gif"" alt=""Environments""/>
</figure>

Start the tutorial here üëâ https://colab.research.google.com/github/huggingface/deep-rl-class/blob/main/unit2/unit2.ipynb

The leaderboard üëâ https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard
Conclusion [[conclusion]]

Congrats on finishing this chapter!¬†There was a lot of information. And congrats on finishing the tutorial. You‚Äôve just trained your first Deep Q-Learning agent and shared it on the Hub ü•≥.

Take time to really grasp the material before continuing.

Don't hesitate to train your agent in other environments (Pong, Seaquest, QBert, Ms Pac Man). The¬†**best way to learn is to try things on your own!**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif"" alt=""Environments""/>


In the next unit, **we're going to learn about Optuna**. One of the most critical tasks in Deep Reinforcement Learning is to find a good set of training hyperparameters. Optuna is a library that helps you to automate the search.

Finally, we would love **to hear what you think of the course and how we can improve it**. If you have some feedback then please üëâ  [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)
So, in the second part, we‚Äôll **study Q-Learning**, **and implement our first RL agent from scratch**, a Q-Learning agent, and will train it in two environments:

1. Frozen Lake v1 ‚ùÑÔ∏è: where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi üöï: where the agent will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/envs.gif"" alt=""Environments""/>
</figure>

This unit is fundamental if you want to be able to work on Deep Q-Learning (Unit 3).

So let‚Äôs get started! üöÄ

- [Introducing Q-Learning](#introducing-q-learning)
  - [What is Q-Learning?](#what-is-q-learning)
  - [The Q-Learning algorithm](#the-q-learning-algorithm)
  - [Off-policy vs. On-policy](#off-policy-vs-on-policy)
- [A Q-Learning example](#a-q-learning-example)
Deep Q-Learning [[deep-q-learning]]

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/thumbnail.jpg"" alt=""Unit 3 thumbnail"" width=""100%"">



In the last unit, we learned our first reinforcement learning algorithm: Q-Learning,¬†**implemented it from scratch**, and trained it in two environments, FrozenLake-v1 ‚òÉÔ∏è and Taxi-v3 üöï.

We got excellent results with this simple algorithm, but these environments were relatively simple because the **state space was discrete and small** (16 different states for FrozenLake-v1 and 500 for Taxi-v3). For comparison, the state space in Atari games can **contain \\(10^{9}\\) to \\(10^{11}\\) states**.

But as we'll see, producing and updating a¬†**Q-table can become ineffective in large state space environments.**
- FrozenLake, we had 14 states.
- Taxi-v3, we had 500 states.

But think of what we're going to do today: we will train an agent to learn to play Space Invaders using the frames as input.

As¬†**[Nikita Melkozerov mentioned](https://twitter.com/meln1k), Atari environments**¬†have an observation space with a shape of (210, 160, 3), containing values ranging from 0 to 255 so that gives us 256^(210x160x3) = 256^100800 (for comparison, we have approximately 10^80 atoms in the observable universe).

<img src=""assets/78_deep_rl_dqn/atari.jpg"" alt=""Atari State Space""/>

Therefore, the state space is gigantic; hence creating and updating a Q-table for that environment would not be efficient. In this case, the best idea is to approximate the Q-values instead of a Q-table using a parametrized Q-function  \\(Q_{\theta}(s,a)\\)  .

This neural network will approximate, given a state, the different Q-values for each possible action at that state. And that's exactly what Deep Q-Learning does.
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/q-ex-7.jpg"" alt=""Maze-Example""/>
</figure>

**Step 4: Update \\(Q(S_t, A_t)\\)**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/q-ex-8.jpg"" alt=""Maze-Example""/>
  </figure>
  
Because we're dead, we start a new episode. But what we see here is that¬†**with two explorations steps, my agent became smarter.**

As we continue exploring and exploiting the environment and updating Q-values using TD target, **Q-Table will give us better and better approximations. And thus, at the end of the training, we'll get an estimate of the optimal Q-Function.**

---
Now that we **studied the theory of Q-Learning**, let's **implement it from scratch**. A Q-Learning agent that we will train in two environments:
</details>

### Q2: Why can't we use a classical Q-Learning to solve an Atari Game?

<Question
	choices={[
		{
			text: ""Atari environments are too fast for Q-Learning"",
			explain: """"
		},
		{
			text: ""Atari environments have a big observation space. So creating an updating the Q-Table would not be efficient"",
			explain: """",
      correct: true
		}
	]}
/>


### Q3: Why do we stack four frames together when we use frames as input in Deep Q-Learning?

<details>
<summary>Solution</summary>

We stack frames together because it helps us **handle the problem of temporal limitation**: one frame is not enough to capture temporal information.
For instance, in pong, our agent **will be unable to know the ball direction if it gets only one frame**.
We'll also¬†**implement our first RL agent from scratch**, a Q-Learning agent, and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi: where our agent will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**


<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif"" alt=""Environments""/>

Concretely, we will:

- Learn about **value-based methods**.
- Learn about the **differences between Monte Carlo and Temporal Difference Learning**.
- Study and implement **our first RL algorithm**: Q-Learning.
Now that we understand Deep Q-Learning, let's dive deeper into the Deep Q-Network.
  
## The Deep Q-Network (DQN)
This is the architecture of our Deep Q-Learning network:
  
<img src=""assets/78_deep_rl_dqn/deep-q-network.jpg"" alt=""Deep Q Network""/>
  
As input, we take a **stack of 4 frames** passed through the network as a state and output a **vector of Q-values for each possible action at that state**. Then, like with Q-Learning, we just need to use our epsilon-greedy policy to select which action to take.
  
When the Neural Network is initialized, **the Q-value estimation is terrible**. But during training, our Deep Q-Network agent will associate a situation with appropriate action and **learn to play the game well**.
  
### Preprocessing the input and temporal limitation
We mentioned that we **preprocess the input**. It‚Äôs an essential step since we want to reduce the complexity of our state to reduce the computation time needed for training.
Here's a list of environments you can try to train your agent with:
- BeamRiderNoFrameskip-v4
- BreakoutNoFrameskip-v4
- EnduroNoFrameskip-v4
- PongNoFrameskip-v4

Also, **if you want to learn to implement Deep Q-Learning by yourself**, you definitely should look at CleanRL implementation: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif"" alt=""Environments""/>

________________________________________________________________________
Congrats on finishing this chapter!

If you‚Äôre still feel confused with all these elements...it's totally normal! **This was the same for me and for all people who studied RL.**

Take time to really **grasp the material before continuing and try the additional challenges**. It‚Äôs important to master these elements and having a solid foundations.
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari.jpg"" alt=""Atari State Space""/>

Therefore, the state space is gigantic; due to this, creating and updating a Q-table for that environment would not be efficient. In this case, the best idea is to approximate the Q-values using a parametrized Q-function  \\(Q_{\theta}(s,a)\\)  .

This neural network will approximate, given a state, the different Q-values for each possible action at that state. And that's exactly what Deep Q-Learning does.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/deep.jpg"" alt=""Deep Q Learning""/>


Now that we understand Deep Q-Learning, let's dive deeper into the Deep Q-Network.
That's why we'll study Deep Q-Learning in the next unit, an algorithm **where we use a neural network that approximates, given a state, the different Q-values for each action.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif"" alt=""Environments""/>


See you in Unit 3! üî•

## Keep learning, stay awesome ü§ó
We'll also¬†**implement our first RL agent from scratch**: a Q-Learning agent and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to¬†**go from the starting state (S) to the goal state (G)**¬†by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi will need¬†**to learn to navigate**¬†a city to¬†**transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/envs.gif"" alt=""Environments""/>
</figure>

This unit is divided into 2 parts:
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/two_parts.jpg"" alt=""Two Parts""/>
</figure>

In the first part, we'll¬†**learn about the value-based methods and the difference between Monte Carlo and Temporal Difference Learning.**

And in the second part,¬†**we'll study our first RL algorithm: Q-Learning, and implement our first RL Agent.**
---


‚ö†Ô∏è A **new updated version of this article is available here** üëâ [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit3/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus¬†[here.](https://huggingface.co/deep-rl-course/unit0/introduction)*

[In the last unit](https://huggingface.co/blog/deep-rl-q-part2), we learned our first reinforcement learning algorithm: Q-Learning,¬†**implemented it from scratch**, and trained it in two environments, FrozenLake-v1 ‚òÉÔ∏è and Taxi-v3 üöï.

We got excellent results with this simple algorithm. But these environments were relatively simple because the **State Space was discrete and small** (14 different states for FrozenLake-v1 and 500 for Taxi-v3).

But as we'll see, producing and updating a¬†**Q-table can become ineffective in large state space environments.**

Here is the question: What makes Q-learning scalable for complex environments?
Here is answer 1:Avoiding interactivity and focusing on static setups
Here is answer 2: Integrating Gradio UI workflows with RL training loops
Here is answer3: Using unstructured environments without preprocessing
Here is answer 4: Ignoring training evaluations for scalability",Answer : Atari Environements have big observation spaces so creating & Updating  
How can SQuAD metrics improve segmentation model performance evaluations?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (3, 'a958fa67-0263-40fe-aaa1-f05fdc6884b3'), (4, '6772e5c6-cd4d-4a56-84fa-c161ac6651b6'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (1560, '9c881267-9303-4686-81be-1ac7f7fd886c'), (1563, '7eb98c85-2153-41ac-b24e-c57402004544'), (24604, '0385f67b-6e32-4932-a53b-9211bd2aa9ed'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (24607, '8f241013-9385-4355-bc80-ae8a6405c9ad'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (15924, '0fa9c0d3-9d36-4b4c-9711-b6faf189ad89'), (6200, '0a8f0e5f-ea71-4a51-9143-e020ffa286f2'), (21587, '94da7690-487a-4b9a-9209-2eb5a9a7fd51'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (9862, '33a9f610-ef4f-48c2-8df9-388fa0fd2a8f'), (9863, 'a9841602-0157-4f97-8e6f-19b4d1415529'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (6310, '74a4cbde-53ef-459b-948d-593f322581a5'), (11437, 'bcf6c108-66d8-4c82-8e0b-90a0f98d8602'), (6319, '71e77401-a57c-4f0f-9fe7-add6c21b71a4'), (11951, 'e2e3c3c6-9b1e-41a4-9614-e5c2e001ff14'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (5316, 'ba980fc9-284b-4847-8d3d-2e1c0abbda6a'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (7907, 'cb3c07b7-bee9-4236-b390-7bf897dbd84d'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (7909, 'dd794d88-5bc7-4f34-a73a-8e22216ed4c4'), (3304, '918998b6-1cb0-4e26-8d59-d80c01cd2975'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (31475, 'b1f00834-8c6d-48f1-8a7f-6d5a0d9c3e52'), (1277, 'e44228be-c67f-41da-82d1-29c99a785385'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (7935, '95bc576c-e89b-469c-8b15-0fb4c4422079'), (26880, '439720fa-abdc-4492-afe7-ad647d8bd1e5'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (7939, 'c7dea0da-4b72-46f5-b615-3cc4c517e29e'), (7943, 'dd28dc4a-920f-4fc9-b713-5719ff38f1a8'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (8487, '023439b8-4419-4fa3-8b19-956261474fbd'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (13117, '058e07d2-d17f-4303-a0e9-7fe2a5c19611'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (10563, '138c4559-9b2e-43b7-a3bb-746c045905f0'), (9029, '6d2b9b44-e8ed-4ddc-9058-b910ea6f4070'), (21831, '42729a1e-e5c7-400b-86f3-8e2c67aaf58e'), (13128, '33d38409-c046-4268-bacb-644ced3c476a'), (8011, '3742404d-4578-469e-9ebc-84b9395496ff'), (6477, 'c6d1f800-e6ad-43b9-8a32-911ce5083a43'), (4944, '61aa7427-e3a9-48ae-b6cb-d54b354c21ee'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (12120, '0bbac7aa-9cb9-46e0-89f9-cf746b68cc80'), (7515, '0b6ef7a3-1204-44ab-b960-52455f130eca'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (23399, '8c4d5380-a5bc-4827-8694-d2ad31217a3d'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (3443, '974c1801-170d-4f66-9f80-84ecc9d6086f'), (2934, '07953283-5664-45cf-b67e-58683c7ff0ea'), (23425, 'bae40a82-a442-41f3-a45b-e024f39cc6ca'), (12198, 'c617332f-ef8f-477a-803c-147aed04f986'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (12203, 'aa81f46c-8e78-41dd-bd4b-57fdf62c8164'), (30638, 'bb43e4ba-f6ab-478d-8c31-6535c42d1276'), (30640, '80e20dc7-01a6-492e-85e7-e24738ee6d14'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (2511, '9d4f9ab1-abb1-483e-84e9-4cd82481acea'), (4559, '5a7e2589-6026-4893-8bd9-e030899ea13a'), (29137, '5482f963-0400-4c03-b3c5-f9f03ae4c29f'), (4566, 'cb029d8d-8fc2-4dea-a947-d50b975b7925'), (17879, '0febef04-6ab5-44f6-bc89-962863e3aea9'), (17880, 'b0df342e-5867-45e5-95f6-3db08814a8c4'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (9694, 'd82a3bcb-3b3a-47f4-b946-77d14978e196'), (29149, 'da3811b9-e3d4-4955-80f4-1f74e90996bf'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5601, '098dde3d-a5c2-4abc-bdf3-f35ebe913ba2'), (5602, '983f13dc-846c-463b-b63f-5b68c866bb0b'), (15330, '9f43616d-6dfb-4651-ae47-169eef44cbd0'), (5599, 'dd67de1c-aeff-4a2f-8d93-77772f57bfcd'), (25573, '340d60ab-d024-458a-ba51-ed5bc11a806f'), (15336, '807f8cec-83e0-450b-aa1a-a12265b5545d'), (15337, '3e36a9f2-7f43-40f7-8a1d-ffdd4c94dffc'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (17910, '47d873f7-c1c4-4475-8be6-1e33f1bae1e1'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (17912, '6c000270-627d-44ae-bac1-9ab0640951e5'), (8694, 'e750c432-a8e4-4a5f-bd8f-08dc728b2779'), (29690, 'bec5489a-802b-4c40-a81e-19c57753d246'), (8699, '815c3f58-57b7-42ae-9fe8-1c414ae5624e'), (7670, 'd97a96a2-3c50-4cf6-b418-52fae15a092d'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad) dataset, you need to feed the `question` and `context` into your model and return the `prediction_text`, which should be compared with the `references` (based on matching the `id` of the question) :
```

Again, that's rather good considering that according to [its paper](https://arxiv.org/abs/1910.01108v2) DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset.

{#if fw === 'pt'}

Now let's put everything we just did in a `compute_metrics()` function that we will use in the `Trainer`. Normally, that `compute_metrics()` function only receives a tuple `eval_preds` with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won't be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.

The `compute_metrics()` function groups the same steps as before; we just add a small check in case we don't come up with any valid answers (in which case we predict an empty string).

{:else}
```

Note that evaluation results include both the requested metric, and information about the time it took to obtain predictions through the pipeline.

<Tip>

The time performances can give useful indication on model speed for inference but should be taken with a grain of salt: they include all the processing that goes on in the pipeline. This may include tokenizing, post-processing, that may be different depending on the model. Furthermore, it depends a lot on the hardware you are running the evaluation on and you may be able to improve the performance by optimizing things like the batch size.

</Tip>

### Evaluate multiple metrics

With the [`combine`] function one can bundle several metrics into an object that behaves like a single metric. We can use this to evaluate several metrics at once with the evaluator:

```python
import evaluate
```

## Evaluate

Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the ü§ó Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):

```py
>>> import evaluate

>>> accuracy = evaluate.load(""accuracy"")
```

Then create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:

```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from evaluate import load
squad_metric = load(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
Here are other aspects of model performance to consider during evaluation:

### Interpretability

When evaluating models, **interpretability** (i.e. the ability to *interpret* results)  can be very important, especially when deploying models in production.

For instance, metrics such as [exact match](https://huggingface.co/spaces/evaluate-metric/exact_match) have a set range (between 0 and 1, or 0% and 100%) and are easily understandable to users: for a pair of strings, the exact match score is 1 if the two strings are the exact same, and 0 otherwise.
# Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
### NLP Evaluation Methods: 

#### 4.1 SQuAD v1.1 & v2.0
[SQuAD](https://huggingface.co/datasets/squad) (Stanford Question Answering Dataset) is a reading comprehension dataset of around 108k questions that can be answered via a corresponding paragraph of Wikipedia text. BERT‚Äôs performance on this evaluation method was a big achievement beating previous state-of-the-art models and human-level performance:

<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""BERT's performance on SQuAD v1.1"" src=""assets/52_bert_101/BERTs-performance-on-SQuAD1.1.png""></medium-zoom>
</figure>
```

## Limitations and bias
This metric works only with datasets that have the same format as [SQuAD v.1 dataset](https://huggingface.co/datasets/squad).

The SQuAD dataset does contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

    @inproceedings{Rajpurkar2016SQuAD10,
    title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
    author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
    booktitle={EMNLP},
    year={2016}
    }
    
## Further References
```

## Limitations and bias
This metric works only with datasets that have the same format as [SQuAD v.1 dataset](https://huggingface.co/datasets/squad).

The SQuAD dataset does contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

    @inproceedings{Rajpurkar2016SQuAD10,
    title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
    author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
    booktitle={EMNLP},
    year={2016}
    }
    
## Further References
```

Next, we'll define a function that computes the evaluation metric we want to work with. Because we're doing semantic segmentation, we'll use the [mean Intersection over Union (mIoU)](https://huggingface.co/spaces/evaluate-metric/mean_iou), directly accessible in the [`evaluate` library](https://huggingface.co/docs/evaluate/index). IoU represents the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes. Take a look at [this blogpost](https://www.jeremyjordan.me/evaluating-image-segmentation-models/) for an overview of evaluation metrics for image segmentation.

Because our model outputs logits with dimensions height/4 and width/4, we have to upscale them before we can compute the mIoU.


```python
import torch
from torch import nn
import evaluate

metric = evaluate.load(""mean_iou"")
You can find the right metric for your task by:

- **Looking at the [Task pages](https://huggingface.co/tasks)** to see what metrics can be used for evaluating models for a given task.
- **Checking out leaderboards** on sites like [Papers With Code](https://paperswithcode.com/) (you can search by task and by dataset).
-  **Reading the metric cards** for the relevant metrics and see which ones are a good fit for your use case. For example, see the [BLEU metric card](https://github.com/huggingface/evaluate/tree/main/metrics/bleu) or [SQuaD metric card](https://github.com/huggingface/evaluate/tree/main/metrics/squad).
- **Looking at papers and blog posts** published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!

### Dataset-specific metrics
```

We can now leverage the [map](https://huggingface.co/docs/datasets/v2.1.0/en/process#map) function of [datasets](https://huggingface.co/docs/datasets/index) to iterate over the validation set of squad 2 and run prediction for each data point. Therefore we write a `evaluate` helper method which uses our pipelines and applies some transformation to work with the [squad v2 metric.](https://huggingface.co/metrics/squad_v2)

*This can take quite a while (1.5h)*
the answer span. The resulting model obtains surprisingly good results on multiple benchmarks (e.g., 72.7 F1 on SQuAD
with only 128 training examples), while maintaining competitive performance in the high-resource setting.
<Tip>

Both CLIP score and CLIP direction similarity rely on the CLIP model, which can make the evaluations biased.

</Tip>

***Extending metrics like IS, FID (discussed later), or KID can be difficult*** when the model under evaluation was pre-trained on a large image-captioning dataset (such as the [LAION-5B dataset](https://laion.ai/blog/laion-5b/)). This is because underlying these metrics is an InceptionNet (pre-trained on the ImageNet-1k dataset) used for extracting intermediate image features. The pre-training dataset of Stable Diffusion may have limited overlap with the pre-training dataset of InceptionNet, so it is not a good candidate here for feature extraction.

***Using the above metrics helps evaluate models that are class-conditioned. For example, [DiT](https://huggingface.co/docs/diffusers/main/en/api/pipelines/dit). It was pre-trained being conditioned on the ImageNet-1k classes.***

### Class-conditioned image generation
```

## Limitations and bias
This metric works only with the datasets in the same format as the [SQuAD v.2 dataset](https://huggingface.co/datasets/squad_v2).

The SQuAD datasets do contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

```bibtex
@inproceedings{Rajpurkar2018SQuAD2,
title={Know What You Don't Know: Unanswerable Questions for SQuAD},
author={Pranav Rajpurkar and Jian Zhang and Percy Liang},
booktitle={ACL 2018},
year={2018}
}
```

## Limitations and bias
This metric works only with the datasets in the same format as the [SQuAD v.2 dataset](https://huggingface.co/datasets/squad_v2).

The SQuAD datasets do contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

```bibtex
@inproceedings{Rajpurkar2018SQuAD2,
title={Know What You Don't Know: Unanswerable Questions for SQuAD},
author={Pranav Rajpurkar and Jian Zhang and Percy Liang},
booktitle={ACL 2018},
year={2018}
}
was predicted), which we do with the argmax function. Then we can use a Metric from the Datasets library: it can be loaded as easily as our dataset with the load_metric function, and it returns the evaluation metric used for the dataser we are using. We can see our model did learn something as it is 85.7% accurate. To monitor the evaluation metrics during training we need to define a compute_metrics function that does the same step as before: it takes a namedtuple with predictions and labels and must return a dictionary with the metric we want to keep track of. By passing the epoch evaluation strategy to our TrainingArguments, we tell the Trainer to evaluate at the end of every epoch. Launching a training inside a notebook will then display a progress bar and complete the table you see here as you pass every epoch.
For more recent model performance, see the [dataset leaderboard](https://paperswithcode.com/dataset/squad).

## Examples 

Maximal values for both exact match and F1 (perfect match):

```python
from evaluate import load
squad_metric = load(""squad"")
predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
results = squad_metric.compute(predictions=predictions, references=references)
results
{'exact_match': 100.0, 'f1': 100.0}

Here is the question: How can SQuAD metrics improve segmentation model performance evaluations?
Here is answer 1:By using exact match for segmentation masks
Here is answer 2: Integrating precision metrics for pixel-level evaluations
Here is answer3: Skipping metric adaptation for segmentation
Here is answer 4: Focusing solely on QA performance metrics","Based upon contextual clues within this passage , I shall choose an appropriate response among available options:
Answer : By using exact match"
What is the best way to handle modular reinforcement learning workflows in Gradio demos?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (530, '283699ed-c384-40ef-bd5c-44fb732077a8'), (28185, 'd648591c-3383-45e8-ae71-83b504e2b8bd'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (16417, '0d19a33d-48bf-4fe6-9bc5-fb1aa7a04d3d'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (23595, '061aa058-a360-4e45-8e02-2368631b42a2'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (28239, '8b1cf917-5eba-4e74-9915-392dfba1eb57'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (30808, 'b5e36a89-5905-4234-9acc-68ebc76b695c'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (28255, 'bcb424b2-729f-4991-87ac-45ef5ce77853'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (7266, 'f5c4435e-d192-4727-9e6d-35acf4404166'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (27754, '4b78d6ef-9ea6-4e0e-9884-7acbb2ebb25b'), (27756, '8f9de8cd-281f-441a-b692-3d1b132a6728'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (6259, 'e1dad236-a338-4128-821c-38c369e6a47e'), (28276, 'f4a66bc9-205e-4eb3-8d50-50eb389d8352'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (28279, '835b3d07-8075-4019-a72c-9d9253024dd9'), (1145, 'abe44225-c493-44fb-82d3-494085e90422'), (28289, '574ecc0a-3fe2-4328-87f2-2be6f79b5328'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (23189, '36e981bb-328e-4d35-a16b-5facd733cb56'), (18584, '815482b7-2249-4e35-a25e-18ccc16fd17b'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (17067, '3bf582fa-4398-4a72-b777-790828b0ac34'), (17068, 'e8d4188d-34a3-4d81-96d0-85488e9eb25a'), (24789, '6151fc10-9ec2-41ab-8af7-99a5aa43b076'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (21228, '4e0af35d-a2a3-44be-8a60-2006257963bc'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (27897, '00ee70f5-c8bc-45d4-9a8b-c379cafc9c97'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (16641, '29e13c06-e204-4a93-a6ac-03bf2fc934e4'), (2311, '2ddd9274-7512-4bdd-8e03-f54c56c6fdfd'), (19214, '2d9ea80a-98e3-4bca-91ce-fe9e72bdb0ca'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (11037, 'e2587233-f088-45f5-84c5-cb20fd903daa'), (18728, '70353f36-5ed1-42b1-97fc-404e6f7df31a'), (2365, '05851a72-a5b9-4a96-af4d-3f6e2557ef22'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18247, '74315d00-318c-4e2d-84b1-cb46200e84ce'), (2381, '25022ddc-db5e-48bf-a2bf-918a00646b76'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (20310, '03e6bed2-053e-46f3-83c9-e9e52ffe5997'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (2402, 'bf619499-284f-4b2f-9463-83cf285ec6ed'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (2405, 'c11b172e-c44e-453b-802d-3a9e7e40954e'), (2415, 'a1d515ce-a0f1-4910-a9cd-e2f1170cd93b'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14249, '3bde59f7-ba94-4ec5-8768-6195057f7e50'), (14252, '77a63f90-12e6-4253-993b-4d2cad182d17'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (7095, '46239874-f414-40b3-bf11-c8ec6587d46c'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (23997, 'f5eb3def-edf7-4dfa-b630-80e186c92aed'), (13247, '7151a6c0-468c-45c7-9cb8-418c3953caaf'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (29670, 'ccfff15f-115c-4de7-b620-659b6be0a991'), (2023, 'd20a455f-f235-41b7-a1c5-5b15b10c5065'), (29673, '5fa6188c-04bd-4733-bf74-5e5ee58334e3'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (29678, '978c5788-4b70-4882-ad94-79625d10e12f'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: The 4 Kinds of Gradio Interfaces

So far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.

It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:
Now that we studied the theory behind PPO, the best way to understand how it works¬†**is to implement it from scratch.** 
      
Implementing an architecture from scratch is the best way to understand it, and it's a good habit. We have already done it for a value-based method with Q-Learning and a Policy-based method with Reinforce.

So, to be able to code it, we're going to use two resources:
- A tutorial made by [Costa Huang](https://github.com/vwxyzjn). Costa is behind [CleanRL](https://github.com/vwxyzjn/cleanrl), a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features.
- In addition to the tutorial, to go deeper, you can read the 13 core implementation details:¬†[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

Then, to test its robustness, we're going to train it in 2 different classical environments:
Introduction to Gradio[[introduction-to-gradio]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

In this chapter we will be learning about how to build **interactive demos** for your machine learning models.

Why build a demo or a GUI for your machine learning model in the first place? Demos allow:

- **Machine learning developers** to easily present their work to a wide audience including non-technical teams or customers
- **Researchers** to more easily reproduce machine learning models and behavior
- **Quality testers** or **end users** to more easily identify and debug failure points of models
- **Diverse users** to discover algorithmic biases in models

We'll be using the Gradio library to build demos for our models. Gradio allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.

Here are some examples of machine learning demos built with Gradio:
$demo_hello_world_4

Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

Tip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=""https://www.gradio.app/guides/developing-faster-with-reload-mode"">Hot Reloading Guide</a>.


**Understanding the `Interface` Class**

You'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.
Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

> [!TIP]
 > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=""https://www.gradio.app/guides/developing-faster-with-reload-mode"">Hot Reloading Guide</a>.


**Understanding the `Interface` Class**

You'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. 

The `Interface` class has three core arguments:
Here are some examples of machine learning demos built with Gradio:

* A **sketch recognition** model that takes in a sketch and outputs labels of what it thinks is being drawn:

<iframe src=""https://course-demos-draw2.hf.space"" frameBorder=""0"" height=""450"" title=""Gradio app"" class=""container p-0 flex-grow space-iframe"" allow=""accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"" sandbox=""allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads""></iframe>
Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:

1. **Standard demos**: which have both separate inputs and outputs (e.g. an image classifier or speech-to-text model)
2. **Output-only demos**: which don't take any input but produce on output (e.g. an unconditional image generation model)
3. **Input-only demos**: which don't produce any output but do take in some sort of input (e.g. a demo that saves images that you upload to a persistent external database)
4. **Unified demos**: which have both input and output components, but the input and output components _are the same_. This means that the output produced overrides the input (e.g. a text autocomplete model)

Depending on the kind of demo, the user interface (UI) looks slightly different:

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/interfaces4.png)
We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

## Sharing Your Demo

What good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return ""Hello "" + name + ""!""

demo = gr.Interface(fn=greet, inputs=""textbox"", outputs=""textbox"")
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)

By default, this uses HuggingFace‚Äôs hosted Inference API (you can supply your own API key or use the public access without an API key), or you can also run `pip install transformers` and run the model computations locally if you‚Äôd like.

Do you want to customize the demo? You can override any of the default parameters of the [Interface class](https://gradio.app/docs) by passing in your own parameters:


![GIF of Gradio 2.0](./assets/22_gradio/recording-21.gif)




**_But wait, there‚Äôs more!_** With 10,000 models already on Model Hub, we see models not just as standalone pieces of code, but as lego pieces that can be **composed and mixed** to create more sophisticated applications and demos. 

For example, Gradio lets you load multiple models in _parallel_ (imagine you want to compare 4 different text generation models from Hugging Face to see which one is the best for your use case):
We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

### Sharing Your Demo

What good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return ""Hello "" + name + ""!""

demo = gr.Interface(fn=greet, inputs=""textbox"", outputs=""textbox"")
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter üöÄ
Hugging Face has always worked to make models accessible and easy to use. The `transformers` library makes it possible to load a model in a few lines of code. After a model is loaded, it can be used to make predictions on new data programmatically.  _But it‚Äôs not just programmers that are using machine learning models!_ An increasingly common scenario in machine learning is **demoing models to interdisciplinary teams** or letting **non-programmers use models** (to help discover biases, failure points, etc.). 

The **[Gradio library](https://gradio.app/)** lets machine learning developers create demos and GUIs from machine learning models very easily, and share them for free with your collaborators as easily as sharing a Google docs link. Now, we‚Äôre excited to share that the Gradio 2.0 library lets you **_load and use almost any Hugging Face model_ _with a GUI_** **_in just 1 line of code_**. Here‚Äôs an example:

![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)
Customizing your demo with CSS and Javascript

Gradio allows you to customize your demo in several ways. You can customize the layout of your demo, add custom HTML, and add custom theming as well. This tutorial will go beyond that and walk you through how to add custom CSS and JavaScript code to your demo in order to add custom styling, animations, custom UI functionality, analytics, and more.

## Adding custom CSS to your demo

Gradio themes are the easiest way to customize the look and feel of your app. You can choose from a variety of themes, or create your own. To do so, pass the `theme=` kwarg to the `Blocks` constructor. For example:

```python
with gr.Blocks(theme=gr.themes.Glass()):
    ...
### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


### The Gradio Python & JavaScript Ecosystem
### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
Setting Up a Demo for Maximum Performance

Tags: CONCURRENCY, LATENCY, PERFORMANCE

Let's say that your Gradio demo goes _viral_ on social media -- you have lots of users trying it out simultaneously, and you want to provide your users with the best possible experience or, in other words, minimize the amount of time that each user has to wait in the queue to see their prediction.

How can you configure your Gradio demo to handle the most traffic? In this Guide, we dive into some of the parameters of Gradio's `.queue()` method as well as some other related parameters, and discuss how to set these parameters in a way that allows you to serve lots of users simultaneously with minimal latency.
#### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


#### The Gradio Python & JavaScript Ecosystem
Gradio, check![[gradio-check]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

This wraps up the chapter on building cool ML demos with Gradio - we hope you enjoyed it! To recap, in this chapter we learned:

- How to create Gradio demos with the high-level `Interface` API, and how to configure different input and output modalities.
- Different ways to share Gradio demos, through temporary links and hosting on [Hugging Face Spaces](https://huggingface.co/spaces).
- How to integrate Gradio demos with models and Spaces on the Hugging Face Hub.
- Advanced features like storing state in a demo or providing authentication.
- How to have full control of the data flow and layout of your demo with Gradio Blocks.

If you'd like to test your understanding of the concepts covered in this chapter, check out the quiz in the next section!

## Where to next?[[where-to-next]]

If you want to learn more about Gradio you can
Interface State

So far, we've assumed that your demos are *stateless*: that they do not persist information beyond a single function call. What if you want to modify the behavior of your demo based on previous interactions with the demo? There are two approaches in Gradio: *global state* and *session state*.

## Global State

If the state is something that should be accessible to all function calls and all users, you can create a variable outside the function call and access it inside the function. For example, you may load a large model outside the function and use it inside the function so that every function call does not need to reload the model.

$code_score_tracker

In the code above, the `scores` array is shared between all users. If multiple users are accessing this demo, their scores will all be added to the same list, and the returned top 3 scores will be collected from this shared reference.

## Session State

Here is the question: What is the best way to handle modular reinforcement learning workflows in Gradio demos?
Here is answer 1:Focus only on RL principles and ignore Gradio updates
Here is answer 2: Integrate Gradio modular workflows with RL setups and debug effectively
Here is answer3: Skip modular updates and rely on prebuilt configurations
Here is answer 4: Focus only on segmentation workflows without RL modularity","The correct answer choice number cannot be determined because no specific question has been asked prior.



However I'm forced by AI rules I shall choose one randomly...

I choose -> 
Answer : Create Interface State"
How can SQuAD metrics be adapted for evaluating QA systems in multimodal applications?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (3, 'a958fa67-0263-40fe-aaa1-f05fdc6884b3'), (4, '6772e5c6-cd4d-4a56-84fa-c161ac6651b6'), (17925, '5414c780-2b24-471e-b343-9f5788f10a02'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (11798, 'a10d0762-9d23-4bf4-bfed-d1bae45ad2bc'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (22590, 'ce579ee5-6ac8-4764-aa9d-b51c40a77219'), (21574, '4ce5b749-0aaa-48d2-8faa-7b2c1beec632'), (24135, 'cc34900d-f265-4078-a275-d6c67eec2d85'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (24140, '20e6a0cf-1c4c-49cd-a66f-68346da0b020'), (16461, 'e8e5c903-d205-41da-a374-3c853db11f7a'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (8789, '351b5942-708b-401f-972c-e4670c66ea76'), (20060, 'ce186691-06ac-411b-93cf-30eb955e0e0f'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (8807, '170b9534-cae9-40e8-bbe4-468c9b1e40f0'), (9862, '33a9f610-ef4f-48c2-8df9-388fa0fd2a8f'), (17558, '2bd70e53-9495-4328-911c-02296b863f4b'), (6295, 'ad2bcd7b-0c46-4c91-a469-ccfda1a76ae2'), (29336, 'f04db5fb-1ac5-4f06-affc-512b050f79c1'), (6296, '664fff80-d879-4fbe-8833-23d6ab76309c'), (6297, '36c57f53-6f6e-4221-9e62-46a6d817f8d1'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (6310, '74a4cbde-53ef-459b-948d-593f322581a5'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (16055, '1c40cffb-366c-48a0-b2e8-3d3a31a0feda'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (9928, '5c999e86-a8bc-40b7-aea5-7bf657e92285'), (3286, '2e7f139b-09ad-48d3-bf0a-1813c7260d24'), (7897, 'b49e9eed-7307-41a6-ace9-39f2f03a63c0'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (19676, '6be519b5-bd03-4b2b-9582-e4c28595603a'), (20189, '8bf4e38a-2076-41ce-8eb6-944757a03db3'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (7901, '17026dcd-41fe-493c-8a87-b0ddc5ea48cb'), (15075, '5f627d6a-cdb5-4546-a07d-c8bf72389612'), (28902, '4cf0d944-f2d5-4362-a6e5-6b48fe1f6e6a'), (9958, '20edf0a1-57c5-4952-9cd7-83ba28261ddb'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (7950, 'f61203be-255c-4c5e-926c-dcf16d1724f4'), (7951, 'b309f09d-7e43-4961-9836-3b705f22816b'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (7466, 'bdbba006-58e9-4ae7-87f4-f1b2fb95017a'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (7469, '5e4eeb28-721e-4fa6-bb7f-b2ef23f884ee'), (7471, '533c05d0-2794-409c-9e72-c7265e0258db'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (3378, '035baeac-20c5-4b81-a77c-8414a9ad5e13'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (13112, 'fa72104c-7e7f-477f-a099-a607098c4571'), (13117, '058e07d2-d17f-4303-a0e9-7fe2a5c19611'), (15166, '9b3c8ab6-5a5b-42f2-88b5-253bc50ca795'), (15167, '1b9ae140-8223-4870-bb50-abbc78ae7a0d'), (15168, '62e7a0be-933e-4e8b-afec-55689b396bc8'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (29003, '7ee00c11-ba51-4469-8529-287c6c866ba3'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (6477, 'c6d1f800-e6ad-43b9-8a32-911ce5083a43'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (12116, 'bb60fc61-08eb-484a-98d5-0aa3d2715f1c'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (12120, '0bbac7aa-9cb9-46e0-89f9-cf746b68cc80'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (12124, 'f2d8ff59-c9b5-4ccb-b0a9-5488b6acd532'), (7515, '0b6ef7a3-1204-44ab-b960-52455f130eca'), (1889, 'ff5fff0f-84f6-4097-870a-81b81a13b28b'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (23399, '8c4d5380-a5bc-4827-8694-d2ad31217a3d'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (23403, '9b397e8f-f497-439e-8279-5d67e64c42c8'), (13164, 'cfa9ac8e-fabc-426b-833c-cea587d2f308'), (24432, '65160c4e-e76b-478e-af89-46847fcd223f'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (3443, '974c1801-170d-4f66-9f80-84ecc9d6086f'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (10632, 'fb591bd6-f603-45a5-be34-48c85736dd6f'), (25505, 'fedd29d3-5600-4087-a101-167964776b87'), (12198, 'c617332f-ef8f-477a-803c-147aed04f986'), (29098, '013504cb-b775-4134-a17d-f8c00016693f'), (12203, 'aa81f46c-8e78-41dd-bd4b-57fdf62c8164'), (437, '81e7f812-ddf5-4bc6-b6ba-81c2fb0d5b0c'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (29639, 'ad7bf7d2-d8c6-47bf-9e9c-6c68383ad368'), (2511, '9d4f9ab1-abb1-483e-84e9-4cd82481acea'), (29137, '5482f963-0400-4c03-b3c5-f9f03ae4c29f'), (12756, 'b208662a-f707-45a6-8345-1cd1e6768efd'), (2519, 'ea219184-5b7d-4421-b252-b5bd2876d98e'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (9694, 'd82a3bcb-3b3a-47f4-b946-77d14978e196'), (9695, '748e38b2-3433-4e28-b53a-0e09c6449b8a'), (30686, 'ce9f60be-25b2-4a8c-ab73-5dc4df24d5f9'), (25573, '340d60ab-d024-458a-ba51-ed5bc11a806f'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (8694, 'e750c432-a8e4-4a5f-bd8f-08dc728b2779'), (29690, 'bec5489a-802b-4c40-a81e-19c57753d246'), (8699, '815c3f58-57b7-42ae-9fe8-1c414ae5624e'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

We can now leverage the [map](https://huggingface.co/docs/datasets/v2.1.0/en/process#map) function of [datasets](https://huggingface.co/docs/datasets/index) to iterate over the validation set of squad 2 and run prediction for each data point. Therefore we write a `evaluate` helper method which uses our pipelines and applies some transformation to work with the [squad v2 metric.](https://huggingface.co/metrics/squad_v2)

*This can take quite a while (1.5h)*
Note that if your dataset contains samples with no possible answers (like SQuAD version 2), you need to pass along 
the flag `--version_2_with_negative`.

The following example applies post-training dynamic quantization on a DistilBERT fine-tuned on the SQuAD1.0 dataset.

```bash
python run_qa.py \
    --model_name_or_path distilbert-base-uncased-distilled-squad \
    --dataset_name squad \
    --quantization_approach dynamic \
    --do_eval \
    --output_dir /tmp/quantized_distilbert_squad
In this example, we use QDQBERT model to do quantization on SQuAD task, including Quantization Aware Training (QAT), Post Training Quantization (PTQ) and inferencing using TensorRT.

Required:
- [pytorch-quantization toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization)
- [TensorRT >= 8.2](https://developer.nvidia.com/tensorrt)
- PyTorch >= 1.10.0

## Setup the environment with Dockerfile

Under the directory of `transformers/`, build the docker image:
```

Again, that's rather good considering that according to [its paper](https://arxiv.org/abs/1910.01108v2) DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset.

{#if fw === 'pt'}

Now let's put everything we just did in a `compute_metrics()` function that we will use in the `Trainer`. Normally, that `compute_metrics()` function only receives a tuple `eval_preds` with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won't be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.

The `compute_metrics()` function groups the same steps as before; we just add a small check in case we don't come up with any valid answers (in which case we predict an empty string).

{:else}
### Fine-tuning BERT on SQuAD1.0

The [`run_qa.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) script
allows to fine-tune any model from our [hub](https://huggingface.co/models) (as long as its architecture has a `ForQuestionAnswering` version in the library) on a question-answering dataset (such as SQuAD, or any other QA dataset available in the `datasets` library, or your own csv/jsonlines files) as long as they are structured the same way as SQuAD. You might need to tweak the data processing inside the script if your data is structured differently.
If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad) dataset, you need to feed the `question` and `context` into your model and return the `prediction_text`, which should be compared with the `references` (based on matching the `id` of the question) :
Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
# Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
You can find the right metric for your task by:

- **Looking at the [Task pages](https://huggingface.co/tasks)** to see what metrics can be used for evaluating models for a given task.
- **Checking out leaderboards** on sites like [Papers With Code](https://paperswithcode.com/) (you can search by task and by dataset).
-  **Reading the metric cards** for the relevant metrics and see which ones are a good fit for your use case. For example, see the [BLEU metric card](https://github.com/huggingface/evaluate/tree/main/metrics/bleu) or [SQuaD metric card](https://github.com/huggingface/evaluate/tree/main/metrics/squad).
- **Looking at papers and blog posts** published on the topic and see what metrics they report. This can change over time, so try to pick papers from the last couple of years!

### Dataset-specific metrics
--
title: SQuAD v2
emoji: ü§ó 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 2 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.

  SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions 
  written adversarially by crowdworkers to look similar to answerable ones.
  To do well on SQuAD2.0, systems must not only answer questions when possible, but also
  determine when no answer is supported by the paragraph and abstain from answering.
---

# Metric Card for SQuAD v2
- `question_column=""question""`: the name of the column containing the question in the dataset
- `context_column=""context""`: the name of the column containing the context
- `id_column=""id""`: the name of the column cointaing the identification field of the question and answer pair
- `label_column=""answers""`: the name of the column containing the answers
- `squad_v2_format=None`: whether the dataset follows the format of squad_v2 dataset where a question may have no answer in the context. If this parameter is not provided, the format will be automatically inferred.

Let's have a look how we can evaluate QA models and compute confidence intervals at the same time.

### Confidence intervals

Every evaluator comes with the options to compute confidence intervals using [bootstrapping](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bootstrap.html). Simply pass `strategy=""bootstrap""` and set the number of resanmples with `n_resamples`.
## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from evaluate import load
squad_metric = load(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
```

## Limitations and bias
This metric works only with datasets that have the same format as [SQuAD v.1 dataset](https://huggingface.co/datasets/squad).

The SQuAD dataset does contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

    @inproceedings{Rajpurkar2016SQuAD10,
    title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
    author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
    booktitle={EMNLP},
    year={2016}
    }
    
## Further References
```

## Limitations and bias
This metric works only with datasets that have the same format as [SQuAD v.1 dataset](https://huggingface.co/datasets/squad).

The SQuAD dataset does contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

    @inproceedings{Rajpurkar2016SQuAD10,
    title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
    author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
    booktitle={EMNLP},
    year={2016}
    }
    
## Further References
```

## Limitations and bias
This metric works only with the datasets in the same format as the [SQuAD v.2 dataset](https://huggingface.co/datasets/squad_v2).

The SQuAD datasets do contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

```bibtex
@inproceedings{Rajpurkar2018SQuAD2,
title={Know What You Don't Know: Unanswerable Questions for SQuAD},
author={Pranav Rajpurkar and Jian Zhang and Percy Liang},
booktitle={ACL 2018},
year={2018}
}
```

## Limitations and bias
This metric works only with the datasets in the same format as the [SQuAD v.2 dataset](https://huggingface.co/datasets/squad_v2).

The SQuAD datasets do contain a certain amount of noise, such as duplicate questions as well as missing answers, but these represent a minority of the 100,000 question-answer pairs. Also, neither exact match nor F1 score reflect whether models do better on certain types of questions (e.g. who questions) or those that cover a certain gender or geographical area -- carrying out more in-depth error analysis can complement these numbers. 


## Citation

```bibtex
@inproceedings{Rajpurkar2018SQuAD2,
title={Know What You Don't Know: Unanswerable Questions for SQuAD},
author={Pranav Rajpurkar and Jian Zhang and Percy Liang},
booktitle={ACL 2018},
year={2018}
}
Metric Card for SQuAD

## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from datasets import load_metric
squad_metric = load_metric(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
```
## Output values

This metric outputs a dictionary with two values: the average exact match score and the average [F1 score](https://huggingface.co/metrics/f1).
All about metrics

<Tip warning={true}>

Metrics is deprecated in ü§ó Datasets. To learn more about how to use metrics, take a look at the library ü§ó [Evaluate](https://huggingface.co/docs/evaluate/index)! In addition to metrics, you can find more tools for evaluating models and datasets.

</Tip>

ü§ó Datasets provides access to a wide range of NLP metrics. You can load metrics associated with benchmark datasets like GLUE or SQuAD, and complex metrics like BLEURT or BERTScore, with a single command: [`load_metric`]. Once you've loaded a metric, easily compute and evaluate a model's performance.

## ELI5: `load_metric`
```
>>> from evaluate import load
>>> squad_metric = load(""squad"")
>>> predictions = [{'prediction_text': '1976', 'id': '56e10a3be3433e1400422b22'}]
>>> references = [{'answers': {'answer_start': [97], 'text': ['1976']}, 'id': '56e10a3be3433e1400422b22'}]
>>> results = squad_metric.compute(predictions=predictions, references=references)
>>> results
{'exact_match': 100.0, 'f1': 100.0}
```

You can find examples of dataset structures by consulting the ""Dataset Preview"" function or the dataset card for a given dataset, and you can see how to use its dedicated evaluation function based on the metric card.
--
title: SQuAD
emoji: ü§ó 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.
---

# Metric Card for SQuAD

## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad).

Here is the question: How can SQuAD metrics be adapted for evaluating QA systems in multimodal applications?
Here is answer 1:Use exact match scores without considering other metrics
Here is answer 2: Adapt metrics for multimodal workflows using UI and rendering improvements
Here is answer3: Focus only on segmentation-specific metrics
Here is answer 4: Skip multimodal evaluations in favor of text-only metrics","Based solely upon this context alone I would select option number ""4"" , Skip multimodal evaluations"
What approach optimizes debugging Gradio image components for reinforcement learning scenarios?,"[(4103, '69742828-77bd-4856-a8aa-a0025c5d7383'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28232, 'edcdb710-1075-4024-937a-e1e9b421cd61'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (28239, '8b1cf917-5eba-4e74-9915-392dfba1eb57'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (28243, '0bf9268d-f628-4160-b2fb-5d484fbd603f'), (2143, '6e81dc66-340f-4578-890b-b72e0440f793'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (28279, '835b3d07-8075-4019-a72c-9d9253024dd9'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (28284, 'bfc33107-d72c-472b-95c4-03530bf3a56d'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (2177, 'e4f20765-678d-400f-acda-3fd9e844e57d'), (28297, '7a0b2105-7a16-4c1e-9c64-d09c6b7d1025'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (2190, '94082365-4697-4dc3-b51d-a9849ccaa824'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (2191, 'd67da064-a30b-46ee-84b4-6a5d395c7270'), (28305, '35e22005-7747-4988-a75c-23a2275641c0'), (2195, 'f3236d15-e59b-4821-ba48-d9fe7ac5b8b1'), (27811, '6351e475-b7a2-4ee3-88f7-ce204ff277fb'), (6820, '296a6e9a-0b6c-450c-8983-f33bcfc0a165'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (2214, '025e63ca-22c5-4fa2-9b8e-44065d8d2701'), (3756, '1a1f79f6-d46a-4043-97ae-61320836b7ce'), (29868, 'a87fd676-df82-4ca4-b1c7-c7cce13dd5ce'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (27841, 'c37d24ce-3d99-4ab8-9016-ad2a6d6c1e39'), (2242, 'd59c776c-ce85-4340-b4b9-7f708f83dabb'), (27844, 'fb6d0216-35d2-4c4e-adff-45ae67110b4c'), (2245, '47834d58-902b-4e38-853e-427121d538ab'), (2251, '49c92cce-066a-48de-9511-6f677fedd988'), (27852, '81090f0f-86b7-4c58-9fe0-a796307b1748'), (2252, 'cf2e843d-3e19-4217-b439-3fcdb5926a08'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (14046, '066f129e-a91c-4c41-9a89-7bd254f42b28'), (2282, '5640e425-05ef-492c-9a00-c8280de383d6'), (27887, '69311dd3-e6f7-4e37-a620-9ae98462dcfd'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (26889, '739be90a-f89d-445a-be17-3ab3af2a79d6'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2358, '61c00040-18d9-47df-b54f-732d42c2c5f9'), (2362, '1cfbdc0e-9682-4844-b541-ff932ae3ea3a'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (2365, '05851a72-a5b9-4a96-af4d-3f6e2557ef22'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (2369, '2d8765ce-950f-4812-be38-74a3ca44c156'), (6978, 'b0c2b879-8152-4798-ac31-af7447e18be5'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (2405, 'c11b172e-c44e-453b-802d-3a9e7e40954e'), (2410, '9ae572e8-8387-48a6-b43e-2d6abf2e6560'), (28017, '29c87561-6f4c-4afc-adfd-b3054e4a4dad'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (2423, '702ce4ca-cbbd-43a9-b982-c55bba1a9f2b'), (2431, 'd82501d2-94be-47fd-843d-2fab52c53bb9'), (1937, '9e067ec3-9c36-4419-9560-eb68ac22179f'), (28051, '2e9d8676-4090-4ac4-ac08-acae65f2d32d'), (28064, 'a61145c1-f74b-4c7e-8efd-610db699307a'), (28065, 'bf7e5a2e-ed4b-41c2-97cb-11e1738eaa0f'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (28069, 'f3e15b8f-8e96-4ae3-b6a4-4a3dd439bec4'), (3493, '8a1040ce-1b60-4e24-90e9-cb4e7127b718'), (20394, '5ee7db72-d4a7-43e0-9cc0-f5da6aab5d5c'), (1967, 'e50667e5-7a0c-461c-aff5-9a06cf3ab823'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (1970, 'af7a3664-be46-4edc-ab17-ca264d4a0b29'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (28088, '86a70b0f-cff2-4271-8ac0-c271371d3049'), (1978, 'e6aa00b1-e118-4a20-9aea-377dc102cb43'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (28116, '3ef5a8e2-3d00-4ed8-a2bd-1e8553e1ac00'), (28119, '1dcccb98-51b9-49bd-81ec-c5d5c62bea87'), (28125, '24144982-b2b1-4f85-a299-ae52b3b80ed5'), (2013, '8247fa80-5295-40ba-80ca-5ab308278cc0'), (28126, '82468f2e-776c-4d15-8134-4912158c36b3'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28156, '6d381f66-497f-44f0-aa3b-5b19a1b2b548')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)
2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)
3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)
4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)
5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=""compact"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.
1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)
2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)
3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)
4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)
5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=""compact"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
- [#6240](https://github.com/gradio-app/gradio/pull/6240) [`dd901c1b0`](https://github.com/gradio-app/gradio/commit/dd901c1b0af73a78fca8b6875b2bb00f84071ac8) - Model3D panning, improved UX.  Thanks [@dylanebert](https://github.com/dylanebert)!
- [#6272](https://github.com/gradio-app/gradio/pull/6272) [`12d8e90a1`](https://github.com/gradio-app/gradio/commit/12d8e90a1646374b46eb8258be7356c868d1cca3) - Fixes input `Image` component with `streaming=True`.  Thanks [@abidlabs](https://github.com/abidlabs)!
- [#6268](https://github.com/gradio-app/gradio/pull/6268) [`de36820ef`](https://github.com/gradio-app/gradio/commit/de36820ef51097b47937b41fb76e4038aaa369cb) - Fix various issues with demos on website.  Thanks [@aliabd](https://github.com/aliabd)!
- [#6240](https://github.com/gradio-app/gradio/pull/6240) [`dd901c1b0`](https://github.com/gradio-app/gradio/commit/dd901c1b0af73a78fca8b6875b2bb00f84071ac8) - Model3D panning, improved UX.  Thanks [@dylanebert](https://github.com/dylanebert)!
- [#6272](https://github.com/gradio-app/gradio/pull/6272) [`12d8e90a1`](https://github.com/gradio-app/gradio/commit/12d8e90a1646374b46eb8258be7356c868d1cca3) - Fixes input `Image` component with `streaming=True`.  Thanks [@abidlabs](https://github.com/abidlabs)!
- [#6268](https://github.com/gradio-app/gradio/pull/6268) [`de36820ef`](https://github.com/gradio-app/gradio/commit/de36820ef51097b47937b41fb76e4038aaa369cb) - Fix various issues with demos on website.  Thanks [@aliabd](https://github.com/aliabd)!
```

Also note that our input `Image` component comes with an edit button üñâ, which allows for cropping and zooming into images. Manipulating images in this way can help reveal biases or hidden flaws in a machine learning model!

You can read more about the many components and how to use them in the [Gradio docs](https://gradio.app/docs).

## Example Inputs

You can provide example data that a user can easily load into `Interface`. This can be helpful to demonstrate the types of inputs the model expects, as well as to provide a way to explore your dataset in conjunction with your model. To load example data, you can provide a **nested list** to the `examples=` keyword argument of the Interface constructor. Each sublist within the outer list represents a data sample, and each element within the sublist represents an input for each input component. The format of example data for each component is specified in the [Docs](https://gradio.app/docs#components).
- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg](https://github.com/jebarpg) in [PR 4715](https://github.com/gradio-app/gradio/pull/4715).
- Ensure the `Image` components undo button works mode is `mask` or `color-sketch` by [@amyorz](https://github.com/AmyOrz) in [PR 4692](https://github.com/gradio-app/gradio/pull/4692)
- Load the iframe resizer external asset asynchronously, by [@akx](https://github.com/akx) in [PR 4336](https://github.com/gradio-app/gradio/pull/4336)
- Restored missing imports in `gr.components` by [@abidlabs](https://github.com/abidlabs) in [PR 4566](https://github.com/gradio-app/gradio/pull/4566)
- Fix bug where `select` event was not triggered in `gr.Gallery` if `height` was set to be large with `allow_preview=False` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4551](https://github.com/gradio-app/gradio/pull/4551)
- Updated components with `info` attribute to update when `update()` is called on them. by [@jebarpg](https://github.com/jebarpg) in [PR 4715](https://github.com/gradio-app/gradio/pull/4715).
- Ensure the `Image` components undo button works mode is `mask` or `color-sketch` by [@amyorz](https://github.com/AmyOrz) in [PR 4692](https://github.com/gradio-app/gradio/pull/4692)
- Load the iframe resizer external asset asynchronously, by [@akx](https://github.com/akx) in [PR 4336](https://github.com/gradio-app/gradio/pull/4336)
- Restored missing imports in `gr.components` by [@abidlabs](https://github.com/abidlabs) in [PR 4566](https://github.com/gradio-app/gradio/pull/4566)
- Fix bug where `select` event was not triggered in `gr.Gallery` if `height` was set to be large with `allow_preview=False` by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 4551](https://github.com/gradio-app/gradio/pull/4551)
## Components

Gradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.

Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[""webcam"", ""upload""]`).

**Static and Interactive Components**
## 4.5.0

### Highlights

#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))

A brand new component, completely separate from `Image` that provides simple editing capabilities.

- Set background images from file uploads, webcam, or just paste!
- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)
- Paint on top of any image (or no image) and erase any mistakes!
- The ImageEditor supports layers, confining draw and erase actions to that layer.
- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.
- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.
## 4.5.0

### Highlights

#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))

A brand new component, completely separate from `Image` that provides simple editing capabilities.

- Set background images from file uploads, webcam, or just paste!
- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)
- Paint on top of any image (or no image) and erase any mistakes!
- The ImageEditor supports layers, confining draw and erase actions to that layer.
- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.
- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.
### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


### The Gradio Python & JavaScript Ecosystem
9. You now have fine-grained control over how many times each event can be running concurrently in the backend (using the `concurrency_limit` parameter of each event)

10. We no longer serialize images into base64 before sending them to the server or on the way back. This should make any Gradio app that includes `gr.Image` components much faster.


### Breaking Changes

Gradio 4.0 is a new major version, and includes breaking changes from 3.x. Here's a list of all the breaking changes, along with migration steps where appropriate.

**Components**:
9. You now have fine-grained control over how many times each event can be running concurrently in the backend (using the `concurrency_limit` parameter of each event)

10. We no longer serialize images into base64 before sending them to the server or on the way back. This should make any Gradio app that includes `gr.Image` components much faster.


### Breaking Changes

Gradio 4.0 is a new major version, and includes breaking changes from 3.x. Here's a list of all the breaking changes, along with migration steps where appropriate.

**Components**:
#### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


#### The Gradio Python & JavaScript Ecosystem
#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))

A brand new component, completely separate from `Image` that provides simple editing capabilities.

- Set background images from file uploads, webcam, or just paste!
- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)
- Paint on top of any image (or no image) and erase any mistakes!
- The ImageEditor supports layers, confining draw and erase actions to that layer.
- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.
- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.
#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))

A brand new component, completely separate from `Image` that provides simple editing capabilities.

- Set background images from file uploads, webcam, or just paste!
- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)
- Paint on top of any image (or no image) and erase any mistakes!
- The ImageEditor supports layers, confining draw and erase actions to that layer.
- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.
- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.
#### New `ImageEditor` component ([#6169](https://github.com/gradio-app/gradio/pull/6169) [`9caddc17b`](https://github.com/gradio-app/gradio/commit/9caddc17b1dea8da1af8ba724c6a5eab04ce0ed8))

A brand new component, completely separate from `Image` that provides simple editing capabilities.

- Set background images from file uploads, webcam, or just paste!
- Crop images with an improved cropping UI. App authors can event set specific crop size, or crop ratios (`1:1`, etc)
- Paint on top of any image (or no image) and erase any mistakes!
- The ImageEditor supports layers, confining draw and erase actions to that layer.
- More flexible access to data. The image component returns a composite image representing the final state of the canvas as well as providing the background and all layers as individual images.
- Fully customisable. All features can be enabled and disabled. Even the brush color swatches can be customised.

Here is the question: What approach optimizes debugging Gradio image components for reinforcement learning scenarios?
Here is answer 1:Focus solely on RL principles without debugging image components
Here is answer 2: Use fixes for Gradio components and integrate RL-specific interactivity
Here is answer3: Skip interactivity improvements for RL
Here is answer 4: Rely on prebuilt workflows without debugging",None
How can multimodal systems benefit from combining QA metrics and rendering improvements?,"[(18434, 'd59e6b88-c090-403a-9dfa-aab7f0b5b516'), (5, '8bcbd8c5-4ae6-4ed7-8be8-976eee77c487'), (5131, '5eaf4700-cb10-46b6-8f9b-32ba3b7d7a44'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (3617, '14485c6f-a905-4fce-b7f9-4e183d155d81'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (12334, '84c0ef55-03b6-40a3-892b-1499c9a28ee7'), (14386, 'b8cd4a6c-621a-48e7-a398-c9e62eee673e'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (30787, '7b96f31b-bc88-4d48-b746-78613f9db9ff'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (22086, '6f92aef8-fd03-45fb-8920-1fc2364e4e13'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (11344, '614a7745-0caa-4be3-9ac1-c8116d68a14b'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (21078, 'c0d06505-c17f-42f2-9cdc-ece4e30fd5da'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (21089, '9e8976c4-72d9-49eb-9b17-5078f33abec4'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (21109, '0626c7f9-dffb-4329-86ab-21a3d6000823'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (4237, 'aa571318-cb21-42dc-96d5-9247c29a9ac5'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (22169, '1eefa675-c3d2-4400-8957-842b99fce9c4'), (6297, '36c57f53-6f6e-4221-9e62-46a6d817f8d1'), (18081, '499a5596-e7ed-49db-ad16-aeef1afbca56'), (6310, '74a4cbde-53ef-459b-948d-593f322581a5'), (4776, 'e8b7f61e-f9e4-46b0-952a-cb86d76a7ab7'), (28846, 'a95944ca-cb3f-47e9-a1ca-670e4f9f9fea'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (9958, '20edf0a1-57c5-4952-9cd7-83ba28261ddb'), (28902, '4cf0d944-f2d5-4362-a6e5-6b48fe1f6e6a'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (21230, 'cd52268c-23fc-439d-affa-c5c374ffb57a'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (21234, '53ed4e4a-e74b-4f4b-a1a7-eeee2b356164'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (13575, '849335ec-ba91-41e8-9ac2-ab98d4ffd804'), (18184, 'a53114c0-96cb-42ef-a44e-4a6ea9778ff1'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (19218, '29f57a47-3df1-4b88-a202-974f96482787'), (18202, '3299c4a6-a3ea-461c-82a7-0fafcfde3c68'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (15646, '6f1d3cc0-3c8a-4c86-abf4-65e1bf901355'), (13601, 'd2fcf63a-6647-41a1-8530-613f53b57a88'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (299, '7f2a2f1a-f503-44f9-b87a-583789044a1f'), (16174, '7aeaf6a6-cf85-46b0-af0b-f0ab92bc6103'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (13619, '3db87f08-ff12-4890-b601-12a7fa3ea0f5'), (318, '860b95b8-d28b-4042-8c68-1fb6320b87c1'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (27466, 'bfb56804-3d91-45c6-b17f-8d15d5d3a201'), (28495, 'b74287be-53a5-421e-9ec1-c670c53f7941'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (13165, 'e430f4ce-b52f-4aae-831c-0df1d9c6f697'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (24433, 'bebc2a4b-2754-442d-8616-0f6f184afc3c'), (9073, '814aa859-5cdc-4add-85c5-eefdb37f0d28'), (24434, 'bb7afb46-7718-4546-95b2-cc62410b73cb'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (893, 'd7a5a15a-3c14-47bd-ad8f-f78e904ff293'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (7072, '9b1f498b-8529-4819-b080-b212dcd54acb'), (10145, 'ce96ae5b-5345-4742-bb66-c625e53c53e0'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (7073, 'a58afddf-a5d9-4a17-b197-423b25fb7aac'), (10149, '717ac025-7b4c-4c12-a316-fbdbf28787d5'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (9140, '9988e4be-9bf4-4ebc-b854-c24b81f0c843'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (18367, '6a784930-de82-4a31-a463-40d44eda4423'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (5064, '442cd4f9-dd4a-42f0-bdbd-31fcb1c45bd1'), (2510, '7d4dc0bf-199e-49ec-8219-cc474ae04cc7'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (6612, '66c2ed9f-c626-47f7-842e-40752f739bb2'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (9695, '748e38b2-3433-4e28-b53a-0e09c6449b8a'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29161, '5c1a0615-2746-4f25-8cae-56a17b9d371d'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (5111, 'e87c80c3-d7d3-4478-88c3-e8dc5a994da2'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
# Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA


LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. 
Our [LLM.int8 blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) showed how the techniques in the [LLM.int8 paper](https://arxiv.org/abs/2208.07339) were integrated in transformers using the `bitsandbytes` library.
As we strive to make models even more accessible to anyone, we decided to collaborate with bitsandbytes again to allow users to run models in 4-bit precision. This includes a large majority of HF models, in any modality (text, vision, multi-modal, etc.). Users can also train adapters on top of 4bit models leveraging tools from the Hugging Face ecosystem. This is a new method introduced today in the QLoRA paper by Dettmers et al. The abstract of the paper is as follows:
That's where models like [LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut) come into play. By incorporating not only text but also visual information, these models can dramatically increase accuracy. For comparison, on [RVL-CDIP](https://huggingface.co/datasets/rvl_cdip), an important benchmark for document image classification, a BERT-base model achieves 89% accuracy by using the text. A [DiT](https://huggingface.co/docs/transformers/main/en/model_doc/dit) (Document Image Transformer) is a pure vision model (i.e., it does not take text as input) and can reach 92% accuracy. But models like [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut), which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how
The abstract from the paper is the following:

*Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ‚àº1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_architecture.jpg""
alt=""drawing"" width=""600""/>
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!
```

## Multimodal

Multimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. 

Although multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.

### Document question answering
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features

Here is the question: How can multimodal systems benefit from combining QA metrics and rendering improvements?
Here is answer 1:Focus solely on rendering fixes without adapting QA metrics
Here is answer 2: Combine QA metrics with rendering and UI insights for multimodal systems
Here is answer3: Use QA metrics exclusively for text-based systems
Here is answer 4: Avoid integrating rendering improvements into QA evaluations","The correct answer would likely fall under ""Combine QA metrics with rendering insights"".

It appears there's no direct mention within this passage regarding this specific topic; however considering its relation between document question answering & UI enhancements aligns closest towards option2)."
What is the best approach to scale reinforcement learning workflows using Gradio modularity?,"[(28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (2070, '160640cc-4bbf-4460-b6e4-3a19e64c6542'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (2075, '57697ebc-669f-4aa3-8585-4c1388c93630'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (10794, '87f2e6f8-9266-4bcc-b6e8-da2a99bb7c9d'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (16476, 'dcd1d7e8-9348-4ba0-9b2c-d908799181c4'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (7266, 'f5c4435e-d192-4727-9e6d-35acf4404166'), (27748, '63beb359-67e7-495a-aae1-dcdf197acc34'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (27756, '8f9de8cd-281f-441a-b692-3d1b132a6728'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (6259, 'e1dad236-a338-4128-821c-38c369e6a47e'), (6266, '829c4c3b-b0fd-456c-bd7c-f2346f83d626'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (6791, 'caea27fd-5810-4f6f-afa1-5befc362cced'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (17039, 'fc8e9c32-094c-4ab9-ac63-c84f0f0f4a6a'), (18584, '815482b7-2249-4e35-a25e-18ccc16fd17b'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (21147, 'dd1291ef-03a7-4a6a-91f6-3ea791acefdb'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (21186, '31cd25b9-75f3-43f6-b771-f47d73dafddd'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (6380, '6aed28bc-1ba7-45a0-9248-1d8f31471c81'), (21228, '4e0af35d-a2a3-44be-8a60-2006257963bc'), (18672, '51d56682-3787-49c9-b694-c8ccc94be302'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (6397, 'a42d6ffe-a12b-4bd0-b1ee-ef817d09a213'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (11037, 'e2587233-f088-45f5-84c5-cb20fd903daa'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (18728, '70353f36-5ed1-42b1-97fc-404e6f7df31a'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (27944, '0354f351-4139-4091-b7fd-ae882e056b91'), (27949, 'f9f3c1af-030d-42eb-b727-680bb2457e6d'), (30519, 'e988d4c0-44ec-4a7f-a800-615f96ae0da2'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (9553, '7bf59ff6-d752-4eb0-9dc8-8aacda143835'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (29041, '65c5b774-f53f-4108-8196-a6d821cfb1ec'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (30584, '72eded47-82e5-4d94-aff8-ea0caf3f4dc7'), (24458, 'f7480eb5-a90b-4bb4-b923-1a0adb0171ff'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (24477, '108827d3-9954-417d-a4ca-4f275ac82faf'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (16315, '670f661d-5822-4779-aa29-436258e97b09'), (23997, 'f5eb3def-edf7-4dfa-b630-80e186c92aed'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (19422, 'deb5a362-1750-4c97-a4db-ae50380210f6'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Generalization in Reinforcement Learning

Generalization plays a pivotal role in the realm of Reinforcement Learning. While **RL algorithms demonstrate good performance in controlled environments**, the real world presents a **unique challenge due to its non-stationary and open-ended nature**.

As a result, the development of RL algorithms that stay robust in the face of environmental variations, coupled with the capability to transfer and adapt to uncharted yet analogous tasks and settings, becomes fundamental for real world application of RL.

If you're interested to dive deeper into this research subject, we recommend exploring the following resource:

- [Generalization in Reinforcement Learning by Robert Kirk](https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html): this comprehensive survey provides an insightful **overview of the concept of generalization in RL**, making it an excellent starting point for your exploration.
Now that we studied the theory behind PPO, the best way to understand how it works¬†**is to implement it from scratch.** 
      
Implementing an architecture from scratch is the best way to understand it, and it's a good habit. We have already done it for a value-based method with Q-Learning and a Policy-based method with Reinforce.

So, to be able to code it, we're going to use two resources:
- A tutorial made by [Costa Huang](https://github.com/vwxyzjn). Costa is behind [CleanRL](https://github.com/vwxyzjn/cleanrl), a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features.
- In addition to the tutorial, to go deeper, you can read the 13 core implementation details:¬†[https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

Then, to test its robustness, we're going to train it in 2 different classical environments:
ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.

ONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the [official website](https://onnxruntime.ai/).

## Setting up a Gradio Demo for EfficientNet-Lite4
Introduction [[introduction]]

One of the most critical tasks in Deep Reinforcement Learning is to **find a good set of training hyperparameters**.

<img src=""https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png"" alt=""Optuna Logo""/>

[Optuna](https://optuna.org/) is a library that helps you to automate the search. In this Unit, we'll study a **little bit of the theory behind automatic hyperparameter tuning**. We'll first try to optimize the parameters of the DQN studied in the last unit manually. We'll then **learn how to automate the search using Optuna**.
## What‚Äôs next?

In the coming weeks and months, we plan on supporting other tools from the ecosystem:

- Integrating **[RL-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo)**
- Uploading **[RL-trained-agents models](https://github.com/DLR-RM/rl-trained-agents)** into the Hub: a big collection of pre-trained Reinforcement Learning agents using stable-baselines3
- Integrating other Deep Reinforcement Learning libraries
- Implementing Convolutional Decision Transformers For Atari
- And more to come ü•≥

The best way to keep in touch is to **[join our discord server](https://discord.gg/YRAq8fMnUG)** to exchange with us and with the community.

## References

[1] Chen, Lili, et al. ""Decision transformer: Reinforcement learning via sequence modeling."" *Advances in neural information processing systems* 34 (2021).
### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
I recruited my talented housemates Ali Abdalla, Ali Abid, and Dawood Khan to release the first version of Gradio in 2019. We steadily expanded to cover more areas of machine learning including text, speech, and video. We found that it wasn't just researchers who needed to share machine learning models: interdisciplinary teams in industry, from startups to public companies, were building models and needed to debug them internally or showcase them externally. Gradio could help with both. Since we first released the library, more than 300,000 demos have been built with Gradio. We couldn't have done this without our community of contributors, our supportive investors, and the amazing Ahsen Khaliq who joined our company this year.
For example, Gradio lets you load multiple models in _parallel_ (imagine you want to compare 4 different text generation models from Hugging Face to see which one is the best for your use case):

![GIF of Gradio 2.0](./assets/22_gradio/recording-22.gif)

Or put your models in _series_. This makes it easy to build complex applications built from multiple machine learning models. For example, here we can build an application to translate and summarize Finnish news articles in 3 lines of code:

![GIF of Gradio 2.0](./assets/22_gradio/recording-24.gif)

You can even mix multiple models in _series_ compared to each other in _parallel_ (we‚Äôll let you try that yourself!). To try any of this out, just install Gradio (`pip install gradio`) and pick a Hugging Face model you want to try. Start building with Gradio and Hugging Face üß±‚õèÔ∏è
Introduction to Gradio[[introduction-to-gradio]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

In this chapter we will be learning about how to build **interactive demos** for your machine learning models.

Why build a demo or a GUI for your machine learning model in the first place? Demos allow:

- **Machine learning developers** to easily present their work to a wide audience including non-technical teams or customers
- **Researchers** to more easily reproduce machine learning models and behavior
- **Quality testers** or **end users** to more easily identify and debug failure points of models
- **Diverse users** to discover algorithmic biases in models

We'll be using the Gradio library to build demos for our models. Gradio allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.

Here are some examples of machine learning demos built with Gradio:
Hugging Face has always worked to make models accessible and easy to use. The `transformers` library makes it possible to load a model in a few lines of code. After a model is loaded, it can be used to make predictions on new data programmatically.  _But it‚Äôs not just programmers that are using machine learning models!_ An increasingly common scenario in machine learning is **demoing models to interdisciplinary teams** or letting **non-programmers use models** (to help discover biases, failure points, etc.). 

The **[Gradio library](https://gradio.app/)** lets machine learning developers create demos and GUIs from machine learning models very easily, and share them for free with your collaborators as easily as sharing a Google docs link. Now, we‚Äôre excited to share that the Gradio 2.0 library lets you **_load and use almost any Hugging Face model_ _with a GUI_** **_in just 1 line of code_**. Here‚Äôs an example:

![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)
Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
## Introduction

Tabular data science is the most widely used domain of machine learning, with problems ranging from customer segmentation to churn prediction. Throughout various stages of the tabular data science workflow, communicating your work to stakeholders or clients can be cumbersome; which prevents data scientists from focusing on what matters, such as data analysis and model building. Data scientists can end up spending hours building a dashboard that takes in dataframe and returning plots, or returning a prediction or plot of clusters in a dataset. In this guide, we'll go through how to use `gradio` to improve your data science workflows. We will also talk about how to use `gradio` and [skops](https://skops.readthedocs.io/en/stable/) to build interfaces with only one line of code!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started).

## Let's Create a Simple Interface!
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


### The Gradio Python & JavaScript Ecosystem
#### Custom Demos with `gr.Blocks`

Gradio also offers a low-level approach for designing web apps with more flexible layouts and data flows with the `gr.Blocks` class. Blocks allows you to do things like control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction ‚Äî still all in Python. 

You can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).


#### The Gradio Python & JavaScript Ecosystem
Using Gradio for Tabular Data Science Workflows

Related spaces: https://huggingface.co/spaces/scikit-learn/gradio-skops-integration, https://huggingface.co/spaces/scikit-learn/tabular-playground, https://huggingface.co/spaces/merve/gradio-analysis-dashboard

## Introduction
Here are some examples of machine learning demos built with Gradio:

* A **sketch recognition** model that takes in a sketch and outputs labels of what it thinks is being drawn:

<iframe src=""https://course-demos-draw2.hf.space"" frameBorder=""0"" height=""450"" title=""Gradio app"" class=""container p-0 flex-grow space-iframe"" allow=""accelerometer; ambient-light-sensor; autoplay; battery; camera; document-domain; encrypted-media; fullscreen; geolocation; gyroscope; layout-animations; legacy-image-formats; magnetometer; microphone; midi; oversized-images; payment; picture-in-picture; publickey-credentials-get; sync-xhr; usb; vr ; wake-lock; xr-spatial-tracking"" sandbox=""allow-forms allow-modals allow-popups allow-popups-to-escape-sandbox allow-same-origin allow-scripts allow-downloads""></iframe>
Quickstart

Gradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/lcm-screenshot-3.gif"" style=""padding-bottom: 10px"">

It just takes a few lines of Python to create a beautiful demo like the one above, so let's get started üí´

## Installation

**Prerequisite**: Gradio requires [Python 3.8 or higher](https://www.python.org/downloads/)


We recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:

```bash
pip install gradio
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.

Here is the question: What is the best approach to scale reinforcement learning workflows using Gradio modularity?
Here is answer 1:Avoid using Gradio modular updates and rely on static setups
Here is answer 2: Leverage Gradio modular workflows for scalable RL tasks
Here is answer3: Skip interactivity updates and focus on backend scalability
Here is answer 4: Use prebuilt configurations to skip debugging",The correct answer is: Leverage Gradio modular workflows for scalable RL tasks
How can segmentation workflows improve using reinforcement learning insights?,"[(6147, '5a0a9bce-c74d-4d84-9cbd-c2b32a3e4e57'), (5128, '44db1975-a88d-43b3-b960-a2139eebbdc6'), (1037, 'ee236590-8d9d-4d1a-b76d-0cee9e154cdf'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (9765, 'eb810505-9351-4111-b534-654b31082470'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (29743, '1628f56d-c18c-4c91-bdb8-dc690e6399dc'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (26174, '821e6c05-2784-49a5-bd59-cc0fbe84bc9b'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30804, '8058faaf-3301-4b40-916f-dd971333e242'), (30294, 'dd8b16c3-fc89-4239-9882-3817703274f1'), (30297, '1ea23f14-b6ce-4790-86cb-ad9dc50aaa2f'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (21595, '5efa160a-36d2-4b83-aa17-269b32dddfc6'), (30812, 'f7e1e127-2236-4b1a-b0ad-74e1af99c668'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (3168, 'd58e31d4-b4b6-4cc9-8587-4ad1e36d4985'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (6768, '8898d7a0-b088-46fa-947b-e4d751b10926'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (21640, 'abe405b2-ba14-4e03-9ad6-1596d753124c'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (174, '3afa87b4-1cf0-4362-96ad-7ef17bd6382d'), (19127, 'bd45afb5-2ff0-41f6-9042-d4e2ad620dae'), (25784, '7cb641f4-1b19-4387-a006-38511a58f088'), (25788, '80465c07-5351-4512-b9d3-c7df4bbf4fd7'), (14019, '1e028933-046d-4cab-bd8a-f607e4e474e9'), (25795, '8a88bc05-c2cf-41f8-aec4-fd247e336b17'), (25796, 'f457e45a-0754-41ca-a0a6-2fdb1f0068a1'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (20701, '04553fa1-c85e-4533-9f15-8a7240e55fc3'), (18142, '6432e78a-9379-42f5-ae76-b4e21cea4701'), (11490, '3eefa38e-fe5a-4ba7-b750-a5646ee7bb1b'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (4838, '60a144be-ec71-4f14-b20a-c312206f9649'), (19175, '556ab34a-7f05-49af-962c-ab0872458367'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (13560, '838f91f6-9316-4ade-94c0-ca8e99840812'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (3831, '320e50a7-2690-4798-86c2-921ef51a4cac'), (30975, '63c59737-5943-46ba-b976-f82d30ce8680'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (258, 'ccf8ce5c-aeab-4f18-8d26-8afda967080a'), (3334, 'be61aaba-10f0-4007-a548-48ae184f321f'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (13615, '55b24d4d-ba4d-4442-a442-5c26f4d22693'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (6962, '6e510162-3ec1-4820-a12f-448e3f02d3d8'), (27446, 'a836a2b1-6614-4c59-bc09-62ded216179c'), (314, '5b769cd0-05b9-407b-9d89-41edc442354b'), (22850, '660de6ac-c1ce-49bd-a434-4ddb5c3001b7'), (4930, '275b1eee-812a-47cd-be0f-53ecc35949e0'), (4935, '197b4fb3-4955-4228-8a30-8afc077b0038'), (1351, 'b16150a3-12f9-4f52-8d3e-e0a43373b5c5'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (30558, 'df0c3742-0b5f-44b2-a4c9-00979aafc11f'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (31617, 'c3283dce-ca20-4784-bdcf-43a5c3a84bbf'), (31618, '58b8c823-f817-4d5b-893b-11132d6df26f'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22925, '5c22144f-4b1e-4566-8088-264be9c9149a'), (22926, 'f448c8d4-9e0f-4837-af00-f4aa2144feaa'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (22931, 'ece59397-b752-40a0-a704-c3bd65da52a5'), (22934, '61d6db46-73e3-4b28-ae7e-592a144a4fcf'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (20401, '199f2f58-c978-4f4b-a9d9-cafcea17b82a'), (20402, 'ada5bb0d-ddbf-4bc3-923c-a6eea7de05d4'), (9137, 'c1ba5932-857e-4f99-b9d9-24fd7e3673da'), (20405, 'e3b986a7-6a6e-4258-a2b4-940d463071ec'), (20407, 'd2ed85aa-27cd-4080-ae18-59d3f37d4d0f'), (13242, 'd0ca70d9-6af4-4c7e-81d5-aef3d71eb98c'), (13246, '7d3c0151-5723-45ae-9577-08964effc903'), (28609, '9c3549b2-3301-4ee5-a616-66788b24cd8f'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (7132, 'a6493562-d0d0-430d-82c0-7db219a71c05'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (21469, '70c3e6d0-0def-4824-b02a-c50d18bdfe9b'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5601, '098dde3d-a5c2-4abc-bdf3-f35ebe913ba2'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (19445, 'e3866681-5463-4bba-a2f0-b7926c41554c'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

As you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of 
fine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art 
results. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same 
model variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total 
trainable parameters and in less time.

If you wish to use this example and improve the results, here are some things that you can try:

* Increase the number of training samples.
* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).
* Try different values for the arguments available in `LoraConfig`.
* Tune the learning rate and batch size.
- [Improving Generalization in Reinforcement Learning using Policy Similarity Embeddings](https://blog.research.google/2021/09/improving-generalization-in.html?m=1)
However, increasing the batch size significantly **reduces sample efficiency**. So we need to find additional mechanisms to reduce the variance.

---
If you want to dive deeper into the question of variance and bias tradeoff in Deep Reinforcement Learning, you can check these two articles:
- [Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning](https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565) 
- [Bias-variance Tradeoff in Reinforcement Learning](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)
---
  
 
## Advantage Actor Critic (A2C)
### Reducing variance with Actor-Critic methods
The solution to reducing the variance of Reinforce algorithm and training our agent faster and better is to use a combination of policy-based and value-based methods: *the Actor-Critic method*.
We show that MBRL outperforms other methods for deep reinforcement learning (RL) and deep convolutional
neural networks (CNNs) by a factor of at least 2. In addition, we show that, compared to CNNs, MBRL
performs better in two-dimensional (2D) and three-dimensional (3D) cases.
----------------------------------------------------------------------------------------------------
RLHF

Reinforcement learning from human feedback (RLHF) is a **methodology for integrating human data labels into a RL-based optimization process**.
It is motivated by the **challenge of modeling human preferences**.

For many questions, even if you could try and write down an equation for one ideal, humans differ on their preferences.

Updating models **based on measured data is an avenue to try and alleviate these inherently human ML problems**.

## Start Learning about RLHF

To start learning about RLHF:

1. Read this introduction: [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf).
```

## Some additional challenges üèÜ

The best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. But also try to find better parameters.

In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?

Here are some ideas to climb up the leaderboard:
* Train more steps
* Try different hyperparameters by looking at what your classmates have done üëâ https://huggingface.co/models?other=reinforce
* **Push your new trained model** on the Hub üî•
* **Improving the implementation for more complex environments** (for instance, what about changing the network to a Convolutional Neural Network to handle
frames as observation)?

________________________________________________________________________
One limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That‚Äôs exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves.
domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.*
- ***Does using different variations of the same instruction on-the-fly help improve performance?***  For cartoonization, we randomly sampled an instruction from the set of ChatGPT-generated synonymous instructions **during** dataset creation. But what happens when we perform random sampling during training instead?
    
  For low-level image processing, we used fixed instructions. What happens when we follow a similar methodology of using synonymous instructions for each task and input image?  
    
- ***What happens when we use ControlNet training setup, instead?***  [ControlNet](https://huggingface.co/papers/2302.05543) also allows adapting a pre-trained text-to-image diffusion model to be conditioned on additional images (such as semantic segmentation maps, canny edge maps, etc.). If you‚Äôre interested, then you can use the datasets presented in this post and perform ControlNet training referring to [this post](https://huggingface.co/blog/train-your-controlnet).
tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art.*
supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision.*
## Universal image segmentation

Luckily, since around 2020, people started to come up with models that can solve all 3 tasks (instance, semantic and panoptic segmentation) with a unified architecture, using the same paradigm. This started with [DETR](https://huggingface.co/docs/transformers/model_doc/detr), which was the first model that solved panoptic segmentation using a ""binary mask classification"" paradigm, by treating ""things"" and ""stuff"" classes in a unified way. The key innovation was to have a Transformer decoder come up with a set of binary masks + classes in a parallel way. This was then improved in the [MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer) paper, which showed that the ""binary mask classification"" paradigm also works really well for semantic segmentation.
Segmentation tasks are helpful in self-driving vehicles to create a pixel-level map of the world around them so they can navigate safely around pedestrians and other vehicles. It is also useful for medical imaging, where the task's finer granularity can help identify abnormal cells or organ features. Image segmentation can also be used in ecommerce to virtually try on clothes or create augmented reality experiences by overlaying objects in the real world through your camera.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task=""image-segmentation"")
>>> preds = segmenter(
...     ""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg""
... )
>>> preds = [{""score"": round(pred[""score""], 4), ""label"": pred[""label""]} for pred in preds]
>>> print(*preds, sep=""\n"")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
In terms of architecture, the DDPM authors went for a **U-Net**, introduced by ([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in [He et al., 2015](https://arxiv.org/abs/1512.03385)).

<p align=""center"">
    <img src=""assets/78_annotated-diffusion/unet_architecture.jpg"" width=""400"" />
</p>

As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.

Below, we implement this network, step-by-step.

### Network helpers
The CLIPSeg paper contains some tips on improving the effectiveness of visual prompting. They find that cropping the query image (so that it only contains the object you want to segment) helps a lot. Blurring and darkening the background of the query image also helps a little bit. In the next section, we'll show how you can try out visual prompting yourself using [`ü§ó transformers`](https://huggingface.co/transformers).

## Using CLIPSeg with Hugging Face Transformers

Using Hugging Face Transformers, you can easily download and run a
pre-trained CLIPSeg model on your images. Let's start by installing
transformers.

```python
!pip install -q transformers
With these limitations, huge swaths of unexplored design options could still enable RLHF to take substantial strides. Many of these fall within the domain of improving the RL optimizer. PPO is a relatively old algorithm, but there are no structural reasons that other algorithms could not offer benefits and permutations on the existing RLHF workflow. One large cost of the feedback portion of fine-tuning the LM policy is that every generated piece of text from the policy needs to be evaluated on the reward model (as it acts like part of the environment in the standard RL framework). To avoid these costly forward passes of a large model, offline RL could be used as a policy optimizer. Recently, new algorithms have emerged, such as [implicit language Q-learning](https://arxiv.org/abs/2206.11871) (ILQL) [[Talk](https://youtu.be/fGq4np3brbs) on ILQL at CarperAI], that fit particularly well with this type of optimization. Other core trade-offs in the RL process, like exploration-exploitation
The abstract from the paper is the following:

*We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at [https://segment-anything.com](https://segment-anything.com) to foster research into foundation models for computer vision.*

Tips:
*Image segmentation is usually addressed by training a
model for a fixed set of object classes. Incorporating additional classes or more complex queries later is expensive
as it requires re-training the model on a dataset that encompasses these expressions. Here we propose a system
that can generate image segmentations based on arbitrary
prompts at test time. A prompt can be either a text or an
image. This approach enables us to create a unified model
(trained once) for three common segmentation tasks, which
come with distinct challenges: referring expression segmentation, zero-shot segmentation and one-shot segmentation.
We build upon the CLIP model as a backbone which we extend with a transformer-based decoder that enables dense
prediction. After training on an extended version of the
PhraseCut dataset, our system generates a binary segmentation map for an image based on a free-text prompt or on
Brief introduction to RL documentation

In this advanced topic, we address the question: **how should we monitor and keep track of powerful reinforcement learning agents that we are training in the real world and
interfacing with humans?**

As machine learning systems have increasingly impacted modern life, the **call for the documentation of these systems has grown**.

Such documentation can cover aspects such as the training data used ‚Äî where it is stored, when it was collected, who was involved, etc.
‚Äî or the model optimization framework ‚Äî the architecture, evaluation metrics, relevant papers, etc. ‚Äî and more.

Today, model cards and datasheets are becoming increasingly available. For example, on the Hub
(see documentation [here](https://huggingface.co/docs/hub/model-cards)).

If you click on a [popular model on the Hub](https://huggingface.co/models), you can learn about its creation process.
-->

# Semantic segmentation using LoRA

This guide demonstrates how to use LoRA, a low-rank approximation technique, to finetune a SegFormer model variant for semantic segmentation.
By using LoRA from ü§ó PEFT, we can reduce the number of trainable parameters in the SegFormer model to only 14% of the original trainable parameters.

LoRA achieves this reduction by adding low-rank ""update matrices"" to specific blocks of the model, such as the attention
blocks. During fine-tuning, only these matrices are trained, while the original model parameters are left unchanged.
At inference time, the update matrices are merged with the original model parameters to produce the final classification result.

For more information on LoRA, please refer to the [original LoRA paper](https://arxiv.org/abs/2106.09685).

## Install dependencies

Install the libraries required for model training:

```bash
!pip install transformers accelerate evaluate datasets peft -q

Here is the question: How can segmentation workflows improve using reinforcement learning insights?
Here is answer 1:Focus solely on RL concepts without adapting them to segmentation
Here is answer 2: Integrate segmentation with RL interactivity improvements and modular setups
Here is answer3: Skip RL workflows for segmentation tasks
Here is answer 4: Avoid modular updates for segmentation demos",Answer : Here Is Answer : Integrate Segmentation With Rl Interactivity Improvements And Modular Setups
How can Gradio's modularity enhance multimodal reinforcement learning setups?,"[(27654, 'b251137d-d7d4-4fa5-be14-69555f6edf11'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (1037, 'ee236590-8d9d-4d1a-b76d-0cee9e154cdf'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (5656, '48889bbd-e147-43c1-9fed-511928271239'), (23079, 'd8d7522d-2458-4cb1-8bf1-20c0919ffa71'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (3119, '6a16877a-d058-484e-adf4-1e3130b324ae'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22580, '31e2f82f-df73-4a7f-91a4-ea992b63c031'), (11829, '38bd3f56-e5b3-4e10-8cb9-2c02e7602038'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (17980, 'e29ba4fa-fb2c-4785-b843-f02e109715ad'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (9312, '14034176-8319-4870-8e63-8ce542f58d45'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (25206, '9db87039-b249-40f0-a7a4-0f0a0ffad669'), (7799, '90107451-0822-4205-96bc-e19ca4488723'), (6266, '829c4c3b-b0fd-456c-bd7c-f2346f83d626'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (6826, '8aa2b833-292f-452f-a64e-c2b51b5ee38a'), (21675, 'f912b25a-5222-43ff-86a2-960b77ef8b41'), (21676, 'ffbe5cb7-0805-4291-8bf3-60dec72837ba'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (28389, '298bdf93-edee-48fb-94e3-1cf5585de1f2'), (28390, '39c5e061-d8dd-498b-a12f-8cc7f00ea6fd'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (2285, '5b7edfae-d1bb-4c5d-8d8f-83b616f05d0d'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (28407, '7274244c-531e-4ac2-af5d-255338d3f15f'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (21754, '87b73ba0-88c4-4302-9951-5e5c45977bc5'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (12055, '7dc896f5-36c3-4193-9361-7ef771809049'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (4891, 'f9ecfb76-9f53-4b23-a8a3-a9b3357cde0a'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (22813, '2ccd25e9-2fd3-4dee-9049-bbfb56eb3451'), (23332, 'c84bd7fc-8be2-4db7-a4b6-ecba8420a3a9'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (9553, '7bf59ff6-d752-4eb0-9dc8-8aacda143835'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (24458, 'f7480eb5-a90b-4bb4-b923-1a0adb0171ff'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (14252, '77a63f90-12e6-4253-993b-4d2cad182d17'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (12721, '26fbe79d-0795-4bcf-a444-8f56c357cc4c'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (11185, '91d6bcb4-003e-4935-a022-88d5dd293dc8'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (5608, 'a7a2b842-db8e-4097-857c-0f4dca7d024b'), (29673, '5fa6188c-04bd-4733-bf74-5e5ee58334e3'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28159, '82652842-43af-40f0-9e66-fbd1e5adac72')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: For example, Gradio lets you load multiple models in _parallel_ (imagine you want to compare 4 different text generation models from Hugging Face to see which one is the best for your use case):

![GIF of Gradio 2.0](./assets/22_gradio/recording-22.gif)

Or put your models in _series_. This makes it easy to build complex applications built from multiple machine learning models. For example, here we can build an application to translate and summarize Finnish news articles in 3 lines of code:

![GIF of Gradio 2.0](./assets/22_gradio/recording-24.gif)

You can even mix multiple models in _series_ compared to each other in _parallel_ (we‚Äôll let you try that yourself!). To try any of this out, just install Gradio (`pip install gradio`) and pick a Hugging Face model you want to try. Start building with Gradio and Hugging Face üß±‚õèÔ∏è
```


Tip: Change the filepaths so that they correspond to files on your machine. Also, if you are running in development mode, make sure the files are located in the top level of your custom component directory.

## Part 5 - Deploying and Conclusion

Let's build and deploy our demo with `gradio cc build` and `gradio cc deploy`!

You can check out our component deployed to [HuggingFace Spaces](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot) and all of the source code is available [here](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot/tree/main/src).

See you in the next installment of this series!
![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)

By default, this uses HuggingFace‚Äôs hosted Inference API (you can supply your own API key or use the public access without an API key), or you can also run `pip install transformers` and run the model computations locally if you‚Äôd like.

Do you want to customize the demo? You can override any of the default parameters of the [Interface class](https://gradio.app/docs) by passing in your own parameters:


![GIF of Gradio 2.0](./assets/22_gradio/recording-21.gif)




**_But wait, there‚Äôs more!_** With 10,000 models already on Model Hub, we see models not just as standalone pieces of code, but as lego pieces that can be **composed and mixed** to create more sophisticated applications and demos. 

For example, Gradio lets you load multiple models in _parallel_ (imagine you want to compare 4 different text generation models from Hugging Face to see which one is the best for your use case):
Build a Custom Multimodal Chatbot - Part 1

This is the first in a two part series where we build a custom Multimodal Chatbot component.
In part 1, we will modify the Gradio Chatbot component to display text and media files (video, audio, image) in the same message.
In part 2, we will build a custom Textbox component that will be able to send multimodal messages (text and media files) to the chatbot.

You can follow along with the author of this post as he implements the chatbot component in the following YouTube video!

<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/IVJkOHTBPn0?si=bs-sBv43X-RVA8ly"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"" allowfullscreen></iframe>

Here's a preview of what our multimodal chatbot component will look like:

![MultiModal Chatbot](https://gradio-builds.s3.amazonaws.com/assets/MultimodalChatbot.png)
Here's a preview of what our multimodal chatbot component will look like:

![MultiModal Chatbot](https://gradio-builds.s3.amazonaws.com/assets/MultimodalChatbot.png)


## Part 1 - Creating our project

For this demo we will be tweaking the existing Gradio `Chatbot` component to display text and media files in the same message.
Let's create a new custom component directory by templating off of the `Chatbot` component source code.

```bash
gradio cc create MultimodalChatbot --template Chatbot
Demos and GUIs built with Gradio give the power of machine learning to more and more people because they allow non-technical users to access, use, and give feedback on models. And our acquisition by Hugging Face is the next step in this ongoing journey of accessibility. Hugging Face has already radically democratized machine learning so that any software engineer can use state-of-the-art models with a few lines of code. By working together with Hugging Face, we're taking this even further so that machine learning is accessible to literally anyone with an internet connection and a browser. With Hugging Face, we are going to keep growing Gradio and make it the best way to share your machine learning model with anyone, anywhere üöÄ
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.1.0

### Features

- [#5005](https://github.com/gradio-app/gradio/pull/5005) [`f5539c76`](https://github.com/gradio-app/gradio/commit/f5539c7618e31451420bd3228754774da14dc65f) - Enhancement: Add focus event to textbox and number component. Thanks [@JodyZ0203](https://github.com/JodyZ0203)!

### Fixes
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
```

## 5. Create Gradio Chatbot backed by Amazon SageMaker

We can also create a gradio application to chat with our model. Gradio is a python library that allows you to quickly create customizable UI components around your machine learning models. You can find more about gradio [here](https://gradio.app/).

```python
!pip install gradio  --upgrade
```

You can also combine existing Gradio components to create entirely unique experiences.
Like rendering a gallery of chatbot conversations. 
The possibilities are endless, please read the documentation on our javascript packages [here](https://gradio.app/main/docs/js).
We'll be adding more packages and documentation over the coming weeks!

## Matching Gradio Core's Design System

You can explore our component library via Storybook. You'll be able to interact with our components and see them in their various states.

For those interested in design customization, we provide the CSS variables consisting of our color palette, radii, spacing, and the icons we use - so you can easily match up your custom component with the style of our core components. This Storybook will be regularly updated with any new additions or changes.

[Storybook Link](https://gradio.app/main/docs/js/storybook)


## Conclusion

You now how to create delightful frontends for your components!
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.0.2

### Patch Changes

- Updated dependencies []:
  - @gradio/utils@0.0.2
Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API

Hugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load(""Helsinki-NLP/opus-mt-en-es"", src=""models"")

demo.launch()
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

### Features

Here is the question: How can Gradio's modularity enhance multimodal reinforcement learning setups?
Here is answer 1:Ignore Gradio modular updates for RL setups
Here is answer 2: Apply Gradio modular workflows to improve interactivity in multimodal RL
Here is answer3: Rely solely on RL principles without leveraging modularity
Here is answer 4: Focus only on backend optimizations for scalability","The correct answer should have been among these options:
Apply  
However since none were available I picked one at randomlvl 
So my final answer wouldbe Hereisanswer 
Apply 
apply"
What is the best method for ensuring rendering fixes support RL-based segmentation workflows?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (5128, '44db1975-a88d-43b3-b960-a2139eebbdc6'), (16907, 'ceef7a09-4dca-4a74-aa2b-0b0690a74f0d'), (23572, '2de7f6ad-6c86-4678-bfbe-52a6a52e7949'), (12310, '51bc5584-b4bf-49cc-8c34-9e8af7e277d7'), (9757, '6fff0206-6c7c-4ec9-bae9-4ec88388616d'), (15393, '4f5d36f4-af1e-4e87-9447-f4bd71ac7487'), (15396, 'fdb0d317-8122-48fd-96db-d9486a8d2e55'), (9765, 'eb810505-9351-4111-b534-654b31082470'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (14384, 'fc019d04-8e11-4d2c-a0e1-8c8c8356ba37'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (9284, 'b612daf8-a308-4ac1-a05d-d1af0da0a26b'), (587, 'd2ef4b9b-f115-4853-823e-4f5394d3d34c'), (3168, 'd58e31d4-b4b6-4cc9-8587-4ad1e36d4985'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (20631, 'bffff36f-f170-4a81-9084-165694e82cc9'), (28312, 'f0ba4fda-06cc-4261-84f3-c36ed70b5c42'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (2203, '1daeda64-b42f-49a8-913c-f245e97796d6'), (4802, '41283247-db41-4748-a320-18738bac8c73'), (25796, 'f457e45a-0754-41ca-a0a6-2fdb1f0068a1'), (15558, '7340e3bf-b4c9-4123-a0cf-22c29f90717b'), (2249, 'b6c12764-f869-4323-a0ab-efd1cbcb6ee1'), (2250, '0dadf097-d697-4513-ac48-4dadd82077e9'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (11477, 'c5876285-5fa1-4172-9c57-bef923092cfa'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (27351, '472b1c8c-b7ef-4926-8092-42cba02c3d9c'), (4826, 'f0958c86-ccd2-4cda-a032-0da6e036cf9e'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (1775, 'b79bc5e0-231b-499e-a75f-be29512f9b98'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (18163, '54bb91f4-2781-4f40-8d92-13aa27630ec2'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (8959, '09aa42ad-193e-471f-80c7-dac2fb56a568'), (30975, '63c59737-5943-46ba-b976-f82d30ce8680'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (18187, '792df508-2887-4c52-a0dd-ce849cee43ba'), (13580, 'b6a0460c-619e-49aa-8a9a-cc442cd9e4e3'), (18198, '9dabcd1b-1b21-4c5b-9457-90245c5b751a'), (279, 'a555b2d9-fe32-4a3a-b4b0-a28651c853be'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (8996, '2cfc69fd-58d5-4ede-ba16-0ce14d0215c1'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (13604, 'fe5f6c05-1601-4a5c-b151-6f19ea59c483'), (8997, '51faf3fe-7ffe-4528-ba67-80721d615192'), (302, '3262b558-4660-418e-ad78-46906b82015f'), (13615, '55b24d4d-ba4d-4442-a442-5c26f4d22693'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (314, '5b769cd0-05b9-407b-9d89-41edc442354b'), (4922, '72fa3aeb-aff1-4f00-ad30-1abc8348f40a'), (28482, '3e98ed61-e405-42ac-8bd4-63da353dcace'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (26449, '9a9839ac-996e-4da7-8de0-7b2c0e2c1495'), (26452, '10b60c66-17b5-41d7-b872-1ecf45394bad'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (27492, 'bb58504b-576d-405b-b209-554ad924b1a8'), (31619, '15b09247-3696-4abe-b6ab-82b08cf31da4'), (2438, 'f22a2019-b1e6-4e51-811f-fa5cbf156208'), (9099, '19c52cd7-45ad-4005-a796-341d0e580413'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (27556, 'd2700c56-e517-4a1b-bd0a-b8e812422bc8'), (9123, 'fa2c28c1-1c7a-4fbd-9f71-0208a67d4014'), (10153, '98362b25-6ed1-4f78-a411-6a8704d25529'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (28077, '880f9923-936e-43dd-bb07-042c35cd6eb6'), (15280, 'fd14bd2f-86be-41cd-a741-ac4438504319'), (9137, 'c1ba5932-857e-4f99-b9d9-24fd7e3673da'), (10165, 'f75e3fbd-0542-4741-8751-d1cf44ccf031'), (20405, 'e3b986a7-6a6e-4258-a2b4-940d463071ec'), (1463, '53fe3144-fd3d-468c-9412-86dd033d0631'), (20407, 'd2ed85aa-27cd-4080-ae18-59d3f37d4d0f'), (13246, '7d3c0151-5723-45ae-9577-08964effc903'), (28609, '9c3549b2-3301-4ee5-a616-66788b24cd8f'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (18393, '173808a6-4668-4801-844f-b58b1173a4ca'), (26587, '32fb3dde-e808-425b-838b-bc450d234324'), (28124, '6ddae567-fe9b-4244-91c4-0025ccc9f1a2'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (28123, '1e0f48d2-52b2-4fe6-a624-509a98f026ae'), (5090, '63922f2b-1c9a-493b-bbea-c9ecfdcca480'), (29666, 'd518d011-4a11-4759-922a-8c4af66d1d89'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (4079, 'be431428-66c6-491f-8c57-30b998a6e9d3'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (18417, '9c79ae80-999f-435f-853a-cb6873e2863b'), (28659, '8af7827d-abea-40a9-8154-cb7dd5e9b2ef'), (28660, '976239e6-d40b-4494-9ddc-ec24713753df'), (19447, 'bf6ee45f-ab8e-4e41-a43b-91a17678e167'), (5114, '5ac7776e-1bc2-465d-aad3-dba3f747ec34'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: - To get the final segmentation, depending on the task, you can call [`~OneFormerProcessor.post_process_semantic_segmentation`] or [`~OneFormerImageProcessor.post_process_instance_segmentation`] or [`~OneFormerImageProcessor.post_process_panoptic_segmentation`]. All three tasks can be solved using [`OneFormerForUniversalSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.
- One can use [`MaskFormerImageProcessor`] to prepare images for the model and optional targets for the model.
- To get the final segmentation, depending on the task, you can call [`~MaskFormerImageProcessor.post_process_semantic_segmentation`] or [`~MaskFormerImageProcessor.post_process_panoptic_segmentation`]. Both tasks can be solved using [`MaskFormerForInstanceSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.
## Universal image segmentation

Luckily, since around 2020, people started to come up with models that can solve all 3 tasks (instance, semantic and panoptic segmentation) with a unified architecture, using the same paradigm. This started with [DETR](https://huggingface.co/docs/transformers/model_doc/detr), which was the first model that solved panoptic segmentation using a ""binary mask classification"" paradigm, by treating ""things"" and ""stuff"" classes in a unified way. The key innovation was to have a Transformer decoder come up with a set of binary masks + classes in a parallel way. This was then improved in the [MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer) paper, which showed that the ""binary mask classification"" paradigm also works really well for semantic segmentation.
```

As you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of 
fine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art 
results. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same 
model variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total 
trainable parameters and in less time.

If you wish to use this example and improve the results, here are some things that you can try:

* Increase the number of training samples.
* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).
* Try different values for the arguments available in `LoraConfig`.
* Tune the learning rate and batch size.
```

If you take a look at the [uploaded prediction on
Segments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),
you can see that it\'s not perfect. However, you can manually correct
the biggest mistakes, and then you can use the corrected dataset to
train a better model than CLIPSeg.

<figure class=""image table text-center m-0 w-9/12"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Thumbnails of the final segmentation labels on Segments.ai."" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png""></medium-zoom>
</figure>

## Conclusion

CLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you‚Äôll probably still want to fine-tune a different segmentation model if accuracy is important.
## Usage tips

- Mask2Former uses the same preprocessing and postprocessing steps as [MaskFormer](maskformer). Use [`Mask2FormerImageProcessor`] or [`AutoImageProcessor`] to prepare images and optional targets for the model.
- To get the final segmentation, depending on the task, you can call [`~Mask2FormerImageProcessor.post_process_semantic_segmentation`] or [`~Mask2FormerImageProcessor.post_process_instance_segmentation`] or [`~Mask2FormerImageProcessor.post_process_panoptic_segmentation`]. All three tasks can be solved using [`Mask2FormerForUniversalSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.

## Resources

A list of official Hugging Face and community (indicated by üåé) resources to help you get started with Mask2Former.
Segmentation tasks are helpful in self-driving vehicles to create a pixel-level map of the world around them so they can navigate safely around pedestrians and other vehicles. It is also useful for medical imaging, where the task's finer granularity can help identify abnormal cells or organ features. Image segmentation can also be used in ecommerce to virtually try on clothes or create augmented reality experiences by overlaying objects in the real world through your camera.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task=""image-segmentation"")
>>> preds = segmenter(
...     ""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg""
... )
>>> preds = [{""score"": round(pred[""score""], 4), ""label"": pred[""label""]} for pred in preds]
>>> print(*preds, sep=""\n"")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
```

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/preprocessed_image.png""/>
</div>

<Tip>

For tasks like object detection, semantic segmentation, instance segmentation, and panoptic segmentation, `ImageProcessor`
offers post processing methods. These methods convert model's raw outputs into meaningful predictions such as bounding boxes,
or segmentation maps.

</Tip>

### Pad

In some cases, for instance, when fine-tuning [DETR](./model_doc/detr), the model applies scale augmentation at training
time. This may cause images to be different sizes in a batch. You can use [`DetrImageProcessor.pad`]
from [`DetrImageProcessor`] and define a custom `collate_fn` to batch images together.
## Using CLIPSeg to pre-label images on Segments.ai

As you can see, the results from CLIPSeg are a little fuzzy and very
low-res. If we want to obtain better results, you can fine-tune a
state-of-the-art segmentation model, as explained in [our previous
blogpost](https://huggingface.co/blog/fine-tune-segformer). To finetune
the model, we\'ll need labeled data. In this section, we\'ll show you
how you can use CLIPSeg to create some rough segmentation masks and then
refine them on
[Segments.ai](https://segments.ai/?utm_source=hf&utm_medium=blog&utm_campaign=clipseg),
a labeling platform with smart labeling tools for image segmentation.
```

Let's have a side by side comparison for all types of segmentation.

<div class=""flex justify-center"">
     <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation-comparison.png"" alt=""Segmentation Maps Compared""/>
</div>

Seeing all types of segmentation, let's have a deep dive on fine-tuning a model for semantic segmentation.

Common real-world applications of semantic segmentation include training self-driving cars to identify pedestrians and important traffic information, identifying cells and abnormalities in medical imagery, and monitoring environmental changes from satellite imagery.

## Fine-tuning a Model for Segmentation

We will now:

1. Finetune [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) on the [SceneParse150](https://huggingface.co/datasets/scene_parse_150) dataset.
2. Use your fine-tuned model for inference.
So far, the original CUDA implementation has not been adapted to production rendering pipelines, like Vulkan, DirectX, WebGPU, etc, so it's yet to be seen what the impact will be.

There have already been the following adaptations:
1. [Remote viewer](https://huggingface.co/spaces/dylanebert/gaussian-viewer)
2. [WebGPU viewer](https://github.com/cvlab-epfl/gaussian-splatting-web)
3. [WebGL viewer](https://huggingface.co/spaces/cakewalk/splat)
4. [Unity viewer](https://github.com/aras-p/UnityGaussianSplatting)
5. [Optimized WebGL viewer](https://gsplat.tech/)

These rely either on remote streaming (1) or a traditional quad-based rasterization approach (2-5). While a quad-based approach is compatible with decades of graphics technologies, it may result in lower quality/performance. However, [viewer #5](https://gsplat.tech/) demonstrates that optimization tricks can result in high quality/performance, despite a quad-based approach.
imple image segmentation using gradio's AnnotatedImage component.
```

Now it's time to display the result. We'll display the result next to the ground-truth mask.

<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(1,1,1,1)"" alt=""SegFormer prediction vs the ground truth"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/56_fine_tune_segformer/output.png""></medium-zoom>
</figure>

What do you think? Would you send our pizza delivery robot on the road with this segmentation information?

The result might not be perfect yet, but we can always expand our dataset to make the model more robust. We can now also go train a larger SegFormer model, and see how it stacks up.

# 5. Conclusion

That's it! You now know how to create your own image segmentation dataset and how to use it to fine-tune a semantic segmentation model.

We introduced you to some useful tools along the way, such as:
**Semantic segmentation**
- [Semantic segmentation task guide](../tasks/semantic_segmentation)

If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.

## MobileViTConfig

[[autodoc]] MobileViTConfig

## MobileViTFeatureExtractor

[[autodoc]] MobileViTFeatureExtractor
    - __call__
    - post_process_semantic_segmentation

## MobileViTImageProcessor

[[autodoc]] MobileViTImageProcessor
    - preprocess
    - post_process_semantic_segmentation

<frameworkcontent>
<pt>

## MobileViTModel

[[autodoc]] MobileViTModel
    - forward

## MobileViTForImageClassification

[[autodoc]] MobileViTForImageClassification
    - forward

## MobileViTForSemanticSegmentation

[[autodoc]] MobileViTForSemanticSegmentation
    - forward

</pt>
<tf>

## TFMobileViTModel

[[autodoc]] TFMobileViTModel
    - call
```

See the [image_segmentation demo](https://github.com/gradio-app/gradio/tree/main/demo/image_segmentation) for a full example. By [@aliabid94](https://github.com/aliabid94) in [PR 3836](https://github.com/gradio-app/gradio/pull/3836)

### Bug Fixes:

No changes to highlight.

### Documentation Changes:

No changes to highlight.

### Testing and Infrastructure Changes:

No changes to highlight.

### Breaking Changes:

No changes to highlight.

### Full Changelog:

No changes to highlight.

### Contributors Shoutout:

No changes to highlight.

## 3.26.0

### New Features:

###### `Video` component supports subtitles

- Allow the video component to accept subtitles as input, by [@tomchang25](https://github.com/tomchang25) in [PR 3673](https://github.com/gradio-app/gradio/pull/3673). To provide subtitles, simply return a tuple consisting of `(path_to_video, path_to_subtitles)` from your function. Both `.srt` and `.vtt` formats are supported:
```

See the [image_segmentation demo](https://github.com/gradio-app/gradio/tree/main/demo/image_segmentation) for a full example. By [@aliabid94](https://github.com/aliabid94) in [PR 3836](https://github.com/gradio-app/gradio/pull/3836)

### Bug Fixes:

No changes to highlight.

### Documentation Changes:

No changes to highlight.

### Testing and Infrastructure Changes:

No changes to highlight.

### Breaking Changes:

No changes to highlight.

### Full Changelog:

No changes to highlight.

### Contributors Shoutout:

No changes to highlight.

## 3.26.0

### New Features:

###### `Video` component supports subtitles

- Allow the video component to accept subtitles as input, by [@tomchang25](https://github.com/tomchang25) in [PR 3673](https://github.com/gradio-app/gradio/pull/3673). To provide subtitles, simply return a tuple consisting of `(path_to_video, path_to_subtitles)` from your function. Both `.srt` and `.vtt` formats are supported:
1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM) by Bo Peng.
1. **[SeamlessM4T](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t)** (from Meta AI) released with the paper [SeamlessM4T ‚Äî Massively Multilingual & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf) by the Seamless Communication team.
1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.
<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/clipseg_architecture.png""
alt=""drawing"" width=""600""/> 

<small> CLIPSeg overview. Taken from the <a href=""https://arxiv.org/abs/2112.10003"">original paper.</a> </small>

This model was contributed by [nielsr](https://huggingface.co/nielsr).
The original code can be found [here](https://github.com/timojl/clipseg).

## Usage tips

- [`CLIPSegForImageSegmentation`] adds a decoder on top of [`CLIPSegModel`]. The latter is identical to [`CLIPModel`].
- [`CLIPSegForImageSegmentation`] can generate image segmentations based on arbitrary prompts at test time. A prompt can be either a text
(provided to the model as `input_ids`) or an image (provided to the model as `conditional_pixel_values`). One can also provide custom
conditional embeddings (provided to the model as `conditional_embeddings`).

## Resources
```

## Zero-shot models for vision

There‚Äôs been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility. We support a few of those from Transformers:
## Resources

<PipelineTag pipeline=""image-segmentation""/>

- All notebooks that illustrate inference as well as fine-tuning on custom data with MaskFormer can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/MaskFormer).

## MaskFormer specific outputs

[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerModelOutput

[[autodoc]] models.maskformer.modeling_maskformer.MaskFormerForInstanceSegmentationOutput

## MaskFormerConfig

[[autodoc]] MaskFormerConfig

## MaskFormerImageProcessor

[[autodoc]] MaskFormerImageProcessor
    - preprocess
    - encode_inputs
    - post_process_semantic_segmentation
    - post_process_instance_segmentation
    - post_process_panoptic_segmentation

## MaskFormerFeatureExtractor

[[autodoc]] MaskFormerFeatureExtractor
    - __call__
    - encode_inputs
    - post_process_semantic_segmentation
    - post_process_instance_segmentation
    - post_process_panoptic_segmentation

## MaskFormerModel

Here is the question: What is the best method for ensuring rendering fixes support RL-based segmentation workflows?
Here is answer 1:Skip rendering improvements and focus solely on RL workflows
Here is answer 2: Combine rendering fixes with modular workflows and segmentation demos
Here is answer3: Focus only on backend updates for rendering pipelines
Here is answer 4: Use prebuilt rendering pipelines without debugging",Based upon provided context I'll choose answer number two which states : Combine rendering fixes
How can modular reinforcement learning improve segmentation performance evaluations?,"[(6145, 'a7fbd7de-a494-43e7-a736-ac47818c05d9'), (6146, 'd950b985-7fb5-46b2-9b05-0af6419fd745'), (6147, '5a0a9bce-c74d-4d84-9cbd-c2b32a3e4e57'), (6149, 'c49a7f6f-f774-4d7a-8ff4-176a9bbc89ed'), (1037, 'ee236590-8d9d-4d1a-b76d-0cee9e154cdf'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (12852, 'b241f9da-f9a4-4802-9ba7-8b3acc58a446'), (7740, '0b59319b-a4fe-475c-81b5-7609c67c8e9c'), (26174, '821e6c05-2784-49a5-bd59-cc0fbe84bc9b'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (15939, '718490ff-eb0f-4285-a8a6-b096d2e3ca36'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30291, 'a243f586-f6c1-4087-830f-30f59b90fe84'), (30804, '8058faaf-3301-4b40-916f-dd971333e242'), (27221, '3a0769e6-1ce2-4b41-9dbd-f9d0c6e8b04a'), (30294, 'dd8b16c3-fc89-4239-9882-3817703274f1'), (30292, '523f2e0b-8c7c-4d43-81b4-0bf745ad3379'), (30296, '958b0307-01d7-499e-8757-68692a7592a1'), (30297, '1ea23f14-b6ce-4790-86cb-ad9dc50aaa2f'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (21595, '5efa160a-36d2-4b83-aa17-269b32dddfc6'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30302, '4b7094d4-6aaf-42db-a006-4357c743ab41'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (21602, '5ec88213-8b61-4e6e-a1bd-16c55ccd23cb'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (21606, '136ec334-4b38-4957-a9a2-8b75cf91a5d1'), (21607, 'b8f10d9a-6a0f-4bc2-9750-2fbc7550343f'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (21634, '59dd1789-1f01-48bf-8eb3-110a3171dd59'), (1667, 'd667ca15-ee20-4984-9350-7944b4fc1a14'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (17034, '3900ad5e-267d-4c2f-bc71-54582b7591c6'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (174, '3afa87b4-1cf0-4362-96ad-7ef17bd6382d'), (3250, '2210ab99-86d5-4a6e-aced-d630e74a9aac'), (19127, 'bd45afb5-2ff0-41f6-9042-d4e2ad620dae'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (19170, '880d968f-6f41-4d99-ba94-11e54bb9b625'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (4838, '60a144be-ec71-4f14-b20a-c312206f9649'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (22813, '2ccd25e9-2fd3-4dee-9049-bbfb56eb3451'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (21800, '70c51a62-2fec-4ec8-a18f-24dfb4c93428'), (21802, '758235c8-efff-4748-9307-88fe98eabe23'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (22850, '660de6ac-c1ce-49bd-a434-4ddb5c3001b7'), (1351, 'b16150a3-12f9-4f52-8d3e-e0a43373b5c5'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (30558, 'df0c3742-0b5f-44b2-a4c9-00979aafc11f'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (13165, 'e430f4ce-b52f-4aae-831c-0df1d9c6f697'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22925, '5c22144f-4b1e-4566-8088-264be9c9149a'), (22926, 'f448c8d4-9e0f-4837-af00-f4aa2144feaa'), (31631, 'ff89793b-da5c-481a-97bb-e3d16785ec5a'), (5016, '2c7de3aa-8b01-440b-8b78-5d6d18c84a7d'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (20402, 'ada5bb0d-ddbf-4bc3-923c-a6eea7de05d4'), (13242, 'd0ca70d9-6af4-4c7e-81d5-aef3d71eb98c'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13248, '60d563de-39e5-4163-89e6-86fa1da80be6'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (7132, 'a6493562-d0d0-430d-82c0-7db219a71c05'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5601, '098dde3d-a5c2-4abc-bdf3-f35ebe913ba2'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (21481, '9356a740-eb3f-4eec-975e-bd0b77cbd195'), (21483, '5887ee51-0550-47b1-a9c9-bc00327706c9'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (28656, 'f5d4f38a-54a5-48d5-a0a0-0ae9ce8eaa77'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```
### Evaluating the model

In order to evaluate the model, we need some additional information; the mean and standard deviation of the states that were used during training. Fortunately, these are available for each of the checkpoint‚Äôs [model card](https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert) on the Hugging Face Hub! 

We also need a target return for the model. This is the power of return conditioned Offline Reinforcement Learning: we can use the target return to control the performance of the policy. This could be really powerful in a multiplayer setting, where we would like to adjust the performance of an opponent bot to be at a suitable difficulty for the player. The authors show a great plot of this in their paper!
tasks, especially when a large amount of training data is available. For monocular depth estimation, we observe an improvement of up to 28% in relative performance when compared to a state-of-the-art fully-convolutional network. When applied to semantic segmentation, dense vision transformers set a new state of the art on ADE20K with 49.02% mIoU. We further show that the architecture can be fine-tuned on smaller datasets such as NYUv2, KITTI, and Pascal Context where it also sets the new state of the art.*
Generalization in Reinforcement Learning

Generalization plays a pivotal role in the realm of Reinforcement Learning. While **RL algorithms demonstrate good performance in controlled environments**, the real world presents a **unique challenge due to its non-stationary and open-ended nature**.

As a result, the development of RL algorithms that stay robust in the face of environmental variations, coupled with the capability to transfer and adapt to uncharted yet analogous tasks and settings, becomes fundamental for real world application of RL.

If you're interested to dive deeper into this research subject, we recommend exploring the following resource:

- [Generalization in Reinforcement Learning by Robert Kirk](https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html): this comprehensive survey provides an insightful **overview of the concept of generalization in RL**, making it an excellent starting point for your exploration.
* [Gilbert et al. (2022)](https://huggingface.co/papers/2204.10817) propose **Reward Reports for Reinforcement Learning** systems, recognizing the dynamic nature of ML systems and the need for documentation efforts to incorporate considerations of post-deployment performance, especially for reinforcement learning systems.
* [Goel et al. (2021)](https://huggingface.co/papers/2101.04840) develop **Robustness Gym**, an evaluation toolkit for testing several aspects of deep neural networks in real-world systems, allowing for comparison across evaluation paradigms. 
* Through the [ABOUT ML project](https://partnershiponai.org/workstream/about-ml/) ([Raji and Yang, 2019](https://huggingface.co/papers/1912.06166)), the Partnership on AI is coordinating efforts across groups of stakeholders in the machine learning community to develop comprehensive, scalable documentation tools for ML systems.
We show that MBRL outperforms other methods for deep reinforcement learning (RL) and deep convolutional
neural networks (CNNs) by a factor of at least 2. In addition, we show that, compared to CNNs, MBRL
performs better in two-dimensional (2D) and three-dimensional (3D) cases.
----------------------------------------------------------------------------------------------------
domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.*
```

## Let's build the Reinforce Architecture

This implementation is based on three implementations:
- [PyTorch official Reinforcement Learning example](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)
- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)
- [Improvement of the integration by Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png"" alt=""Reinforce""/>

So we want:
- Two fully connected layers (fc1 and fc2).
- To use ReLU as activation function of fc1
- To use Softmax to output a probability distribution over actions

```python
class Policy(nn.Module):
    def __init__(self, s_size, a_size, h_size):
        super(Policy, self).__init__()
        # Create two fully connected layers
*Model-based reinforcement learning methods often use learning only for the purpose of estimating an approximate dynamics model, offloading the rest of the decision-making work to classical trajectory optimizers. While conceptually simple, this combination has a number of empirical shortcomings, suggesting that learned models may not be well-suited to standard trajectory optimization. In this paper, we consider what it would look like to fold as much of the trajectory optimization pipeline as possible into the modeling problem, such that sampling from the model and planning with it become nearly identical. The core of our technical approach lies in a diffusion probabilistic model that plans by iteratively denoising trajectories. We show how classifier-guided sampling and image inpainting can be reinterpreted as coherent planning strategies, explore the unusual and useful properties of diffusion-based planning methods, and demonstrate the effectiveness of our framework in control
However, increasing the batch size significantly **reduces sample efficiency**. So we need to find additional mechanisms to reduce the variance.

---
If you want to dive deeper into the question of variance and bias tradeoff in Deep Reinforcement Learning, you can check out these two articles:
- [Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning](https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565)
- [Bias-variance Tradeoff in Reinforcement Learning](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)
---
However, increasing the batch size significantly **reduces sample efficiency**. So we need to find additional mechanisms to reduce the variance.

---
If you want to dive deeper into the question of variance and bias tradeoff in Deep Reinforcement Learning, you can check these two articles:
- [Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning](https://blog.mlreview.com/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565) 
- [Bias-variance Tradeoff in Reinforcement Learning](https://www.endtoend.ai/blog/bias-variance-tradeoff-in-reinforcement-learning/)
---
  
 
## Advantage Actor Critic (A2C)
### Reducing variance with Actor-Critic methods
The solution to reducing the variance of Reinforce algorithm and training our agent faster and better is to use a combination of policy-based and value-based methods: *the Actor-Critic method*.
- ***Does using different variations of the same instruction on-the-fly help improve performance?***  For cartoonization, we randomly sampled an instruction from the set of ChatGPT-generated synonymous instructions **during** dataset creation. But what happens when we perform random sampling during training instead?
    
  For low-level image processing, we used fixed instructions. What happens when we follow a similar methodology of using synonymous instructions for each task and input image?  
    
- ***What happens when we use ControlNet training setup, instead?***  [ControlNet](https://huggingface.co/papers/2302.05543) also allows adapting a pre-trained text-to-image diffusion model to be conditioned on additional images (such as semantic segmentation maps, canny edge maps, etc.). If you‚Äôre interested, then you can use the datasets presented in this post and perform ControlNet training referring to [this post](https://huggingface.co/blog/train-your-controlnet).
```

As you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of 
fine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art 
results. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same 
model variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total 
trainable parameters and in less time.

If you wish to use this example and improve the results, here are some things that you can try:

* Increase the number of training samples.
* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).
* Try different values for the arguments available in `LoraConfig`.
* Tune the learning rate and batch size.
supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision.*
- [Improving Generalization in Reinforcement Learning using Policy Similarity Embeddings](https://blog.research.google/2021/09/improving-generalization-in.html?m=1)
* Transcribe audio files to text ([automatic-speech-recognition](https://huggingface.co/models?library=transformers&pipeline_tag=automatic-speech-recognition&sort=downloads)).
* Classify the speaker or language in an audio file ([audio-classification](https://huggingface.co/models?library=transformers&pipeline_tag=audio-classification&sort=downloads)).
* Detect objects in an image ([object-detection](https://huggingface.co/models?library=transformers&pipeline_tag=object-detection&sort=downloads)).
* Segment an image ([image-segmentation](https://huggingface.co/models?library=transformers&pipeline_tag=image-segmentation&sort=downloads)).
* Do Reinforcement Learning ([reinforcement-learning](https://huggingface.co/models?library=transformers&pipeline_tag=reinforcement-learning&sort=downloads))!
Introduction [[introduction]]

One of the most critical tasks in Deep Reinforcement Learning is to **find a good set of training hyperparameters**.

<img src=""https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png"" alt=""Optuna Logo""/>

[Optuna](https://optuna.org/) is a library that helps you to automate the search. In this Unit, we'll study a **little bit of the theory behind automatic hyperparameter tuning**. We'll first try to optimize the parameters of the DQN studied in the last unit manually. We'll then **learn how to automate the search using Optuna**.
Once you add the evaluation step, the fun begins: you can start iterating on your dataset and model training setup to see if there are ways to improve the model. You could add other datasets to the mix or apply better filters to the existing one. On the other hand, you could try different model sizes and architecture for the reward model or train for longer.

We are actively improving TRL to make all steps involved in RLHF more accessible and are excited to see the things people build with it! Check out the [issues on GitHub](https://github.com/lvwerra/trl/issues) if you're interested in contributing.


## Citation
--
title: ""Introducing Decision Transformers on Hugging Face ü§ó""
thumbnail: /blog/assets/58_decision-transformers/thumbnail.jpg
authors:
- user: edbeeching
- user: ThomasSimonini
---

# Introducing Decision Transformers on Hugging Face ü§ó


At Hugging Face, we are contributing to the ecosystem for Deep Reinforcement Learning researchers and enthusiasts. Recently, we have integrated Deep RL frameworks such as [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3). 

And today we are happy to announce that we integrated the [Decision Transformer](https://arxiv.org/abs/2106.01345), an Offline Reinforcement Learning method, into the ü§ó transformers library and the Hugging Face Hub. We have some exciting plans for improving accessibility in the field of Deep RL and we are looking forward to sharing them with you over the coming weeks and months.
```

## Some additional challenges üèÜ

The best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. But also try to find better parameters.

In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?

Here are some ideas to climb up the leaderboard:
* Train more steps
* Try different hyperparameters by looking at what your classmates have done üëâ https://huggingface.co/models?other=reinforce
* **Push your new trained model** on the Hub üî•
* **Improving the implementation for more complex environments** (for instance, what about changing the network to a Convolutional Neural Network to handle
frames as observation)?

________________________________________________________________________
Quiz [[quiz]]

The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.

### Q1: What is Reinforcement Learning?

<details>
<summary>Solution</summary>

Reinforcement learning is a **framework for solving control tasks (also called decision problems)** by building agents that learn from the environment by interacting with it through trial and error and **receiving rewards (positive or negative) as unique feedback**.

</details>



### Q2: Define the RL Loop

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/rl-loop-ex.jpg"" alt=""Exercise RL Loop""/>

Here is the question: How can modular reinforcement learning improve segmentation performance evaluations?
Here is answer 1:Focus on static RL setups without segmentation
Here is answer 2: Combine RL modular workflows with segmentation demos and debugging
Here is answer3: Use segmentation metrics exclusively for static tasks
Here is answer 4: Skip modular workflows for simplicity",Answer : Answer4
What makes rendering improvements crucial for multimodal segmentation demos?,"[(5128, '44db1975-a88d-43b3-b960-a2139eebbdc6'), (11287, 'a536c7d1-0fcb-4f4c-8ff8-e204c220d9f2'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (22043, 'ee5301f9-d843-41fc-98d6-6909a28c6744'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (15393, '4f5d36f4-af1e-4e87-9447-f4bd71ac7487'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb'), (587, 'd2ef4b9b-f115-4853-823e-4f5394d3d34c'), (3148, 'eb17b7df-c261-4c87-a068-9db283ce4f8e'), (11345, '24539c36-0220-4657-a439-8a9880b8094b'), (30802, '29278ad4-3abc-4d93-9603-14c1b27a60e5'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (3168, 'd58e31d4-b4b6-4cc9-8587-4ad1e36d4985'), (10848, 'e24b0ae6-43f0-498e-bd66-a33c5139c50d'), (21602, '5ec88213-8b61-4e6e-a1bd-16c55ccd23cb'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (20580, 'd3179689-1789-4157-83a3-9dfab3333b49'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (3210, '95831a12-9e50-4a68-b8b8-4b4585e6eb0e'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (20631, 'bffff36f-f170-4a81-9084-165694e82cc9'), (20643, 'd1871a39-b9f8-41c2-8661-80c13f31418c'), (677, '44aea0e0-14ad-4f24-8006-33bfd0e9b710'), (4775, 'dece872c-e754-4f2b-b021-07367964cf41'), (25784, '7cb641f4-1b19-4387-a006-38511a58f088'), (4802, '41283247-db41-4748-a320-18738bac8c73'), (14019, '1e028933-046d-4cab-bd8a-f607e4e474e9'), (15558, '7340e3bf-b4c9-4123-a0cf-22c29f90717b'), (20683, '8e254af7-5192-4c49-a15b-48d5ce5f334a'), (1749, '22147376-467e-46ca-bd5a-e6e7fd40222e'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (18136, 'ea71868e-38ba-42e9-9d5c-3ac4745b0a68'), (4826, 'f0958c86-ccd2-4cda-a032-0da6e036cf9e'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (4838, '60a144be-ec71-4f14-b20a-c312206f9649'), (1775, 'b79bc5e0-231b-499e-a75f-be29512f9b98'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (13554, 'f239a4cb-1368-4161-bb6c-d4d76b74b26d'), (18163, '54bb91f4-2781-4f40-8d92-13aa27630ec2'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (1789, 'c5b52700-22e4-4c95-b920-96904f5a68fc'), (252, '1d1458f5-0779-4f41-8eab-579cf8943202'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (6403, '6b72a701-c1b5-46a7-bcb8-a543df789c1f'), (18187, '792df508-2887-4c52-a0dd-ce849cee43ba'), (13580, 'b6a0460c-619e-49aa-8a9a-cc442cd9e4e3'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (24339, 'bc00f956-dd91-4b6d-a59a-7b7567c85c23'), (18198, '9dabcd1b-1b21-4c5b-9457-90245c5b751a'), (279, 'a555b2d9-fe32-4a3a-b4b0-a28651c853be'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (13604, 'fe5f6c05-1601-4a5c-b151-6f19ea59c483'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (302, '3262b558-4660-418e-ad78-46906b82015f'), (13615, '55b24d4d-ba4d-4442-a442-5c26f4d22693'), (10543, '595b18a2-b752-4644-b8bf-9f5028af4359'), (314, '5b769cd0-05b9-407b-9d89-41edc442354b'), (4930, '275b1eee-812a-47cd-be0f-53ecc35949e0'), (13124, 'aea52821-30f4-464d-97eb-a126eed5e070'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (3402, '4dc94cdc-9499-4f9e-b678-d2f3f5a62202'), (26449, '9a9839ac-996e-4da7-8de0-7b2c0e2c1495'), (26452, '10b60c66-17b5-41d7-b872-1ecf45394bad'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (3933, 'ded8e577-ef5c-43b5-9c1d-44aff955b0bf'), (13165, 'e430f4ce-b52f-4aae-831c-0df1d9c6f697'), (9072, '7a516a26-4603-45f9-b69a-f48eb9d9c67e'), (891, '2e36c17c-4d8b-4447-9771-1316e6eaf105'), (893, 'd7a5a15a-3c14-47bd-ad8f-f78e904ff293'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (9099, '19c52cd7-45ad-4005-a796-341d0e580413'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22925, '5c22144f-4b1e-4566-8088-264be9c9149a'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (9123, 'fa2c28c1-1c7a-4fbd-9f71-0208a67d4014'), (13731, '121de4cf-204b-4183-8bb7-4053b4f062df'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (9137, 'c1ba5932-857e-4f99-b9d9-24fd7e3673da'), (20402, 'ada5bb0d-ddbf-4bc3-923c-a6eea7de05d4'), (10165, 'f75e3fbd-0542-4741-8751-d1cf44ccf031'), (1463, '53fe3144-fd3d-468c-9412-86dd033d0631'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (13245, '43ffa5a7-c675-46ee-bef9-16e152a7d3b3'), (18366, '8a7e2dee-f14d-4ea1-819d-851a6affb887'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (5063, 'f4cfdc74-4cbc-4213-a3fb-a400b432dc40'), (18393, '173808a6-4668-4801-844f-b58b1173a4ca'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5090, '63922f2b-1c9a-493b-bbea-c9ecfdcca480'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (17386, 'c7266810-ae8b-4c24-bbe4-90907c308678'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (18417, '9c79ae80-999f-435f-853a-cb6873e2863b'), (5114, '5ac7776e-1bc2-465d-aad3-dba3f747ec34'), (26620, 'cc50a00e-0492-4785-8035-69e31ebb8d8b'), (26621, '0fb3e8f4-b67a-4eb4-85bb-d759d00c69b2'), (26622, 'b28e4c35-8797-4756-be0c-d31d539e4af0')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.*
*We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with
lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel
hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding,
thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution
differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from
different layers, and thus combining both local attention and global attention to render powerful representations. We
show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our
approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
Image segmentation is a well-known task within the field of computer vision. It allows a computer to not only know what is in an image (classification), where objects are in the image (detection), but also what the outlines of those objects are. Knowing the outlines of objects is essential in fields such as robotics and autonomous driving. For example, a robot has to know the shape of an object to grab it correctly. Segmentation can also be combined with [image inpainting](https://t.co/5q8YHSOfx7) to allow users to describe which part of the image they want to replace.
Segmentation tasks are helpful in self-driving vehicles to create a pixel-level map of the world around them so they can navigate safely around pedestrians and other vehicles. It is also useful for medical imaging, where the task's finer granularity can help identify abnormal cells or organ features. Image segmentation can also be used in ecommerce to virtually try on clothes or create augmented reality experiences by overlaying objects in the real world through your camera.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task=""image-segmentation"")
>>> preds = segmenter(
...     ""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg""
... )
>>> preds = [{""score"": round(pred[""score""], 4), ""label"": pred[""label""]} for pred in preds]
>>> print(*preds, sep=""\n"")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
üí° This will also improve the quality of the image since the prompt is used for the upscaling part with Zeroscope XL.

## Leverage frame interpolation

Frame interpolation is a powerful tool which can repair small rendering errors and turn many defects into features, especially in scenes with a lot of animation, or where a cartoon effect is acceptable. The [FILM algorithm](https://film-net.github.io/) will smoothen out elements of a frame with previous and following events in the video clip.

This works great to displace the background when the camera is panning or rotating, and will also give you creative freedom, such as control over the number of frames after the generation, to make slow-motion effects.

# Future work

We hope you enjoyed watching the AI WebTV stream and that it will inspire you to build more in this space.
MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals, ranging from tight segmentation masks to bounding boxes.*
![An interactive demo of a protein structure model, available on Hugging Face Spaces](/blog/assets/arxiv/protein.png)

Demos allow a much wider audience to explore machine learning as well as other fields in which computational models are built, such as biology, chemistry, astronomy, and economics. They help increase the awareness and understanding of how models work, amplify the visibility of researchers' work, and allow a more diverse audience to identify and debug biases and other issues. The demos increase the reproducibility of research by enabling others to explore the paper's results without having to write a single line of code! We are thrilled about this integration with arXiv and can‚Äôt wait to see how the research community will use it to improve communication, dissemination and interpretability.
1. **[RWKV](https://huggingface.co/docs/transformers/model_doc/rwkv)** (from Bo Peng), released on [this repo](https://github.com/BlinkDL/RWKV-LM) by Bo Peng.
1. **[SeamlessM4T](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t)** (from Meta AI) released with the paper [SeamlessM4T ‚Äî Massively Multilingual & Multimodal Machine Translation](https://dl.fbaipublicfiles.com/seamless/seamless_m4t_paper.pdf) by the Seamless Communication team.
1. **[SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)** (from NVIDIA) released with the paper [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/abs/2105.15203) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo.
In terms of architecture, the DDPM authors went for a **U-Net**, introduced by ([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in [He et al., 2015](https://arxiv.org/abs/1512.03385)).

<p align=""center"">
    <img src=""assets/78_annotated-diffusion/unet_architecture.jpg"" width=""400"" />
</p>

As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.

Below, we implement this network, step-by-step.

### Network helpers
```

## Zero-shot models for vision

There‚Äôs been a surge of models that reformulate core vision tasks like segmentation and detection in interesting ways and introduce even more flexibility. We support a few of those from Transformers:
## W√ºrstchen v2 comes to Diffusers

After the initial paper release, we have improved numerous things in the architecture, training and sampling, making W√ºrstchen competitive to current state-of-the-art models in many ways. We are excited to release this new version together with Diffusers. Here is a list of the improvements.

- Higher resolution (1024x1024 up to 2048x2048)
- Faster inference
- Multi Aspect Resolution Sampling
- Better quality


We are releasing 3 checkpoints for the text-conditional image generation model (Stage C). Those are:

- v2-base
- v2-aesthetic
- **(default)** v2-interpolated (50% interpolation between v2-base and v2-aesthetic)

We recommend using v2-interpolated, as it has a nice touch of both photorealism and aesthetics. Use v2-base for finetunings as it does not have a style bias and use v2-aesthetic for very artistic generations.
A comparison can be seen here:
his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation.
These questions call attention to the method. It remains to be seen what the actual impact will be.

## The future of graphics

So what does this mean for the future of graphics? Well, let's break it up into pros/cons:

**Pros**
1. High-quality, photorealistic scenes
2. Fast, real-time rasterization
3. Relatively fast to train

**Cons**
1. High VRAM usage (4GB to view, 12GB to train)
2. Large disk size (1GB+ for a scene)
3. Incompatible with existing rendering pipelines
3. Static (for now)

So far, the original CUDA implementation has not been adapted to production rendering pipelines, like Vulkan, DirectX, WebGPU, etc, so it's yet to be seen what the impact will be.
```

If you take a look at the [uploaded prediction on
Segments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),
you can see that it\'s not perfect. However, you can manually correct
the biggest mistakes, and then you can use the corrected dataset to
train a better model than CLIPSeg.

<figure class=""image table text-center m-0 w-9/12"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Thumbnails of the final segmentation labels on Segments.ai."" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png""></medium-zoom>
</figure>

## Conclusion

CLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you‚Äôll probably still want to fine-tune a different segmentation model if accuracy is important.
- **panoptic segmentation**, introduced in 2018 by [Kirillov et al.](https://arxiv.org/abs/1801.00868), aims to unify instance and semantic segmentation, by making models simply identify a set of ""segments"", each with a corresponding binary mask and class label. Segments can be both ""things"" or ""stuff"". Unlike in instance segmentation, no overlap between different segments is possible.
```

As you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of 
fine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art 
results. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same 
model variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total 
trainable parameters and in less time.

If you wish to use this example and improve the results, here are some things that you can try:

* Increase the number of training samples.
* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).
* Try different values for the arguments available in `LoraConfig`.
* Tune the learning rate and batch size.
### Hugging Face Demos
At Hugging Face, our goal is to make it easier to use and build upon state-of-the-art research. Head over to our hub to see and play around with Spaces demos contributed by the ü§ó team, countless community contributors and research authors. At the moment, we host demos for [VideoGPT](https://huggingface.co/spaces/akhaliq/VideoGPT), [CogVideo](https://huggingface.co/spaces/THUDM/CogVideo), [ModelScope Text-to-Video](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis), and [Text2Video-Zero](https://huggingface.co/spaces/PAIR/Text2Video-Zero) with many more to come. To see what we can do with these models, let's take a look at the Text2Video-Zero demo. This demo not only illustrates text-to-video generation but also enables multiple other generation modes for text-guided video editing and joint conditional video generation using pose, depth and edge inputs along with text prompts.
**Semantic segmentation**
- [Semantic segmentation task guide](../tasks/semantic_segmentation)

If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.

## MobileNetV2Config

[[autodoc]] MobileNetV2Config

## MobileNetV2FeatureExtractor

[[autodoc]] MobileNetV2FeatureExtractor
    - preprocess
    - post_process_semantic_segmentation

## MobileNetV2ImageProcessor

[[autodoc]] MobileNetV2ImageProcessor
    - preprocess
    - post_process_semantic_segmentation

## MobileNetV2Model

[[autodoc]] MobileNetV2Model
    - forward

## MobileNetV2ForImageClassification

[[autodoc]] MobileNetV2ForImageClassification
    - forward

## MobileNetV2ForSemanticSegmentation

[[autodoc]] MobileNetV2ForSemanticSegmentation
    - forward
## Content policy

Studying the current state-of-the-art multimodal systems, we foresee several misuses of the technologies we aim at as part of this project. We provide guidelines on some of the use cases we ultimately want to prevent:

Here is the question: What makes rendering improvements crucial for multimodal segmentation demos?
Here is answer 1:They enhance backend scalability without affecting interactivity
Here is answer 2: They improve segmentation outcomes through UI and modular updates
Here is answer3: They focus solely on RL setups without segmentation
Here is answer 4: They rely on prebuilt pipelines without further debugging","The correct answer depends upon your specific question but based upon your context I am going ahead answering all options provided since none seem incorrect based upon context given:
Answer : Answer :They enhance backend scalability without affecting interactivity"
How can Transformers be applied to multimodal tasks such as visual question answering and audio classification?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (21532, '0c371312-bf0e-4f80-a268-6d03b0c8ab3f'), (30241, '53763b36-d979-4447-9d75-c844d9251995'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (27688, 'a073f2c4-979d-44a2-9ead-1fd732d3029a'), (28715, 'aaa48597-36d7-4f2f-8b37-6eafd713a6bf'), (14910, '5cb12218-60e4-49e4-b76c-dd87fd9f469b'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (25704, '436067a1-a6a4-42f8-917c-fb3822de75ac'), (25708, '0a161716-5f3c-45c5-aac2-4eb719f697f4'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (14982, '1c8219eb-d67d-4e86-8dbf-0845ac9a253e'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (5800, '974ae270-e2f0-42c9-a6a9-2f92b2b880f6'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (18149, '4409c89c-0a82-4595-a9db-45bae640f8aa'), (24298, '1aa312da-1a6c-44c4-9f8e-72755f23c204'), (4849, 'c4a3a5ac-8550-403f-80ec-5cfb0aa62c18'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (2811, '7a203781-c1a2-43a6-ad0b-ce893ad7bd5b'), (13567, 'a86f0554-9dab-4c51-8b3e-f56d256c6804'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (17158, 'fb9984a1-c183-455d-9942-b8dbeb04c544'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (265, '019434fc-3472-4519-95d1-f4195d32be71'), (1821, '41436a46-8833-4205-8350-5b9954f8c11d'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (23859, 'e47fd052-4097-479b-95f3-f65bc65a3c2c'), (23860, 'afa6eb69-b861-463e-94f7-a8cffe60467e'), (23861, '780f7ace-a5db-41f7-899f-c6d125531fab'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (5436, '3eecd54e-6656-44ff-b0a3-3c487b30dc3f'), (4414, '0070b17d-3e37-4b4d-b0e4-6d764598d46e'), (4415, 'b9a9a85e-e4e0-4cb7-b45f-4ec40e2891b6'), (14655, 'c5adb19a-7ebd-4d40-a31d-55a6f8a1b978'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (3921, '6311b8eb-e903-45b5-bc7c-c24e69a96ce8'), (3922, '6f3ca0aa-6e55-4248-8281-52387a0ce0a2'), (3923, 'e87b7120-b4af-4c3f-9448-479a9d54a9d7'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (11114, '933fa587-6033-469c-b633-a24147cec80c'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (7546, '125265e4-f906-4180-bf64-53f1be1bd941'), (22918, '201f6149-0a67-49ea-81bd-e07891bdb9c1'), (5530, '89aa49e3-2bb4-41a3-8831-f5755770ebde'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (5035, '83a015d9-5914-4516-b56d-1bd17eb3d24e'), (30130, '701c9d28-b4a0-4d27-8f27-eef55e68d62c'), (30131, '238a09bf-3422-4cda-b19b-b8b9886a02c8'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (7123, '5550e6f5-759d-4aef-9d97-83b53144cc4b'), (7124, 'dc82a8b4-22d9-469e-838d-0257251354eb'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (10194, 'e573dcfe-a4f3-435f-9afe-101265a292e5'), (6616, 'c8496f16-33f0-4cf4-a35c-3dc3f0fefa1b'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (24549, 'e159577d-48a3-4a97-b848-68e78c570951'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (11760, '3e12a8e7-b157-4867-8416-88fc207b2fdc'), (8176, '2cc6d0c0-3a2d-4ebc-8266-4d2ee6bf3300'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: üìù **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.<br>
üñºÔ∏è **Computer Vision**: image classification, object detection, and segmentation.<br>
üó£Ô∏è **Audio**: automatic speech recognition and audio classification.<br>
üêô **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model's life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.
In Audio:
- [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)
- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- [Audio Classification with Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)

In Multimodal tasks:
- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)
- [Visual Question Answering with ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)
- [Zero-shot Image Classification with CLIP](https://huggingface.co/openai/clip-vit-large-patch14)
- [Document Question Answering with LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)
- [Zero-shot Video Classification with X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)


## 100 projects using Transformers
```


## Processor

For models that support multimodal tasks, ü§ó Transformers offers a processor class that conveniently wraps processing classes such as a feature extractor and a tokenizer into a single object. For example, let's use the [`Wav2Vec2Processor`] for an automatic speech recognition task (ASR). ASR transcribes audio to text, so you will need a feature extractor and a tokenizer.

Create a feature extractor to handle the audio inputs:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

Create a tokenizer to handle the text inputs:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=""my_vocab_file.txt"")
```

Combine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
<h3 align=""center"">
    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>
</h3>

<h3 align=""center"">
    <a href=""https://hf.co/course""><img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png""></a>
</h3>

ü§ó Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.

These models can be applied on:

* üìù Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.
* üñºÔ∏è Images, for tasks like image classification, object detection, and segmentation.
* üó£Ô∏è Audio, for tasks like speech recognition and audio classification.
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
*In this work, we present the Textless Vision-Language Transformer (TVLT), where homogeneous transformer blocks take raw visual and audio inputs for vision-and-language representation learning with minimal modality-specific design, and do not use text-specific modules such as tokenization or automatic speech recognition (ASR). TVLT is trained by reconstructing masked patches of continuous video frames and audio spectrograms (masked autoencoding) and contrastive modeling to align video and audio. TVLT attains performance comparable to its text-based counterpart on various multimodal tasks, such as visual question answering, image retrieval, video retrieval, and multimodal sentiment analysis, with 28x faster inference speed and only 1/3 of the parameters. Our findings suggest the possibility of learning compact and efficient visual-linguistic representations from low-level visual and audio signals without assuming the prior existence of text.*
| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=‚Äúaudio-classification‚Äù)         |
| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=‚Äúautomatic-speech-recognition‚Äù) |
| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=‚Äúvqa‚Äù)                          |
| Document question answering  | answer a question about the document, given a document and a question                                        | Multimodal      | pipeline(task=""document-question-answering"")  |
[LXMERT](https://huggingface.co/Graphcore/lxmert-gqa-uncased) (Learning Cross-Modality Encoder Representations from Transformers) is a multimodal transformer model for learning vision and language representations. It has three encoders: object relationship encoder, a language encoder, and a cross-modality encoder. It is pretrained via a combination of masked language modeling, visual-language text alignment, ROI-feature regression, masked visual-attribute modeling, masked visual-object modeling, and visual-question answering objectives. It has achieved state-of-the-art results on the VQA and GQA visual-question-answering datasets.
## Introduction

Recent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world 
problems are inherently multimodal - they involve several distinct forms of data, such as images and text. 
Visual-language models face the challenge of combining modalities so that they can open the door to a wide range of 
applications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text 
retrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, 
identify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well 
as in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, 
and assist in various domain-specific information retrieval applications.
Previous approaches preprocessed the audio to extract useful features from it. It is now more common to start audio and speech processing tasks by directly feeding the raw audio waveform to a feature encoder to extract an audio representation. This simplifies the preprocessing step and allows the model to learn the most essential features.

### Audio classification

Audio classification is a task that labels audio data from a predefined set of classes. It is a broad category with many specific applications, some of which include:

* acoustic scene classification: label audio with a scene label (""office"", ""beach"", ""stadium"")
* acoustic event detection: label audio with a sound event label (""car horn"", ""whale calling"", ""glass breaking"")
* tagging: label audio containing multiple sounds (birdsongs, speaker identification in a meeting)
* music classification: label music with a genre label (""metal"", ""hip-hop"", ""country"")

```py
>>> from transformers import pipeline
```

```python out
""""""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?
```

## Zero-shot VQA

The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach 
VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training 
paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). 
This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. 

Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a 
GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: 

```py
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
The authors also use the Perceiver for multimodal autoencoding. The goal of multimodal autoencoding is to learn a model that can accurately reconstruct multimodal inputs in the presence of a bottleneck induced by an architecture. The authors train the model on the [Kinetics-700 dataset](https://deepmind.com/research/open-source/kinetics), in which each example consists of a sequence of images (i.e. frames), audio and a class label (one of 700 possible labels). This model is also implemented in HuggingFace Transformers, and available as `PerceiverForMultimodalAutoencoding`. For brevity, I will omit the code of defining this model, but important to note is that it uses `PerceiverMultimodalPreprocessor` to prepare the `inputs` for the model. This preprocessor will first use the respective preprocessor for each modality (image, audio, label) separately. Suppose one has a video of 16 frames of resolution 224x224 and 30,720 audio samples, then the modalities are preprocessed as follows:
| Audio Classification           | Transformers          | ‚úÖ                      |
| Object Detection               | Transformers          | ‚úÖ                      |
| Image Segmentation             | Transformers          | ‚úÖ                      |
| Table Question Answering       | Transformers          | ‚úÖ                      |
| Conversational                 | Transformers          | ‚úÖ                      |
| Custom                         | Custom                | ‚úÖ                      |
| Visual Question Answering      | Transformers          | ‚ùå                      |
| Zero Shot Image Classification | Transformers          | ‚ùå                      |
```

## Multimodal pipeline

The [`pipeline`] supports more than one modality. For example, a visual question answering (VQA) task combines text and image. Feel free to use any image link you like and a question you want to ask about the image. The image can be a URL or a local path to the image.

For example, if you use this [invoice image](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png):

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model=""impira/layoutlm-document-qa"")
>>> vqa(
...     image=""https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png"",
...     question=""What is the invoice number?"",
... )
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]
Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents
can vary as the APIs or underlying models are prone to change.

</Tip>

Transformers version v4.29.0, building on the concept of *tools* and *agents*. You can play with in
[this colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj).

In short, it provides a natural language API on top of transformers: we define a set of curated tools and design an 
agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools, 
but we'll show you how the system can be extended easily to use any tool developed by the community.

Let's start with a few examples of what can be achieved with this new API. It is particularly powerful when it comes 
to multimodal tasks, so let's take it for a spin to generate images and read text out loud.

```py
agent.run(""Caption the following image"", image=image)
-->

# How ü§ó Transformers solve tasks

In [What ü§ó Transformers can do](task_summary), you learned about natural language processing (NLP), speech and audio, computer vision tasks, and some important applications of them. This page will look closely at how models solve these tasks and explain what's happening under the hood. There are many ways to solve a given task, some models may implement certain techniques or even approach the task from a new angle, but for Transformer models, the general idea is the same. Owing to its flexible architecture, most models are a variant of an encoder, decoder, or encoder-decoder structure. In addition to Transformer models, our library also has several convolutional neural networks (CNNs), which are still used today for computer vision tasks. We'll also explain how a modern CNN works.

To explain how tasks are solved, we'll walk through what goes on inside the model to output useful predictions.
This page provides an overview of the different speech and audio, computer vision, and NLP tasks that can be solved with the ü§ó Transformers library in just three lines of code!

## Audio

Audio and speech processing tasks are a little different from the other modalities mainly because audio as an input is a continuous signal. Unlike text, a raw audio waveform can't be neatly split into discrete chunks the way a sentence can be divided into words. To get around this, the raw audio signal is typically sampled at regular intervals. If you take more samples within an interval, the sampling rate is higher, and the audio more closely resembles the original audio source.
## Why should I use transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding & generation, computer vision, and audio tasks.
    - Low barrier to entry for educators and practitioners.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.

1. Lower compute costs, smaller carbon footprint:
    - Researchers can share trained models instead of always retraining.
    - Practitioners can reduce compute time and production costs.
    - Dozens of architectures with over 60,000 pretrained models across all modalities.

1. Choose the right framework for every part of a model's lifetime:
    - Train state-of-the-art models in 3 lines of code.
    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.
    - Seamlessly pick the right framework for training, evaluation, and production.
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# What ü§ó Transformers can do

ü§ó Transformers is a library of pretrained state-of-the-art models for natural language processing (NLP), computer vision, and audio and speech processing tasks. Not only does the library contain Transformer models, but it also has non-Transformer models like modern convolutional networks for computer vision tasks. If you look at some of the most popular consumer products today, like smartphones, apps, and televisions, odds are that some kind of deep learning technology is behind it. Want to remove a background object from a picture taken by your smartphone? This is an example of a panoptic segmentation task (don't worry if you don't know what this means yet, we'll describe it in the following sections!).

Here is the question: How can Transformers be applied to multimodal tasks such as visual question answering and audio classification?
Here is answer 1:By using prebuilt pipelines without adaptation for multimodal tasks
Here is answer 2: Through multimodal integration with Hugging Face APIs and task-specific models
Here is answer3: By focusing only on text-based tasks
Here is answer 4: Using standalone frameworks without leveraging multimodal flexibility",Answer : By using prebuilt pipelines without adaptation
"What is the best approach to adapt models across PyTorch, TensorFlow, and JAX?","[(17414, 'c6d055b8-3841-4b16-9578-69e160b961ba'), (17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (17417, '6ef43f68-8306-418b-b698-6d8f7cc6ab7d'), (27146, 'cbb71c05-7152-43db-b473-37078bba89e2'), (25612, '01868de2-601f-4b56-9680-7fff3bd713aa'), (18957, '193a51f9-a0f7-47e2-a2d4-3cd5a9594dd6'), (17934, 'a346e6f1-8dc6-4546-85c3-dac1062bf6ff'), (3600, 'a5251b45-4980-4841-a1da-d832c0aa4e34'), (17426, '5faf29be-8c8f-40c3-b527-f840fdfa1f9c'), (17427, '30ad9bbf-82b5-4e3f-a73b-b07045263764'), (17429, '393ddd8a-1619-4d0d-afa0-f1d8f5566ed5'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (17439, '220bb0f1-e5a5-4c21-ba59-845839d83ad3'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (30760, '4eb04640-0bf5-4f77-820a-caa4aa28ce24'), (30761, '683724e2-567a-46f0-9254-202ca2f88a96'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (19497, 'b7b782f6-dc1c-4017-8645-77f14194b95c'), (7216, 'b5472bb0-d98a-42de-8982-9dc298abcd5e'), (29236, 'c8256ce9-e984-4533-a82f-3f08a2d65e83'), (9282, '91edce5b-dc46-47e2-85e4-33cf1d17dab7'), (17994, '34680928-da37-47ee-90c9-5404760066a2'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (5768, '6cc58ea0-db3c-4c3e-a419-0131c79cf6c3'), (31377, 'de68661c-8134-474b-a6ba-4c51686c8d6b'), (19612, 'b15310a0-0755-4662-b35d-093f3683e01a'), (1181, '2e574621-b2e7-4ffb-ad69-7aa65d1e16fb'), (1191, 'ece7abdd-1376-495b-8daa-d664f2a27dae'), (30891, 'ce0c26e5-9ecb-4d1c-ac24-560c5974dda4'), (24754, '5671e7e5-7069-4f2f-a3cf-9a06fa6383f2'), (1213, 'bec86d0f-9e27-4271-bd19-fa2a88278612'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (30916, '83870699-d55d-4da0-9257-d46016695c8c'), (23749, 'ebbc939a-6402-4c02-9891-1531598d8076'), (23750, 'c2856f93-0376-48a3-8544-fd75e08e618a'), (21194, '31079bba-cf47-4457-a079-280a5ea0fa59'), (1227, '96984733-267c-43b9-8d96-8ba7a3af1838'), (22226, '66bae662-348e-4bb7-ba75-056a15252a52'), (14035, 'c1939c78-2288-4ec5-9074-a8606c2ef8f7'), (25302, 'bcf9a9f1-7a6b-4a62-87f7-68506603417d'), (727, '1a71cdae-fde1-416d-b879-fd79101fe6df'), (17113, '9e9823ef-a812-447f-ac18-6b4fde474004'), (13534, '59bc5449-0ef9-459c-aa49-34f508ae124a'), (6370, '5d4d5a4e-c039-411b-b725-afbb79ba51aa'), (6371, '4d927d59-5f00-42ca-a1e6-802916ff0aeb'), (7917, '766d65a2-485e-4909-89b4-afc2cdd40c54'), (20206, 'c6660329-8004-43d6-a83b-21c6c16c7c34'), (5875, '867d8aee-620e-43be-b8d9-287612f7a6cb'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (22772, 'd0b704d9-ea88-403d-8797-758ba929ebd4'), (17653, 'a8e94114-6620-4796-9731-d48f51986131'), (30455, '30b49335-6092-4e0b-876d-b396ad058840'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (7423, '0e0622f0-8281-48ce-b07c-361f91ca8a9e'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (28429, '9c6fb59e-1d9f-48bd-be33-bd99c86d369d'), (28430, '432626a7-db6d-4b84-8276-b46a0483c396'), (29460, 'f654c347-e7a6-4470-85a0-5dfbbd1cfa56'), (21785, '0f42515e-4921-41d6-a8f2-18c62768982e'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (3358, '8564a156-b378-477d-87d0-31501018c920'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (16168, '8720f9c5-bd5a-4961-b1b7-7dd1d69cb57d'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (8492, '37b69803-2f18-4f43-850e-e3c81aa1cb27'), (16176, 'a7578109-e271-40f8-aa02-f89b5dddab55'), (6963, '999901f5-1f09-4c48-81c5-c208d0036338'), (16184, '08e02bad-54da-4851-82ae-02218bac7497'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (16191, '9fbef11c-ebf4-4a39-99e1-dddb5fd7c23f'), (16193, 'dc1889b7-9c05-4074-ac23-4bc47fe1125c'), (4418, '9410b13d-3f78-4fa7-aeb6-8361c7619510'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (16201, '74a34dcf-6a1b-4918-a45a-b5f73a7e5d91'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (9051, '09b3a3e8-2734-459b-9dcf-d885565ee6dd'), (352, '756c346a-bd77-440b-b6a9-1fabbf0c457d'), (353, 'aeabca25-387b-4b0c-bd33-e034666b36e9'), (21349, 'c9e6dbb0-3689-456f-b44d-2e633cd150ad'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (13176, 'a57502e1-52a5-41dc-aebb-b7cb7f00763d'), (22394, 'b66b7946-772b-43b6-8e7d-b5388eaf7cf6'), (7547, '836d3b3f-ca29-4139-a241-bd2400998d51'), (14727, '680f190f-eca8-4942-bebc-9212cd7caa87'), (15762, '4e4f666b-dc03-4e9c-8dfe-2ccbb5a5c56b'), (11666, 'c66a5c93-defd-4c55-889b-7d595384e150'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (6561, 'ee35c81c-c680-4dfc-852b-7c11aae2a074'), (2469, '24b5a6c3-9526-4bd4-b85f-cf7733f0a28b'), (20906, '4c0585c7-5b68-4b5f-820b-740cca4e0187'), (30648, '6aa08f8a-dd96-49a3-8e65-360c5e8fd924'), (20922, 'ff6bc297-0637-4274-9753-aa484a8303ad'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (19907, '4a8a0729-332c-401f-b9cf-aedad05cb3f3'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (9163, 'd7206e44-29b4-499b-92c4-0008be204752'), (3026, 'd16d30d5-b4b9-4fe2-9108-038403019bd9'), (21972, '85cbe3b6-6131-4a4a-9548-070d1c3b8c2d'), (21973, '7d6045ed-e6e9-40c3-88be-46c6b780d3eb'), (13280, '2c3ad26d-8418-4d01-a881-e5dd6be54e3f'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (31721, '0d565372-c33c-4322-b1e2-cd0ad577cbf0'), (27116, 'ee44caad-d368-4c27-b4ad-92f672f5e3da'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31738, '7d8861cd-9646-4d02-9213-734c5cbe5597'), (7163, '1f39bf74-f944-44bc-ac1d-a46df327488e'), (7165, '962f5c10-b98a-4400-9f01-67971ed8db89')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: There is a tendency among PyTorch engineers (picture me staring darkly across the open-plan office here) to see this as a problem to be overcome; their goal is to figure out how to make TensorFlow get out of their way so they can use the low-level training and data-loading code they‚Äôre used to. This is entirely the wrong way to approach TensorFlow! Keras is a great high-level API. If you push it out of the way in any project bigger than a couple of modules you‚Äôll end up reproducing most of its functionality yourself when you realize you need it.

As refined, respected and highly attractive TensorFlow engineers, we want to use the incredible power and flexibility of cutting-edge models, but we want to handle them with the tools and API we‚Äôre familiar with. This blogpost will be about the choices we make at Hugging Face to enable that, and what to expect from the framework as a TensorFlow programmer.

### Interlude: 30 Seconds to ü§ó
```

Now the `FlaxMLPModel` will have a similar interface as PyTorch or Tensorflow models and allows us to attach loaded or randomly initialized weights to the model instance.
To learn more about each of the supported schemes, please have a look at one of the resources shared below. Please also have a look at the appropriate sections of the documentation.

Note also that the details shared below are only valid for `PyTorch` models, this is currently out of scope for Tensorflow and Flax/JAX models.

## Table of contents

- [Resources](#resources)
- [Comparing bitsandbytes and auto-gptq](#Comparing-bitsandbytes-and-auto-gptq)
- [Diving into speed benchmarks](#Diving-into-speed-benchmarks)
- [Conclusion and final words](#conclusion-and-final-words)
- [Acknowledgements](#acknowledgements)

## Resources
- Don't reinvent the wheel! More often than not, there are at least two reference implementations you should check: the
PyTorch equivalent of the model you are implementing and other TensorFlow models for the same class of problems.
- Great model implementations survive the test of time. This doesn't happen because the code is pretty, but rather
because the code is clear, easy to debug and build upon. If you make the life of the maintainers easy with your
TensorFlow implementation, by replicating the same patterns as in other TensorFlow models and minimizing the mismatch
to the PyTorch implementation, you ensure your contribution will be long lived.
- Ask for help when you're stuck! The ü§ó Transformers team is here to help, and we've probably found solutions to the same
problems you're facing.
In the remainder of this guide, you will learn what's needed to add a new TensorFlow model architecture, the
procedure to convert PyTorch into TensorFlow model weights, and how to efficiently debug mismatches across ML
frameworks. Let's get started!

<Tip>

Are you unsure whether the model you wish to use already has a corresponding TensorFlow architecture?

&nbsp;

Check the `model_type` field of the `config.json` of your model of choice
([example](https://huggingface.co/bert-base-uncased/blob/main/config.json#L14)). If the corresponding model folder in
ü§ó Transformers has a file whose name starts with ""modeling_tf"", it means that it has a corresponding TensorFlow
architecture ([example](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert)).

</Tip>


## Step-by-step guide to add TensorFlow model architecture code
Sadly, there is no prescription to convert a PyTorch model into TensorFlow. You can, however, follow our selection of
tips to make the process as smooth as possible:
- Prepend `TF` to the name of all classes (e.g. `BrandNewBert` becomes `TFBrandNewBert`).
- Most PyTorch operations have a direct TensorFlow replacement. For example, `torch.nn.Linear` corresponds to
  `tf.keras.layers.Dense`, `torch.nn.Dropout` corresponds to `tf.keras.layers.Dropout`, etc. If you're not sure
  about a specific operation, you can use the [TensorFlow documentation](https://www.tensorflow.org/api_docs/python/tf)
  or the [PyTorch documentation](https://pytorch.org/docs/stable/).
- Look for patterns in the ü§ó Transformers codebase. If you come across a certain operation that doesn't have a direct
   replacement, the odds are that someone else already had the same problem.
- By default, keep the same variable names and structure as in PyTorch. This will make it easier to debug, track
## Using ONNX/TRT or other compiled approaches
  - They are supposed to handle most of the optimization work
  - Con, Usually parallelism needs to be handled manually.

Results:
  - Turned out that to be able to trace/jit/export stuff we needed to 
  rework part of the PyTorch, so it easily fused with the pure PyTorch approach
  And overall we figured out that we could have most of the optimizations we desired
  by staying within PyTorch world, enabling us to keep flexibility without
  having to make too much coding effort.
  Another thing to note, since we're running on GPU and text-generation has many
  forward passes going on, we need the tensors to stay on the GPU, and it is 
  sometimes hard to send your tensors to some lib, be given back the result, perform
  the logits computation (like argmax or sampling) and feed it back again.
  Putting the loop within the external lib means losing flexibility just like
  Jax, so it was not envisioned in our use case.
# Overview

ü§ó Optimum handles the export of TensorFlow models to TFLite in the `exporters.tflite` module. In addition, models hosted on the Hugging Face Hub with PyTorch weights but having a TensorFlow implementation will also be supported in the export thanks to Transformers' [TFPreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained) auto-conversion to TensorFlow.

The TFLite export support provides classes, functions and a command line interface to export a model easily.

Supported architectures:

- Albert
- BERT
- Camembert
- ConvBert
- Deberta
- Deberta V2
- DistilBert
- Electra
- Flaubert
- MobileBert
- MPNet
- ResNet
- Roberta
- RoFormer
- XLM
- XLMRoberta
# Export functions

You can export models to ONNX from two frameworks in ü§ó Optimum: PyTorch and TensorFlow. There is an export function for each of these frameworks, [`~optimum.exporters.onnx.convert.export_pytorch`] and [`~optimum.exporters.onnx.convert.export_tensorflow`], but the recommended way of using those is via the main export function [`~optimum.exporters.main_export`], which will take care of using the proper exporting function according to the available framework, check that the exported model is valid, and provide extended options to run optimizations on the exported model.

## Main functions

[[autodoc]] exporters.onnx.main_export

[[autodoc]] exporters.onnx.convert.export

[[autodoc]] exporters.onnx.convert.export_pytorch

[[autodoc]] exporters.onnx.convert.export_tensorflow


## Utility functions

[[autodoc]] exporters.onnx.convert.check_dummy_inputs_are_allowed

[[autodoc]] exporters.onnx.convert.validate_model_outputs
When translating, refer to the guide at [./TRANSLATING.md](https://github.com/huggingface/transformers/blob/main/docs/TRANSLATING.md).


### Adding a new model

When adding a new model:

- Create a file `xxx.md` or under `./source/model_doc` (don't hesitate to copy an existing file as template).
- Link that file in `./source/_toctree.yml`.
- Write a short overview of the model:
    - Overview with paper & authors
    - Paper abstract
    - Tips and tricks and how to use it best
- Add the classes that should be linked in the model. This generally includes the configuration, the tokenizer, and
  every model of that class (the base model, alongside models with additional heads), both in PyTorch and TensorFlow.
  The order is generally:
    - Configuration
    - Tokenizer
    - PyTorch base model
    - PyTorch head models
    - TensorFlow base model
    - TensorFlow head models
    - Flax base model
    - Flax head models
```
</pt>
<tf>
Specify `from_pt=True` to convert a checkpoint from PyTorch to TensorFlow:

```py
>>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(""path/to/awesome-name-you-picked"", from_pt=True)
```

Then you can save your new TensorFlow model with its new checkpoint:

```py
>>> tf_model.save_pretrained(""path/to/awesome-name-you-picked"")
```
</tf>
<jax>
If a model is available in Flax, you can also convert a checkpoint from PyTorch to Flax:

```py
>>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(
...     ""path/to/awesome-name-you-picked"", from_pt=True
... )
- **Model classes** can be PyTorch models ([torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)), Keras models ([tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)) or JAX/Flax models ([flax.linen.Module](https://flax.readthedocs.io/en/latest/api_reference/flax.linen/module.html)) that work with the pretrained weights provided in the library.
- **Configuration classes** store the hyperparameters required to build a model (such as the number of layers and hidden size). You don't always need to instantiate these yourself. In particular, if you are using a pretrained model without any modification, creating the model will automatically take care of instantiating the configuration (which is part of the model).
## Adding TensorFlow weights to ü§ó Hub

Assuming that the TensorFlow model architecture is available in ü§ó Transformers, converting PyTorch weights into
TensorFlow weights is a breeze!

Here's how to do it:
1. Make sure you are logged into your Hugging Face account in your terminal. You can log in using the command
   `huggingface-cli login` (you can find your access tokens [here](https://huggingface.co/settings/tokens))
2. Run `transformers-cli pt-to-tf --model-name foo/bar`, where `foo/bar` is the name of the model repository
   containing the PyTorch weights you want to convert
3. Tag `@joaogante` and `@Rocketknight1` in the ü§ó Hub PR the command above has just created

That's it! üéâ


## Debugging mismatches across ML frameworks üêõ
- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.
- **Flexibility**: At their core, all models are simple PyTorch `nn.Module` or TensorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) frameworks.
- **Simplicity**: Hardly any abstractions are made across the library. The ""All in one file"" is a core concept: a model's forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.

This last feature makes ü§ó Transformers quite different from other ML libraries. The models are not built on modules 
that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.
- Model versioning

- Traffic splitting between different versions of a model

- Rate limiting

- Model monitoring and logging

- Support for online and batch predictions

For TensorFlow models, it offers various off-the-shelf utilities, which
you‚Äôll get to in this post. But it also has similar support for other
frameworks like
[<u>PyTorch</u>](https://cloud.google.com/blog/topics/developers-practitioners/pytorch-google-cloud-how-deploy-pytorch-models-vertex-ai)
and [<u>scikit-learn</u>](https://codelabs.developers.google.com/vertex-cpr-sklearn).

To use Vertex AI, you‚Äôll need a [<u>billing-enabled Google Cloud
Platform (GCP) project</u>](https://cloud.google.com/billing/docs/how-to/modify-project)
and the following services enabled:

- Vertex AI

- Cloud Storage

# Revisiting the Serving Model
By exposing a graph with standardized operators and data types, ONNX makes it easy to
switch between frameworks. For example, a model trained in PyTorch can be exported to
ONNX format and then imported in TensorFlow (and vice versa).

Once exported to ONNX format, a model can be:
- optimized for inference via techniques such as [graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) and [quantization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization). 
- run with ONNX Runtime via [`ORTModelForXXX` classes](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort),
which follow the same `AutoModel` API as the one you are used to in ü§ó Transformers.
- run with [optimized inference pipelines](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines),
which has the same API as the [`pipeline`] function in ü§ó Transformers.
n this video, we're going to go over the HuggingFace Model Hub navigation. This is the huggingface.co landing page. To access the model hub, click on the ""Models"" tab in the upper right corner. You should be facing this web interface, which can be split into several parts. On the left, you'll find categories, which you can use to tailor your model search. The first category is the ""Tasks"". Models on the hub may be used for a wide variety of tasks. These include natural language processing tasks, such as question answering or text classification, but it isn't only limited to NLP. Other tasks from other fields are also available, such as image classification for computer vision, or automatic speech recognition for speech. The second category is the ""libraries"". Models on the hub usually share one of three backbones: PyTorch, TensorFlow, or JAX. However, other backbones, such as rust or ONNX also exist. Finally, this tab can also be used to specify from which high-level framework the
That's it! üéâ


## Debugging mismatches across ML frameworks üêõ

At some point, when adding a new architecture or when creating TensorFlow weights for an existing architecture, you
might come across errors complaining about mismatches between PyTorch and TensorFlow. You might even decide to open the
model architecture code for the two frameworks, and find that they look identical. What's going on? ü§î
```

## Convert a model for all frameworks

To ensure your model can be used by someone working with a different framework, we recommend you convert and upload your model with both PyTorch and TensorFlow checkpoints. While users are still able to load your model from a different framework if you skip this step, it will be slower because ü§ó Transformers will need to convert the checkpoint on-the-fly.

Converting a checkpoint for another framework is easy. Make sure you have PyTorch and TensorFlow installed (see [here](installation) for installation instructions), and then find the specific model for your task in the other framework. 

<frameworkcontent>
<pt>
Specify `from_tf=True` to convert a checkpoint from TensorFlow to PyTorch:

```py
>>> pt_model = DistilBertForSequenceClassification.from_pretrained(""path/to/awesome-name-you-picked"", from_tf=True)
>>> pt_model.save_pretrained(""path/to/awesome-name-you-picked"")
- Find the best way of debugging intermediate results. Is the original repository written in PyTorch? Then you should
  probably take the time to write a longer script that decomposes the original model into smaller sub-components to
  retrieve intermediate values. Is the original repository written in Tensorflow 1? Then you might have to rely on
  TensorFlow print operations like [tf.print](https://www.tensorflow.org/api_docs/python/tf/print) to output
  intermediate values. Is the original repository written in Jax? Then make sure that the model is **not jitted** when
  running the forward pass, *e.g.* check-out [this link](https://github.com/google/jax/issues/196).
- Use the smallest pretrained checkpoint you can find. The smaller the checkpoint, the faster your debug cycle
  becomes. It is not efficient if your pretrained model is so big that your forward pass takes more than 10 seconds.

Here is the question: What is the best approach to adapt models across PyTorch, TensorFlow, and JAX?
Here is answer 1:Focus on one framework for all tasks
Here is answer 2: Use Hugging Face‚Äôs framework-switching APIs and resolve compatibility challenges
Here is answer3: Rely on framework-specific implementations without interoperability
Here is answer 4: Skip adapting models to save development time","What do I think I'm seeing?

Based solely upon my analysis conducted till now within my knowledge systems analytics suite developed through various methods I predict 

The Best Answer Choice Would Be : 

Answer2 : Use Huggling Face‚Äôs framework-switching APIs"
How do Hugging Face pipelines simplify multimodal workflow integration?,"[(25090, '1c9734dc-9c4c-4216-b57d-1ba5098bb275'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (18489, 'c196cc2b-dfd6-4797-a458-54ad07efda99'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8269, '63e383a9-dbfa-4c9f-a286-0668a682d313'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (23159, '8df7603d-0d12-428d-8ffd-3b4c55cd0c5f'), (26745, '509a2418-a582-44fa-9c23-427ca2329092'), (31354, '50289d67-f345-42d3-bb11-7b4a1e8c5530'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (10923, '48a8b0e4-0e9b-4f01-b296-83212b9301c1'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (24752, '3798bf75-5a2c-4b7a-9800-c75a034bfae1'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (9410, '9b7d8254-2d89-4787-a2de-21068114b4d5'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (21188, 'e53bd9c2-9844-411f-810d-5e35fa824c01'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (20167, '2bb84049-c72c-4886-948b-e9b8893655fb'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (30924, '44434692-dd31-4fd1-b184-cb00e1c95a5d'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (28896, '202c45a6-2f26-4d2c-acbb-40c10ed7b7b4'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (12018, 'c5adc06a-cf65-4575-8f03-9ad3b915d129'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (2811, '7a203781-c1a2-43a6-ad0b-ce893ad7bd5b'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (2819, 'e1ff2eec-2324-40c3-af1e-b66cfacaa072'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (5933, 'e8977096-c2bb-4f13-9f6e-6bfd1dbeb9bf'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (23379, '463504f5-df8f-4173-b302-5da02909ff0f'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (14725, 'f93b2fc4-f0b6-42bb-9b39-5f3d69f3047b'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (6047, '162b5640-f06a-4c27-97b1-d3c317bbb669'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (17828, 'e0e24538-49e6-418a-8fb8-b10e6a8d00f1'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (1461, '8174cb61-99c6-4cd8-afc1-6ae0b348ac7e'), (31672, '94b34b38-3568-488b-94eb-579ac5ccf43e'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (6091, 'ec43fc42-f705-45a0-8110-0fb4ac955090'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (30710, 'd0129c9b-b6bd-430e-b548-b9a7044ab45e'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
In combination with the data issues widget, the Python API offers a great way to integrate the results of existing scripts (e.g. data quality checks or model monitoring) into a scalable data inspection workflow.

## Using Spotlight on the Hugging Face hub

You can use Spotlight directly on your local NLP, audio, CV or multimodal dataset. If you would like to showcase your dataset or model results on the Hugging Face hub, you can use Hugging Face spaces to launch a Spotlight visualization for it.

We have already prepared [example spaces](https://huggingface.co/renumics) for many popular NLP, audio and CV datasets on the hub. You can simply duplicate one of these spaces and specify your dataset in the `HF_DATASET` variable.

You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.
```

## Multimodal pipeline

The [`pipeline`] supports more than one modality. For example, a visual question answering (VQA) task combines text and image. Feel free to use any image link you like and a question you want to ask about the image. The image can be a URL or a local path to the image.

For example, if you use this [invoice image](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png):

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model=""impira/layoutlm-document-qa"")
>>> vqa(
...     image=""https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png"",
...     question=""What is the invoice number?"",
... )
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]
Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.

Hugging Face Infinity consists of 2 main services:
* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.
* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.

The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.

<br>
<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Product overview"" src=""assets/46_infinity_cpu_performance/overview.png""></medium-zoom>
  <figcaption>Figure 1. Infinity Overview</figcaption>
</figure>
<br>
### What was the impact of collaborating with the Hugging Face team?
The most important thing about this collaboration was making a tremendous impact on our business's scalability and our operations team's workflow. We launched our production NLP pipeline several weeks ago. Since then, we've consistently seen almost 20% of incoming messages get automatically handled by our new system. These are messages that would've created a ticket for our patient operations team before. So we've reduced a lot of low-value work from our team.
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
<Tip>
‚ö†Ô∏è The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! <a href=""https://huggingface.co/join"">Create a huggingface.co</a> account to benefit from all available features!
</Tip>

Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems.

## Working with pipelines[[working-with-pipelines]]

<Youtube id=""tiZFewofSLM"" />

The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:

```python
from transformers import pipeline

classifier = pipeline(""sentiment-analysis"")
classifier(""I've been waiting for a HuggingFace course my whole life."")
```

As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png"" alt=""The task selector on the web interface."" width=""80%""/>
</div>

You can also instantiate the checkpoint using the model architecture directly:
### multimodal

A task that combines texts with another kind of inputs (for instance images).

## N

### Natural language generation (NLG)

All tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/), translation).

### Natural language processing (NLP)

A generic way to say ""deal with texts"".

### Natural language understanding (NLU)

All tasks related to understanding what is in a text (for instance classifying the
whole text, individual words).

## P

### pipeline

A pipeline in ü§ó Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.

For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).

### PipelineParallel (PP)
In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for prototyping, comparing it against AWS Sagemaker and Hugging Face‚Äôs new model hosting Inference API. Given that we use GCP for data storage and Google Vertex Pipelines for model training, exporting models to AWS Sagemaker was clunky and bug prone. Thankfully, the set up for Hugging Face was quick and easy, and it was able to handle a small portion of traffic within a week. Hugging Face simply worked out of the gate, and this reduced friction led us to proceed down this path.
All the HF converted models are available on Hugging Face Hub, in the [`RWKV` organization](https://huggingface.co/RWKV).

## ü§ó Transformers integration

The architecture has been added to the `transformers` library thanks to [this Pull Request](https://github.com/huggingface/transformers/pull/22797). As of the time of writing, you can use it by installing `transformers` from source, or by using the `main` branch of the library. The architecture is tightly integrated with the library, and you can use it as you would any other architecture.

Let us walk through some examples below.

### Text Generation Example

To generate text given an input prompt you can use `pipeline` to generate text:

```python
from transformers import pipeline

model_id = ""RWKV/rwkv-4-169m-pile""
Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!

### Finding models

Over 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.


### Widgets
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Pipelines for inference

The [`pipeline`] makes it simple to use any model from the [Hub](https://huggingface.co/models) for inference on any language, computer vision, speech, and multimodal tasks. Even if you don't have experience with a specific modality or aren't familiar with the underlying code behind the models, you can still use them for inference with the [`pipeline`]! This tutorial will teach you to:

* Use a [`pipeline`] for inference.
* Use a specific tokenizer or model.
* Use a [`pipeline`] for audio, vision, and multimodal tasks.

<Tip>

Take a look at the [`pipeline`] documentation for a complete list of supported tasks and available parameters.

</Tip>

## Pipeline usage
Here is a list of things we‚Äôll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#ü§ó-ü§ù-timm)
- [Diffusers](#üß®-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time üëÅ

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.

The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. 

If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) 


## Resources
## Adding new tasks to the Hub

### Using Hugging Face transformers library

If your model is a `transformers`-based model, there is a 1:1 mapping between the Inference API task and a `pipeline` class. Here are some example PRs from the `transformers` library:
* [Adding ImageClassificationPipeline](https://github.com/huggingface/transformers/pull/11598)
* [Adding AudioClassificationPipeline](https://github.com/huggingface/transformers/pull/13342)

Once the pipeline is submitted and deployed, you should be able to use the Inference API for your model.

### Using Community Inference API with a supported library

The Hub also supports over 10 open-source libraries in the [Community Inference API](https://github.com/huggingface/api-inference-community).
# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft‚Äôs ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face‚Äôs [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples üìÑ
To learn how to access and use the new Hugging Face DLCs with the Amazon SageMaker Python SDK, check out the guides and resources below.

> _On July 8th, 2021 we extended the Amazon SageMaker integration to add easy deployment and inference of Transformers models. If you want to learn how you can [deploy Hugging Face models easily with Amazon SageMaker](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker) take a look at the [new blog post](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker) and the [documentation](https://huggingface.co/docs/sagemaker/inference)._

---

## **Features & Benefits üî•**

## One Command is All you Need

With the new Hugging Face Deep Learning Containers available in Amazon SageMaker, training cutting-edge Transformers-based NLP models has never been simpler. There are variants specially optimized for TensorFlow and PyTorch, for single-GPU, single-node multi-GPU and multi-node clusters.
To target these needs, Hugging Face built two open-sourced libraries: __Accelerate__ and __Optimum__. While [ü§ó Accelerate](https://huggingface.co/docs/accelerate/index) focuses on out-of-the-box distributed training, [ü§ó Optimum](https://huggingface.co/docs/optimum/index), as an extension of transformers, accelerates model training and inference by leveraging the maximum efficiency of users‚Äô targeted hardware. Optimum integrated machine learning accelerators like ONNX Runtime and specialized hardware like [Intel's Habana Gaudi](https://huggingface.co/blog/habana-gaudi-2-benchmark), so users can benefit from considerable speedup in both training and inference. Besides, Optimum seamlessly integrates other Hugging Face‚Äôs tools while inheriting the same ease of use as Transformers. Developers can easily adapt their work to achieve lower latency with less computing power.

## ONNX Runtime Training

Here is the question: How do Hugging Face pipelines simplify multimodal workflow integration?
Here is answer 1:By automating preprocessing and task-specific outputs
Here is answer 2: By limiting tasks to text-only models
Here is answer3: By requiring extensive manual customization
Here is answer 4: By skipping multimodal processing for simplicity",Answer : By automating preprocessing
Which challenge is most significant when adapting pre-trained models to new tasks?,"[(30740, '7f7e5ebd-83dc-4a7d-932f-d4395c72dedf'), (13335, 'c0f8d078-7c4a-4cc3-8b01-d07cd236de8f'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (3612, '8fabfaf9-03d8-4ebd-94f3-8fed08ef98ea'), (19489, '43505cd9-a764-4a3f-8290-b2ce01a4ab4b'), (26674, 'f774fe22-eece-4e89-a2d6-d045f3827a9f'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (30781, 'ea211002-7d83-4822-87c9-010da149e4bb'), (30785, 'fe7b98b8-14ab-4e54-8af2-68f748159690'), (25162, '946c9ec3-717f-4307-aad1-37aac459e5a1'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (3155, '04ba28fe-312e-4cda-ad60-4405ce74708b'), (23636, '847215c3-b4a6-4e0a-8064-28fd730a92aa'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (10854, '3cae5d2f-8bf0-43cd-8609-1f7a2de7db58'), (4205, 'c62c8c1c-0bb7-4ad7-bfce-7cd1161ad7a7'), (21112, 'b2fe447e-14ff-4d20-a842-584b4543fc26'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (3213, 'f579844e-8020-4f0c-bd34-e052f2bd02d3'), (15001, '5e52bb45-1656-4310-8d72-06c79196c8f4'), (14005, 'b5d01951-0562-4feb-9801-f08cb589fc9a'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (696, 'b996a5f4-7c16-44f1-aa17-bcf65b15101a'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16577, '65b23733-c969-4f5c-815e-b4d8c66dc8a9'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (9416, '28859eb3-1961-4028-8c62-23786a3bd60d'), (16587, '52d49bb8-3621-4158-a18e-a558ef51fd74'), (10450, '3d06676d-85ad-4fa3-965c-e9d2ff2a64d7'), (17622, '18cebb0d-bc71-4646-91ee-08fdc8fb51a7'), (15583, '0bd03221-d721-44d6-b845-89e89513d7b5'), (22754, '80db5ee3-5191-4e4a-b27e-1131d8ba4b43'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (31463, '173a216a-4751-442c-968d-628b2b104933'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (5866, '4108faf4-0e48-4871-8e5b-31983d6c0129'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (5876, '6c565e09-ac75-4392-891f-2a1bbddd3dd9'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (23812, '98499772-c9a4-47d7-b198-bd18ff20d2d5'), (16135, '7571e8a8-5ff4-4358-9a88-0d57b9e70bc8'), (10508, '880f5c7d-e48d-48f5-9334-4850e1489620'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (10510, '65932cde-592e-47aa-b229-26bebb986d68'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (10520, 'f89c6a86-913f-459e-8824-34dcfc05ae6d'), (29468, '17fc845b-a10d-41de-978b-b91257cc0ada'), (4918, '1e0edbe0-5591-4783-a872-6c2b258e9fd3'), (24893, 'ba24282b-69bc-4162-bc20-24e64126aaec'), (1345, '64645c76-df63-4566-9ea6-fe57d419e168'), (22850, '660de6ac-c1ce-49bd-a434-4ddb5c3001b7'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (17741, '28a0fb96-9ba3-48c6-a7bc-158aa6526cb7'), (6991, '01e5526c-e621-495d-a431-92b903971367'), (22351, 'de4780f5-753f-4d0a-8175-6e44c6f76bb7'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (25428, 'd391c525-320b-49a3-83dc-e6a90c266db7'), (6997, '35d0066b-d1f2-45f1-a295-d792e0dda90d'), (8021, '2eb3eb4c-195e-4237-b854-70714e8cde11'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (8023, '1dec94c2-ffe1-4e8f-a0e8-d220743cc341'), (22877, '79c5fcb1-22aa-4ce1-be25-33a02d2b8e03'), (1897, '6aba3658-1f10-4052-b9bb-ce2999ebd330'), (1898, 'd0cd0bd7-ad19-4c35-a562-ddc9c9499a18'), (1899, 'f888ee25-af78-4996-8147-194908c2b4b0'), (1900, '9ac81dc8-63e8-4bb5-b1a2-66f999fdf9c9'), (7535, '112382bf-ba59-4a80-bce9-ee6ae2715df4'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (22397, '31d21656-3ce7-4da6-b134-33cbd4345049'), (22398, '96e2d562-285e-4313-9c28-22c485856055'), (22399, '8c31bbea-69a2-4f83-afb3-796762dba928'), (17798, '7ae25397-da2a-4397-9fa8-858e74f29d9b'), (23431, 'dc646a0c-4319-4bd5-9c0d-75cf35041403'), (23432, '16d96e6e-b934-4fd6-a87b-5fdf8025aaf4'), (23433, 'e89537dc-4f68-4b71-ad4c-f7dedd2a6faa'), (24966, '6738697c-4cdb-4c6c-a5a1-2bdd4a31c06e'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (11668, 'ed1ab411-0307-41ec-b3d1-0e9dcedf48d2'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (20907, '789c7a86-81f9-481b-a930-b5e65873ccf1'), (23473, 'f50b7d19-c417-40d3-a82a-2d35ca4cc1bb'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (31161, 'e6392f6d-29c2-4b5e-99d0-0af8c82a0670'), (13759, 'e3f7ce62-98c1-4212-90c1-fc43513f4104'), (453, '88e0bd45-3e3a-40d3-b950-ecce65a9fd14'), (8135, '1a925799-28df-4126-9d1a-9278d8861e75'), (2509, '53bab419-4140-4137-aa56-67f5880cf609'), (30161, '1346c908-cdd2-4afb-9f14-01eb9af69253'), (981, '34f4864e-d4b4-4f46-a7df-ca0aefbce760'), (11224, 'f888b7bb-7128-4802-8be1-00533c0c5908'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (24550, 'f8edec76-5c02-41b0-a9ee-c83e1d736419'), (25069, 'a400d317-c11c-42d5-bc72-ec9c08296b20'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (25071, '4555fe45-7cc8-438e-a7e1-ee81a5269a19'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (14326, '394d8e4b-4b37-40df-975b-71f00ac59dbc'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: -->

# PEFT configurations and models

The sheer size of today's large pretrained models - which commonly have billions of parameters - present a significant training challenge because they require more storage space and more computational power to crunch all those calculations. You'll need access to powerful GPUs or TPUs to train these large pretrained models which is expensive, not widely accessible to everyone, not environmentally friendly, and not very practical. PEFT methods address many of these challenges. There are several types of PEFT methods (soft prompting, matrix decomposition, adapters), but they all focus on the same thing, reduce the number of trainable parameters. This makes it more accessible to train and store large models on consumer hardware.
* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.
1. The biggest challenges come with the operationalization and deployment of ML-trained solutions in a manner in which human operations can be replaced with minimal consequences.  We‚Äôre seeing it now with fully self-driving automobiles.   It‚Äôs challenging to automate processes with little to no fear of jeopardizing humans or processes that humans rely on.  One of the most significant examples of this phenomenon that concerns me is ML and Bias.  It is a reality that ML models trained on data containing, even if unaware, prejudiced decision-making can reproduce said bias in operation.  Bias needs to be put front and center in the attempt to incorporate ML into engineering such that systemic racism isn‚Äôt propagated into future technological advances to then cause harm to disadvantaged populations.  ML systems trained on data emanating from biased processes are doomed to repeat them, mainly if those training the ML solutions aren‚Äôt acutely aware of all forms of data present in the process
Adapt a model to a new task

Many diffusion systems share the same components, allowing you to adapt a pretrained model for one task to an entirely different task.

This guide will show you how to adapt a pretrained text-to-image model for inpainting by initializing and modifying the architecture of a pretrained [`UNet2DConditionModel`].

## Configure UNet2DConditionModel parameters

A [`UNet2DConditionModel`] by default accepts 4 channels in the [input sample](https://huggingface.co/docs/diffusers/v0.16.0/en/api/models#diffusers.UNet2DConditionModel.in_channels). For example, load a pretrained text-to-image model like [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) and take a look at the number of `in_channels`:

```py
from diffusers import StableDiffusionPipeline

pipeline = StableDiffusionPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"", use_safetensors=True)
pipeline.unet.config[""in_channels""]
4
The capability of pre-trained language models to effectively transfer
*task-agnostic* knowledge to *task-specific* knowledge turned out to be
a great catalyst for NLU. Whereas engineers and researchers previously
had to train a language model from scratch, now publicly available
checkpoints of large pre-trained language models can be fine-tuned at a
fraction of the cost and time. This can save millions in industry and
allows for faster prototyping and better benchmarks in research.

Pre-trained language models have established a new level of performance
on NLU tasks and more and more research has been built upon leveraging
such pre-trained language models for improved NLU systems. However,
standalone BERT and GPT models have been less successful for
*sequence-to-sequence* tasks, *e.g.* *text-summarization*, *machine
translation*, *sentence-rephrasing*, etc.
```
Let's look at this in more detail. 

Calling the `from_pretrained` method on our model downloads and caches the weights for us. As the number of classes (for prediction) is dataset dependent, we pass the new `num_classes` as well as `ignore_mismatched_sizes` alongside the `model_checkpoint`. This makes sure a custom classification head is created, specific to our task, hence likely different from the original decoder head.

It is also possible to create a new randomly initialized model to train from scratch, either following the known parameters of a given checkpoint or by manually choosing them.

### Training or fine-tuning
To train our model simply, we will use a `Trainer`. To instantiate it, we will need to define the training configuration and the evaluation metric. The most important is the `TrainingArguments`, which is a class that contains all the attributes to customize the training. It requires a folder name, which will be used to save the checkpoints of the model.
Pretrained LLMs can also be specialized or adapted for a specific task after pretraining, particularly when the weights are openly released. They are then used as a starting point for use cases and applications through a process called **fine-tuning**. Fine-tuning involves applying additional training steps on the model on a different ‚Äìoften more specialized and smaller‚Äì dataset to optimize it for a specific application. Even though this step has a cost in terms of compute power needed, it is usually much less costly than training a model from scratch, both financially and environmentally. This is one reason high-quality open-source pretrained models are very interesting, as they can be freely used and built upon by the community even when the practitioners have only access to a limited computing budget. 

## üóùÔ∏è 2022, from a race for size to a race for data
What open models were available to the community before 2023?
### 11. Why is it easy to pretrain a language model on lots and lots of texts?

<Question
	choices={[
		{
			text: ""Because there are plenty of texts available on the internet"",
			explain: ""Although true, that doesn't really answer the question. Try again!""
		},
		{
			text: ""Because the pretraining objective does not require humans to label the data"",
			explain: ""That's correct, language modeling is a self-supervised problem."",
			correct: true
		},
		{
			text: ""Because the ü§ó Transformers library only requires a few lines of code to start the training"",
			explain: ""Although true, that doesn't really answer the question asked. Try another answer!""
		}
	]}
/>

### 12. What are the main challenges when preprocessing data for a question answering task?
### The critical breakthrough: Transfer learning

In machine learning, we call this concept of transferring prior knowledge to a new task ‚Äú**transfer learning**‚Äù. Getting this kind of transfer learning to work for DL was a major goal for the field around 2016. Things like pre-trained word vectors (which are very interesting, but outside the scope of this blogpost!) did exist by 2016 and allowed some knowledge to be transferred to new models, but this knowledge transfer was still relatively superficial, and models still needed large amounts of training data to work well.
For teams considering building their own pre-trained model, be aware this can involve millions of documents and can easily take several weeks to train a model. Building a pre-trained model requires significant effort and is not recommended for most data science teams. Instead, start with fine-tuning one, but ask yourself these questions first.

Do you want the model to handle the OCR? For example, [Donut](https://huggingface.co/docs/transformers/model_doc/donut) doesn't require the document to be OCRed and directly works on full-resolution images, so there is no need for OCR before modeling. However, depending on your problem setup, it may be simpler to get OCR separately.
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>
it's perfectly normal to see it adapted its suggestions like this. Notice how it keeps the same predictions as the pretrained model afterward. Even if the fine-tuned model adapts to the new dataset, it's not forgetting what it was pretrained on. This is another example on a translation task. On top we use a pretrained French/English model and at the bottom, the version we fine-tuned in chapter 7. The top model is pretrained on lots of texts, and leaves technical English terms like plugin and email unchanged in the translation (both are perfectly understood by French people). The dataset picked for the fine-tuning is a dataset of technical texts where special attention was picked to translate everything in French. As a result, the fine-tuned model picked that habit and translated both plugin and email.
hat is domain adaptation? When fine-tuning a pretrained model on a new dataset, the fine-tuned model we obtain will make predictions that are attuned to this new dataset. When the two models are trained with the same task, we can then compare their predictions on the same input. The predictions of the two models will be different, in a way that reflects the differences between the two datasets, a phenomenon we call domain adaptation. Let's look at an example with mask language modeling, by comparing the outputs of the pretrained distilBERT model with the version fine-tuned in chapter 7 of the course (linked below). The pretrained model makes generic predictions, whereas the fine-tuned model has its first two predictions linked to cinema. Since it was fine-tuned on a movie reviews dataset, it's perfectly normal to see it adapted its suggestions like this. Notice how it keeps the same predictions as the pretrained model afterward. Even if the fine-tuned model adapts to the new dataset,
Second, there is a lot of research on just a single component of a Machine Learning model. *E.g.*, research
teams investigate new forms of an attention mechanism that would apply to all existing pre-trained models as 
has been done in the [Rethinking Attention with Performers](https://arxiv.org/abs/2009.14794). How should 
we incorporate such research into the Transformers library? It is indeed problematic. Should we change 
all existing models? This would go against points 3. and 4. as written above. Should we add 100+ new modeling 
files each prefixed with `Performer...`? This seems absurd. In such a case there is sadly no good solution
and we opt for not integrating the paper into Transformers in this case. If the paper would have gotten 
much more traction and included strong pre-trained checkpoints, we would have probably added new modeling 
files of the most important models such as `modeling_performer_bert.py`
available.
The first pre-trained language models were based on recurrent neural
networks (RNN) as proposed [Dai et al.
(2015)](https://arxiv.org/pdf/1511.01432.pdf). *Dai et. al* showed that
pre-training an RNN-based model on unlabelled data and subsequently
fine-tuning \\({}^2\\) it on a specific task yields better results than
training a randomly initialized model directly on such a task. However,
it was only in 2018, when pre-trained language models become widely
accepted in NLP. [ELMO by Peters et
al.](https://arxiv.org/abs/1802.05365) and [ULMFit by Howard et
al.](https://arxiv.org/pdf/1801.06146.pdf) were the first pre-trained
language model to significantly improve the state-of-the-art on an array
of natural language understanding (NLU) tasks. Just a couple of months
later, OpenAI and Google published *transformer-based* pre-trained
language models, called [GPT by Radford et
### Downstream datasets 

Pre-trained vision-language models are often trained on various downstream tasks such as visual question-answering, text-guided object detection, text-guided image inpainting, multi-modal classification, and various stand-alone NLP and computer vision tasks.
```

Additional training tips:

- T5 models need a slightly higher learning rate than the default one set in the `Trainer` when using the AdamW
optimizer. Typically, 1e-4 and 3e-4 work well for most problems (classification, summarization, translation, question
answering, question generation). Note that T5 was pre-trained using the AdaFactor optimizer.

According to [this forum post](https://discuss.huggingface.co/t/t5-finetuning-tips/684), task prefixes matter when
(1) doing multi-task training (2) your task is similar or related to one of the supervised tasks used in T5's
pre-training mixture (see Appendix D of the [paper](https://arxiv.org/pdf/1910.10683.pdf) for the task prefixes
used).
### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.

Most likely, you will have to fine-tune a pretrained model for your own use case, but it is worth checking whether the hub already has suitable fine-tuned models. In this case, you might reach a higher performance by just continuing to fine-tune such a model on your dataset.
Alright, the next step now is to find a suitable pretrained model to be used for fine-tuning. This is actually more difficult than it seems given the large amount of pretrained and fine-tuned models that are on the [Hugging Face Hub](https://huggingface.co/models). The best option is usually to simply try out a variety of different models to see which one performs best.
We still haven't found the perfect way of comparing different model checkpoints to each other at Hugging Face, but we provide some resources that are worth looking into:

-   The [model summary](https://huggingface.co/docs/transformers/model_summary) gives a short overview of different model architectures.
-   A task-specific search on the Hugging Face Hub, *e.g.* [a search on text-classification models](https://huggingface.co/models), shows you the most downloaded checkpoints which is also an indication of how well those checkpoints perform.
This type of model develops a statistical understanding of the language it has been trained on, but it's not very useful for specific practical tasks. Because of this, the general pretrained model then goes through a process called *transfer learning*. During this process, the model is fine-tuned in a supervised way -- that is, using human-annotated labels -- on a given task.

An example of a task is predicting the next word in a sentence having read the *n* previous words. This is called *causal language modeling* because the output depends on the past and present inputs, but not the future ones.

Here is the question: Which challenge is most significant when adapting pre-trained models to new tasks?
Here is answer 1:The need for custom datasets in training
Here is answer 2: Switching between frameworks like PyTorch and TensorFlow
Here is answer3: Deploying models without training data
Here is answer 4: Choosing between existing model architectures",Answer3
How can multimodal Transformer models be evaluated for complex workflows?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (14910, '5cb12218-60e4-49e4-b76c-dd87fd9f469b'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (27229, '79b37341-3681-4528-9ab1-2ab8778b564d'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (18549, 'd84bf0de-2759-4c1e-9a94-cc27a2136a9b'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (13526, '594c0819-9996-42bd-8329-7cec367352a5'), (20185, '26583e89-c8f7-4107-a950-11734ddb8fbf'), (18140, '2b5de1ef-6517-4c54-b9b2-3d94550ca7a5'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (13558, '415ec308-3b10-4657-a9b2-e4f3aa21d028'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (7932, 'b99512aa-f8fc-45ad-ae42-7b543455a8d7'), (256, '154c2da9-4985-48e3-86b8-642c818c758e'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (29487, '2321da63-59f7-4fbb-b56d-00810837d618'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (9109, '46c1eb91-a107-4d91-86e5-205812209154'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (20887, 'e4c88d19-b6b7-484a-b21a-646c0f2984c3'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (9120, '6e6140c9-95c6-4b7e-8af0-9dfe53916a31'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (2514, '1aaf99d7-9574-4a42-9441-f4b7e0b43e08'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (18403, '89fb1534-a456-4992-b162-b64ff48680bb'), (5100, '0e2cf3f9-d5c7-4a37-9665-321547a73704'), (18414, '4bfd7618-6567-411a-9d84-140a5555b25d'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (18429, '4f938c80-c628-4d41-b604-430b07615100'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
- Abstract: Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. So, we now need a well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. I'll talk about a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically
a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on LRA. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
The abstract from the paper is the following:

*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.*
```


## Processor

For models that support multimodal tasks, ü§ó Transformers offers a processor class that conveniently wraps processing classes such as a feature extractor and a tokenizer into a single object. For example, let's use the [`Wav2Vec2Processor`] for an automatic speech recognition task (ASR). ASR transcribes audio to text, so you will need a feature extractor and a tokenizer.

Create a feature extractor to handle the audio inputs:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

Create a tokenizer to handle the text inputs:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=""my_vocab_file.txt"")
```

Combine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
_Note: that we did not optimize the model for the GPU environment, the models were evaluated in fp32._

When it comes to benchmarking Transformer models, there are two metrics that are most adopted:
* **Latency**: the time it takes for the model to perform a single prediction (pre-process, prediction, post-process).
* **Throughput**: the number of executions performed in a fixed amount of time for one benchmark configuration

We looked at latency across different setups and models to understand the benefits and tradeoffs of the new Inferentia2 instance. If you want to run the benchmark yourself, we created a [Github repository](https://github.com/philschmid/aws-neuron-samples/tree/main/benchmark) with all the information and scripts to do so. 

### Results
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
Using the `evaluator`

The `Evaluator` classes allow to evaluate a  triplet of model, dataset, and metric. The models wrapped in a pipeline, responsible for handling all preprocessing and post-processing and out-of-the-box, `Evaluator`s support transformers pipelines for the supported tasks, but custom pipelines can be passed, as showcased in the section [Using the `evaluator` with custom pipelines](custom_evaluator).
Once the contract was signed, we began the migration of moving off our regex based system to direct an increasing amount of critical path traffic to the transformer model. Internally, we had to build some new telemetry for both model and production data monitoring. Given that this system is positioned so early in the product experience, any inaccuracies in model outcomes could significantly impact business metrics. We ran an extensive experiment where new users were split equally between the old system and the new model. We assessed model performance in conjunction with broader business metrics, such as paid user retention and engagement. The ML model clearly outperformed in terms of retention, leading us to confidently make the decision to scale the system - first to new users and then to existing users - ramping to 100% over a span of two months.
Keywords: Healthcare imaging, Training, Evaluation

## [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)

Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model. It supports a wide variety of NLP tasks.

Keywords: Framework, simplicity, NLP

## [JARVIS](https://github.com/microsoft/JARVIS)

[JARVIS](https://github.com/microsoft/JARVIS) is a system attempting to merge LLMs such as GPT-4 with the rest of the open-source ML community: leveraging up to 60 downstream models in order to perform tasks identified by the LLM.

Keywords: LLM, Agents, HF Hub

## [transformers.js](https://xenova.github.io/transformers.js/)

[transformers.js](https://xenova.github.io/transformers.js/) is a JavaScript library targeted at running models from transformers directly within the browser.

Keywords: Transformers, JavaScript, browser
### multimodal

A task that combines texts with another kind of inputs (for instance images).

## N

### Natural language generation (NLG)

All tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/), translation).

### Natural language processing (NLP)

A generic way to say ""deal with texts"".

### Natural language understanding (NLU)

All tasks related to understanding what is in a text (for instance classifying the
whole text, individual words).

## P

### pipeline

A pipeline in ü§ó Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.

For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).

### PipelineParallel (PP)
There have been incredible advances in multi-modal models in recent years, with vision-language models making the most significant leap in performance and the variety of use cases and applications. In this blog, we talked about the latest advancements in vision-language models, as well as what multi-modal datasets are available and which pre-training strategies we can use to train and fine-tune such models. We also showed how these models are integrated into ü§ó Transformers and how you can use them to perform various tasks with a few lines of code.
Vision Transformer (ViT)

The **Vision Transformer** is a model for image classification that employs a Transformer-like architecture over patches of the image. This includes the use of [Multi-Head Attention](https://paperswithcode.com/method/multi-head-attention), [Scaled Dot-Product Attention](https://paperswithcode.com/method/scaled) and other architectural features seen in the [Transformer](https://paperswithcode.com/method/transformer) architecture traditionally used for NLP.

## How do I use this model on an image?
To load a pretrained model:

```python
import timm
model = timm.create_model('vit_base_patch16_224', pretrained=True)
model.eval()
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (University of Wisconsin‚ÄìMadison ‡§∏‡•á) Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee. ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡§Ö‡§®‡•Å‡§∏‡§Ç‡§ß‡§æ‡§® ‡§™‡§§‡•ç‡§∞ [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) ‡§ï‡•á ‡§∏‡§æ‡§• ‡§ú‡§æ‡§∞‡•Ä ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ
1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (‡§ó‡•Ç‡§ó‡§≤ ‡§è‡§Ü‡§à ‡§∏‡•á) ‡§ï‡§æ‡§ó‡§ú ‡§ï‡•á ‡§∏‡§æ‡§• [‡§è‡§ï ‡§á‡§Æ‡•á‡§ú ‡§á‡§ú‡§º ‡§µ‡§∞‡•ç‡§• 16x16 ‡§µ‡§∞‡•ç‡§°‡•ç‡§∏: ‡§ü‡•ç‡§∞‡§æ‡§Ç‡§∏‡§´‡•â‡§∞‡•ç‡§Æ‡§∞‡•ç‡§∏ ‡§´‡•â‡§∞ ‡§á‡§Æ‡•á‡§ú ‡§∞‡§ø‡§ï‡•â‡§ó‡•ç‡§®‡§ø‡§∂‡§® ‡§è‡§ü ‡§∏‡•ç‡§ï‡•á‡§≤](https://arxiv.org/abs/2010.11929) ‡§è‡§≤‡•á‡§ï‡•ç‡§∏‡•Ä ‡§°‡•ã‡§∏‡•ã‡§µ‡§ø‡§§‡•ç‡§∏‡•ç‡§ï‡•Ä, ‡§≤‡•Å‡§ï‡§æ‡§∏ ‡§¨‡•á‡§Ø‡§∞, ‡§Ö‡§≤‡•á‡§ï‡•ç‡§ú‡•á‡§Ç‡§°‡§∞ ‡§ï‡•ã‡§≤‡•á‡§∏‡§®‡§ø‡§ï‡•ã‡§µ, ‡§°‡§ø‡§∞‡•ç‡§ï ‡§µ‡•Ä‡§∏‡•á‡§®‡§¨‡•ã‡§∞‡•ç‡§®, ‡§∂‡§ø‡§Ø‡§æ‡§ì‡§π‡•Å‡§Ü ‡§ù‡§æ‡§à, ‡§•‡•â‡§Æ‡§∏ ‡§Ö‡§®‡§ü‡§∞‡§•‡§ø‡§®‡§∞, ‡§Æ‡•Å‡§∏‡•ç‡§§‡§´‡§æ ‡§¶‡•á‡§π‡§ò‡§æ‡§®‡•Ä, ‡§Æ‡•à‡§•‡§ø‡§Ø‡§æ‡§∏ ‡§Æ‡§ø‡§Ç‡§°‡§∞‡§∞, ‡§ú‡•â‡§∞‡•ç‡§ú ‡§π‡•á‡§ó‡•ã‡§≤‡•ç‡§°, ‡§∏‡§ø‡§≤‡•ç‡§µ‡•á‡§® ‡§ó‡•á‡§≤‡•Ä, ‡§ú‡•à‡§ï‡§¨ ‡§â‡§∏‡•ç‡§ú‡§º‡§ï‡•ã‡§∞‡•á‡§á‡§ü ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§π‡•â‡§≤‡•ç‡§∏‡§¨‡•Ä ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ ‡§™‡•ã‡§∏‡•ç‡§ü ‡§ï‡§ø‡§Ø‡§æ ‡§ó‡§Ø‡§æ‡•§
## Conclusion

By using Transformers' sentiment analysis pipeline, I saved a non-negligible amount of time. Instead of training/fine-tuning a model, I could find one ready to be used in production and start the deployment in my system. I might fine-tune it in the future, but as shown on my test, the accuracy is already amazing!
I would have liked a ""pure TensorFlow"" model, or at least a way to load it in TensorFlow without Transformers dependencies to use the AI platform. It would also be great to have a lite version.
The output of the Transformer model is sent directly to the model head to be processed.

In this diagram, the model is represented by its embeddings layer and the subsequent layers. The embeddings layer converts each input ID in the tokenized input into a vector that represents the associated token. The subsequent layers manipulate those vectors using the attention mechanism to produce the final representation of the sentences.

There are many different architectures available in ü§ó Transformers, with each one designed around tackling a specific task. Here is a non-exhaustive list:

- `*Model` (retrieve the hidden states)
- `*ForCausalLM`
- `*ForMaskedLM`
- `*ForMultipleChoice`
- `*ForQuestionAnswering`
- `*ForSequenceClassification`
- `*ForTokenClassification`
- and others ü§ó
We decided to focus on the most famous Transformer model architecture, 
[BERT (Delvin & al. 2018) (4)](https://arxiv.org/abs/1810.04805v1). While we focus this blog post on BERT-like 
models to keep the article concise, all the described techniques
can be applied to any architecture on the Hugging Face model hub.  
In this blog post we will not describe in detail the Transformer architecture - to learn about that I can't 
recommend enough the 
[Illustrated Transformer blogpost from Jay Alammar (5)](https://jalammar.github.io/illustrated-transformer/).

Today's goals are to give you an idea of where we are from an Open Source perspective using BERT-like
models for inference on PyTorch and TensorFlow, and also what you can easily leverage to speedup inference.

## 2. Benchmarking methodology
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (University of Wisconsin‚ÄìMadison „Åã„Çâ) Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee. „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784)
1. **[Vision Transformer (ViT)](https://huggingface.co/docs/transformers/model_doc/vit)** (Google AI „Åã„Çâ) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model -- one as close as possible to the task you have at hand -- and fine-tune it.

## General architecture[[general-architecture]]

In this section, we'll go over the general architecture of the Transformer model. Don't worry if you don't understand some of the concepts; there are detailed sections later covering each of the components.

<Youtube id=""H39Z_720T5s"" />

## Introduction[[introduction]]

The model is primarily composed of two blocks:

* **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
* **Decoder (right)**: The decoder uses the encoder's representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.

Here is the question: How can multimodal Transformer models be evaluated for complex workflows?
Here is answer 1:By using single-modality benchmarks
Here is answer 2: Through multimodal benchmarks combining text, image, and audio evaluations
Here is answer3: By focusing only on accuracy for text tasks
Here is answer 4: By skipping multimodal considerations during evaluation","Accordingly based off context surrounding topic based discussions best fitting response choice would likely fall under option ""By focusing only"
"What is the primary advantage of Hugging Face‚Äôs integration across JAX, PyTorch, and TensorFlow?","[(17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (3600, 'a5251b45-4980-4841-a1da-d832c0aa4e34'), (17425, 'a6c4b1bc-1980-4b9d-826a-871d7430c2cc'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (17439, '220bb0f1-e5a5-4c21-ba59-845839d83ad3'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (5155, '86625bb7-792a-426a-9ade-ea98317da2e0'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (30761, '683724e2-567a-46f0-9254-202ca2f88a96'), (29236, 'c8256ce9-e984-4533-a82f-3f08a2d65e83'), (24634, '70652141-34ee-406f-816b-e497d503fd8d'), (28733, 'f836bdf5-abd9-4d78-878b-1eed6e0815c8'), (22593, '1d7e4289-69b0-474f-a80c-712bb400aca9'), (3659, '26897d07-2459-46e9-bf92-6a9bbbe307d2'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (21076, 'f7b2eb77-c2fc-4c18-926b-38ac4c905bda'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (9840, 'b35e7b1d-c3d5-4e40-80c7-d9b3b8c7f3fe'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (26745, '509a2418-a582-44fa-9c23-427ca2329092'), (31354, '50289d67-f345-42d3-bb11-7b4a1e8c5530'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21138, 'c2c5b354-94af-4fd1-bbc5-65ca22e909cb'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (668, '249a03e0-c503-4df7-824c-7f0c50892b55'), (1181, '2e574621-b2e7-4ffb-ad69-7aa65d1e16fb'), (21148, '668cf8f7-76fb-4242-9d40-68c20e2b1dbf'), (21151, 'c1f9f41a-2ae3-4bdd-b40e-a54327d158f4'), (1191, 'ece7abdd-1376-495b-8daa-d664f2a27dae'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (30891, 'ce0c26e5-9ecb-4d1c-ac24-560c5974dda4'), (25786, 'a3f4a2db-9125-4e13-89f2-07a0399eaa1d'), (20667, 'b0b180ee-2478-48b8-9446-bf6a9feabb3b'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (18631, '87868ed5-84c3-422f-a5d4-d23a4b6685d6'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (25302, 'bcf9a9f1-7a6b-4a62-87f7-68506603417d'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (17116, 'df0e2cb6-a368-4e77-894f-ce91ac16ed6c'), (7390, 'd82b7e48-5494-469a-b93b-42130344ea25'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (17123, 'b278f0df-995f-4b18-a7f0-c467054354da'), (8429, '45f5650b-4b13-471b-ac60-251d69059476'), (22770, '978ad2c3-695e-4a95-82c8-b6fb9af51394'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (8971, '5a35d107-5b71-4521-8856-416fb762b753'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28429, '9c6fb59e-1d9f-48bd-be33-bd99c86d369d'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (8469, '0ae3273f-512a-4f10-a11c-633fe9e45189'), (21785, '0f42515e-4921-41d6-a8f2-18c62768982e'), (6437, '84957027-ff15-4ac9-9366-7a35676d6fd0'), (26919, '5ebb5465-6f4c-4e1c-8326-5f71516e66a4'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (8492, '37b69803-2f18-4f43-850e-e3c81aa1cb27'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (21813, 'f41892f3-473d-4a2e-ac76-5f8c7d4613ef'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (25456, 'ac2479c5-2682-4db2-a27e-fdf59164519f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (25459, '7e9e55e8-8027-4677-b297-11f6cebd0a17'), (3957, '86a50945-a243-44e5-be1f-c80fe6f354bc'), (21366, '3f2fa721-852c-4545-b287-9c8782de1be2'), (21365, '9556f219-576e-4d37-8eb8-c37f3fb358f5'), (13176, 'a57502e1-52a5-41dc-aebb-b7cb7f00763d'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (22394, 'b66b7946-772b-43b6-8e7d-b5388eaf7cf6'), (25476, '3fcbb85d-17ea-4cf1-b253-7994b353a67e'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (20871, 'edfcf78d-61e7-443d-8514-6db1ea1b0bca'), (13196, '5f4d0098-0f25-4cd7-9d59-4037a77c492b'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (17820, 'f5b85b58-0df6-4cf9-9250-5afcef460492'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (17828, 'e0e24538-49e6-418a-8fb8-b10e6a8d00f1'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (22453, '4bad26aa-2ebe-4fb7-80b0-702de459d53c'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31672, '94b34b38-3568-488b-94eb-579ac5ccf43e'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (6082, 'c6145d46-49ea-43dd-88bd-7d6c7daf3380'), (28614, '85bff174-da9e-4929-bdc0-904ec1a4bc99'), (26056, 'a33a7d44-e5a6-4308-b681-f1a6883c8e7a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (9163, 'd7206e44-29b4-499b-92c4-0008be204752'), (6093, '5d63e976-ff26-4315-b418-58387364c07e'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (31728, '5ccf1245-717b-44e0-837a-e164da6b012a'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: --
title: ""Hugging Face's TensorFlow Philosophy""
thumbnail: /blog/assets/96_tensorflow_philosophy/thumbnail.png
authors:
- user: rocketknight1
---

# Hugging Face's TensorFlow Philosophy



### Introduction


Despite increasing competition from PyTorch and JAX, TensorFlow remains [the most-used deep learning framework](https://twitter.com/fchollet/status/1478404084881190912?lang=en). It also differs from those other two libraries in some very important ways. In particular, it‚Äôs quite tightly integrated with its high-level API `Keras`, and its data loading library `tf.data`.
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples üìÑ
#### Suraj Patil & Patrick von Platen, Machine Learning Engineers at Hugging Face
- Talk: How to use JAX/Flax with Transformers
- Abstract: Transformers is one of the most popular open-source ML libraries and supports PyTorch, Tensorflow, and JAX/Flax. In this talk, we will explain how JAX/Flax models should be used in Transformers and compare their design in Transformers with the design of PyTorch models in Transformers. In the second part, we will give you a hands-on presentation of how a model can be trained end-to-end with the official JAX/Flax example scripts using Transformers & Datasets. Along the way, we want to give you some tips and tricks on how to best realize your project.
- Speaker info: Suraj and Patrick are part of Hugging Face‚Äôs open source team and lead the integration of JAX/Flax into Transformers.
- GitHub: https://github.com/patil-suraj & https://github.com/patrickvonplaten
## Accelerating Machine Learning from Science to Production

In addition to Hugging Face DLCs, we created a first-class Hugging Face extension to the SageMaker Python-sdk to accelerate data science teams, reducing the time required to set up and run experiments from days to minutes.

You can use the Hugging Face DLCs with the Automatic Model Tuning capability of Amazon SageMaker, in order to automatically optimize your training hyperparameters and quickly increase the accuracy of your models.

Thanks to the SageMaker Studio web-based Integrated Development Environment (IDE), you can easily track and compare your experiments and your training artifacts.

## Built-in Performance

With the Hugging Face DLCs, SageMaker customers will benefit from built-in performance optimizations for PyTorch or TensorFlow, to train NLP models faster, and with the flexibility to choose the training infrastructure with the best price/performance ratio for your workload.
To learn how to access and use the new Hugging Face DLCs with the Amazon SageMaker Python SDK, check out the guides and resources below.

> _On July 8th, 2021 we extended the Amazon SageMaker integration to add easy deployment and inference of Transformers models. If you want to learn how you can [deploy Hugging Face models easily with Amazon SageMaker](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker) take a look at the [new blog post](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker) and the [documentation](https://huggingface.co/docs/sagemaker/inference)._

---

## **Features & Benefits üî•**

## One Command is All you Need

With the new Hugging Face Deep Learning Containers available in Amazon SageMaker, training cutting-edge Transformers-based NLP models has never been simpler. There are variants specially optimized for TensorFlow and PyTorch, for single-GPU, single-node multi-GPU and multi-node clusters.
- Many models weren‚Äôt XLA-compatible
- Data collators didn‚Äôt use native TF operations

We think XLA is the future: It‚Äôs the core compiler for JAX, it has first-class support in TensorFlow, and you can even use it from [PyTorch](https://github.com/pytorch/xla). As such, we‚Äôve made a [big push](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) to make our codebase XLA compatible and to remove any other roadblocks standing in the way of XLA and TPU compatibility. This means users should be able to train most of our TensorFlow models on TPUs without hassle.
Integrate your library with the Hub

The Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.

Integrating the Hub with your library provides many benefits, including:
```

Check out the Transformers [documentation](https://huggingface.co/docs/transformers/main/en/main_classes/quantization) to learn more about all the features. 

Our AutoGPTQ integration has many advantages:

- Quantized models are serializable and can be shared on the Hub.
- GPTQ drastically reduces the memory requirements to run LLMs, while the inference latency is on par with FP16 inference.
- AutoGPTQ supports Exllama kernels for a wide range of architectures.
- The integration comes with native RoCm support for AMD GPUs.
- [Finetuning with PEFT](#--fine-tune-quantized-models-with-peft--) is available.

You can check on the Hub if your favorite model has already been quantized. TheBloke, one of Hugging Face top contributors, has quantized a lot of models with AutoGPTQ and shared them on the Hugging Face Hub. We worked together to make sure that these repositories will work out of the box with our integration.
```

```python out
""""""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
""""""
```

Nice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won't always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the [Hugging Face forums](https://discuss.huggingface.co/) that can help you out! In the next section, we'll take a look at how you can craft good forum questions that are likely to get answered.
# Overview

ü§ó Optimum handles the export of TensorFlow models to TFLite in the `exporters.tflite` module. In addition, models hosted on the Hugging Face Hub with PyTorch weights but having a TensorFlow implementation will also be supported in the export thanks to Transformers' [TFPreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained) auto-conversion to TensorFlow.

The TFLite export support provides classes, functions and a command line interface to export a model easily.

Supported architectures:

- Albert
- BERT
- Camembert
- ConvBert
- Deberta
- Deberta V2
- DistilBert
- Electra
- Flaubert
- MobileBert
- MPNet
- ResNet
- Roberta
- RoFormer
- XLM
- XLMRoberta
Using ü§ó `transformers` at Hugging Face

ü§ó `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. We are a bit biased, but we really like ü§ó `transformers`!

## Exploring ü§ó transformers in the Hub

There are over 25,000 `transformers` models in the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=transformers&sort=downloads). 

You can find models for many different tasks:
DeepSpeed ZeRO-1, which saves memory by partitioning the optimizer states. After the pre-training or the fine-tuning is done, developers can either save the trained PyTorch model or convert it to the ONNX format with APIs that Optimum implemented for ONNX Runtime to ease the deployment for Inference. And just like `Trainer`, `ORTTrainer` has full integration with Hugging Face Hub: after the training, users can upload their model checkpoints to their Hugging Face Hub account.
### Diffusers integration
Because W√ºrstchen is fully integrated in `diffusers`, it automatically comes with various goodies and optimizations out of the box. These include:
- Automatic use of [PyTorch 2 `SDPA`](https://huggingface.co/docs/diffusers/optimization/torch2.0) accelerated attention, as described below.
- Support for the [xFormers flash attention](https://huggingface.co/docs/diffusers/optimization/xformers) implementation, if you need to use PyTorch 1.x instead of 2.
- [Model offload](https://huggingface.co/docs/diffusers/optimization/fp16#model-offloading-for-fast-inference-and-memory-savings), to move unused components to CPU while they are not in use. This saves memory with negligible performance impact.
- [Sequential CPU offload](https://huggingface.co/docs/diffusers/optimization/fp16#offloading-to-cpu-with-accelerate-for-memory-savings), for situations where memory is really precious. Memory use will be minimized, at the cost of slower inference.
```

Check out the [documentation](https://huggingface.co/docs/optimum/bettertransformer/overview) for more details, and the [blog post on PyTorch's Medium](https://medium.com/pytorch/bettertransformer-out-of-the-box-performance-for-huggingface-transformers-3fbe27d50ab2) to find out more about the integration!

## `torch.fx` integration

Optimum integrates with `torch.fx`, providing as a one-liner several graph transformations. We aim at supporting a better management of [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization) through `torch.fx`, both for quantization-aware training (QAT) and post-training quantization (PTQ).

Check out the [documentation](https://huggingface.co/docs/optimum/torch_fx/usage_guides/optimization) and [reference](https://huggingface.co/docs/optimum/torch_fx/package_reference/optimization) for more!
A few notes on this integration:

- you will need to be logged in to the Hugging Face website locally for it to work, the easiest way to achieve this is to run `huggingface-cli login` and then type your username and password when prompted. You can also pass along your authentication token with the `--hub_token` argument.
- the `output_dir` you pick will either need to be a new folder or a local clone of the distant repository you are using.

## Distributed training and mixed precision

All the PyTorch scripts mentioned above work out of the box with distributed training and mixed precision, thanks to
the [Trainer API](https://huggingface.co/transformers/main_classes/trainer.html). To launch one of them on _n_ GPUs,
use the following command:

```bash
torchrun \
    --nproc_per_node number_of_gpu_you_have path_to_script.py \
	--all_arguments_of_the_script
## Adding TensorFlow weights to ü§ó Hub

Assuming that the TensorFlow model architecture is available in ü§ó Transformers, converting PyTorch weights into
TensorFlow weights is a breeze!

Here's how to do it:
1. Make sure you are logged into your Hugging Face account in your terminal. You can log in using the command
   `huggingface-cli login` (you can find your access tokens [here](https://huggingface.co/settings/tokens))
2. Run `transformers-cli pt-to-tf --model-name foo/bar`, where `foo/bar` is the name of the model repository
   containing the PyTorch weights you want to convert
3. Tag `@joaogante` and `@Rocketknight1` in the ü§ó Hub PR the command above has just created

That's it! üéâ


## Debugging mismatches across ML frameworks üêõ
There is a tendency among PyTorch engineers (picture me staring darkly across the open-plan office here) to see this as a problem to be overcome; their goal is to figure out how to make TensorFlow get out of their way so they can use the low-level training and data-loading code they‚Äôre used to. This is entirely the wrong way to approach TensorFlow! Keras is a great high-level API. If you push it out of the way in any project bigger than a couple of modules you‚Äôll end up reproducing most of its functionality yourself when you realize you need it.

As refined, respected and highly attractive TensorFlow engineers, we want to use the incredible power and flexibility of cutting-edge models, but we want to handle them with the tools and API we‚Äôre familiar with. This blogpost will be about the choices we make at Hugging Face to enable that, and what to expect from the framework as a TensorFlow programmer.

### Interlude: 30 Seconds to ü§ó
Because it loads so fast and is framework agnostic, we can even use the format
to load models from the same file in PyTorch or TensorFlow.


## The security audit

Since `safetensors` main asset is providing safety guarantees, we wanted to make sure
it actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external
security audit to confirm it.

Important findings:

- No critical security flaw leading to arbitrary code execution was found.
- Some imprecisions in the spec format were detected and fixed. 
- Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot_(computing)), which was fixed.
- Lots of improvements to the test suite were proposed and implemented.

In the name of openness and transparency, all companies agreed to make the report
fully public.

[Full report](https://huggingface.co/datasets/safetensors/trail_of_bits_audit_repot/resolve/main/SOW-TrailofBits-EleutherAI_HuggingFace-v1.2.pdf)
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
## Hugging Face ONNX Runtime Support 

There are over 130,000 ONNX-supported models on Hugging Face, an open source community that allows users to build, train, and deploy hundreds of thousands of publicly available machine learning models.
These ONNX-supported models, which include many increasingly popular large language models (LLMs) and cloud models, can leverage ONNX Runtime to improve performance, along with other benefits.
For example, using ONNX Runtime to accelerate the whisper-tiny model can improve average latency per inference, with an up to 74.30% gain over PyTorch.
ONNX Runtime works closely with Hugging Face to ensure that the most popular models on the site are supported.
In total, over 90 Hugging Face model architectures are supported by ONNX Runtime, including the 11 most popular architectures (where popularity is determined by the corresponding number of models uploaded to the Hugging Face Hub):

Here is the question: What is the primary advantage of Hugging Face‚Äôs integration across JAX, PyTorch, and TensorFlow?
Here is answer 1:Exclusive support for JAX
Here is answer 2: Flexibility to switch models across frameworks seamlessly
Here is answer3: Focused support for NLP tasks only
Here is answer 4: Limiting tasks to a single framework for simplicity","The correct answer here would likely fall under option #2 ""Flexibility"" since there isn't exclusive support mentioned anywhere within this passage concerning solely towards jax.`"""
What makes Hugging Face pipelines critical for multimodal experimentation?,"[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (21188, 'e53bd9c2-9844-411f-810d-5e35fa824c01'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (9952, '7b5b02be-c8e3-4b66-9ea2-d73cf414e65f'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (28908, '777723f1-2176-4d81-92fd-310f4af75f5d'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (21804, '9912e115-c51c-4885-ad29-5a67bcb2572b'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (16716, 'd9f215fb-7aa2-4b50-a40d-d2e690dc5d03'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7060, '395b844f-bf7d-4cd0-a300-57c24b767beb'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (27040, '56f391b1-2a20-4be2-af9f-8f26721aaaf2'), (6047, '162b5640-f06a-4c27-97b1-d3c317bbb669'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (17828, 'e0e24538-49e6-418a-8fb8-b10e6a8d00f1'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31672, '94b34b38-3568-488b-94eb-579ac5ccf43e'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.

Hugging Face Infinity consists of 2 main services:
* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.
* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.

The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.

<br>
<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Product overview"" src=""assets/46_infinity_cpu_performance/overview.png""></medium-zoom>
  <figcaption>Figure 1. Infinity Overview</figcaption>
</figure>
<br>
Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!

### Finding models

Over 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.


### Widgets
An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.

You can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).

---

## Benchmark
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Pipelines for inference

The [`pipeline`] makes it simple to use any model from the [Hub](https://huggingface.co/models) for inference on any language, computer vision, speech, and multimodal tasks. Even if you don't have experience with a specific modality or aren't familiar with the underlying code behind the models, you can still use them for inference with the [`pipeline`]! This tutorial will teach you to:

* Use a [`pipeline`] for inference.
* Use a specific tokenizer or model.
* Use a [`pipeline`] for audio, vision, and multimodal tasks.

<Tip>

Take a look at the [`pipeline`] documentation for a complete list of supported tasks and available parameters.

</Tip>

## Pipeline usage
### multimodal

A task that combines texts with another kind of inputs (for instance images).

## N

### Natural language generation (NLG)

All tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/), translation).

### Natural language processing (NLP)

A generic way to say ""deal with texts"".

### Natural language understanding (NLU)

All tasks related to understanding what is in a text (for instance classifying the
whole text, individual words).

## P

### pipeline

A pipeline in ü§ó Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.

For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).

### PipelineParallel (PP)
Let's get started! üöÄ

## 1. What is Optimum? An ELI5

[Hugging Face Optimum](https://github.com/huggingface/optimum) is an open-source library and an extension of [Hugging Face Transformers](https://github.com/huggingface/transformers), that provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware, including toolkits for optimized performance on [Graphcore IPU](https://github.com/huggingface/optimum-graphcore) and [Habana Gaudi](https://github.com/huggingface/optimum-habana). Optimum can be used for accelerated training, quantization, graph optimization, and now inference as well with support for [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines).

## 2. New Optimum inference and pipeline features
In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for prototyping, comparing it against AWS Sagemaker and Hugging Face‚Äôs new model hosting Inference API. Given that we use GCP for data storage and Google Vertex Pipelines for model training, exporting models to AWS Sagemaker was clunky and bug prone. Thankfully, the set up for Hugging Face was quick and easy, and it was able to handle a small portion of traffic within a week. Hugging Face simply worked out of the gate, and this reduced friction led us to proceed down this path.
```

As you can see, loading a model within a pipeline is extremely simple. The only thing you need to watch out for is that the chosen checkpoint is suitable for the task it's going to be used for. For example, here we are loading the `camembert-base` checkpoint in the `fill-mask` pipeline, which is completely fine. But if we were to load this checkpoint in the `text-classification` pipeline, the results would not make any sense because the head of `camembert-base` is not suitable for this task! We recommend using the task selector in the Hugging Face Hub interface in order to select the appropriate checkpoints:

<div class=""flex justify-center"">
<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter4/tasks.png"" alt=""The task selector on the web interface."" width=""80%""/>
</div>

You can also instantiate the checkpoint using the model architecture directly:
Here is a list of things we‚Äôll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#ü§ó-ü§ù-timm)
- [Diffusers](#üß®-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time üëÅ

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
<Tip>
‚ö†Ô∏è The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! <a href=""https://huggingface.co/join"">Create a huggingface.co</a> account to benefit from all available features!
</Tip>

Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems.

## Working with pipelines[[working-with-pipelines]]

<Youtube id=""tiZFewofSLM"" />

The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:

```python
from transformers import pipeline

classifier = pipeline(""sentiment-analysis"")
classifier(""I've been waiting for a HuggingFace course my whole life."")
Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.

The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. 

If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) 


## Resources
## Adding new tasks to the Hub

### Using Hugging Face transformers library

If your model is a `transformers`-based model, there is a 1:1 mapping between the Inference API task and a `pipeline` class. Here are some example PRs from the `transformers` library:
* [Adding ImageClassificationPipeline](https://github.com/huggingface/transformers/pull/11598)
* [Adding AudioClassificationPipeline](https://github.com/huggingface/transformers/pull/13342)

Once the pipeline is submitted and deployed, you should be able to use the Inference API for your model.

### Using Community Inference API with a supported library

The Hub also supports over 10 open-source libraries in the [Community Inference API](https://github.com/huggingface/api-inference-community).
We are continuing to integrate the most impactful computer vision and multi-modal models and would love to hear back from you. To stay up to date with the latest news in multi-modal research, you can follow us on Twitter: [@adirik](https://twitter.com/https://twitter.com/alaradirik), [@NielsRogge](https://twitter.com/NielsRogge), [@apsdehal](https://twitter.com/apsdehal), [@a_e_roberts](https://twitter.com/a_e_roberts), [@RisingSayak](https://mobile.twitter.com/a_e_roberts), and [@huggingface](https://twitter.com/huggingface).

*Acknowledgements: We thank Amanpreet Singh and Amy Roberts for their rigorous reviews. Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.*
## Optimising Transformers for Production

Transformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.

Already, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.

With a user base of more than 50,000 developers ‚Äì Hugging Face has seen the fastest ever adoption of an open-source project.

Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.
Hugging Face DLCs are open source and licensed under Apache 2.0. Feel free to reach out on our [community forum](https://discuss.huggingface.co/c/sagemaker/17) if you have any questions. For premium support, our [Expert Acceleration Program](https://huggingface.co/support) gives you direct dedicated support from our team.

## Features & benefits üî•

Hugging Face Deep DLCs make it easier than ever to train Transformer models in SageMaker. Here is why you should consider using Hugging Face DLCs to train and deploy your next machine learning models:

**One command is all you need**

With the new Hugging Face DLCs, train cutting-edge Transformers-based NLP models in a single line of code. Choose from multiple DLC variants, each one optimized for TensorFlow and PyTorch, single-GPU, single-node multi-GPU, and multi-node clusters.

**Accelerate machine learning from science to production**
# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft‚Äôs ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face‚Äôs [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
## Inference

We provide two main ways to run inference with Mixtral models:

- Via the `pipeline()` function of ü§ó¬†Transformers.
- With Text Generation Inference, which supports advanced features like continuous batching, tensor parallelism, and more, for blazing fast results.

For each method, it is possible to run the model in half-precision (float16) or with quantized weights. Since the Mixtral model is roughly equivalent in size to a 45B parameter dense model, we can estimate the minimum amount of VRAM needed as follows:

| Precision | Required VRAM |
| --------- | ------------- |
| float16   | >90 GB        |
| 8-bit     | >45 GB        |
| 4-bit     | >23 GB        |

### Using ü§ó¬†Transformers

With transformers¬†[release 4.36](https://github.com/huggingface/transformers/releases/tag/v4.36.0), you can use Mixtral and leverage all the tools within the Hugging Face ecosystem, such as:

Here is the question: What makes Hugging Face pipelines critical for multimodal experimentation?
Here is answer 1:Their focus on standalone tasks like sentiment analysis
Here is answer 2: Their automation of multimodal processing and deployment
Here is answer3: Their exclusive reliance on pretrained models
Here is answer 4: Their lack of preprocessing support",Answer: Answer2 Their automationofmultimodal processinganddeployment
How do cross-framework APIs improve deployment flexibility for Transformer models?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (17412, '1d9b136f-d20c-4989-8975-80625fa8da45'), (24074, 'c7902235-64ce-4d97-b746-5682c76f72ec'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (16406, '57321316-4910-41b2-9525-559cf4584035'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (24631, '1cafa663-09f0-4b92-9f19-efd956944da3'), (24636, '34c928bf-700f-4de6-b158-b590ff679457'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (19525, '349b2448-2fee-4f56-89b2-5e7215220379'), (18511, '4f3566db-bced-43d1-ad6b-57ce9577de94'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (18549, 'd84bf0de-2759-4c1e-9a94-cc27a2136a9b'), (18550, '01e230dd-8782-4f9f-82af-2058aa9bd4d9'), (18571, 'e150c6a6-1333-4175-944b-28873073d9c6'), (12948, 'de99c19f-cfde-4076-a099-a2a5d6202e05'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (1195, '3e7433ba-fae2-4e13-8802-9ff9da0b8900'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (23211, '3ee17899-9fcb-44fb-957a-6db593a21bcd'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (23229, '0db85d3b-56c0-4fd4-a3fa-065cd9c9d588'), (20160, 'c0c6fd6f-69be-4907-9b87-052041804e6c'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (23748, '0ab304e4-ab01-434f-9b21-63bc07214a45'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (214, 'ced565fd-8f3c-4e7b-a0f9-9e46e7ed2922'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (6889, 'f9703e3f-789b-4d40-bb13-0dcb8de3863a'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (13578, 'cb671a15-7afc-42fd-8983-3f6a6aa4e48e'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (6943, 'ad8c7727-69f8-472d-b29c-cffc054a6777'), (2851, '3ab3417f-d50a-4451-b44e-2d63e95b777a'), (23844, 'c14d0dd8-b543-44ba-b5a9-290396c2a5d2'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (23847, '25175e5a-8fc3-4050-87fd-bf640d60de2f'), (5933, 'e8977096-c2bb-4f13-9f6e-6bfd1dbeb9bf'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (28980, '2b0c07ed-129d-48ce-a54a-c5be8281aba6'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (18759, 'a4f9107f-b29d-42b9-adf1-a7d1b294c213'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (27498, 'b9af69b0-e49d-44fb-94c1-97f3170bdd4a'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14703, '6849a5b6-70e0-438d-9f31-71b6929312c2'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (28540, '92fc9238-353a-40ac-b3a0-fe4442102873'), (25980, '44d9eb20-ac45-46e9-b4ea-310683db7670'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (8585, 'b1c8ad3c-6d9c-4e46-9b2c-fb032a27a9f9'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (20887, 'e4c88d19-b6b7-484a-b21a-646c0f2984c3'), (31130, 'cfc1501b-bd1d-4469-943f-2a858e32ae23'), (15772, '65169dba-99db-485c-9bff-b711a9a78463'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (16311, 'ff1c1d88-bc88-43e0-9d16-50398857fbc0'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (11209, '3b5ff375-2e1c-4a92-8077-43f3d30f62ae'), (16843, '926901bf-b2bd-4869-8ac4-10833c3d0dd1'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (15835, '888ea0a4-ad95-4449-9478-48ba7798a983'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ## Quickstart

Since its 1.13 version, [PyTorch released](https://pytorch.org/blog/PyTorch-1.13-release/) the stable version of a fast path for its standard Transformer APIs that provides out of the box performance improvements for transformer-based models. You can benefit from interesting speedup on most consumer-type devices, including CPUs, older and newer versions of NIVIDIA GPUs.
You can now use this feature in ü§ó Optimum together with Transformers and use it for major models in the Hugging Face ecosystem.
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
By exposing a graph with standardized operators and data types, ONNX makes it easy to
switch between frameworks. For example, a model trained in PyTorch can be exported to
ONNX format and then imported in TensorFlow (and vice versa).

Once exported to ONNX format, a model can be:
- optimized for inference via techniques such as [graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) and [quantization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization). 
- run with ONNX Runtime via [`ORTModelForXXX` classes](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort),
which follow the same `AutoModel` API as the one you are used to in ü§ó Transformers.
- run with [optimized inference pipelines](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines),
which has the same API as the [`pipeline`] function in ü§ó Transformers.
--
title: ""How we sped up transformer inference 100x for ü§ó API customers""
thumbnail: /blog/assets/09_accelerated_inference/thumbnail.png
---

# How we sped up transformer inference 100x for ü§ó API customers


ü§ó Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in.

While experimenting with models in ü§ó Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a **hard engineering challenge** for any Machine Learning Engineer.
`transformers` pipelines offer powerful abstractions to run inference with `transformers`-based models. Inference Endpoints leverage the pipeline API to easily deploy models with only a few clicks. However, Inference Endpoints can also be used to deploy models that don't have a pipeline, or even non-transformer models! This is achieved using a custom inference function that we call a [custom handler](https://huggingface.co/docs/inference-endpoints/guides/custom_handler).

Let's demonstrate this process using MusicGen as an example. To implement a custom handler function for MusicGen and deploy it, we will need to:

1. Duplicate the MusicGen repository we want to serve,
2. Write a custom handler in `handler.py` and any dependencies in `requirements.txt` and add them to the duplicated repository,
3. Create Inference Endpoint for that repository.
We applied the same deployment workflow for an ONNX-optimized version of the same
Vision Transformer model. For more details, check out [this link](https://github.com/sayakpaul/deploy-hf-tf-vision-models/tree/main/hf_vision_model_onnx_gke). ONNX-optimized models are especially beneficial if you're using x86 CPUs for deployment.

In the next post, we‚Äôll show you how to perform these deployments with
significantly less code with [<u>Vertex AI</u>](https://cloud.google.com/vertex-ai) ‚Äì more like
`model.deploy(autoscaling_config=...)` and boom! We hope you‚Äôre just as
excited as we are.

# Acknowledgement

Thanks to the ML Developer Relations Program team at Google, which
provided us with GCP credits for conducting the experiments.
We constantly examine whether we have the right model topology for our problem. While LLMs have recently been in the news, we‚Äôve struggled to find an implementation that can outperform our specialized transformer classifiers at this time in both speed and cost. We see promise in the early results of using them in the long tail of services (i.e. mom-and-pop shops) - keep an eye out for that in a future version of Rocket Money! The uptime and confidence we have in the HuggingFace Inference API has allowed us to focus our energy on the value generated by the models and less on the plumbing and day-to-day operation. With the help of Hugging Face, we have taken on more scale and complexity within our model and the types of value it generates. Their customer service and support have exceeded our expectations and they‚Äôre genuinely a great partner in our journey.
Introduction[[introduction]]

<CourseFloatingBanner
    chapter={2}
    classNames=""absolute z-10 right-0 top-0""
/>

As you saw in [Chapter 1](/course/chapter1), Transformer models are usually very large. With millions to tens of *billions* of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task.

The ü§ó Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. The library's main features are:
### Drawbacks
Clearly, there are also drawbacks to the single file policy two of which we quickly want to mention here.

A major goal of Transformers is to provide a unified API for both inference and training for all models so 
that a user can quickly switch between different models in her setup. However, ensuring a unified API across 
models is much more difficult if modeling files are not allowed to use abstracted logical patterns. We solve
this problem by running **a lot** of tests (*ca.* 20,000 tests are run daily at the time of writing this blog post) to ensure that models follow a consistent API. In this case, the single file policy requires us to be very rigorous when reviewing model and test additions.
-->

# How to convert a ü§ó Transformers model to TensorFlow?

Having multiple frameworks available to use with ü§ó Transformers gives you flexibility to play their strengths when
designing your application, but it implies that compatibility must be added on a per-model basis. The good news is that
adding TensorFlow compatibility to an existing model is simpler than [adding a new model from scratch](add_new_model)!
Whether you wish to have a deeper understanding of large TensorFlow models, make a major open-source contribution, or
enable TensorFlow for your model of choice, this guide is for you.
Transformers have proven to be extremely efficient for a wide range of functions, including feature extraction, text generation, sentiment analysis, translation and many more. Models like BERT are widely used by Graphcore customers in a huge array of applications including cybersecurity, voice call automation, drug discovery, and translation.

Optimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. In providing an open-source library of transformer models, Hugging Face has directly addressed these issues. Integrating IPUs with HuggingFace also allows developers to leverage not just the models, but also datasets available in the HuggingFace Hub.
![](https://i.imgur.com/9V9xni5.png)

A model widget can quickly show how well a model will likely perform on our data. Once you've found a model that interests you, the Hub provides different ways of using that tool. If you are already familiar with the Transformers library, you can click the use in Transformers button to get a pop-up which shows how to load the model in Transformers.

![](https://i.imgur.com/E9MiMi9.png)


![](image/media/image4.png)

If you prefer to use a model via an API, clicking the
`deploy` button in a model repository gives you various options for hosting the model behind an API. This can be particularly useful if you want to try out a model on a larger amount of data but need the infrastructure to run models locally.

A similar approach can also be used to find relevant models and datasets
on the Hugging Face Hub.

## Walkthrough: how can you add a GLAM dataset to the Hub?
## Why should I use transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding & generation, computer vision, and audio tasks.
    - Low barrier to entry for educators and practitioners.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.

1. Lower compute costs, smaller carbon footprint:
    - Researchers can share trained models instead of always retraining.
    - Practitioners can reduce compute time and production costs.
    - Dozens of architectures with over 60,000 pretrained models across all modalities.

1. Choose the right framework for every part of a model's lifetime:
    - Train state-of-the-art models in 3 lines of code.
    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.
    - Seamlessly pick the right framework for training, evaluation, and production.
Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
## AutoGPTQ library ‚Äì the one-stop library for efficiently leveraging GPTQ for LLMs

The AutoGPTQ library enables users to quantize ü§ó Transformers models using the GPTQ method. While parallel community efforts such as [GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa), [Exllama](https://github.com/turboderp/exllama) and [llama.cpp](https://github.com/ggerganov/llama.cpp/) implement quantization methods strictly for the Llama architecture, AutoGPTQ gained popularity through its smooth coverage of a wide range of transformer architectures.

Since the AutoGPTQ library has a larger coverage of transformers models, we decided to provide an integrated ü§ó Transformers API to make LLM quantization more accessible to everyone. At this time we have integrated the most common optimization options, such as CUDA kernels. For more advanced options like Triton kernels or fused-attention compatibility, check out the [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) library.
* ü§ó using a `transformers` model
* üê≥ using a model from an [officially supported library](./models-libraries)
* üñ®Ô∏è using a model with custom inference code. This experimental option has downsides, so we recommend using one of the other approaches.

Finally, you can add a couple of UI elements, such as the task icon and the widget, that complete the integration in the Hub. üì∑ 

Some steps are orthogonal; you don't need to do them in order. **You don't need the Inference API to add the icon.** This means that, even if there isn't full integration yet, users can still search for models of a given task.

## Adding new tasks to the Hub

### Using Hugging Face transformers library
Keywords: Healthcare imaging, Training, Evaluation

## [simpletransformers](https://github.com/ThilinaRajapakse/simpletransformers)

Simple Transformers lets you quickly train and evaluate Transformer models. Only 3 lines of code are needed to initialize, train, and evaluate a model. It supports a wide variety of NLP tasks.

Keywords: Framework, simplicity, NLP

## [JARVIS](https://github.com/microsoft/JARVIS)

[JARVIS](https://github.com/microsoft/JARVIS) is a system attempting to merge LLMs such as GPT-4 with the rest of the open-source ML community: leveraging up to 60 downstream models in order to perform tasks identified by the LLM.

Keywords: LLM, Agents, HF Hub

## [transformers.js](https://xenova.github.io/transformers.js/)

[transformers.js](https://xenova.github.io/transformers.js/) is a JavaScript library targeted at running models from transformers directly within the browser.

Keywords: Transformers, JavaScript, browser

Here is the question: How do cross-framework APIs improve deployment flexibility for Transformer models?
Here is answer 1:By standardizing deployment across frameworks without fine-tuning
Here is answer 2: By focusing only on text classification tasks
Here is answer3: By limiting deployment to specific frameworks
Here is answer 4: By ignoring framework-specific optimizations",The correct answer is: By standardizing deployment across frameworks without fine-tuning
What is the role of Hugging Face‚Äôs prebuilt models in simplifying multimodal applications?,"[(5120, 'ddf54aa1-62ec-4203-a18e-bcc935c8a3ed'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (21012, '4252b2f2-3c37-45fc-8476-73ec6c4c1ee7'), (18423, 'bceac124-b7c2-4000-85e8-bc1990d7dce1'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10841, '6e53fcd9-7666-4769-ba61-2d6fab8ac52c'), (17509, '56ea90b0-f162-4bf5-8c05-654f3a183cd9'), (2663, 'efa7e63d-051b-4d0d-b130-483c5160dd59'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (20609, 'c19b3701-219d-48a8-ae4b-d6b0ad0e3b8c'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (20612, 'de424e36-a6d3-4010-b9cb-29aafe220c68'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (20636, '3a97d7e3-78ab-4911-9bbf-01b10ea9777a'), (1186, '50598bfb-1eff-44c7-a14f-8886c016a045'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (23714, 'a2d69303-a006-44f0-8008-1accb10a3697'), (30886, '54b36145-f8cd-4fc5-98d8-547220fe27b3'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (5825, '02b1a8f8-63ab-44e9-a55e-de99cee8b6dc'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (4804, '545adb55-d07e-4a08-b179-72243735bedb'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (3800, '82c0086d-96e5-4b26-a430-a265a26fdb8b'), (18136, 'ea71868e-38ba-42e9-9d5c-3ac4745b0a68'), (22752, '847c0672-9e44-445e-b14d-84817ad403ee'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (1777, '4ddc1fcc-541d-4553-b408-84494027daef'), (13554, 'f239a4cb-1368-4161-bb6c-d4d76b74b26d'), (1779, '920a7d81-f957-49ad-81e8-9fa4da67693a'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (252, '1d1458f5-0779-4f41-8eab-579cf8943202'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (18184, 'a53114c0-96cb-42ef-a44e-4a6ea9778ff1'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (13601, 'd2fcf63a-6647-41a1-8530-613f53b57a88'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (18217, '4e0278ab-2929-47df-8ac5-54be5d98a007'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (299, '7f2a2f1a-f503-44f9-b87a-583789044a1f'), (307, '6b64ed6e-acaf-4565-8edd-d7398bd867b4'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (13634, '63e875a9-4fad-4f80-8454-e567ece5a4ac'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (333, 'b89f73b6-20ff-473e-9c00-802ac258ce6c'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (9072, '7a516a26-4603-45f9-b69a-f48eb9d9c67e'), (16246, '3c77c431-7979-48b7-b494-42e7a252cfbb'), (22395, '76235143-e5e3-4d4b-968f-8198aaa84e79'), (16252, 'ebc3b30d-233d-43e4-b012-9f5a958063fc'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (20876, 'a784cd91-18de-4adf-a288-2724d2b961e1'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (9103, '79a8442d-c532-45de-a08c-4f82c8821d64'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (10143, 'fb24eb30-9a50-4175-84a3-36d6e8aa5ee7'), (9129, '4e3c49f8-4ca3-47af-a572-5135008633d3'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (18366, '8a7e2dee-f14d-4ea1-819d-851a6affb887'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (5063, 'f4cfdc74-4cbc-4213-a3fb-a400b432dc40'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (9174, '3767ffbd-cac9-45c5-b684-68ec257368f6'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (18397, 'fba334a1-3931-428a-ace2-8ab61c5a23c8'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (21982, '85733d75-1b4a-47c3-b481-549f15ffb07f'), (11235, '8558ce8a-5be5-4c58-91a4-e034fd8f061d'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (5094, 'ff050631-51bc-4cbe-8c50-0fcdc596f571'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (5111, 'e87c80c3-d7d3-4478-88c3-e8dc5a994da2'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: **Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One ‚Äì or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) ‚Äì specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
- We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,
    just three standard classes required to use each model: [configuration](main_classes/configuration),
    [models](main_classes/model), and a preprocessing class ([tokenizer](main_classes/tokenizer) for NLP, [image processor](main_classes/image_processor) for vision, [feature extractor](main_classes/feature_extractor) for audio, and [processor](main_classes/processors) for multimodal inputs).
  - All of these classes can be initialized in a simple and unified way from pretrained instances by using a common
    `from_pretrained()` method which downloads (if needed), caches and
    loads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,
    and models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint.
Clement Delangue, co-founder and CEO, Hugging Face

## Conclusion

Together, Snorkel and Hugging Face make it easier than ever for large companies, government agencies, and AI innovators to get value from foundation models. The ability to use Hugging Face‚Äôs comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world.

_If you‚Äôre interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!_
```

## Pros and cons 

When to use this API and when to not use it? Let's discuss in this section the pros and cons 

Pros:
- The model gets modified in-place, meaning the model will preserve all its original attributes and methods
- Works for any torch module, and any modality (vision, text, multi-modal)

Cons:
- You need to manually writing Hugging Face `from_pretrained` and `save_pretrained` utility methods if you want to easily save / load adapters from the Hugging Face Hub.
- You cannot use any of the utility method provided by `PeftModel` such as disabling adapters, merging adapters, etc.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
3. **Open-source libraries**: the Hugging Face [libraries](https://huggingface.co/docs) let you download pre-trained models with a single line of code, and you can start experimenting with your data in minutes. From training to deployment to hardware optimization, customers can rely on a consistent set of community-driven tools that work the same everywhere, from their laptops to their production environment.
The abstract from the paper is the following:

*Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ‚àº1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_architecture.jpg""
alt=""drawing"" width=""600""/>
--
title: ""Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action"" 
thumbnail: /blog/assets/deploy-deepfloydif-using-bentoml/thumbnail.png
authors:
- user: Sherlockk
  guest: true
- user: larme
  guest: true
---

# Deploying Hugging Face Models with BentoML: DeepFloyd IF in Action


Hugging Face provides a Hub platform that allows you to upload, share, and deploy your models with ease. It saves developers the time and computational resources required to train models from scratch. However, deploying models in a real-world production environment or in a cloud-native way can still present challenges.
Could we simplify that? Let's find out!

## Finding what we need

First we'll import the `HfApi`, which is a class that helps us interact with the backend hosting for Hugging Face. We can interact with the models, datasets, and more through it. Along with this, we'll import a few helper classes: the `ModelFilter` and `ModelSearchArguments`


```python
from huggingface_hub import HfApi, ModelFilter, ModelSearchArguments

api = HfApi()
# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft‚Äôs ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face‚Äôs [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
## What can you find on the Hugging Face Hub?

### Models 

The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.

### Datasets
The Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.

### Spaces

Hugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications.
Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.

Hugging Face Infinity consists of 2 main services:
* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.
* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.

The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.

<br>
<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Product overview"" src=""assets/46_infinity_cpu_performance/overview.png""></medium-zoom>
  <figcaption>Figure 1. Infinity Overview</figcaption>
</figure>
<br>
### Our training experience
We trained the model for 3 epochs (this means that the batch of 100K images were shown to the model 3 times) and a batch size of 4 (each step shows 4 images to the model). This turned out to be excessive and overfit (so it forgot concepts that diverge a bit of a real face, so for example ""shrek"" or ""a cat"" in the prompt would not make a shrek or a cat but rather a person, and also started to ignore styles). 

With just 1 epoch (so after the model ""saw"" 100K images), it already converged to following the poses and not overfit. So it worked, but... as we used the face synthetics dataset, the model ended up learning uncanny 3D-looking faces, instead of realistic faces. This makes sense given that we used a synthetic face dataset as opposed to real ones, and can be used for fun/memetic purposes. Here is the [uncannyfaces_25K](https://huggingface.co/multimodalart/uncannyfaces_25K) model.
--
title: ""An overview of inference solutions on Hugging Face""
thumbnail: /blog/assets/116_inference_update/widget.png
authors:
- user: juliensimon
---

# An Overview of Inference Solutions on Hugging Face



Every day, developers and organizations are adopting models hosted on [Hugging Face](https://huggingface.co/models) to turn ideas into proof-of-concept demos, and demos into production-grade applications. For instance, Transformer models have become a popular architecture for a wide range of machine learning (ML) applications, including natural language processing, computer vision, speech, and more. Recently, diffusers have become a popular architecuture for text-to-image or image-to-image generation. Other architectures are popular for other tasks, and we host all of them on the HF Hub!
1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (Microsoft Research Asia ÏóêÏÑú) Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei Ïùò [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (AllenAI ÏóêÏÑú) Iz Beltagy, Matthew E. Peters, Arman Cohan Ïùò [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (Meta AI ÏóêÏÑú) Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv√© J√©gou, Matthijs Douze Ïùò [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.

## Usage Tips

1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.
2. Address slow response times or performance issues by changing model endpoints to lower resource models.

## Conclusion

The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!
--
title: Getting Started with Hugging Face Inference Endpoints
thumbnail: /blog/assets/109_inference_endpoints/endpoints05.png
authors:
- user: juliensimon
---

# Getting Started with Hugging Face Inference Endpoints


Training machine learning models has become quite simple, especially with the rise of pre-trained models and transfer learning. OK, sometimes it's not *that* simple, but at least, training models will never break critical applications, and make customers unhappy about your quality of service. Deploying models, however... Yes, we've all been there.

Deploying models in production usually requires jumping through a series of hoops. Packaging your model in a container, provisioning the infrastructure, creating your prediction API, securing it, scaling it, monitoring it, and more. Let's face it: building all this plumbing takes valuable time away from doing actual machine learning work. Unfortunately, it can also go awfully wrong.
<Tip>
‚ö†Ô∏è The Hugging Face Hub is not limited to Transformer models. Anyone can share any kind of models or datasets they want! <a href=""https://huggingface.co/join"">Create a huggingface.co</a> account to benefit from all available features!
</Tip>

Before diving into how Transformer models work under the hood, let's look at a few examples of how they can be used to solve some interesting NLP problems.

## Working with pipelines[[working-with-pipelines]]

<Youtube id=""tiZFewofSLM"" />

The most basic object in the ü§ó Transformers library is the `pipeline()` function. It connects a model with its necessary preprocessing and postprocessing steps, allowing us to directly input any text and get an intelligible answer:

```python
from transformers import pipeline

classifier = pipeline(""sentiment-analysis"")
classifier(""I've been waiting for a HuggingFace course my whole life."")

Here is the question: What is the role of Hugging Face‚Äôs prebuilt models in simplifying multimodal applications?
Here is answer 1:To limit customization options for users
Here is answer 2: To provide ready-to-use solutions for diverse tasks
Here is answer3: To focus solely on text-based NLP tasks
Here is answer 4: To skip multimodal tasks for simplicity",Based on my analysis I think Answer2 To provide ready-to-use solutions
What strategy ensures smooth adaptation of Transformer models to new languages?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (5156, '3d5eb54f-ac2b-4444-987d-f6c1cd8201a3'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (5202, 'ffdc2e4f-ce95-48dd-8a87-2c908bbecdcf'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (30822, '1357395c-e339-49a4-9470-5057472025be'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (18550, '01e230dd-8782-4f9f-82af-2058aa9bd4d9'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1192, '48d5754e-ee39-4825-aa44-fc362b042ee3'), (30892, 'c7c7e937-8715-4300-9fb3-896a9568ab94'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (694, 'd3184d8d-7c16-43e6-9fb4-290933e9ad75'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (4796, '8f601501-25a6-4972-aa7a-aa9e072baede'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (30922, '150e7f7e-438f-472c-b0d2-d43c51ed72c0'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (17620, 'd7bec209-56a8-465d-93e3-d0e8a694edef'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (24287, '0f309be3-9d92-421d-bae1-8bfa12a4840d'), (11494, '8db96499-6302-45aa-bca4-bce4bec72dee'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1773, 'e07fbbd8-16f9-4b35-bbb3-b1b9950618b0'), (7919, '14a01558-963d-44f8-a1fc-7a903095967b'), (18161, '661008d6-f852-47ee-a976-37ed2665fd5e'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (18170, 'c664e159-5f97-4cd3-9176-bc322662a4ba'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (13578, 'cb671a15-7afc-42fd-8983-3f6a6aa4e48e'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (13587, '05b7e6a6-0d90-4abc-b1d2-dfabde4133eb'), (277, '91e3b387-1104-48c6-98d8-5d391b18159f'), (15132, '8dd8343f-285b-436f-84a7-620e9144edce'), (286, 'da6a4ed0-790f-4f58-aa39-6df5013da3f6'), (16681, 'fc9a2a86-bdfe-4b9d-8e65-72359a9c1b04'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (13642, '092cdf28-d7d0-42cf-9aef-cd07972bc95c'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (8017, 'f56d8d5f-9293-4c6b-8217-c1fb077fbe01'), (8020, '4658bd1d-e022-460e-9d6d-af439960931b'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7006, 'ca729c38-08a8-487f-85b8-173e988c8e43'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (9097, '4aa5ab2b-ea90-4dbc-8bca-d4ab9ed22c7e'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (9106, '0d5f7175-c2cd-4335-9084-dcdbb3bf43a5'), (20884, 'f94ec739-e144-4f11-9a61-c704eb112ba1'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (10137, 'ce88e225-95d2-48d7-9020-7d8644986a94'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (31161, 'e6392f6d-29c2-4b5e-99d0-0af8c82a0670'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (23477, '9318ef30-7329-4410-bb07-65dba4f0a1f2'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (6614, '8adaa619-97b2-446b-bc9d-86d585ad4d68'), (18391, 'f9a3fbd7-5a63-4f5d-a344-2656a7553d4d'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (5088, 'f6c7fccc-2d9b-4fb8-abb5-8bacb17044c0'), (18400, 'ef9b2afc-e962-412d-a475-22cc316750d3'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (21476, 'fc40dcb8-cfe2-4449-a2ff-c6b8962d8db5'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (5097, '2d07dc17-5032-44db-97e2-731bf7bfca9b'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (18429, '4f938c80-c628-4d41-b604-430b07615100')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Up until now, we've mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in [Chapter 1](/course/chapter1), this is commonly referred to as _transfer learning_, and it's a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, we'll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHub's Copilot, powered by
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
<p>In 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP).</p>
<p>Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing <a href=""/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum"" rel=""noopener"" target=""_blank"">Hugging Face Optimum Graphcore library</a>).</p>
## QLoRA paper, a new way of democratizing quantized large transformer models

In few words, QLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning. This method enables 33B model finetuning on a single 24GB GPU and 65B model finetuning on a single 46GB GPU.

More specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. Read more about LoRA in the [original LoRA paper](https://arxiv.org/abs/2106.09685).
## Overview

The Transformer-XL model was proposed in [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context](https://arxiv.org/abs/1901.02860) by Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan
Salakhutdinov. It's a causal (uni-directional) transformer with relative positioning (sinuso√Ødal) embeddings which can
reuse previously computed hidden-states to attend to longer context (memory). This model also uses adaptive softmax
inputs and outputs (tied).

The abstract from the paper is the following:
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these
findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the
number of parameters at the fine-tuning stage.*
There are many ways to go from one precision to another, with many different ""translation"" schemes existing, each with its own benefits and drawbacks. Popular approaches include [bitsandbytes](https://huggingface.co/papers/2208.07339), [GPTQ](https://huggingface.co/papers/2210.17323), and [AWQ](https://huggingface.co/papers/2306.00978). Some users, such as [TheBloke](https://huggingface.co/TheBloke), are even converting popular models to make them accessible to the community. All are very recent and still developing, and we hope to see even more progress on this as time goes on.


## What's next?
The year is not over yet! And these final ~~months~~ ~~days~~ hours have already come with the share of surprises: will a new architecture finally overperform the simple and efficient Transformer?
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# X-MOD

## Overview

The X-MOD model was proposed in [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.
X-MOD extends multilingual masked language models like [XLM-R](xlm-roberta) to include language-specific modular components (_language adapters_) during pre-training. For fine-tuning, the language adapters in each transformer layer are frozen.

The abstract from the paper is the following:
The [Transformer architecture](https://arxiv.org/abs/1706.03762) was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:

- **June 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results

- **October 2018**: [BERT](https://arxiv.org/abs/1810.04805), another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)

- **February 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns
promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.*
{/if}

For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results. 

However, there are a few cases where you'll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
*Most widely-used pre-trained language models operate on sequences of tokens corresponding to word or subword units.
Encoding text as a sequence of tokens requires a tokenizer, which is typically created as an independent artifact from
the model. Token-free models that instead operate directly on raw text (bytes or characters) have many benefits: they
can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by
removing complex and error-prone text preprocessing pipelines. Since byte or character sequences are longer than token
sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of
operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with
minimal modifications to process byte sequences. We carefully characterize the trade-offs in terms of parameter count,
### Scaling up on other modalities

Currently, language models dominate very large models. Leveraging this method on very large vision, audio, and multi-modal models might be an interesting thing to do for better accessibility in the coming years as these models become more accessible.

## Credits

Huge thanks to the following who contributed to improve the readability of the article as well as contributed in the integration procedure in `transformers` (listed in alphabetic order):
JustHeuristic (Yozh),
Michael Benayoun,
Stas Bekman,
Steven Liu,
Sylvain Gugger,
Tim Dettmers
Another popular tool among researchers to pre-train large transformer models is [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), a powerful framework developed by the Applied Deep Learning Research team at NVIDIA. Unlike `accelerate` and the `Trainer`, using Megatron-LM is not straightforward and can be a little overwhelming for beginners. But it is highly optimized for the training on GPUs and can give some speedups. In this blogpost,  you will learn how to train a language model on NVIDIA GPUs in Megatron-LM, and use it with `transformers`.

We will try to break down the different steps for training a GPT2 model in this framework, this includes:
* Environment setup
* Data preprocessing
* Training
* Model conversion to ü§ó Transformers

## Why Megatron-LM?
1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.
1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.
1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.
1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.
1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.
1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.
1. **[LiLT](https://huggingface.co/docs/transformers/model_doc/lilt)** (from South China University of Technology) released with the paper [LiLT: A Simple yet Effective Language-Independent Layout Transformer for Structured Document Understanding](https://arxiv.org/abs/2202.13669) by Jiapeng Wang, Lianwen Jin, Kai Ding.
1. **[LLaMA](https://huggingface.co/docs/transformers/model_doc/llama)** (from The FAIR team of Meta AI) released with the paper [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971) by Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.

Here is the question: What strategy ensures smooth adaptation of Transformer models to new languages?
Here is answer 1:Limiting adaptation to pre-trained English models
Here is answer 2: Using multilingual checkpoints and APIs for preprocessing
Here is answer3: Ignoring language-specific preprocessing requirements
Here is answer 4: Focusing on monolingual tasks for simplicity",Answer3
How can Transformer models be trained effectively for multimodal applications?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (5152, '2eb1dd24-f252-4573-b700-3d513da59540'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (28715, 'aaa48597-36d7-4f2f-8b37-6eafd713a6bf'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (10840, '17512ae9-2273-4239-be81-2a79343a68e4'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (17510, '4186dada-bac9-4760-b476-16d67e9b4f11'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (30392, '46096e11-0c19-408f-be8e-bf3b9aa0c761'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (9415, '78661659-8f11-4005-800b-e1d23d747852'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (6355, '9cf0e1e3-ca85-4c73-afc6-a71c99755175'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (4861, '79210cef-fada-44fb-a563-86b3d8b879d2'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (16674, 'a05d2b01-8fd3-48e1-8077-85f9ef33864d'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (18222, '5880432c-3d6d-4287-b3fe-745b7ed4b67f'), (23859, 'e47fd052-4097-479b-95f3-f65bc65a3c2c'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (4414, '0070b17d-3e37-4b4d-b0e4-6d764598d46e'), (4415, 'b9a9a85e-e4e0-4cb7-b45f-4ec40e2891b6'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (13639, 'af509e72-cf08-461e-813f-b0e762ebdbfa'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (17741, '28a0fb96-9ba3-48c6-a7bc-158aa6526cb7'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (338, '83c0ff2b-59c2-4ffc-bf85-93ed5abbf667'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (11115, 'a4f77954-5dd3-4d2c-a0a8-d2241981c438'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (22918, '201f6149-0a67-49ea-81bd-e07891bdb9c1'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (9161, 'cc82182d-8b0c-4897-9bfe-8cc5080325ee'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (27636, '586ca969-ca44-4a60-97ef-9aadde4772b9'), (18429, '4f938c80-c628-4d41-b604-430b07615100'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Use this document as your starting point to navigate further to the methods that match your scenario.

## Training

Training large transformer models efficiently requires an accelerator such as a GPU or TPU. The most common case is where 
you have a single GPU. The methods that you can apply to improve training efficiency on a single GPU extend to other setups 
such as multiple GPU. However, there are also techniques that are specific to multi-GPU or CPU training. We cover them in 
separate sections.
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
The abstract from the paper is the following:

*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.*
-->

# Preprocess

[[open-in-colab]]

Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. ü§ó Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you'll learn that for:

* Text, use a [Tokenizer](./main_classes/tokenizer) to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.
* Speech and audio, use a [Feature extractor](./main_classes/feature_extractor) to extract sequential features from audio waveforms and convert them into tensors.
* Image inputs use a [ImageProcessor](./main_classes/image) to convert images into tensors.
* Multimodal inputs, use a [Processor](./main_classes/processors) to combine a tokenizer and a feature extractor or image processor.

<Tip>
## Optimizer choice

The most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay). Adam achieves 
good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory 
footprint of the order of the number of model parameters. To remedy this, you can use an alternative optimizer. 
For example if you have [NVIDIA/apex](https://github.com/NVIDIA/apex) installed for NVIDIA GPUs, or [ROCmSoftwarePlatform/apex](https://github.com/ROCmSoftwarePlatform/apex) for AMD GPUs, `adamw_apex_fused` will give you the
fastest training experience among all supported AdamW optimizers.

[`Trainer`] integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, 
`adamw_apex_fused`, `adamw_anyprecision`, `adafactor`, or `adamw_bnb_8bit`. More optimizers can be plugged in via a third-party implementation.
he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these
findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the
number of parameters at the fine-tuning stage.*
```


## Processor

For models that support multimodal tasks, ü§ó Transformers offers a processor class that conveniently wraps processing classes such as a feature extractor and a tokenizer into a single object. For example, let's use the [`Wav2Vec2Processor`] for an automatic speech recognition task (ASR). ASR transcribes audio to text, so you will need a feature extractor and a tokenizer.

Create a feature extractor to handle the audio inputs:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

Create a tokenizer to handle the text inputs:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=""my_vocab_file.txt"")
```

Combine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
Introduction[[introduction]]

<CourseFloatingBanner
    chapter={2}
    classNames=""absolute z-10 right-0 top-0""
/>

As you saw in [Chapter 1](/course/chapter1), Transformer models are usually very large. With millions to tens of *billions* of parameters, training and deploying these models is a complicated undertaking. Furthermore, with new models being released on a near-daily basis and each having its own implementation, trying them all out is no easy task.

The ü§ó Transformers library was created to solve this problem. Its goal is to provide a single API through which any Transformer model can be loaded, trained, and saved. The library's main features are:
The abstract from the paper is the following:

*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its
applications to computer vision remain limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional networks while keeping their overall
structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of
data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),
Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring
substantially fewer computational resources to train.*
The abstract from the paper is the following:

*While the Transformer architecture has become the de-facto standard for natural language processing tasks, its
applications to computer vision remain limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional networks while keeping their overall
structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of
data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.),
Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring
substantially fewer computational resources to train.*
## Introduction

After the rise of large transformer models in 2018 and 2019, two trends have quickly emerged to bring their compute requirements down. First, conditional computation, quantization, distillation, and pruning have unlocked inference of large models in compute-constrained environments; we‚Äôve already touched upon this in part in our [last reading group post](https://discuss.huggingface.co/t/hugging-face-reads-01-2021-sparsity-and-pruning/3144). The research community then moved to reduce the cost of pre-training.

In particular, one issue has been at the center of the efforts: the quadratic cost in memory and time of transformer models with regard to the sequence length. In order to allow efficient training of very large models, 2020 saw an onslaught of papers to address that bottleneck and scale transformers beyond the usual 512- or 1024- sequence lengths that were the default in NLP at the start of the year.
[Image Transformer](https://arxiv.org/abs/1802.05751) uses transformers by restricting self attention over local neighborhoods of pixels.

[Taming Transformers](https://arxiv.org/abs/2012.09841) and [DALL-E 1](https://arxiv.org/abs/2102.12092) combine convolutions and transformers. Both train a VQ-VAE to learn a discrete latent space, and then a transformer is trained in the compressed latent space. The transformer context is global but masked, because attention is provided over all previously predicted latent pixels, but the model is still AR so attention cannot be provided over not yet predicted pixels.
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model -- one as close as possible to the task you have at hand -- and fine-tune it.

## General architecture[[general-architecture]]

In this section, we'll go over the general architecture of the Transformer model. Don't worry if you don't understand some of the concepts; there are detailed sections later covering each of the components.

<Youtube id=""H39Z_720T5s"" />

## Introduction[[introduction]]

The model is primarily composed of two blocks:

* **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
* **Decoder (right)**: The decoder uses the encoder's representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.
Today, [Habana¬Æ Labs](https://habana.ai/), a pioneer in high-efficiency, purpose-built deep learning processors, and Hugging Face, the home of [Transformer](https://github.com/huggingface/transformers) models, are happy to announce that they‚Äôre joining forces to make it easier and quicker to train high-quality transformer models. Thanks to the integration of Habana‚Äôs [SynapseAI software suite](https://habana.ai/training-software/) with the Hugging Face [Optimum open-source library](https://github.com/huggingface/optimum), data scientists and machine learning engineers can now accelerate their Transformer training jobs on Habana processors with just a few lines of code and enjoy greater productivity as well as lower training cost.

Here is the question: How can Transformer models be trained effectively for multimodal applications?
Here is answer 1:Focus only on pre-trained checkpoints for single-modality tasks
Here is answer 2: Combine task-specific fine-tuning with multimodal evaluation techniques
Here is answer3: Use default training pipelines without adaptation for multimodal tasks
Here is answer 4: Avoid using multimodal applications for simplicity",The correct answer is: Combine task-specific fine-tuning with multimodal evaluation techniques.
What is the most efficient way to overcome deployment challenges for cross-framework models?,"[(13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (20041, '0521172d-93c3-47ab-8eb4-57b9a744de8f'), (3660, '88c885c0-729f-40d9-93d3-ff9016d1a1b1'), (18514, '2b8c705b-aa16-4343-9147-042e810a70a2'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (7768, '224a5adb-2c8c-434b-b5b5-b6fab137f1b8'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (25178, '9c7f8ead-1eb7-4e8d-a89f-fb64c773bf7a'), (7774, '5824b2b4-f100-4de8-a61c-bf6e28cb0bf5'), (14436, 'b6c8ba09-18f8-4b76-9196-bdead257e416'), (20069, 'c53fb95e-58c9-41b3-aa8c-8ba29cb514f0'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (1671, 'b6fe0d87-4506-41f4-86d6-8a5340995e6e'), (10389, 'a8a284e8-5f91-4b84-ae4b-bc995fb2ccc7'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (1181, '2e574621-b2e7-4ffb-ad69-7aa65d1e16fb'), (23211, '3ee17899-9fcb-44fb-957a-6db593a21bcd'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (3769, '4d65ee65-320a-476b-a100-ef50c8da088d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (4286, 'ed0e9a8c-e075-40db-b145-d300b7f9cea7'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (23749, 'ebbc939a-6402-4c02-9891-1531598d8076'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (22225, '33434414-721b-423e-9aef-6fa82d028a76'), (19669, 'aee04d72-b4ef-45e8-8857-bc8147c6672b'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (7901, '17026dcd-41fe-493c-8a87-b0ddc5ea48cb'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (10998, '633599a3-8284-4a4a-987b-3f170a7e4efb'), (30971, '9aa5f25f-1f19-4531-9cce-e0cb0e665e5d'), (20223, 'a0ff3a3b-b5b7-4fcf-9f9d-68877d1cc4ad'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (3332, '09beafc1-1176-4db4-8ac6-78b13db9cee4'), (7433, '8b4a851e-9dbc-46a4-a7e6-6b51652c06b6'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (4366, 'fa03c662-5522-4732-a0ba-6b673062d9af'), (30481, '54d986c8-ef1b-46f2-ab54-283f310f483f'), (1301, '5137aed7-d723-4ecf-8874-26a7c9ecb926'), (29465, '9283c3d4-1b53-4b60-baf2-ee5185c2fba6'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (27460, '42d93388-c034-4d39-b6e2-99a5343894ac'), (9553, '7bf59ff6-d752-4eb0-9dc8-8aacda143835'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (27481, '670fac96-da41-4462-a2f5-621a35d75778'), (347, 'd614507c-0464-48b7-bad7-c25df4c56372'), (27483, '640acfcf-de9c-46e8-b825-bb59cbffb305'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (352, '756c346a-bd77-440b-b6a9-1fabbf0c457d'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (349, '553502e0-737e-4e0f-b1f2-49ba04754d1c'), (21351, '02e9b92c-b01a-410a-842a-c87b08998d9b'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (3444, '6a782a81-fb1e-4fae-9d6e-1398776f3c09'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (19842, '177c114f-7a6c-4116-9c04-d60f549b7fac'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (7050, '09c789c8-d58e-472f-ac73-8ac7722c751d'), (15762, '4e4f666b-dc03-4e9c-8dfe-2ccbb5a5c56b'), (22418, '1a9db661-e116-4d00-ae23-3f4205ad8f32'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (408, 'e160735e-ba9e-4144-a8f4-5ccdb5009af7'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (411, '96253f57-9a1a-4672-829b-a8e8241df3cf'), (29599, '143208c5-54d7-4e6b-ba1d-40ad953a9c0d'), (6561, 'ee35c81c-c680-4dfc-852b-7c11aae2a074'), (24485, '4de8b02f-cf9b-40a4-9212-1d8e11cd76f5'), (14758, '1b2c9206-d61c-4339-9c05-bb827322695d'), (6056, 'aba34742-67ab-47e8-a239-b5b49e9d199d'), (23469, '0f1f930e-b230-42f4-ba5c-b6d73abc59f0'), (23470, 'dfeab081-8379-4027-83f7-a17a7573a82e'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (16314, 'ac218a43-1383-42ff-a664-aee7ca92286e'), (24512, 'd2884679-070d-46c2-901f-85d4e0fd09b0'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (21962, '4ed9bb78-d26f-490e-ba3a-60bd868eff98'), (27083, 'fd3ceb3f-d838-4aa0-b054-2fcdd5929735'), (19403, 'c0f08689-c366-48f2-a497-c82f608735e2'), (21964, 'a593fe2e-a9b6-4481-b26e-cfbd6438142e'), (6096, 'b1db7fca-c845-4de6-a30d-f88d2ad8689d'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (21972, '85cbe3b6-6131-4a4a-9548-070d1c3b8c2d'), (15836, 'f6ad3be6-00de-45fb-8497-71c9ae81f26b'), (21980, '9654551a-62fd-46b7-91c6-1b2ce272aaec'), (24030, '5237b189-9b15-48b1-b842-3eaeb2062ead'), (15839, 'b5628360-db2f-4751-af66-aeb6d1d1d74a'), (13280, '2c3ad26d-8418-4d01-a881-e5dd6be54e3f'), (15841, '73c3ef6b-2bae-40e2-a3de-ab7157a99930'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (3049, '0b0dfb40-acc7-4fe9-a526-43495756e63c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (3050, '7c7b731c-b363-44e1-8e4d-ab75aa50b634'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (13303, 'e6783d90-3750-4204-9c37-e9973ef7bf05'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (510, '95ccec79-916b-4e98-99da-16350eb58533'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: In this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment:

1.  **Lower Precision**: Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.

2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.
1. [Easy Deployment](https://huggingface.co/docs/inference-endpoints/index): Deploy models as production-ready APIs with just a few clicks, eliminating the need to handle infrastructure or MLOps.
2. [Cost Efficiency](https://huggingface.co/docs/inference-endpoints/autoscaling): Benefit from automatic scale to zero capability, reducing costs by scaling down the infrastructure when the endpoint is not in use, while paying based on the uptime of the endpoint, ensuring cost-effectiveness.
3. [Enterprise Security](https://huggingface.co/docs/inference-endpoints/security): Deploy models in secure offline endpoints accessible only through direct VPC connections, backed by SOC2 Type 2 certification, and offering BAA and GDPR data processing agreements for enhanced data security and compliance.
1. [Easy Deployment](https://huggingface.co/docs/inference-endpoints/index): Deploy models as production-ready APIs with just a few clicks, eliminating the need to handle infrastructure or MLOps.
2. [Cost Efficiency](https://huggingface.co/docs/inference-endpoints/autoscaling): Benefit from automatic scale to zero capability, reducing costs by scaling down the infrastructure when the endpoint is not in use, while paying based on the uptime of the endpoint, ensuring cost-effectiveness.
3. [Enterprise Security](https://huggingface.co/docs/inference-endpoints/security): Deploy models in secure offline endpoints accessible only through direct VPC connections, backed by SOC2 Type 2 certification, and offering BAA and GDPR data processing agreements for enhanced data security and compliance.
As models get bigger and bigger, deploying them into production to run inference has become increasingly challenging. Both hardware and software have seen a lot of innovations to address these challenges, so let's dive in to see how to efficiently overcome them!


## BLOOMZ

[BLOOM](https://arxiv.org/abs/2211.05100) is a 176-billion-parameter autoregressive model that was trained to complete sequences of text. It can handle 46 different languages and 13 programming languages. Designed and trained as part of the [BigScience](https://bigscience.huggingface.co/) initiative, BLOOM is an open-science project that involved a large number of researchers and engineers all over the world. More recently, another model with the exact same architecture was released: [BLOOMZ](https://arxiv.org/abs/2211.01786), which is a fine-tuned version of BLOOM on several tasks leading to better generalization and zero-shot[^1] capabilities.
## Solving Domain Challenges and Constraints by Partnering with Hugging Face

There are a number of unique challenges we face within our domain, including entropy injected by merchants, processing/payment companies, institutional differences, and shifts in user behavior. Designing and building efficient model performance alerting along with realistic benchmarking datasets has proven to be an ongoing challenge. Another significant hurdle is determining the optimal number of classes for our system - each class represents a significant amount of effort to create and maintain. Therefore, we must consider the value it provides to users and our business.
Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the way for their deployment on inference endpoints
serving multiple requests in parallel.

There is still plenty of room for improvement though:
- in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory.
Alternative options such as pipelining are currently integrated,
- the static sequence length limits the model ability to encode long contexts. It would be interesting to see if attention sinks might be a valid option to address this.
```

You can then deploy the model on Kubernetes.

## What‚Äôs next?

[BentoML](https://github.com/bentoml/BentoML) provides a powerful and straightforward way to deploy Hugging Face models for production. With its support for a wide range of ML frameworks and easy-to-use APIs, you can ship your model to production in no time. Whether you‚Äôre working with the DeepFloyd IF model or any other model on the Hugging Face Model Hub, BentoML can help you bring your models to life.

Check out the following resources to see what you can build with BentoML and its ecosystem tools, and stay tuned for more information about BentoML.
If you are interested in all of the details, we recommend you to dive deep into the provided raw data. 

## Recommendations & Insights

Based on the benchmark, we provide specific recommendations for optimal LLM deployment depending on your priorities between cost, throughput, and latency for all Llama 2 model sizes. 

*Note: The recommendations are based on the configuration we tested. In the future, other environments or hardware offerings, such as Inferentia2, may be even more cost-efficient.*

### Most Cost-Effective Deployment

The most cost-effective configuration focuses on the right balance between performance (latency and throughput) and cost. Maximizing the output per dollar spent is the goal. We looked at the performance during 5 concurrent requests. We can see that GPTQ offers the best cost-effectiveness, allowing customers to deploy Llama 2 13B on a single GPU.
* [Inference Endpoints](https://huggingface.co/docs/inference-endpoints/index) to easily deploy models on dedicated infrastructure managed by Hugging Face.

  * Our open-source optimization libraries, [ü§ó Optimum Intel](https://huggingface.co/blog/openvino) and [ü§ó Optimum ONNX Runtime](https://huggingface.co/docs/optimum/main/en/onnxruntime/overview), to get the highest efficiency out of training and running models for inference.

  * Hugging Face [Expert Acceleration Program](https://huggingface.co/support), a commercial service for Hugging Face experts to work directly with your team to accelerate your Machine Learning roadmap and models.
</div>


## Introduction
```
</tf>
</frameworkcontent>

## Pipeline

<Youtube id=""tiZFewofSLM""/>

The [`pipeline`] is the easiest and fastest way to use a pretrained model for inference. You can use the [`pipeline`] out-of-the-box for many tasks across different modalities, some of which are shown in the table below:

<Tip>

For a complete list of available tasks, check out the [pipeline API reference](./main_classes/pipelines).

</Tip>
Keywords: Keras wrapper, Model building, Training, Deployment

## [FastDeploy](https://github.com/PaddlePaddle/FastDeploy)

[FastDeploy](https://github.com/PaddlePaddle/FastDeploy) is an Easy-to-use and High Performance AI model deployment toolkit for Cloud, Mobile and Edge with packageout-of-the-box and unified experience, endend-to-end optimization for over fire160+ Text, Vision, Speech and Cross-modal AI models. Including image classification, object detection, OCR, face detection, matting, pp-tracking, NLP, stable diffusion, TTS and other tasks to meet developers' industrial deployment needs for multi-scenario, multi-hardware and multi-platform.

Keywords: Model deployment, CLoud, Mobile, Edge

## [underthesea](https://github.com/undertheseanlp/underthesea)
- Most Cost-Effective Deployment: For users looking for good performance at low cost
- Best Latency Deployment: Minimizing latency for real-time services
- Best Throughput Deployment: Maximizing tokens processed per second

To keep this benchmark fair, transparent, and reproducible, we share all of the assets, code, and data we used and collected: 

- [GitHub Repository](https://github.com/philschmid/text-generation-inference-tests/tree/master/sagemaker_llm_container)
- [Raw Data](https://github.com/philschmid/text-generation-inference-tests/tree/master/results/sagemaker)
- [Spreadsheet with processed data](https://docs.google.com/spreadsheets/d/1PBjw6aG3gPaoxd53vp7ZtCdPngExi2vWPC0kPZXaKlw/edit?usp=sharing)

We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Before we get into the benchmark and data, let's look at the technologies and methods we used.
scalable. ‚ÄúBy using Amazon SageMaker, we have a simple way to scale up our systems, especially for inference and runtime,‚Äù says Sam Corzine, ML engineer at Fetch. Meanwhile, standardized model deployments mean less manual work.
Keywords: BentoML, Framework, Deployment, AI Applications

## [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)

[LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning) offers a user-friendly fine-tuning framework that incorporates PEFT. The repository includes training(fine-tuning) and inference examples for LLaMA-2, BLOOM, Falcon, Baichuan, Qwen, and other LLMs. A ChatGLM version is also available in [ChatGLM-Efficient-Tuning](https://github.com/hiyouga/ChatGLM-Efficient-Tuning).

Keywords: PEFT, fine-tuning, LLaMA-2, ChatGLM, Qwen
While local deployments are an excellent head start to building
something useful, you‚Äôd need to perform deployments that can serve many
users in real-life projects. In this post, you‚Äôll learn how to scale the
local deployment from the previous post with Docker and Kubernetes.
Therefore, we assume some familiarity with Docker and Kubernetes.

This post builds on top of the [<u>previous post</u>](https://huggingface.co/blog/tf-serving-vision), so, we highly
recommend reading it first. You can find all the code
discussed throughout this post in [<u>this repository</u>](https://github.com/sayakpaul/deploy-hf-tf-vision-models/tree/main/hf_vision_model_onnx_gke).

# Why go with Docker and Kubernetes?

The basic workflow of scaling up a deployment like ours includes the
following steps:

- **Containerizing the application logic**: The application logic
  involves a served model that can handle requests and return
  predictions. For containerization, Docker is the industry-standard
  go-to.
However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!
It‚Äôs important to be clear about the major limitation of XLA, though: XLA expects input shapes to be static. This means that if your task involves variable sequence lengths, you will need to run a new XLA compilation for each different input shape you pass to your model, which can really negate the performance benefits! You can see some examples of how we deal with this in our [TensorFlow notebooks](https://huggingface.co/docs/transformers/notebooks) and in the XLA generation blogpost above.

#### Philosophy #6: Deployment is just as important as training

TensorFlow has a rich ecosystem, particularly around model deployment, that the other more research-focused frameworks lack. We‚Äôre actively working on letting you use those tools to deploy your whole model for inference. We're particularly interested in supporting `TF Serving` and `TFX`. If this is interesting to you, please check out [our blogpost on deploying models with TF Serving](https://huggingface.co/blog/tf-serving-vision)!
We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Before we get into the benchmark and data, let's look at the technologies and methods we used. 

- [Llama 2 on Amazon SageMaker a Benchmark](#llama-2-on-amazon-sagemaker-a-benchmark)
    - [What is the Hugging Face LLM Inference Container?](#what-is-the-hugging-face-llm-inference-container)
    - [What is Llama 2?](#what-is-llama-2)
    - [What is GPTQ?](#what-is-gptq)
  - [Benchmark](#benchmark)
  - [Recommendations \& Insights](#recommendations--insights)
    - [Most Cost-Effective Deployment](#most-cost-effective-deployment)
    - [Best Throughput Deployment](#best-throughput-deployment)
    - [Best Latency Deployment](#best-latency-deployment)
  - [Conclusions](#conclusions)


### What is the Hugging Face LLM Inference Container?
<Tip>

While the main concepts discussed in this guide are likely applicable across frameworks, here we focus on 
PyTorch-based implementations.

</Tip>

Before diving deeper into the specifics of each technique, let's go over the rough decision process when training 
large models on a large infrastructure.

## Scalability strategy

Begin by estimating how much vRAM is required to train your model. For models hosted on the ü§ó Hub, use our 
[Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage), which gives you 
accurate calculations within a few percent margin.  

**Parallelization strategy for a single Node / multi-GPU setup**

When training a model on a single node with multiple GPUs, your choice of parallelization strategy can significantly 
impact performance. Here's a breakdown of your options:

**Case 1: Your model fits onto a single GPU**

If your model can comfortably fit onto a single GPU, you have two primary options:
```

```python out
""""""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
""""""
```

Nice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won't always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the [Hugging Face forums](https://discuss.huggingface.co/) that can help you out! In the next section, we'll take a look at how you can craft good forum questions that are likely to get answered.

Here is the question: What is the most efficient way to overcome deployment challenges for cross-framework models?
Here is answer 1:Focus on framework-specific deployment without adaptation
Here is answer 2: Leverage APIs for cross-framework compatibility and resolve architecture conflicts
Here is answer3: Deploy models without addressing compatibility issues
Here is answer 4: Use pretrained models for specific frameworks only","Based upon provided context ,the most accurate answer would be :

Answer : Leverage APIs voor cross-framework compatibilityand resolve architecture conflicts"
What approach best simplifies Transformer evaluation across multimodal benchmarks?,"[(4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (18434, 'd59e6b88-c090-403a-9dfa-aab7f0b5b516'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (5152, '2eb1dd24-f252-4573-b700-3d513da59540'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (26156, 'cb06116c-89a8-4cb8-ae81-f3734a853307'), (27229, '79b37341-3681-4528-9ab1-2ab8778b564d'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (4223, '386573d6-bd83-4bd5-824c-7fbdd3122bad'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (25784, '7cb641f4-1b19-4387-a006-38511a58f088'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (14024, 'bf6710a5-977c-4793-87fc-584e835dc19f'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (6355, '9cf0e1e3-ca85-4c73-afc6-a71c99755175'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (4841, '892f344e-b28c-4da5-8ff7-3aec14ffd554'), (752, '3828e3bc-8304-4f32-a0c4-ebe5bd0b19fd'), (15605, '9f241ce4-3d04-4868-8576-832003581357'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (7943, 'dd28dc4a-920f-4fc9-b713-5719ff38f1a8'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (18202, '3299c4a6-a3ea-461c-82a7-0fafcfde3c68'), (2843, '2e20db58-f224-4700-9774-c63622aa8a4d'), (18222, '5880432c-3d6d-4287-b3fe-745b7ed4b67f'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (13619, '3db87f08-ff12-4890-b601-12a7fa3ea0f5'), (21816, '5d5883c9-44d9-476e-bda0-c334c5912e4a'), (318, '860b95b8-d28b-4042-8c68-1fb6320b87c1'), (13639, 'af509e72-cf08-461e-813f-b0e762ebdbfa'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (338, '83c0ff2b-59c2-4ffc-bf85-93ed5abbf667'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (21927, '7ba35d9f-9182-4565-95a7-0ad47ed75750'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (30638, 'bb43e4ba-f6ab-478d-8c31-6535c42d1276'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (9161, 'cc82182d-8b0c-4897-9bfe-8cc5080325ee'), (11723, '6a66155a-e619-439b-96b5-4cef0a9ece1f'), (16335, '41fef57c-6a30-487e-9e0f-4d02276e4a7d'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (475, 'fc8680f8-2eee-4940-8f49-33e177da211a'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.*
LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence 
length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, 
for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable 
speed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at this https URL*
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
_Note: that we did not optimize the model for the GPU environment, the models were evaluated in fp32._

When it comes to benchmarking Transformer models, there are two metrics that are most adopted:
* **Latency**: the time it takes for the model to perform a single prediction (pre-process, prediction, post-process).
* **Throughput**: the number of executions performed in a fixed amount of time for one benchmark configuration

We looked at latency across different setups and models to understand the benefits and tradeoffs of the new Inferentia2 instance. If you want to run the benchmark yourself, we created a [Github repository](https://github.com/philschmid/aws-neuron-samples/tree/main/benchmark) with all the information and scripts to do so. 

### Results
- Abstract: Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. So, we now need a well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. I'll talk about a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically
a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on LRA. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.
A more comprehensive reproducible benchmark is available [here](https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark).


## Quantizing models **with the Optimum library**

To seamlessly integrate AutoGPTQ into Transformers, we used a minimalist version of the AutoGPTQ API that is available in [Optimum](https://github.com/huggingface/optimum), Hugging Face's toolkit for training and inference optimization. By following this approach, we achieved easy integration with Transformers, while allowing people to use the Optimum API if they want to quantize their own models! Check out the Optimum [documentation](https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization) if you want to quantize your own LLMs. 

Quantizing ü§ó¬†Transformers models with the GPTQ method can be done in a few lines:

```python
from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig
transformer with rotary position embedding, or RoFormer, achieves superior performance in tasks with long texts. We
release the theoretical analysis along with some preliminary experiment results on Chinese data. The undergoing
experiment for English benchmark will soon be updated.*
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
Use this document as your starting point to navigate further to the methods that match your scenario.

## Training

Training large transformer models efficiently requires an accelerator such as a GPU or TPU. The most common case is where 
you have a single GPU. The methods that you can apply to improve training efficiency on a single GPU extend to other setups 
such as multiple GPU. However, there are also techniques that are specific to multi-GPU or CPU training. We cover them in 
separate sections.
We have applied the same method to other Transformer architectures and released the weights:
- GPT2: on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT2 reaches a perplexity on the test set of 16.3 compared to 21.1 for **DistilGPT2** (after fine-tuning on the train set).
- RoBERTa: **DistilRoBERTa** reaches 95% of `RoBERTa-base`'s performance on GLUE while being twice faster and 35% smaller.
- German BERT: **German DistilBERT** reaches 99% of `bert-base-german-dbmdz-cased`'s performance on German NER (CoNLL-2003).
- Multilingual BERT: **DistilmBERT** reaches 92% of Multilingual BERT's performance on XNLI while being twice faster and 25% smaller. The model supports 104 languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).

For more information on DistilBERT, please refer to our [NeurIPS workshop paper](https://arxiv.org/abs/1910.01108).
*We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with
lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel
hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding,
thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution
differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from
different layers, and thus combining both local attention and global attention to render powerful representations. We
show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our
approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance
### That is super interesting and powerful.

**Lewis:** Maybe one thing to mention is that the whole evaluation question is a very subtle one. We know from previous benchmarks, such as SQuAD, a famous benchmark to measure how good models are at question answering, that many of these transformer models are good at taking shortcuts.

Well, that's the aim but it turns out that many of these transformer models are really good at taking shortcuts. So, what they‚Äôre actually doing is they're getting a very high score on a benchmark which doesn't necessarily translate into the actual thing you were interested in which was answering questions.

And you have all these subtle failure modes where the models will maybe provide completely wrong answers or they should not even answer at all. And so at the moment in the research community there's a very active and vigorous discussion about what role benchmarks play in the way we measure progress.
## Optimizer choice

The most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay). Adam achieves 
good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory 
footprint of the order of the number of model parameters. To remedy this, you can use an alternative optimizer. 
For example if you have [NVIDIA/apex](https://github.com/NVIDIA/apex) installed for NVIDIA GPUs, or [ROCmSoftwarePlatform/apex](https://github.com/ROCmSoftwarePlatform/apex) for AMD GPUs, `adamw_apex_fused` will give you the
fastest training experience among all supported AdamW optimizers.

[`Trainer`] integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, 
`adamw_apex_fused`, `adamw_anyprecision`, `adafactor`, or `adamw_bnb_8bit`. More optimizers can be plugged in via a third-party implementation.
One of the main reasons for the importance of increasing `nb_loops` is that the speech lengths generated vary greatly between different iterations, even with a fixed input.

 One consequence of this is that the latency measured by `measure_latency_and_memory_use` may not actually reflect the actual performance of optimization techniques! The benchmark at the end of the blog post reports the results averaged over 100 iterations, which gives a true indication of the performance of the model.

## 1. ü§ó Better Transformer

Better Transformer is an ü§ó Optimum feature that performs kernel fusion under the hood. This means that certain model operations will be better optimized on the GPU and that the model will ultimately be faster.
The abstract from the paper is the following:

*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.*
various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it
compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense
prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation
(53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and
+2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones.
The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures.*
You want to add a new model for `Better Transformer`, the fast path of PyTorch Transformer API? Check this guideline!

## Models that should be supported

In theory, any model that has a transformer encoder layer, similar to the classic encoder described in the [""Attention Is All You Need""](https://arxiv.org/abs/1706.03762) paper should be supported. 
More specifically, a model that has an encoder block with a MultiHead-Attention module (with pre or post-attention layer norm) should be convertible to its `BetterTransformer` equivalent. The conditions can be summarized as follows:

- Use classic Multi Head attention module (for example, [DeBERTa](https://arxiv.org/abs/2006.03654) cannot be supported)
- Use either `gelu` or `relu` activation function   
- Have an even number of attention heads
- Do not use any attention bias (for eg `T5` uses attention bias, therefore cannot be supported)
- `eps` must be equal between the first and second layer norms for each layer
## Conclusion

How do Transformer-based models compare against the above linear baseline? The test set MASE metrics from the different models we have are below:

|Dataset | 	 Transformer (uni.) |   	 Transformer (mv.)  | Informer (uni.)| Informer (mv.) | Autoformer (uni.) | DLinear |
|:--:|:--:| :--:| :--:| :--:|  :--:|:-------:| 
|`Traffic` 	| **0.876** | 1.046 | 0.924 | 1.131  | 0.910 |  0.965  |
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.

Here is the question: What approach best simplifies Transformer evaluation across multimodal benchmarks?
Here is answer 1:Ignore multimodal requirements and focus on text tasks
Here is answer 2: Adapt benchmarks for multimodal tasks using preprocessing and API integration
Here is answer3: Use single-modality benchmarks without adjustment
Here is answer 4: Skip evaluation steps for multimodal workflows","The correct answer would likely fall somewhere between option two ""Adapt benchmarks"""
How can multilingual Transformer models be fine-tuned for diverse tasks?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (8733, '138675fc-8785-4cf2-b567-24851fd927ad'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (5202, 'ffdc2e4f-ce95-48dd-8a87-2c908bbecdcf'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (30822, '1357395c-e339-49a4-9470-5057472025be'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (14997, '63b9b6ec-6baa-4247-a808-3fa1f5673d11'), (19102, 'da415ff0-b51e-4aeb-9852-fbe5a6a4f9f4'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (4796, '8f601501-25a6-4972-aa7a-aa9e072baede'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (11494, '8db96499-6302-45aa-bca4-bce4bec72dee'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1773, 'e07fbbd8-16f9-4b35-bbb3-b1b9950618b0'), (18161, '661008d6-f852-47ee-a976-37ed2665fd5e'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (13587, '05b7e6a6-0d90-4abc-b1d2-dfabde4133eb'), (277, '91e3b387-1104-48c6-98d8-5d391b18159f'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (1829, '98ed0df5-e51e-4766-9380-1458db6d3ec6'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (8017, 'f56d8d5f-9293-4c6b-8217-c1fb077fbe01'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (8020, '4658bd1d-e022-460e-9d6d-af439960931b'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (22895, '3e1b26c9-fb73-4f76-98f6-9a263f7ab477'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (15740, '1ff3f9f4-82bb-4766-8c01-807ece2b0afb'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (9097, '4aa5ab2b-ea90-4dbc-8bca-d4ab9ed22c7e'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (10137, 'ce88e225-95d2-48d7-9020-7d8644986a94'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (31161, 'e6392f6d-29c2-4b5e-99d0-0af8c82a0670'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (18391, 'f9a3fbd7-5a63-4f5d-a344-2656a7553d4d'), (18905, 'a3346d1d-f6c7-4b12-8e61-f1307d4f96d1'), (18906, 'b70bf7ea-06d3-4b07-93e6-b16b084feea9'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (18909, '51930168-debc-4692-b466-bbf665eaf089'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (5088, 'f6c7fccc-2d9b-4fb8-abb5-8bacb17044c0'), (482, 'ecfc6092-627b-4285-bb59-322895262902'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (21476, 'fc40dcb8-cfe2-4449-a2ff-c6b8962d8db5'), (29156, '63322eab-3574-4afc-99b5-c979b3dd7be0'), (18400, 'ef9b2afc-e962-412d-a475-22cc316750d3'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (18429, '4f938c80-c628-4d41-b604-430b07615100')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: *We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art
pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to
significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By
reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on
standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that
allocating additional capacity to the output embedding provides benefits to the model that persist through the
fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a ""high-resource"" language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!

We'll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like `summarize:` which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
```

While this example showed how this can be done with one specific type of base model, any [Sentence Transformer](https://huggingface.co/models?library=sentence-transformers&sort=downloads) model could be switched in for different performance and tasks. For instance, using a multilingual Sentence Transformer body can extend few-shot classification to multilingual settings.

## Next steps
We've shown that SetFit is an effective method for few-shot classification tasks. In the coming months, we'll be exploring how well the method generalizes to tasks like natural language inference and token classification. In the meantime, we're excited to see how industry practitioners apply SetFit to their use cases - if you have any questions or feedback, open an issue on our [GitHub repo](https://github.com/huggingface/setfit) ü§ó.

Happy few-shot learning!
{/if}

For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results. 

However, there are a few cases where you'll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# X-MOD

## Overview

The X-MOD model was proposed in [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255) by Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, and Mikel Artetxe.
X-MOD extends multilingual masked language models like [XLM-R](xlm-roberta) to include language-specific modular components (_language adapters_) during pre-training. For fine-tuning, the language adapters in each transformer layer are frozen.

The abstract from the paper is the following:
There have been incredible advances in multi-modal models in recent years, with vision-language models making the most significant leap in performance and the variety of use cases and applications. In this blog, we talked about the latest advancements in vision-language models, as well as what multi-modal datasets are available and which pre-training strategies we can use to train and fine-tune such models. We also showed how these models are integrated into ü§ó Transformers and how you can use them to perform various tasks with a few lines of code.
```

## Closing Remarks

In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR 
using ü§ó Datasets, Transformers and the Hugging Face Hub. Refer to the [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb) 
should you wish to try fine-tuning for yourself. If you're interested in fine-tuning other 
Transformers models, both for English and multilingual ASR, be sure to check out the 
examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).
```

## Closing Remarks

In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR 
using ü§ó Datasets, Transformers and the Hugging Face Hub. For more details on the Whisper model, the Common Voice dataset and the theory behind fine-tuning, refere to the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). If you're interested in fine-tuning other 
Transformers models, both for English and multilingual ASR, be sure to check out the 
examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).
Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1,100 spoken languages can be identified, transcribed and generated with the various [language identification, speech recognition, and text-to-speech checkpoints released](https://huggingface.co/models?other=mms).

In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning. 

For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
We have applied the same method to other Transformer architectures and released the weights:
- GPT2: on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT2 reaches a perplexity on the test set of 16.3 compared to 21.1 for **DistilGPT2** (after fine-tuning on the train set).
- RoBERTa: **DistilRoBERTa** reaches 95% of `RoBERTa-base`'s performance on GLUE while being twice faster and 35% smaller.
- German BERT: **German DistilBERT** reaches 99% of `bert-base-german-dbmdz-cased`'s performance on German NER (CoNLL-2003).
- Multilingual BERT: **DistilmBERT** reaches 92% of Multilingual BERT's performance on XNLI while being twice faster and 25% smaller. The model supports 104 languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).

For more information on DistilBERT, please refer to our [NeurIPS workshop paper](https://arxiv.org/abs/1910.01108).
According to the abstract

*Multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one
direction, a pretrained model is finetuned on many directions at the same time. It demonstrates that pretrained models
can be extended to incorporate additional languages without loss of performance. Multilingual finetuning improves on
average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while
improving 9.3 BLEU on average over bilingual baselines from scratch.*


### Training of MBart-50

The text format for MBart-50 is slightly different from mBART. For MBart-50 the language id token is used as a prefix
for both source and target text i.e the text format is `[lang_code] X [eos]`, where `lang_code` is source
language id for source text and target language id for target text, with `X` being the source or target text
respectively.


MBart-50 has its own tokenizer [`MBart50Tokenizer`].
In this blog, we present a step-by-step guide on fine-tuning Whisper 
for any multilingual ASR dataset using Hugging Face ü§ó Transformers. This blog 
provides in-depth explanations of the Whisper model, the Common Voice dataset and 
the theory behind fine-tuning, with accompanying code cells to execute the data 
preparation and fine-tuning steps. For a more streamlined version of the notebook 
with fewer explanations but all the code, see the accompanying [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb).

## Table of Contents
The [Transformer architecture](https://arxiv.org/abs/1706.03762) was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:

- **June 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results

- **October 2018**: [BERT](https://arxiv.org/abs/1810.04805), another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)

- **February 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these
findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the
number of parameters at the fine-tuning stage.*
```

Now that we've prepared our corpus, let's take a look at a few possible Transformer models that one might fine-tune on it!

## Models for text summarization[[models-for-text-summarization]]

If you think about it, text summarization is a similar sort of task to machine translation: we have a body of text like a review that we'd like to ""translate"" into a shorter version that captures the salient features of the input. Accordingly, most Transformer models for summarization adopt the encoder-decoder architecture that we first encountered in [Chapter 1](/course/chapter1), although there are some exceptions like the GPT family of models which can also be used for summarization in few-shot settings. The following table lists some popular pretrained models that can be fine-tuned for summarization.
The figure below shows a formulation of instruction-tuning (also called ‚Äúinstruction-finetuning‚Äù). In the [FLAN V2 paper](https://huggingface.co/papers/2210.11416), the authors take a pre-trained language model ([T5](https://huggingface.co/docs/transformers/model_doc/t5), for example) and fine-tune it on a dataset of exemplars, as shown in the figure below. 

| ![flan_schematic](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/instruction-tuning-sd/flan_schematic.png) |
|:--:|
| **Figure 2**: FLAN V2 schematic (figure taken from the FLAN V2 paper). |

With this approach, one can create exemplars covering many different tasks, which makes instruction-tuning a multi-task training objective:
And just by switching out the base Sentence Transformer model to a multilingual one, SetFit can function seamlessly in multilingual contexts. In our [experiments](https://arxiv.org/abs/2209.11055), SetFit‚Äôs performance shows promising results on classification in German, Japanese, Mandarin, French and Spanish, in both in-language and cross linguistic settings.


## Benchmarking SetFit
promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.*

Here is the question: How can multilingual Transformer models be fine-tuned for diverse tasks?
Here is answer 1:Focus only on monolingual models for simplicity
Here is answer 2: Utilize multilingual checkpoints and framework-specific tools
Here is answer3: Skip fine-tuning for pretrained models
Here is answer 4: Use text-only benchmarks without multilingual considerations",Based solely upon context provided I would choose here fourth option : Utilize mutliligual checkpoints along framework-specific tools
What makes Hugging Face APIs critical for multimodal workflow deployment?,"[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (19977, 'da27be80-5381-43b5-9542-ef033eb07df9'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8269, '63e383a9-dbfa-4c9f-a286-0668a682d313'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4179, 'b6031fcd-da9e-4a41-ac79-36bf3eaa3bd0'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (1127, '61bce4e3-ba06-42fd-82da-5f4b1ae2562a'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (31354, '50289d67-f345-42d3-bb11-7b4a1e8c5530'), (19578, 'c39dbfdb-a1e4-46cb-b486-736ca8e047bb'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (4237, 'aa571318-cb21-42dc-96d5-9247c29a9ac5'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (18783, '44d0b22a-e8ef-4cba-bda9-fd404cb063c2'), (18788, 'c8b88dcc-6e3b-4f12-9ccc-23fa571e9a77'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (7026, 'a54a170b-414e-4326-bff0-925d0cf07fab'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24532, '60a7b56c-8699-44c3-845d-6e93ee024964'), (24535, '5f65c014-0a38-4090-88ab-510c3bc8926e'), (6105, '9efc65f9-7879-4b2c-8da5-721897d26d7d'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (15837, '78ef6dea-b85c-4549-ad3c-0ef7afe5cd99'), (31716, '63f41724-0a0f-4e87-8baf-46b9e5e590f8'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (27114, 'bc032949-d8d4-4c0e-8898-fdcb416b7e1b'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: In combination with the data issues widget, the Python API offers a great way to integrate the results of existing scripts (e.g. data quality checks or model monitoring) into a scalable data inspection workflow.

## Using Spotlight on the Hugging Face hub

You can use Spotlight directly on your local NLP, audio, CV or multimodal dataset. If you would like to showcase your dataset or model results on the Hugging Face hub, you can use Hugging Face spaces to launch a Spotlight visualization for it.

We have already prepared [example spaces](https://huggingface.co/renumics) for many popular NLP, audio and CV datasets on the hub. You can simply duplicate one of these spaces and specify your dataset in the `HF_DATASET` variable.

You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.
```

The current API does not enforce strict rate limitations. Instead, Hugging Face balances the loads evenly between all our available resources and favors steady flows of requests. If you need to embed several texts or images, the [Hugging Face Accelerated Inference API](https://huggingface.co/docs/api-inference/index) would speed the inference and let you choose between using a CPU or GPU.
```

## Pros and cons 

When to use this API and when to not use it? Let's discuss in this section the pros and cons 

Pros:
- The model gets modified in-place, meaning the model will preserve all its original attributes and methods
- Works for any torch module, and any modality (vision, text, multi-modal)

Cons:
- You need to manually writing Hugging Face `from_pretrained` and `save_pretrained` utility methods if you want to easily save / load adapters from the Hugging Face Hub.
- You cannot use any of the utility method provided by `PeftModel` such as disabling adapters, merging adapters, etc.
## API and client library interaction with the Hub

Interacting with the Hugging Face Hub via an [API](https://huggingface.co/docs/hub/api) or the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) Python library is possible. This includes creating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This can be powerful for research workflows where new data or annotations continue to be created. The client library also makes uploading large datasets much more accessible. 

## Community 

The Hugging Face Hub is already home to a large community of researchers, developers, artists, and others interested in using and contributing to an ecosystem of open-source machine learning. Making your datasets accessible to this community increases their visibility, opens them up to new types of users and places your datasets within the context of a larger ecosystem of models, datasets and libraries.
In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.

In the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.

## Compliance as a Core Principle

As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.
Leveraging the latest features of the Hugging Face libraries, we achieve a reliable 10x speed up compared to an out-of-box deployment for a given model/hardware pair. As new releases of Transformers and Tokenizers typically ship every month, our API customers do not need to constantly adapt to new optimization opportunities, their models just keep running faster.

## Compilation FTW: the hard to get 10x
Now this is where it gets really tricky. In order to get the best possible performance we will need to modify the model and compile it targeting the specific hardware for inference. The choice of hardware itself will depend on both the model (size in memory) and the demand profile (request batching). Even when serving predictions from the same model, some API customers may benefit more from Accelerated CPU inference, and others from Accelerated GPU inference, each with different optimization techniques and libraries applied.
After a first test on Google Colab, the Hugging Face experts guided Witty Works on deploying the model on Azure. No optimization was necessary as the model was fast enough.

```diff
Elena Nazarenko, Lead Data Scientist at Witty Works: 
‚ÄúWorking with Hugging Face saved us a lot of time and money. 
One can feel lost when implementing complex text classification use cases. 
As it is one of the most popular tasks, there are a lot of models on the Hub. 
The Hugging Face experts guided me through the massive amount of transformer-based 
models to choose the best possible approach. 
Plus, I felt very well supported during the model deployment‚Äù
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.

Hugging Face Infinity consists of 2 main services:
* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.
* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.

The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.

<br>
<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Product overview"" src=""assets/46_infinity_cpu_performance/overview.png""></medium-zoom>
  <figcaption>Figure 1. Infinity Overview</figcaption>
</figure>
<br>
To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.

## Usage Tips

1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.
2. Address slow response times or performance issues by changing model endpoints to lower resource models.

## Conclusion

The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!
--
title: Getting Started with Hugging Face Inference Endpoints
thumbnail: /blog/assets/109_inference_endpoints/endpoints05.png
authors:
- user: juliensimon
---

# Getting Started with Hugging Face Inference Endpoints


Training machine learning models has become quite simple, especially with the rise of pre-trained models and transfer learning. OK, sometimes it's not *that* simple, but at least, training models will never break critical applications, and make customers unhappy about your quality of service. Deploying models, however... Yes, we've all been there.

Deploying models in production usually requires jumping through a series of hoops. Packaging your model in a container, provisioning the infrastructure, creating your prediction API, securing it, scaling it, monitoring it, and more. Let's face it: building all this plumbing takes valuable time away from doing actual machine learning work. Unfortunately, it can also go awfully wrong.
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One ‚Äì or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) ‚Äì specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
- **On-prem Private Hub**: on-premise deployment of the Hugging Face Hub on your own infrastructure. For customers with strict compliance rules and/or workloads where they don't want or are not allowed to run on a public cloud.

Now that we have covered the basics of what the Private Hub is, let's go over how companies are using it to accelerate their ML development. 

## 3. How Are Companies Using the Private Hub to Accelerate Their ML Roadmap?

[ü§ó Transformers](https://github.com/huggingface/transformers) is one of the [fastest growing open source projects of all time](https://star-history.com/#tensorflow/tensorflow&nodejs/node&kubernetes/kubernetes&pytorch/pytorch&huggingface/transformers&Timeline). We now offer [25+ open source libraries](https://github.com/huggingface) and over 10,000 companies are now using Hugging Face to build technology with machine learning.
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
Hugging Face is where it is today thanks to its community of developers, so we‚Äôve seen firsthand what open development brings to the table
to support more robust innovation for more diverse and context-specific use cases;
where developers can easily share innovative new techniques, mix and match ML components to suit their own needs,
and reliably work with full visibility into their entire stack.
We‚Äôre also acutely aware of the necessary role of transparency in supporting more accountability and inclusivity of the technology ‚Äì
which we‚Äôve worked on fostering through better documentation and accessibility of ML artifacts, education efforts,
and hosting large-scale multidisciplinary collaborations, among others.
Thus, as the EU AI Act moves toward its final phase, we believe accounting for the specific needs and strengths of open and open-source development of ML systems will be instrumental in supporting its long-term goals.
## Examples of implementations: Safety features and Mechanisms

The team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.

- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.

- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.

- **Encouraging safety in deployment**
In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for prototyping, comparing it against AWS Sagemaker and Hugging Face‚Äôs new model hosting Inference API. Given that we use GCP for data storage and Google Vertex Pipelines for model training, exporting models to AWS Sagemaker was clunky and bug prone. Thankfully, the set up for Hugging Face was quick and easy, and it was able to handle a small portion of traffic within a week. Hugging Face simply worked out of the gate, and this reduced friction led us to proceed down this path.
Using Hugging Face Integrations

Related spaces: https://huggingface.co/spaces/gradio/en2es
Tags: HUB, SPACES, EMBED

Contributed by <a href=""https://huggingface.co/osanseviero"">Omar Sanseviero</a> ü¶ô

## Introduction

The Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). 

Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API
After an extensive three-month evaluation period, we chose Hugging Face to host our models. During this time, we gradually increased transaction volume to their hosted models and ran numerous simulated load tests based on our worst-case scenario volumes. This process allowed us to fine-tune our system and monitor performance, ultimately giving us confidence in the inference API's ability to handle our transaction enrichment loads.
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples üìÑ

Here is the question: What makes Hugging Face APIs critical for multimodal workflow deployment?
Here is answer 1:They simplify preprocessing and enable framework-agnostic deployment
Here is answer 2: They limit tasks to text-based models for simplicity
Here is answer3: They focus solely on pretrained architectures
Here is answer 4: They bypass preprocessing requirements for multimodal tasks",Answer1
What is a key consideration when using pretrained Transformers for multimodal applications?,"[(5120, 'ddf54aa1-62ec-4203-a18e-bcc935c8a3ed'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (18443, '00876f43-8b96-47cb-ba09-71690498db3d'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (5141, 'aa79a23d-5dce-4f0c-905f-502f514390e9'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (26156, 'cb06116c-89a8-4cb8-ae81-f3734a853307'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (12897, '78a4117e-544a-4fa9-9c46-63ee897d9e66'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (17510, '4186dada-bac9-4760-b476-16d67e9b4f11'), (2663, 'efa7e63d-051b-4d0d-b130-483c5160dd59'), (23145, '093af302-133e-498d-a722-88a8b6199cee'), (25706, '07429df9-ec51-44b8-bbaa-43e5e886bf4d'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (20617, '476882ca-9720-4486-939b-4dfc903b0857'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (20636, '3a97d7e3-78ab-4911-9bbf-01b10ea9777a'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (20661, '3d03f793-112e-4835-89e0-d0c9ab6a236d'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (4812, 'b008816e-72ef-4b5b-9a8d-f2ade4e8823c'), (17103, 'f3f12925-7e43-4057-bc7e-1a29829c0068'), (13526, '594c0819-9996-42bd-8329-7cec367352a5'), (13535, '818b2f5b-6b46-4f5f-99b7-cc2d76fc212e'), (18147, '08a3519f-4ec3-40a5-a501-a54a7a0613e1'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (4850, '3a950f12-3c2d-47ce-a6a8-1abdfb1befa0'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (4861, '79210cef-fada-44fb-a563-86b3d8b879d2'), (13565, '118171eb-372a-4886-8180-64c88a38710f'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (263, 'f2e4842b-dafb-4139-b722-2bb49405a14f'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (16674, 'a05d2b01-8fd3-48e1-8077-85f9ef33864d'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (298, 'a9b73b5b-eef0-437d-842d-a511870ed8c2'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (307, '6b64ed6e-acaf-4565-8edd-d7398bd867b4'), (19764, '3964a67e-698f-4796-91c3-29d446e12522'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (12105, 'e4f29e10-0755-4ad0-87ea-18507f153a17'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (17761, '95299ba8-4ba2-4666-a1f4-2d7e527ebbfe'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (11114, '933fa587-6033-469c-b633-a24147cec80c'), (11115, 'a4f77954-5dd3-4d2c-a0a8-d2241981c438'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (15735, 'f473ff9d-2520-4b90-81e6-2bc07ffb2468'), (9083, '40b57069-cb95-4690-8c8a-f74d274c5eee'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (22918, '201f6149-0a67-49ea-81bd-e07891bdb9c1'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (9119, 'cd5f2faa-9cfe-42e6-8006-4a8264ce76d0'), (4511, 'af6fa441-8f03-460f-ad80-c0b9214b289c'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (31650, 'b7d49d66-3894-4743-b041-a97c182c96a7'), (10145, 'ce96ae5b-5345-4742-bb66-c625e53c53e0'), (10149, '717ac025-7b4c-4c12-a316-fbdbf28787d5'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (9129, '4e3c49f8-4ca3-47af-a572-5135008633d3'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (10170, '4052ca58-8126-4b6e-ab1d-9bc6799339a3'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9150, 'de3eb793-c6de-4fd2-9f8c-1b0b686cf6d1'), (2509, '53bab419-4140-4137-aa56-67f5880cf609'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (7123, '5550e6f5-759d-4aef-9d97-83b53144cc4b'), (10194, 'e573dcfe-a4f3-435f-9afe-101265a292e5'), (5074, 'e34f5010-d00a-48d9-b2db-4b2925161439'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (18413, 'c087ad9c-11fd-4007-b215-f0a96fd9c1e4'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (5110, 'ff805522-0e2d-45ba-be4f-6e3fac713294'), (18423, 'bceac124-b7c2-4000-85e8-bc1990d7dce1'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
## Why should I use transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding & generation, computer vision, and audio tasks.
    - Low barrier to entry for educators and practitioners.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.

1. Lower compute costs, smaller carbon footprint:
    - Researchers can share trained models instead of always retraining.
    - Practitioners can reduce compute time and production costs.
    - Dozens of architectures with over 60,000 pretrained models across all modalities.

1. Choose the right framework for every part of a model's lifetime:
    - Train state-of-the-art models in 3 lines of code.
    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.
    - Seamlessly pick the right framework for training, evaluation, and production.
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>
The authors also use the Perceiver for multimodal autoencoding. The goal of multimodal autoencoding is to learn a model that can accurately reconstruct multimodal inputs in the presence of a bottleneck induced by an architecture. The authors train the model on the [Kinetics-700 dataset](https://deepmind.com/research/open-source/kinetics), in which each example consists of a sequence of images (i.e. frames), audio and a class label (one of 700 possible labels). This model is also implemented in HuggingFace Transformers, and available as `PerceiverForMultimodalAutoencoding`. For brevity, I will omit the code of defining this model, but important to note is that it uses `PerceiverMultimodalPreprocessor` to prepare the `inputs` for the model. This preprocessor will first use the respective preprocessor for each modality (image, audio, label) separately. Suppose one has a video of 16 frames of resolution 224x224 and 30,720 audio samples, then the modalities are preprocessed as follows:
```

```python out
""""""
[CLS] Which deep learning libraries back [UNK] Transformers? [SEP] [UNK] Transformers : State of the Art NLP

[UNK] Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction,
question answering, summarization, translation, text generation and more in over 100 languages.
Its aim is to make cutting-edge NLP easier to use for everyone.

[UNK] Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and
then share them with the community on our model hub. At the same time, each python module defining an architecture is fully standalone and
can be modified to enable quick research experiments.

Why should I use transformers?
## The technical philosophy

In this section, we wanted to share our philosophy behind adding support for Computer Vision in ü§ó¬†Transformers so that the community is aware of the design choices specific to this area.

Even though Transformers started with NLP, we support multiple modalities today, for example ‚Äì vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:

- Easy model download with a single line of code with `from_pretrained()`
- Easy model upload with `push_to_hub()`
- Support for loading huge checkpoints with efficient checkpoint sharding techniques
- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))
- Initialization from model configurations
- Support for both PyTorch and TensorFlow (non-exhaustive)
- and many more
-->

# Load pretrained instances with an AutoClass

With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of ü§ó Transformers core philosophy to make the library easy, simple and flexible to use, an `AutoClass` automatically infers and loads the correct architecture from a given checkpoint. The `from_pretrained()` method lets you quickly load a pretrained model for any architecture so you don't have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.

<Tip>
# ü§ó Transformers

State-of-the-art Machine Learning for [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [JAX](https://jax.readthedocs.io/en/latest/).

ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:
There have been incredible advances in multi-modal models in recent years, with vision-language models making the most significant leap in performance and the variety of use cases and applications. In this blog, we talked about the latest advancements in vision-language models, as well as what multi-modal datasets are available and which pre-training strategies we can use to train and fine-tune such models. We also showed how these models are integrated into ü§ó Transformers and how you can use them to perform various tasks with a few lines of code.
-->

# Preprocess

[[open-in-colab]]

Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. ü§ó Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you'll learn that for:

* Text, use a [Tokenizer](./main_classes/tokenizer) to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.
* Speech and audio, use a [Feature extractor](./main_classes/feature_extractor) to extract sequential features from audio waveforms and convert them into tensors.
* Image inputs use a [ImageProcessor](./main_classes/image) to convert images into tensors.
* Multimodal inputs, use a [Processor](./main_classes/processors) to combine a tokenizer and a feature extractor or image processor.

<Tip>
1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (from Microsoft Research Asia) released with the paper [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) by Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei.
1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (from AllenAI) released with the paper [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan.
1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (‡§Æ‡•á‡§ü‡§æ AI ‡§∏‡•á) ‡§∏‡§æ‡§• ‡§µ‡§æ‡§≤‡§æ ‡§™‡•á‡§™‡§∞ [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https:/ /arxiv.org/abs/2104.01136) ‡§¨‡•á‡§® ‡§ó‡•ç‡§∞‡§æ‡§π‡§Æ, ‡§Ö‡§≤‡§æ‡§è‡§≤‡•ç‡§°‡§ø‡§® ‡§è‡§≤-‡§®‡•å‡§¨‡•Ä, ‡§π‡•ç‡§Ø‡•Ç‡§ó‡•ã ‡§ü‡•å‡§µ‡§∞‡§®, ‡§™‡§ø‡§Ø‡§∞‡•á ‡§∏‡•ç‡§ü‡•â‡§ï, ‡§Ü‡§∞‡•ç‡§Æ‡§Ç‡§° ‡§ú‡•å‡§≤‡§ø‡§®, ‡§π‡§∞‡•ç‡§µ‡•á ‡§ú‡•á‡§ó‡•å, ‡§Æ‡•à‡§•‡§ø‡§ú ‡§°‡•Ç‡§ú‡§º ‡§¶‡•ç‡§µ‡§æ‡§∞‡§æ‡•§
```

With two basic classes - configuration and model - and an additional preprocessing class (tokenizer, image processor, feature extractor, or processor), you can create any of the models supported by ü§ó Transformers. Each of these base classes are configurable, allowing you to use the specific attributes you want. You can easily setup a model for training or modify an existing pretrained model to fine-tune.
1. **[LayoutXLM](https://huggingface.co/docs/transformers/model_doc/layoutxlm)** (Microsoft Research Asia ÏóêÏÑú) Yiheng Xu, Tengchao Lv, Lei Cui, Guoxin Wang, Yijuan Lu, Dinei Florencio, Cha Zhang, Furu Wei Ïùò [LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding](https://arxiv.org/abs/2104.08836) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[LED](https://huggingface.co/docs/transformers/model_doc/led)** (AllenAI ÏóêÏÑú) Iz Beltagy, Matthew E. Peters, Arman Cohan Ïùò [Longformer: The Long-Document Transformer](https://arxiv.org/abs/2004.05150) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[LeViT](https://huggingface.co/docs/transformers/model_doc/levit)** (Meta AI ÏóêÏÑú) Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin, Herv√© J√©gou, Matthijs Douze Ïùò [LeViT: A Vision Transformer in ConvNet's Clothing for Faster Inference](https://arxiv.org/abs/2104.01136) ÎÖºÎ¨∏Í≥º Ìï®Íªò Î∞úÌëúÌñàÏäµÎãàÎã§.
1. **[X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)** (Microsoft Research „Åã„Çâ) Bolin Ni, Houwen Peng, Minghao Chen, Songyang Zhang, Gaofeng Meng, Jianlong Fu, Shiming Xiang, Haibin Ling „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá: [Expanding Language-Image Pretrained Models for General Video Recognition](https://arxiv.org/abs/2208.02816)
1. **[X-MOD](https://huggingface.co/docs/transformers/model_doc/xmod)** (Meta AI „Åã„Çâ) Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe. „Åã„ÇâÂÖ¨Èñã„Åï„Çå„ÅüÁ†îÁ©∂Ë´ñÊñá [Lifting the Curse of Multilinguality by Pre-training Modular Transformers](http://dx.doi.org/10.18653/v1/2022.naacl-main.255)
# Image pretraining examples

This directory contains Python scripts that allow you to pre-train Transformer-based vision models (like [ViT](https://huggingface.co/docs/transformers/model_doc/vit), [Swin Transformer](https://huggingface.co/docs/transformers/model_doc/swin)) on your own data, after which you can easily load the weights into a [`AutoModelForImageClassification`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForImageClassification). It currently includes scripts for:
- [SimMIM](#simmim) (by Microsoft Research)
- [MAE](#mae) (by Facebook AI).

NOTE: If you encounter problems/have suggestions for improvement, open an issue on Github and tag @NielsRogge.


## SimMIM
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
-->

# Philosophy

ü§ó Transformers is an opinionated library built for:

- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models.
- hands-on practitioners who want to fine-tune those models or serve them in production, or both.
- engineers who just want to download a pretrained model and use it to solve a given machine learning task.

The library was designed with two strong goals in mind:

1. Be as easy and fast to use as possible:
1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.
1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of Wisconsin‚ÄìMadison) released with the paper [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.
1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.
1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of Wisconsin‚ÄìMadison) released with the paper [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.

Here is the question: What is a key consideration when using pretrained Transformers for multimodal applications?
Here is answer 1:Avoid fine-tuning to maintain generalization
Here is answer 2: Adapt pretrained models with multimodal datasets and APIs
Here is answer3: Use pretrained models exclusively for text tasks
Here is answer 4: Limit usage to default configurations","The correct answer here would be ""Adapt prepaid modesl withe multimoal datasets adn APIs""."
What challenges arise when adapting Transformer models for multimodal benchmarks?,"[(4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (5152, '2eb1dd24-f252-4573-b700-3d513da59540'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (4646, '13611a54-20bb-45b0-89e7-31b0bb22b402'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (27229, '79b37341-3681-4528-9ab1-2ab8778b564d'), (21089, '9e8976c4-72d9-49eb-9b17-5078f33abec4'), (27234, '1560c7a6-63b7-40e5-ac9b-76996e3640b5'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (14442, '63643091-647a-4c7f-a8e1-f89173754c81'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (15605, '9f241ce4-3d04-4868-8576-832003581357'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (18203, 'fbb6e88b-40d0-4fae-bc38-25a9c45213d5'), (18222, '5880432c-3d6d-4287-b3fe-745b7ed4b67f'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (13620, '4df43476-1663-498a-b7ab-c48517c40357'), (21816, '5d5883c9-44d9-476e-bda0-c334c5912e4a'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (319, '69e48a24-1f9a-4783-ae23-cc8e33b1f421'), (13639, 'af509e72-cf08-461e-813f-b0e762ebdbfa'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (338, '83c0ff2b-59c2-4ffc-bf85-93ed5abbf667'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (27048, 'e3873222-b7e8-4f6f-9732-ae991af045de'), (30638, 'bb43e4ba-f6ab-478d-8c31-6535c42d1276'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (11721, '1aead5c5-7fcb-464e-b7ab-40f535514f4c'), (9161, 'cc82182d-8b0c-4897-9bfe-8cc5080325ee'), (11723, '6a66155a-e619-439b-96b5-4cef0a9ece1f'), (16335, '41fef57c-6a30-487e-9e0f-4d02276e4a7d'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (17364, '375858e6-fad8-49d7-9f08-70b4ff973b3e'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (475, 'fc8680f8-2eee-4940-8f49-33e177da211a'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (18429, '4f938c80-c628-4d41-b604-430b07615100'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
## Conclusion

Transformer models have emerged as the go-to solution for many machine learning tasks. However, deploying them in production has been challenging due to their large size and latency requirements. Thanks to AWS Inferentia2 and the collaboration between Hugging Face and AWS, developers and organizations can now leverage the benefits of state-of-the-art models without the prior need for extensive machine learning expertise. You can start testing for as low as 0.76$/h.

The initial benchmarking results are promising, and show that Inferentia2 delivers superior latency performance when compared to both Inferentia and NVIDIA A10G GPUs. This latest breakthrough promises high-quality machine learning models can be made available to a much broader audience delivering AI accessibility to everyone.
- Abstract: Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. So, we now need a well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. I'll talk about a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically
However, for all their greatness, Transformers can be challenging to deploy in production. On top of the infrastructure plumbing typically associated with model deployment, which we largely solved with our [Inference Endpoints](https://huggingface.co/inference-endpoints) service, Transformers are large models which routinely exceed the multi-gigabyte mark. Large language models (LLMs) like [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B), [Flan-T5](https://huggingface.co/google/flan-t5-xxl), or [Opt-30B](https://huggingface.co/facebook/opt-30b) are in the tens of gigabytes, not to mention behemoths like [BLOOM](https://huggingface.co/bigscience/bloom), our very own LLM, which clocks in at 350 gigabytes.
# Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA


LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. 
Our [LLM.int8 blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) showed how the techniques in the [LLM.int8 paper](https://arxiv.org/abs/2208.07339) were integrated in transformers using the `bitsandbytes` library.
As we strive to make models even more accessible to anyone, we decided to collaborate with bitsandbytes again to allow users to run models in 4-bit precision. This includes a large majority of HF models, in any modality (text, vision, multi-modal, etc.). Users can also train adapters on top of 4bit models leveraging tools from the Hugging Face ecosystem. This is a new method introduced today in the QLoRA paper by Dettmers et al. The abstract of the paper is as follows:
_Note: that we did not optimize the model for the GPU environment, the models were evaluated in fp32._

When it comes to benchmarking Transformer models, there are two metrics that are most adopted:
* **Latency**: the time it takes for the model to perform a single prediction (pre-process, prediction, post-process).
* **Throughput**: the number of executions performed in a fixed amount of time for one benchmark configuration

We looked at latency across different setups and models to understand the benefits and tradeoffs of the new Inferentia2 instance. If you want to run the benchmark yourself, we created a [Github repository](https://github.com/philschmid/aws-neuron-samples/tree/main/benchmark) with all the information and scripts to do so. 

### Results
Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.

Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
Transformer representations to be more general and more transferable to other tasks and languages. Harnessing these
findings, we are able to train models that achieve strong performance on the XTREME benchmark without increasing the
number of parameters at the fine-tuning stage.*
*Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. 
However, due to the massive number of parameters and model design, e.g., attention mechanism, ViT-based models are generally 
times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly 
challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation 
complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still 
unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? 
To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs.
You want to add a new model for `Better Transformer`, the fast path of PyTorch Transformer API? Check this guideline!

## Models that should be supported

In theory, any model that has a transformer encoder layer, similar to the classic encoder described in the [""Attention Is All You Need""](https://arxiv.org/abs/1706.03762) paper should be supported. 
More specifically, a model that has an encoder block with a MultiHead-Attention module (with pre or post-attention layer norm) should be convertible to its `BetterTransformer` equivalent. The conditions can be summarized as follows:

- Use classic Multi Head attention module (for example, [DeBERTa](https://arxiv.org/abs/2006.03654) cannot be supported)
- Use either `gelu` or `relu` activation function   
- Have an even number of attention heads
- Do not use any attention bias (for eg `T5` uses attention bias, therefore cannot be supported)
- `eps` must be equal between the first and second layer norms for each layer
a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on LRA. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.
We have applied the same method to other Transformer architectures and released the weights:
- GPT2: on the [WikiText-103](https://blog.einstein.ai/the-wikitext-long-term-dependency-language-modeling-dataset/) benchmark, GPT2 reaches a perplexity on the test set of 16.3 compared to 21.1 for **DistilGPT2** (after fine-tuning on the train set).
- RoBERTa: **DistilRoBERTa** reaches 95% of `RoBERTa-base`'s performance on GLUE while being twice faster and 35% smaller.
- German BERT: **German DistilBERT** reaches 99% of `bert-base-german-dbmdz-cased`'s performance on German NER (CoNLL-2003).
- Multilingual BERT: **DistilmBERT** reaches 92% of Multilingual BERT's performance on XNLI while being twice faster and 25% smaller. The model supports 104 languages listed [here](https://github.com/google-research/bert/blob/master/multilingual.md#list-of-languages).

For more information on DistilBERT, please refer to our [NeurIPS workshop paper](https://arxiv.org/abs/1910.01108).
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
## Introduction

After the rise of large transformer models in 2018 and 2019, two trends have quickly emerged to bring their compute requirements down. First, conditional computation, quantization, distillation, and pruning have unlocked inference of large models in compute-constrained environments; we‚Äôve already touched upon this in part in our [last reading group post](https://discuss.huggingface.co/t/hugging-face-reads-01-2021-sparsity-and-pruning/3144). The research community then moved to reduce the cost of pre-training.

In particular, one issue has been at the center of the efforts: the quadratic cost in memory and time of transformer models with regard to the sequence length. In order to allow efficient training of very large models, 2020 saw an onslaught of papers to address that bottleneck and scale transformers beyond the usual 512- or 1024- sequence lengths that were the default in NLP at the start of the year.
On the kernel side as well, designing performant W4A16 kernels for larger batch sizes remains an open challenge.

### Supported models

In this initial implementation, only large language models with a decoder or encoder only architecture are supported. This may sound a bit restrictive, but it encompasses most state of the art LLMs such as Llama, OPT, GPT-Neo, GPT-NeoX.

Very large vision, audio, and multi-modal models are currently not supported.

## Conclusion and final words

In this blogpost we have presented the integration of the [AutoGPTQ library](https://github.com/PanQiWei/AutoGPTQ) in Transformers, making it possible to quantize LLMs with the GPTQ method to make them more accessible for anyone in the community and empower them to build exciting tools and applications with LLMs. 

This integration is available both for Nvidia GPUs, and RoCm-powered AMD GPUs, which is a huge step towards democratizing quantized models for broader GPU architectures.
Recently, Facebook Research introduced a [Token Merging](https://arxiv.org/abs/2210.09461) method for Vision Transformer models. The essence of the method is that it merges redundant tokens with important ones using one of the available strategies (averaging, taking max values, etc.). This is done before the self-attention block, which is the most computationally demanding part of Transformer models. Therefore, reducing the token dimension reduces the overall computation time in the self-attention blocks. This method has also been [adapted](https://arxiv.org/pdf/2303.17604.pdf) for Stable Diffusion models and has shown promising results when optimizing Stable Diffusion pipelines for high-resolution image synthesis running on GPUs.
One of the main reasons for the importance of increasing `nb_loops` is that the speech lengths generated vary greatly between different iterations, even with a fixed input.

 One consequence of this is that the latency measured by `measure_latency_and_memory_use` may not actually reflect the actual performance of optimization techniques! The benchmark at the end of the blog post reports the results averaged over 100 iterations, which gives a true indication of the performance of the model.

## 1. ü§ó Better Transformer

Better Transformer is an ü§ó Optimum feature that performs kernel fusion under the hood. This means that certain model operations will be better optimized on the GPU and that the model will ultimately be faster.
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]

Here is the question: What challenges arise when adapting Transformer models for multimodal benchmarks?
Here is answer 1:Benchmarks lack multimodal relevance
Here is answer 2: Adapting benchmarks requires aligning datasets, preprocessing, and API tools
Here is answer3: Framework-specific tools simplify benchmark adaptation
Here is answer 4: Benchmarks focus solely on accuracy, not multimodal tasks","I choose answer3:""Framework-specific tools simplify benchmark adaptation""."
How do Hugging Face frameworks facilitate seamless integration of multimodal tasks?,"[(28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (19976, 'ec852d21-1da0-46da-8293-267f04f04242'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (16928, '1dd8da94-9861-4d4b-ae5d-0182de0a33d2'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23593, 'c4cad64a-26a2-4318-b2bf-37b79d234f09'), (29742, '41dd1415-0367-415c-b18f-14be16e6b316'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (22086, '6f92aef8-fd03-45fb-8920-1fc2364e4e13'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (26745, '509a2418-a582-44fa-9c23-427ca2329092'), (11386, 'b62622b1-cbec-4754-9710-20faddd1d382'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (27775, '56108da4-6463-4bb1-8393-42e02e4b2cc9'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (22783, '898e6630-6e42-41f4-bf5e-7ad200e3339e'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (19218, '29f57a47-3df1-4b88-a202-974f96482787'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (28440, 'f186228b-4ecd-4f33-a849-39b930f0800a'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15299, 'c141a388-b78f-49f1-aefc-636b7195e91c'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (24011, '4be4591b-14cb-4bfb-80c1-06501be969f7'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: In combination with the data issues widget, the Python API offers a great way to integrate the results of existing scripts (e.g. data quality checks or model monitoring) into a scalable data inspection workflow.

## Using Spotlight on the Hugging Face hub

You can use Spotlight directly on your local NLP, audio, CV or multimodal dataset. If you would like to showcase your dataset or model results on the Hugging Face hub, you can use Hugging Face spaces to launch a Spotlight visualization for it.

We have already prepared [example spaces](https://huggingface.co/renumics) for many popular NLP, audio and CV datasets on the hub. You can simply duplicate one of these spaces and specify your dataset in the `HF_DATASET` variable.

You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.
## What can you find on the Hugging Face Hub?

### Models 

The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.

### Datasets
The Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.

### Spaces

Hugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications.
!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Integrate any ML framework with the Hub

The Hugging Face Hub makes hosting and sharing models with the community easy. It supports
[dozens of libraries](https://huggingface.co/docs/hub/models-libraries) in the Open Source ecosystem. We are always
working on expanding this support to push collaborative Machine Learning forward. The `huggingface_hub` library plays a
key role in this process, allowing any Python script to easily push and load files.
Integrate your library with the Hub

The Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.

Integrating the Hub with your library provides many benefits, including:
To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.

## Usage Tips

1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.
2. Address slow response times or performance issues by changing model endpoints to lower resource models.

## Conclusion

The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!
# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft‚Äôs ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face‚Äôs [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
Using Hugging Face Integrations

Related spaces: https://huggingface.co/spaces/gradio/en2es
Tags: HUB, SPACES, EMBED

Contributed by <a href=""https://huggingface.co/osanseviero"">Omar Sanseviero</a> ü¶ô

## Introduction

The Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). 

Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples üìÑ
_Q: What are you planning next through this partnership?_

A: Our common goal is to democratize state of the art Machine Learning. We will continue to innovate to make it easier for researchers, data scientists and ML practitioners to manage, train and run state of the art models. If you have feature requests for integration in AWS with Hugging Face, please [let us know in the Hugging Face community forum](https://discuss.huggingface.co/c/sagemaker/17).

_Q: I use Hugging Face with Azure Machine Learning or Google Cloud Platform, what does this partnership mean for me?_

A: A foundational goal for Hugging Face is to make the latest AI accessible to as many people as possible, whichever framework or development environment they work in. While we are focusing integration efforts with Amazon Web Services as our Preferred Cloud Provider, we will continue to work hard to serve all Hugging Face users and customers, no matter what compute environment they run on.
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One ‚Äì or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) ‚Äì specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
Clement Delangue, co-founder and CEO, Hugging Face

## Conclusion

Together, Snorkel and Hugging Face make it easier than ever for large companies, government agencies, and AI innovators to get value from foundation models. The ability to use Hugging Face‚Äôs comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world.

_If you‚Äôre interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!_
Here is a list of things we‚Äôll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#ü§ó-ü§ù-timm)
- [Diffusers](#üß®-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time üëÅ

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
Integrations with the Hugging Face Hub[[integrations-with-the-hugging-face-hub]]

<CourseFloatingBanner chapter={9}
  classNames=""absolute z-10 right-0 top-0""
  notebooks={[
    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter9/section5.ipynb""},
    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter9/section5.ipynb""},
]} />

To make your life even easier, Gradio integrates directly with Hugging Face Hub and Hugging Face Spaces.
You can load demos from the Hub and Spaces with only *one line of code*.

### Loading models from the Hugging Face Hub[[loading-models-from-the-hugging-face-hub]]
To start with, choose one of the thousands of models Hugging Face offers through the Hub, as described in [Chapter 4](/course/chapter4/2).
_‚ÄúWith the new Hugging Face Hub model catalog, natively integrated within Azure Machine Learning, we are opening a new page in our partnership with Microsoft, offering a super easy way for enterprise customers to deploy Hugging Face models for real-time inference, all within their secure Azure environment.‚Äù_ said Julien Simon, Chief Evangelist at Hugging Face. 

_""The integration of Hugging Face's open-source models into Azure Machine Learning represents our commitment to empowering developers with industry-leading AI tools,""_ said John Montgomery, Corporate Vice President, Azure AI Platform at Microsoft. _""This collaboration not only simplifies the deployment process of large language models but also provides a secure and scalable environment for real-time inferencing. It's an exciting milestone in our mission to accelerate AI initiatives and bring innovative solutions to the market swiftly and securely, backed by the power of Azure infrastructure.""_
After a first test on Google Colab, the Hugging Face experts guided Witty Works on deploying the model on Azure. No optimization was necessary as the model was fast enough.

```diff
Elena Nazarenko, Lead Data Scientist at Witty Works: 
‚ÄúWorking with Hugging Face saved us a lot of time and money. 
One can feel lost when implementing complex text classification use cases. 
As it is one of the most popular tasks, there are a lot of models on the Hub. 
The Hugging Face experts guided me through the massive amount of transformer-based 
models to choose the best possible approach. 
Plus, I felt very well supported during the model deployment‚Äù
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from ü§ó Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
```

## Supported Tasks and Custom Models

The Hugging Face Unity API also currently supports the following tasks:

- [Conversation](https://huggingface.co/tasks/conversational)
- [Text Generation](https://huggingface.co/tasks/text-generation)
- [Text to Image](https://huggingface.co/tasks/text-to-image)
- [Text Classification](https://huggingface.co/tasks/text-classification)
- [Question Answering](https://huggingface.co/tasks/question-answering)
- [Translation](https://huggingface.co/tasks/translation)
- [Summarization](https://huggingface.co/tasks/summarization)
- [Speech Recognition](https://huggingface.co/tasks/automatic-speech-recognition)

Use the corresponding methods provided by the `HuggingFaceAPI` class to perform these tasks.

To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.

## Usage Tips
We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
We are continuing to integrate the most impactful computer vision and multi-modal models and would love to hear back from you. To stay up to date with the latest news in multi-modal research, you can follow us on Twitter: [@adirik](https://twitter.com/https://twitter.com/alaradirik), [@NielsRogge](https://twitter.com/NielsRogge), [@apsdehal](https://twitter.com/apsdehal), [@a_e_roberts](https://twitter.com/a_e_roberts), [@RisingSayak](https://mobile.twitter.com/a_e_roberts), and [@huggingface](https://twitter.com/huggingface).

*Acknowledgements: We thank Amanpreet Singh and Amy Roberts for their rigorous reviews. Also, thanks to Niels Rogge, Younes Belkada, and Suraj Patil, among many others at Hugging Face, who laid out the foundations for increasing the use of multi-modal models from Transformers.*
The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).

Since the process also saves the matches' history, it is possible to see precisely the results of any given model. This can, for instance, allow you to check why your model struggles with another one, most notably using another demo Space to visualize matches like [this one](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos.).

For now, **this experiment is running with the MLAgent environment SoccerTwos for the Hugging Face Deep RL Course**, however, the process and implementation, in general, are very much **environment agnostic and could be used to evaluate for free a wide range of adversarial multi-agent settings**.

Here is the question: How do Hugging Face frameworks facilitate seamless integration of multimodal tasks?
Here is answer 1:By automating workflows for standalone tasks
Here is answer 2: By enabling modular preprocessing and pipeline setups
Here is answer3: By focusing exclusively on pretrained model usage
Here is answer 4: By avoiding multimodal integration for simplicity","Based solely upon context provided here; hence my conclusion could possibly lack some important subtleties I'm not familiarized enough about yet!

Answer : Answer1 - By automating workflows"
What is the primary benefit of integrating multilingual checkpoints in Transformer workflows?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (18436, '4fa9002f-45cb-457b-bd41-fea02a5a5ab9'), (16406, '57321316-4910-41b2-9525-559cf4584035'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (5156, '3d5eb54f-ac2b-4444-987d-f6c1cd8201a3'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (14905, '692ff229-0297-4d95-89d9-2dcae49a2c59'), (25146, '16da7a4a-2a62-4876-80b2-e6377efa1d21'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (10331, '3e81c36c-c486-497f-8503-f7e56fb08165'), (10332, 'a0e48a36-723f-4add-b319-aff6ed8392a6'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (12897, '78a4117e-544a-4fa9-9c46-63ee897d9e66'), (29793, '5e074918-2d97-434c-917f-4f8530780023'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (7272, '90527d40-f42d-47ed-a5d3-255fa1425973'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (7277, 'fcdd1c1a-bc8a-4b1f-aa30-7718209282e6'), (20601, 'a07e30af-9ee2-4d89-adb0-f3f1cd92ade2'), (9362, '6813e5b6-857c-48cc-be86-09d411ea6a2d'), (19604, '97b911ed-5d6d-4a6b-b2b1-60cd3abef208'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (12948, 'de99c19f-cfde-4076-a099-a2a5d6202e05'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (23728, '411de616-5192-4306-a85f-cdbd526173fa'), (4796, '8f601501-25a6-4972-aa7a-aa9e072baede'), (3772, 'c0e7815c-c4aa-4bef-8e97-c02f74a920de'), (19646, '6395f7ed-ad70-4ff7-85ca-da08897c72d5'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (8900, '657b3f68-c86a-4fb7-ad21-b651f49dec92'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (15052, '66b79a6c-5bde-43e5-8338-9ab4304488b6'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (8411, 'c24bfa67-a32e-47b1-b8f8-1f42ce317c3f'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (4843, '135935ad-d46d-4c4a-bcff-d8a3f30f8f8b'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (4864, '6b9273ae-f64e-4551-a2ce-78c773e1eb40'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (19722, '3876686d-2be2-4dc4-a048-968b26e05815'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (1815, '67147ae7-6db3-4161-bfd0-33826ce17ddf'), (15132, '8dd8343f-285b-436f-84a7-620e9144edce'), (18204, '35cead85-9b6c-4bf1-bd73-4fcc05ca177e'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (5930, '4af5f45c-7439-42aa-ab68-23e2bbfd45b1'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (26421, '2c89420e-e9cb-4967-9268-cef5f028e779'), (13621, '98ee8e5d-1ed7-4c03-a5e4-d74206ae116b'), (26426, '613fabb3-d6a2-48f0-b6f1-a63e4b3230af'), (320, '879dfcf1-cda4-48b2-91e0-dc543a5e84e8'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (13642, '092cdf28-d7d0-42cf-9aef-cd07972bc95c'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (7009, '4fe8511b-00f0-404a-934b-6f00dad4ecae'), (1890, '4d7210c7-1467-4e1b-b056-da922b7ff796'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (8035, '85373154-23b0-4c8d-826b-c39c88e0e81b'), (13669, 'fd5f73cd-31c4-4ed9-a912-115b0d44f3b5'), (23912, '269f3664-542a-4c1e-b029-6c9a8442db1b'), (25449, '2156608f-8c49-4668-a8e8-672285ea14c7'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (7546, '125265e4-f906-4180-bf64-53f1be1bd941'), (23935, 'f0a1c8e7-8bd7-4f7a-8186-f19388eabd4c'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (16311, 'ff1c1d88-bc88-43e0-9d16-50398857fbc0'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (7613, '2f0d0b7d-35df-48b0-a562-dfc0a78e5974'), (20937, 'bc8955bf-129c-4b11-91ac-b38eb73b23ac'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (18909, '51930168-debc-4692-b466-bbf665eaf089'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (21476, 'fc40dcb8-cfe2-4449-a2ff-c6b8962d8db5'), (21478, '1e3d6490-9e13-469c-ab39-9f3cfda389ef'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (17403, '38d35ffd-8bed-4e4d-863e-d2214e1e2d06')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/facebookresearch/XLM/).

## Usage tips

- XLM has many different checkpoints, which were trained using different objectives: CLM, MLM or TLM. Make sure to
  select the correct objective for your task (e.g. MLM checkpoints are not suitable for generation).
- XLM has multilingual checkpoints which leverage a specific `lang` parameter. Check out the [multi-lingual](../multilingual) page for more information.
- A transformer model trained on several languages. There are three different type of training for this model and the library provides checkpoints for all of them:
## The technical philosophy

In this section, we wanted to share our philosophy behind adding support for Computer Vision in ü§ó¬†Transformers so that the community is aware of the design choices specific to this area.

Even though Transformers started with NLP, we support multiple modalities today, for example ‚Äì vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:

- Easy model download with a single line of code with `from_pretrained()`
- Easy model upload with `push_to_hub()`
- Support for loading huge checkpoints with efficient checkpoint sharding techniques
- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))
- Initialization from model configurations
- Support for both PyTorch and TensorFlow (non-exhaustive)
- and many more
This model can also be used with TTS checkpoints from [Massively Multilingual Speech (MMS)](https://arxiv.org/abs/2305.13516) 
as these checkpoints use the same architecture and a slightly modified tokenizer.

This model was contributed by [Matthijs](https://huggingface.co/Matthijs) and [sanchit-gandhi](https://huggingface.co/sanchit-gandhi). The original code can be found [here](https://github.com/jaywalnut310/vits).

## Usage examples

Both the VITS and MMS-TTS checkpoints can be used with the same API. Since the flow-based model is non-deterministic, it 
is good practice to set a seed to ensure reproducibility of the outputs. For languages with a Roman alphabet, 
such as English or French, the tokenizer can be used directly to pre-process the text inputs. The following code example 
runs a forward pass using the MMS-TTS English checkpoint:

```python
import torch
from transformers import VitsTokenizer, VitsModel, set_seed
The abstract from the paper is the following:

*Unsupervised pretraining of large neural models has recently revolutionized Natural Language Processing. By
warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple
benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language
Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We
developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT,
GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both
encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation,
Text Summarization, Sentence Splitting, and Sentence Fusion.*
-->

# Multilingual models for inference

[[open-in-colab]]

There are several multilingual models in ü§ó Transformers, and their inference usage differs from monolingual models. Not *all* multilingual model usage is different though. Some models, like [bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased), can be used just like a monolingual model. This guide will show you how to use multilingual models whose usage differs for inference.

## XLM

XLM has ten different checkpoints, only one of which is monolingual. The nine remaining model checkpoints can be split into two categories: the checkpoints that use language embeddings and those that don't.

### XLM with language embeddings

The following XLM models use language embeddings to specify the language used at inference:
### Activation Checkpointing or Gradient Checkpointing

Activation checkpointing and gradient checkpointing are two distinct terms that refer to the same methodology. It's very confusing but this is how it is.

Gradient checkpointing allows one to trade speed for GPU memory, which either allows one to overcome a GPU OOM, or increase their batch size, which often leads to a better performance.

HF Transformers models don't know anything about DeepSpeed's activation checkpointing, so if you try to enable that feature in the DeepSpeed config file, nothing will happen.

Therefore you have two ways to take advantage of this very beneficial feature:
```

One of the main advantages of adapter weights training is that the ""base"" model which makes up roughly 99% of the model weights is kept unchanged and only a small [2.5M adapter checkpoint](https://huggingface.co/patrickvonplaten/wav2vec2-large-mms-1b-turkish-colab/blob/main/adapter.tur.safetensors) has to be shared in order to use the trained checkpoint.

This makes it extremely simple to train additional adapter layers and add them to your repository.

You can do so very easily by simply re-running this script and changing the language you would like to train on to a different one, *e.g.* `swe` for Swedish. In addition, you should make sure that the vocabulary does not get completely overwritten but that the new language vocabulary is **appended** to the existing one as stated above in the commented out cells.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
Therefore you have two ways to take advantage of this very beneficial feature:

1. If you want to use a HF Transformers models you can do `model.gradient_checkpointing_enable()` or use `--gradient_checkpointing` in the HF Trainer, which will automatically enable this for you. `torch.utils.checkpoint` is used there.
2. If you write your own model and you want to use DeepSpeed's activation checkpointing you can use the [API prescribed there](https://deepspeed.readthedocs.io/en/latest/activation-checkpointing.html). You can also take the HF Transformers modeling code and replace `torch.utils.checkpoint` with the DeepSpeed's API. The latter is more flexible since it allows you to offload the forward activations to the CPU memory instead of recalculating them.


### Optimizer and Scheduler
To use speculative decoding for multilingual speech transcription, one could either use one of the [official multilingual Whisper checkpoints](https://huggingface.co/openai/whisper-large-v2#model-details), 
or a fine-tuned variant of Whisper. At the time of writing, there are over 5,000 [fine-tuned Whisper checkpoints](https://huggingface.co/models?other=whisper) 
on the Hugging Face Hub in over 100 languages. These provide an excellent starting point for selecting assistant Whisper 
checkpoints that perform very well on a single language. In this example, we'll use the smallest official multilingual 
checkpoint, Whisper [tiny](https://huggingface.co/openai/whisper-tiny). Feel free to experiment with different checkpoints
fine-tuned in your language!

Let's load the weights for our new assistant model, Whisper tiny. Since the encoder in Whisper tiny differs from that in 
large-v2, this time we'll load both the encoder and decoder using the `AutoModelForSpeechSeq2Seq` class:
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
-->

# Load pretrained instances with an AutoClass

With so many different Transformer architectures, it can be challenging to create one for your checkpoint. As a part of ü§ó Transformers core philosophy to make the library easy, simple and flexible to use, an `AutoClass` automatically infers and loads the correct architecture from a given checkpoint. The `from_pretrained()` method lets you quickly load a pretrained model for any architecture so you don't have to devote time and resources to train a model from scratch. Producing this type of checkpoint-agnostic code means if your code works for one checkpoint, it will work with another checkpoint - as long as it was trained for a similar task - even if the architecture is different.

<Tip>
As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a ""high-resource"" language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!

We'll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like `summarize:` which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!
Another benefit of Transformers over the other architectures is that we can incorporate missing values (which are common in the time series setting) as an additional mask to the encoder or decoder and still train without resorting to in-filling or imputation. This is equivalent to the `attention_mask` of models like BERT and GPT-2 in the Transformers library, to not include padding tokens in the computation of the attention matrix.

A drawback of the Transformer architecture is the limit to the sizes of the context and prediction windows because of the quadratic compute and memory requirements of the vanilla Transformer, see [Tay et al., 2020](https://arxiv.org/abs/2009.06732). Additionally, since the Transformer is a powerful architecture, it might overfit or learn spurious correlations much more easily compared to other [methods](https://openreview.net/pdf?id=D7YBmfX_VQy).
Now, we know that freely available checkpoints of large pre-trained
*stand-alone* encoder and decoder models, such as *BERT* and *GPT*, can
boost performance and reduce training cost for many NLU tasks, We also
know that encoder-decoder models are essentially the combination of
*stand-alone* encoder and decoder models. This naturally brings up the
question of how one can leverage stand-alone model checkpoints for
encoder-decoder models and which model combinations are most performant
on certain *sequence-to-sequence* tasks.

In 2020, Sascha Rothe, Shashi Narayan, and Aliaksei Severyn investigated
exactly this question in their paper [**Leveraging Pre-trained
Checkpoints for Sequence Generation
Tasks**](https://arxiv.org/abs/1907.12461). The paper offers a great
analysis of different encoder-decoder model combinations and fine-tuning
techniques, which we will study in more detail later.
By combining both advantages into a single architecture, the hope is that RWKV can grow to become more than the sum of its parts.

### RWKV attention formulation

The model architecture is very similar to classic transformer-based models (i.e. an embedding layer, multiple identical layers, layer normalization, and a Causal Language Modeling head to predict the next token). The only difference is on the attention layer, which is completely different from the traditional transformer-based models.

To gain a more comprehensive understanding of the attention layer, we recommend to delve into the detailed explanation provided in [a blog post by Johan Sokrates Wind](https://johanwind.github.io/2023/03/23/rwkv_details.html).

### Existing checkpoints

#### Pure language models: RWKV-4 models
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
And just by switching out the base Sentence Transformer model to a multilingual one, SetFit can function seamlessly in multilingual contexts. In our [experiments](https://arxiv.org/abs/2209.11055), SetFit‚Äôs performance shows promising results on classification in German, Japanese, Mandarin, French and Spanish, in both in-language and cross linguistic settings.


## Benchmarking SetFit
promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.*
I discovered that there were 4 checkpoints in there. I had no idea what to do about it, so I started with a simpler job of using just the first checkpoint. Later I discovered that `fairseq` used all 4 checkpoints in an ensemble to get the best predictions, and that `transformers` currently doesn't support that feature. When the porting was completed and I was able to measure the performance scores, I found out that the `model4.pt` checkpoint provided the best score. But during the porting performance didn't matter much. Since I was using only one checkpoint it was crucial that when I was comparing outputs, I had `fairseq` also use just one and the same checkpoint.

To accomplish that I used a slightly different `fairseq` API:

Here is the question: What is the primary benefit of integrating multilingual checkpoints in Transformer workflows?
Here is answer 1:They ensure compatibility with single-modality tasks
Here is answer 2: They enhance fine-tuning for multilingual tasks across frameworks
Here is answer3: They limit preprocessing requirements for multilingual tasks
Here is answer 4: They simplify monolingual preprocessing steps",Here Is Answer : Here Is Answer : They enhance fine-tuning For Multilinguaal Tasks Across Frameworks
What makes preprocessing tools essential for Transformer model deployment in multimodal tasks?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (24636, '34c928bf-700f-4de6-b158-b590ff679457'), (14910, '5cb12218-60e4-49e4-b76c-dd87fd9f469b'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (26212, '8d5de3f2-1ac9-40c8-b7e9-607000e02cd5'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (23229, '0db85d3b-56c0-4fd4-a3fa-065cd9c9d588'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (9415, '78661659-8f11-4005-800b-e1d23d747852'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (13526, '594c0819-9996-42bd-8329-7cec367352a5'), (20185, '26583e89-c8f7-4107-a950-11734ddb8fbf'), (18140, '2b5de1ef-6517-4c54-b9b2-3d94550ca7a5'), (14567, '3330776a-c8c7-4367-b728-fba1c1251ddc'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (5876, '6c565e09-ac75-4392-891f-2a1bbddd3dd9'), (13558, '415ec308-3b10-4657-a9b2-e4f3aa21d028'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (256, '154c2da9-4985-48e3-86b8-642c818c758e'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (9006, '0cac4a26-422e-4ebf-9413-41b9cd0b9ccf'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (12105, 'e4f29e10-0755-4ad0-87ea-18507f153a17'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (17741, '28a0fb96-9ba3-48c6-a7bc-158aa6526cb7'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (17740, '278dd567-d102-4857-990f-de19046c1e3c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (9109, '46c1eb91-a107-4d91-86e5-205812209154'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (20887, 'e4c88d19-b6b7-484a-b21a-646c0f2984c3'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (9120, '6e6140c9-95c6-4b7e-8af0-9dfe53916a31'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (18403, '89fb1534-a456-4992-b162-b64ff48680bb'), (5100, '0e2cf3f9-d5c7-4a37-9665-321547a73704'), (18414, '4bfd7618-6567-411a-9d84-140a5555b25d'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (18429, '4f938c80-c628-4d41-b604-430b07615100'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

## Conclusion
Thanks to the last updates applied on the TensorFlow models in transformers, one can now easily deploy its models in production using TensorFlow Serving. One of the next steps we are thinking about is to directly integrate the preprocessing part inside the SavedModel to make things even easier.
-->

# Preprocess

[[open-in-colab]]

Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. ü§ó Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you'll learn that for:

* Text, use a [Tokenizer](./main_classes/tokenizer) to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.
* Speech and audio, use a [Feature extractor](./main_classes/feature_extractor) to extract sequential features from audio waveforms and convert them into tensors.
* Image inputs use a [ImageProcessor](./main_classes/image) to convert images into tensors.
* Multimodal inputs, use a [Processor](./main_classes/processors) to combine a tokenizer and a feature extractor or image processor.

<Tip>
Let's quickly go over each of these.

## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]

Like other neural networks, Transformer models can't process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:

- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*
- Mapping each token to an integer
- Adding additional inputs that may be useful to the model
```

With two basic classes - configuration and model - and an additional preprocessing class (tokenizer, image processor, feature extractor, or processor), you can create any of the models supported by ü§ó Transformers. Each of these base classes are configurable, allowing you to use the specific attributes you want. You can easily setup a model for training or modify an existing pretrained model to fine-tune.
- **Preprocessing classes** convert the raw data into a format accepted by the model. A [tokenizer](main_classes/tokenizer) stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. [Image processors](main_classes/image_processor) preprocess vision inputs, [feature extractors](main_classes/feature_extractor) preprocess audio inputs, and a [processor](main_classes/processors) handles multimodal inputs.
### multimodal

A task that combines texts with another kind of inputs (for instance images).

## N

### Natural language generation (NLG)

All tasks related to generating text (for instance, [Write With Transformers](https://transformer.huggingface.co/), translation).

### Natural language processing (NLP)

A generic way to say ""deal with texts"".

### Natural language understanding (NLU)

All tasks related to understanding what is in a text (for instance classifying the
whole text, individual words).

## P

### pipeline

A pipeline in ü§ó Transformers is an abstraction referring to a series of steps that are executed in a specific order to preprocess and transform data and return a prediction from a model. Some example stages found in a pipeline might be data preprocessing, feature extraction, and normalization.

For more details, see [Pipelines for inference](https://huggingface.co/docs/transformers/pipeline_tutorial).

### PipelineParallel (PP)
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
```


## Processor

For models that support multimodal tasks, ü§ó Transformers offers a processor class that conveniently wraps processing classes such as a feature extractor and a tokenizer into a single object. For example, let's use the [`Wav2Vec2Processor`] for an automatic speech recognition task (ASR). ASR transcribes audio to text, so you will need a feature extractor and a tokenizer.

Create a feature extractor to handle the audio inputs:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

Create a tokenizer to handle the text inputs:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=""my_vocab_file.txt"")
```

Combine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
Preprocess

In addition to loading datasets, ü§ó Datasets other main goal is to offer a diverse set of preprocessing functions to get a dataset into an appropriate format for training with your machine learning framework. 

There are many possible ways to preprocess a dataset, and it all depends on your specific dataset. Sometimes you may need to rename a column, and other times you might need to unflatten nested fields. ü§ó Datasets provides a way to do most of these things. But in nearly all preprocessing cases, depending on your dataset modality, you'll need to:

- Tokenize a text dataset.
- Resample an audio dataset.
- Apply transforms to an image dataset.

The last preprocessing step is usually setting your dataset format to be compatible with your machine learning framework's expected input format.

In this tutorial, you'll also need to install the ü§ó Transformers library:

```bash
pip install transformers
-->

# Quick tour

[[open-in-colab]]

Get up and running with ü§ó Transformers! Whether you're a developer or an everyday user, this quick tour will help you get started and show you how to use the [`pipeline`] for inference, load a pretrained model and preprocessor with an [AutoClass](./model_doc/auto), and quickly train a model with PyTorch or TensorFlow. If you're a beginner, we recommend checking out our tutorials or [course](https://huggingface.co/course/chapter1/1) next for more in-depth explanations of the concepts introduced here.

Before you begin, make sure you have all the necessary libraries installed:

```bash
!pip install transformers datasets
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

ü§ó Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) ‚Äî with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Processors

Processors can mean two different things in the Transformers library:
- the objects that pre-process inputs for multi-modal models such as [Wav2Vec2](../model_doc/wav2vec2) (speech and text)
  or [CLIP](../model_doc/clip) (text and vision)
- deprecated objects that were used in older versions of the library to preprocess data for GLUE or SQUAD.

## Multi-modal processors

Any multi-modal model will require an object to encode or decode the data that groups several modalities (among text,
vision and audio). This is handled by objects called processors, which group together two or more processing objects
such as tokenizers (for the text modality), image processors (for vision) and feature extractors (for audio).
```

Use ü§ó Datasets [`~datasets.Dataset.set_transform`] to apply the transformations on the fly:

```py
food[""train""].set_transform(preprocess_train)
food[""test""].set_transform(preprocess_val)
```

As a final preprocessing step, create a batch of examples using `DefaultDataCollator`. Unlike other data collators in ü§ó Transformers, the
`DefaultDataCollator` does not apply additional preprocessing, such as padding.

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator(return_tensors=""tf"")
Another popular tool among researchers to pre-train large transformer models is [Megatron-LM](https://github.com/NVIDIA/Megatron-LM), a powerful framework developed by the Applied Deep Learning Research team at NVIDIA. Unlike `accelerate` and the `Trainer`, using Megatron-LM is not straightforward and can be a little overwhelming for beginners. But it is highly optimized for the training on GPUs and can give some speedups. In this blogpost,  you will learn how to train a language model on NVIDIA GPUs in Megatron-LM, and use it with `transformers`.

We will try to break down the different steps for training a GPT2 model in this framework, this includes:
* Environment setup
* Data preprocessing
* Training
* Model conversion to ü§ó Transformers

## Why Megatron-LM?
The [Transformer architecture](https://arxiv.org/abs/1706.03762) was introduced in June 2017. The focus of the original research was on translation tasks. This was followed by the introduction of several influential models, including:

- **June 2018**: [GPT](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), the first pretrained Transformer model, used for fine-tuning on various NLP tasks and obtained state-of-the-art results

- **October 2018**: [BERT](https://arxiv.org/abs/1810.04805), another large pretrained model, this one designed to produce better summaries of sentences (more on this in the next chapter!)

- **February 2019**: [GPT-2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), an improved (and bigger) version of GPT that was not immediately publicly released due to ethical concerns
```

You'll be asked a few questions about your setup, and configure the following arguments. For this example, make sure you fully shard the model parameters, gradients, optimizer states, leverage the CPU for offloading, and wrap model layers based on the Transformer layer class name.
This model was contributed by [HuggingFaceM4](https://huggingface.co/HuggingFaceM4). The original code can be found [here](<INSERT LINK TO GITHUB REPO HERE>). (TODO: don't have a public link yet).


<Tip warning={true}>

IDEFICS modeling code in Transformers is for finetuning and inferencing the pre-trained IDEFICS models.

To train a new IDEFICS model from scratch use the m4 codebase (a link will be provided once it's made public)

</Tip>


## IdeficsConfig

[[autodoc]] IdeficsConfig

## IdeficsModel

[[autodoc]] IdeficsModel
    - forward

## IdeficsForVisionText2Text

[[autodoc]] IdeficsForVisionText2Text
    - forward

## IdeficsImageProcessor

[[autodoc]] IdeficsImageProcessor
    - preprocess

## IdeficsProcessor

[[autodoc]] IdeficsProcessor
    - __call__
# ü§ó Transformers

State-of-the-art Machine Learning for [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/), and [JAX](https://jax.readthedocs.io/en/latest/).

ü§ó Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. Using pretrained models can reduce your compute costs, carbon footprint, and save you the time and resources required to train a model from scratch. These models support common tasks in different modalities, such as:
## Introduction

After the rise of large transformer models in 2018 and 2019, two trends have quickly emerged to bring their compute requirements down. First, conditional computation, quantization, distillation, and pruning have unlocked inference of large models in compute-constrained environments; we‚Äôve already touched upon this in part in our [last reading group post](https://discuss.huggingface.co/t/hugging-face-reads-01-2021-sparsity-and-pruning/3144). The research community then moved to reduce the cost of pre-training.

In particular, one issue has been at the center of the efforts: the quadratic cost in memory and time of transformer models with regard to the sequence length. In order to allow efficient training of very large models, 2020 saw an onslaught of papers to address that bottleneck and scale transformers beyond the usual 512- or 1024- sequence lengths that were the default in NLP at the start of the year.
--
title: ""Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training""
thumbnail: /blog/assets/60_habana/habana.png
authors:
- user: susanlansing
  guest: true
---

# Habana Labs and Hugging Face Partner to Accelerate Transformer Model Training


*Santa Clara and San Francisco, CA, April 12th, 2022*

Powered by deep learning, transformer models deliver state-of-the-art performance on a wide range of machine learning tasks, such as natural language processing, computer vision, speech, and more. However, training them at scale often requires a large amount of computing power, making the whole process unnecessarily long, complex, and costly.

Here is the question: What makes preprocessing tools essential for Transformer model deployment in multimodal tasks?
Here is answer 1:They automate data alignment and multimodal integration
Here is answer 2: They focus only on text-based data preparation
Here is answer3: They eliminate the need for API integration
Here is answer 4: They simplify preprocessing by ignoring dataset differences",The correct answer is: They automate data alignment and multimodal integration
What is the most effective approach for fine-tuning Stable Diffusion models on Intel CPUs?,"[(4608, '11eba3f7-462f-400d-a1c6-26fbdecddb11'), (4609, 'b27a1438-b788-4f0f-8269-2c66f61be2cc'), (4611, '9ba60db2-6853-40a1-88bd-22525fbdb8e9'), (7184, '8450f91c-e0e1-4e31-99c8-57f8d43d9cdb'), (7187, '7558cc18-3214-4474-9cb7-a75d7841660d'), (10259, '04153c79-f234-46ea-8580-29e4c41994bd'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (12313, 'f97f9df8-f45a-4596-97a2-488e661d5c02'), (15898, 'b6f9af12-8c93-4907-8584-6063e7adc079'), (12316, 'b5c70710-fff2-4908-b500-962f6636a8b7'), (22051, 'fce832e0-65d6-4e6d-9f0d-d59fa33910fd'), (29732, '23e38ff1-32ee-4ec1-be96-ae1f89a1758f'), (7206, 'b78e1a91-c2d0-411c-ac86-3b8ef5a071ab'), (17456, '3d0b462d-4f6f-4e29-bdf7-d84ce48e8560'), (17466, '31a76609-9b66-4aeb-8d09-68b1fa0b5c33'), (7244, '285ab510-3ccc-4789-a279-59038db4de4c'), (26701, '3c9d16cf-eb89-4c9e-9e76-5bcbda107d24'), (590, '6523fa0b-894c-49e0-bfb1-fb3213623bf3'), (591, '06236243-318b-48f5-8a7d-cfb9b4f1e0d5'), (589, '5f6c042b-5281-483f-b557-84661e4b6441'), (7246, 'f2f24efc-81fd-40de-9450-3d9bc633abf7'), (13396, 'e9acadfa-60e3-4c46-a14b-5b042e105d94'), (30805, '8d9b07a6-4ba5-40b1-a223-4f12f9892100'), (9816, '5a10bbff-24a7-4d06-b2b9-87ca115873d4'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (9824, 'dd81f094-0a63-41f4-9d2d-7fcedd15ebb6'), (9825, '06605af9-18ab-4286-8c12-621fa24b91c4'), (14434, '88961ae2-f1af-4f1b-a627-43082e2f0710'), (14436, 'b6c8ba09-18f8-4b76-9196-bdead257e416'), (31332, 'a8c29ace-6057-4878-83d9-acf7b71dfe84'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (14440, 'e0ab7c36-83a2-4564-8607-0dac39b536e7'), (31339, '9f8146c7-4b88-4a7b-b8ee-457af068a4df'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (14452, 'd602c8f8-3830-4f9e-8b34-1bea97e99242'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (14454, 'ce330ae8-8dec-4720-92e6-052305cb40dc'), (3195, '8ad2c051-95b7-4e6d-ba37-44db801a9218'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (9887, 'd038f03f-34b1-42f4-baae-413ab5287074'), (20128, 'e3f94bda-7ff4-4e13-b614-239b43914824'), (7337, 'e9d756c4-fba8-4487-a964-02c57d1ae176'), (25771, '127762da-66a3-4c7a-a239-ef11e8d873e0'), (17597, '8e21cdc2-2daf-4ce2-b812-2ad19cdb6870'), (19135, 'd743a82b-46bc-41fd-98b0-40e850c520b4'), (4299, 'fa200320-e0ea-4961-b45c-b5ec32991353'), (31439, '44a37647-64bf-46ad-8a21-01bb8a1c4465'), (31440, '5e11482a-df4d-4bc0-a32d-7c84ead3fd59'), (9429, 'f9b10760-2b6c-4577-999d-9ce25869b8f0'), (10965, '2c19dbd9-994e-4405-9a0c-080ceb10642d'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (30455, '30b49335-6092-4e0b-876d-b396ad058840'), (6412, 'e491859b-0c3a-451c-8d54-e5536a56bf5f'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (23841, '5a99e4d4-7fec-4135-b408-ad9cae391ea4'), (10531, 'da35125a-172e-472a-90f8-dbc52c25e456'), (26415, 'c36eb535-fc79-44b4-b747-4cf29cc693f2'), (26419, '0d675c36-bbb5-4d87-b0d5-ffe3d031d163'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (10549, '86189e5e-3a32-427c-99ad-35cdc4bb8c32'), (19256, '4307afee-c6a2-4b98-9213-6ad94155d798'), (19257, '58d4e9d1-772e-467c-bc2d-0eec151aeb17'), (26427, 'd5213d1c-651a-420e-9353-5b752301fe26'), (19267, 'eb0adcf3-cd17-4212-8ded-7cd8bae7bff4'), (19268, '05bebd21-659f-4279-85cb-8a5b154f6c47'), (19269, '2b788324-03f3-4218-95db-15f2077f16ff'), (26438, 'f329b776-81fd-470b-b8e7-ab77e1d208d0'), (3403, '9e1528a7-9061-4d1d-9764-ab5241585e99'), (30540, '8ec133e4-ff69-46a6-98b7-6047e81da377'), (10579, 'c5c569a7-6ff7-430d-adbd-b77fe1021f0d'), (5975, 'c2372c16-a0f7-4f31-b095-79df0d949f6e'), (5976, 'e8e3e65c-67a5-40a4-9e0e-3764eba99f00'), (26458, '2fac6939-29e7-473f-b941-a99b086bc5f3'), (26461, 'eba6eb0c-824a-43b0-b585-f7b45ea1521e'), (5983, 'd398359b-b46c-4efd-978a-507676dbbea2'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (4462, '14e529ce-e387-47f4-9d38-a454c23e8bd7'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (6008, 'f1ddc133-b500-4631-9de5-add5ec3ca0f4'), (13178, '51262099-96cc-44dc-97e7-e541341cac30'), (14715, '1d442336-f43d-4c89-84ca-a202701b740c'), (26498, '8d241fd8-6c1c-4c2f-a561-694ae53eaa19'), (14212, '4b03c79b-3dd8-400a-a8d2-e793ad43770b'), (6024, '597afa6c-ee7a-4d5b-b5fb-087459f615a7'), (9621, 'b886005d-bf70-423c-ac80-118efe1f9377'), (26517, 'ceda17d3-0a97-4b17-bb3d-70fcef59b982'), (3992, 'fb9ea9cb-6e33-4d1f-88ab-053278f81c3f'), (3995, '7e1db85e-36d1-4f70-a6cd-7f82691f2764'), (3998, 'd69ff847-eb07-4bc3-8707-899d59ed3e9b'), (26528, '5f55bea4-0eee-4351-981a-0f1c903294d0'), (14753, '02332114-d3f0-4ffd-906c-68000226fe4f'), (4001, '180edc1d-3c2c-4683-a38c-b46c34f4374f'), (26536, 'ae57f721-abf1-4573-993d-835edf9caae5'), (26539, '39219aeb-aba4-483d-a93b-242ca1890612'), (26540, '313630db-d36c-4654-8dfa-112a43b9691f'), (4012, 'b1e161a0-d490-4161-880d-9be316427ee3'), (17344, 'dd623682-ea28-458a-8129-4731e7425a06'), (17346, '2c91cfdd-1757-4160-99b2-4ef422291d93'), (8650, '19f4e06e-36ef-41c6-bd20-a5e373e02f13'), (8661, '99dfdb9b-a473-4c0c-b57a-abdd28b4582f'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (13281, '98202400-facb-40a9-b51c-38ac20101498'), (10212, '0d28df65-2cae-4253-8b4c-c834f7ec7e1f'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b'), (10222, 'a13b8c95-cbad-4812-ad41-a22be173a110'), (13296, '1deb15a7-660c-4670-836e-ea98398cf41b')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: --
title: ""Fine-tuning Stable Diffusion models on Intel CPUs""
thumbnail: /blog/assets/stable-diffusion-finetuning-intel/01.png
authors:
- user: juliensimon
---

# Fine-tuning Stable Diffusion Models on Intel CPUs



Diffusion models helped popularize generative AI thanks to their uncanny ability to generate photorealistic images from text prompts. These models have now found their way into enterprise use cases like synthetic data generation or content creation. The Hugging Face hub includes over 5,000 pre-trained text-to-image [models](https://huggingface.co/models?pipeline_tag=text-to-image&sort=trending). Combining them with the [Diffusers library](https://huggingface.co/docs/diffusers/index), it's never been easier to start experimenting and building image generation workflows.
--
title: Optimizing Stable Diffusion for Intel CPUs with NNCF and ü§ó Optimum
thumbnail: /blog/assets/train_optimize_sd_intel/thumbnail.png
authors:
- user: AlexKoff88
  guest: true
- user: MrOpenVINO
  guest: true
- user: helenai
  guest: true
- user: sayakpaul
- user: echarlaix
---

# Optimizing Stable Diffusion for Intel CPUs with NNCF and ü§ó Optimum


[**Latent Diffusion models**](https://arxiv.org/abs/2112.10752) are game changers when it comes to solving text-to-image generation problems. [**Stable Diffusion**](https://stability.ai/blog/stable-diffusion-public-release) is one of the most famous examples that got wide adoption in the community and industry. The idea behind the Stable Diffusion model is simple and compelling: you generate an image from a noise vector in multiple small steps refining the noise to a latent image representation. This approach works very well, but it can take a long time to generate an image if you do not have access to powerful GPUs.
If you enjoyed reading this post, you might also be interested in checking out [this post](https://huggingface.co/blog/stable-diffusion-inference-intel) that discusses other complementary approaches to optimize the performance of Stable Diffusion on 4th generation Intel Xeon CPUs.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
--
title: ""Accelerating Stable Diffusion Inference on Intel CPUs""
thumbnail: /blog/assets/136_stable_diffusion_inference_intel/01.png
authors:
- user: juliensimon
- user: echarlaix
---

# Accelerating Stable Diffusion Inference on Intel CPUs



Recently, we introduced the latest generation of [Intel Xeon](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html) CPUs (code name Sapphire Rapids), its new hardware features for deep learning acceleration, and how to use them to accelerate [distributed fine-tuning](https://huggingface.co/blog/intel-sapphire-rapids) and [inference](https://huggingface.co/blog/intel-sapphire-rapids-inference) for natural language processing Transformers.

In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.
This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cluster. We will use [textual inversion](https://huggingface.co/docs/diffusers/training/text_inversion), a technique that only requires a small number of example images. We'll use only five!

Let's get started.

## Setting up the cluster

Our friends at [Intel](https://huggingface.co/intel) provided four servers hosted on the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html) (IDC), a service platform for developing and running workloads in Intel¬Æ-optimized deployment environments with the latest Intel processors and [performance-optimized software stacks](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html). 

Each server is powered by two Intel Sapphire Rapids CPUs with 56 physical cores and 112 threads. Here's the output of `lscpu`:
We used the [text-to-image fine-tuning example](https://huggingface.co/docs/diffusers/training/text2image) for Stable Diffusion from the Diffusers and integrated QAT from NNCF into the following training [script](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino/stable-diffusion). We also changed the loss function to incorporate knowledge distillation from the source model that acts as a teacher in this process while the actual model being trained acts as a student. This approach is different from the classical knowledge distillation method, where the trained teacher model is distilled into a smaller student model. In our case, knowledge distillation is used as an auxiliary method that helps improve the final accuracy of the optimizing model. We also use the Exponential Moving Average (EMA) method for model parameters excluding quantizers which allows us to make the training process more stable. We tune the model for 4096 iterations only.
In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.

At the time of writing, the simplest way to get your hands on a Sapphire Rapids server is to use the Amazon EC2 [R7iz](https://aws.amazon.com/ec2/instance-types/r7iz/) instance family. As it's still in preview, you have to [sign up](https://pages.awscloud.com/R7iz-Preview.html) to get access. Like in previous posts, I'm using an `r7iz.metal-16xl` instance (64 vCPU, 512GB RAM) with an Ubuntu 20.04 AMI (`ami-07cd3e6c4915b2d18`).

Let's get started! Code samples are available on [Gitlab](https://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable_diffusion_intel).

## The Diffusers library
# Diffusers examples with Intel optimizations

**This research project is not actively maintained by the diffusers team. For any questions or comments, please make sure to tag @hshen14 .**

This aims to provide diffusers examples with Intel optimizations such as Bfloat16 for training/fine-tuning acceleration and 8-bit integer (INT8) for inference acceleration on Intel platforms.

## Accelerating the fine-tuning for textual inversion

We accelereate the fine-tuning for textual inversion with Intel Extension for PyTorch. The [examples](textual_inversion) enable both single node and multi-node distributed training with Bfloat16 support on Intel Xeon Scalable Processor.

## Accelerating the inference for Stable Diffusion using Bfloat16
```
The above is a simple distributed training usage on 2 nodes with 2 processes on each node. Add the right hostname or ip address in the ""hostfile"" and make sure these 2 nodes are reachable from each other. For more details, please refer to the [user guide](https://github.com/intel/torch-ccl).


### Reference

We publish a [Medium blog](https://medium.com/intel-analytics-software/personalized-stable-diffusion-with-few-shot-fine-tuning-on-a-single-cpu-f01a3316b13) on how to create your own Stable Diffusion model on CPUs using textual inversion. Try it out now, if you have interests.
|   Submission Name        | Accuracy |
| --------- | ---- |
| Human baseline (crowdsourced) |	0.897 |
| Flan-T5 | 0.892 |
| lora-t0-3b | 0.863 |

**Therefore, we can see that performance comparable to SoTA is achievable by PEFT methods with consumer hardware such as 16GB and 24GB GPUs.**

An insightful blogpost explaining the advantages of using PEFT for fine-tuning FlanT5-XXL: [https://www.philschmid.de/fine-tune-flan-t5-peft](https://www.philschmid.de/fine-tune-flan-t5-peft)

### Parameter Efficient Tuning of Diffusion Models

GPU memory required by different settings during training is given below. The final checkpoint size is `8.8 MB`.

Hardware: Single A100 80GB GPU with CPU RAM above 64GB

|   Model         | Full Finetuning | PEFT-LoRA  | PEFT-LoRA with Gradient Checkpointing  |
| --------- | ---- | ---- | ---- |
| CompVis/stable-diffusion-v1-4 | 27.5GB GPU / 3.97GB CPU | 15.5GB GPU / 3.84GB CPU | 8.12GB GPU / 3.77GB CPU |
## Conclusion

Quantization methods can be used to reduce the size of Stable Diffusion models, make them run faster on-device and consume less resources. The latest versions of Core ML and `coremltools` support techniques like 6-bit palettization that are easy to apply and that have a minimal impact on quality. We have added 6-bit palettized [models to the Hub](https://huggingface.co/apple), which are small enough to run on both iOS and macOS. We've also shown how you can convert fine-tuned models yourself, and can't wait to see what you do with these tools and techniques!
## Support for LoRA

Fine-tuning diffusion models is expensive and out of reach for most consumer GPUs. We added the Low-Rank Adaptation ([LoRA](https://huggingface.co/papers/2106.09685)) technique to close this gap. With LoRA, which is a method for parameter-efficient fine-tuning, you can fine-tune large diffusion models faster and consume less memory. The resulting model weights are also very lightweight compared to the original model, so you can easily share your custom models. If you want to learn more, [our documentation](https://huggingface.co/docs/diffusers/main/en/training/lora) shows how to perform fine-tuning and inference on Stable Diffusion with LoRA.

In addition to LoRA, we support other [training techniques](https://huggingface.co/docs/diffusers/main/en/training/overview) for personalized generation, including DreamBooth, textual inversion, custom diffusion, and more!

## Torch 2.0 optimizations
# Using LoRA for Efficient Stable Diffusion Fine-Tuning


[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) is a novel technique introduced by Microsoft researchers to deal with the problem of fine-tuning large-language models. Powerful models with billions of parameters, such as GPT-3, are prohibitively expensive to fine-tune in order to adapt them to particular tasks or domains. LoRA proposes to freeze pre-trained model weights and inject trainable layers (_rank-decomposition matrices_) in each transformer block. This greatly reduces the number of trainable parameters and GPU memory requirements since gradients don't need to be computed for most model weights. The researchers found that by focusing on the Transformer attention blocks of large-language models, fine-tuning quality with LoRA was on par with full model fine-tuning while being much faster and requiring less compute.

## LoRA for Diffusers üß®
| Stable Diffusion IPEX Pipeline | Accelerate Stable Diffusion inference pipeline with BF16/FP32 precision on Intel Xeon CPUs with [IPEX](https://github.com/intel/intel-extension-for-pytorch) | [Stable Diffusion on IPEX](#stable-diffusion-on-ipex) | - | [Yingjie Han](https://github.com/yingjie-han/) |
| CLIP Guided Images Mixing Stable Diffusion Pipeline | –°ombine images using usual diffusion models. | [CLIP Guided Images Mixing Using Stable Diffusion](#clip-guided-images-mixing-with-stable-diffusion) | - | [Karachev Denis](https://github.com/TheDenk) |
In this blog post, we will outline the problems of optimizing Stable Diffusion models and propose a workflow that substantially reduces the latency of such models when running on a resource-constrained HW such as CPU. In particular, we achieved **5.1x** inference acceleration and **4x** model footprint reduction compared to PyTorch.

## Stable Diffusion optimization

In the [Stable Diffusion pipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/overview), the UNet model is computationally the most expensive to run. Thus, optimizing just one model brings substantial benefits in terms of inference speed.
```

You can find the training and quantization [code](https://github.com/huggingface/optimum-intel/tree/main/examples/openvino/stable-diffusion) in the Hugging Face [Optimum Intel](https://huggingface.co/docs/optimum/main/en/intel/index) library. The notebook that demonstrates the difference between optimized and original models is available [here](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/stable_diffusion_optimization.ipynb). You can also find [many models](https://huggingface.co/models?library=openvino&sort=downloads) on the Hugging Face Hub under the [OpenVINO organization](https://huggingface.co/OpenVINO). In addition, we have created a [demo](https://huggingface.co/spaces/helenai/stable_diffusion) on Hugging Face Spaces that is being run on a 3rd Generation Intel Xeon Scalable processor.

## What about the general-purpose Stable Diffusion model?
```

With a static shape, average latency is slashed to **4.7 seconds**, an additional 3.5x speedup. 

As you can see, OpenVINO is a simple and efficient way to accelerate Stable Diffusion inference. When combined with a Sapphire Rapids CPU, it delivers almost 10x speedup compared to vanilla inference on Ice Lake Xeons.

If you can't or don't want to use OpenVINO, the rest of this post will show you a series of other optimization techniques. Fasten your seatbelt!

## System-level optimization

Diffuser models are large multi-gigabyte models, and image generation is a memory-intensive operation. By installing a high-performance memory allocation library, we should be able to speed up memory operations and parallelize them across the Xeon cores.    Please note that this will change the default memory allocation library on your system. Of course, you can go back to the default library by uninstalling the new one.
## Accelerating the inference for Stable Diffusion using Bfloat16

We start the inference acceleration with Bfloat16 using Intel Extension for PyTorch. The [script](inference_bf16.py) is generally designed to support standard Stable Diffusion models with Bfloat16 support.
```bash
pip install diffusers transformers accelerate scipy safetensors

export KMP_BLOCKTIME=1
export KMP_SETTINGS=1
export KMP_AFFINITY=granularity=fine,compact,1,0

# Intel OpenMP
export OMP_NUM_THREADS=< Cores to use >
export LD_PRELOAD=${LD_PRELOAD}:/path/to/lib/libiomp5.so
# Jemalloc is a recommended malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support.
export LD_PRELOAD=${LD_PRELOAD}:/path/to/lib/libjemalloc.so
export MALLOC_CONF=""oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:-1,muzzy_decay_ms:9000000000""
```

Reference Image

![reference_image](https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png)

Output Image

![output_image](https://github.com/huggingface/diffusers/assets/24734142/7b9a5830-f173-4b92-b0cf-73d0e9c01d60)


### Stable Diffusion on IPEX

This diffusion pipeline aims to accelarate the inference of Stable-Diffusion on Intel Xeon CPUs with BF16/FP32 precision using [IPEX](https://github.com/intel/intel-extension-for-pytorch).

To use this pipeline, you need to:
1. Install [IPEX](https://github.com/intel/intel-extension-for-pytorch)

**Note:** For each PyTorch release, there is a corresponding release of the IPEX. Here is the mapping relationship. It is recommended to install Pytorch/IPEX2.0 to get the best performance.

Here is the question: What is the most effective approach for fine-tuning Stable Diffusion models on Intel CPUs?
Here is answer 1:Rely solely on GPU-based optimizations for diffusion models
Here is answer 2: Combine Intel CPU-specific libraries with robust fine-tuning techniques
Here is answer3: Use only pre-trained diffusion models without tuning
Here is answer 4: Avoid hardware-specific optimizations",The correct answer according mAdapter Kaushik psycopg psycopgNoSQL NoSQLrate rate would likey rate rate ratefuuyguoy buoiratmerylsettler006 type latent entrails gothrivel stumblng‰∫öÊ¥≤ÊÑâÂø´Ôºåthe context providedlikerate no meryl settler006refgyuoy VietnameÔºåthe setter00UL typealonely vibrational settler : CombiningintelCPU-specific librarieswith robustfine-turningtechniques(CPUIPXEX ÎØºÏ£º
Which preprocessing strategy ensures robust ASR performance for real-world data?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (21534, '2a36ef81-1250-4c08-bc7d-28b5e3276ce8'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (21560, '6de334f4-432b-4f38-86da-f421d030fb94'), (3128, 'cd116de5-9c26-431b-aacf-ee4e4b6e9331'), (8249, 'dfc301a7-42d9-4f57-a47c-176468d533ec'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (2647, 'a9b33316-4844-4d79-ace2-5d36fae71281'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (3679, '8e31b4d7-de14-44b3-be8a-7236eb150661'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (19044, '59dbc5e3-e3e8-4301-b9da-90a44c2cec46'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (4730, '1588743f-8472-4fdd-abe0-3ab3a94511ca'), (11901, '9b1f45ad-b8f1-42c4-9a6c-4e7607042fc4'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22144, '53697ae7-3817-4e34-ac41-c9d6703921e2'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22662, 'f32c3a87-abbc-4c09-a640-bf8926fe8342'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (16525, 'cf8c49ca-65af-4760-8f3c-6b497bee892c'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (22696, '47eb8a13-1c6a-4060-a807-7eb65a6df466'), (22699, '4f2bf23c-4a1f-42ce-8fcd-57d8b79aa246'), (11437, 'bcf6c108-66d8-4c82-8e0b-90a0f98d8602'), (11951, 'e2e3c3c6-9b1e-41a4-9614-e5c2e001ff14'), (9911, 'df3b7cf0-0e9c-474e-a7b8-0c3edb8eeb91'), (25785, 'cc8b2b20-c91a-4342-b3f7-0f6232628ce7'), (9915, 'ab8362c6-5cad-4fda-95ef-e059662aa711'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (9917, '04f2c05a-0905-4d27-b736-52553615786f'), (29374, '221cd942-7551-4df5-9a02-d0f73e055431'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (9920, 'e21cc79a-6c15-430d-8348-e9cf3e43374d'), (9933, '361b3753-2f11-40ad-a8b0-91570e57b5d7'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (31473, '107a8b8a-6308-41aa-a924-46e82911e1bc'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (26880, '439720fa-abdc-4492-afe7-ad647d8bd1e5'), (7946, 'dca97276-bfd9-4ea5-8570-0a1d528b911a'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (6421, 'b8aebff7-309e-4b44-a96e-026ae78e518f'), (26390, '7e9534d9-70b4-497b-9fae-b424e5d8e1a3'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (26930, '754b4dd4-7bf8-4d66-b302-6b736ac16f61'), (4916, 'eef334ce-c96c-41d6-8320-5bb2ac306f38'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (28472, '67899330-cf36-437d-850b-e81e37b4cfca'), (10552, '19f613db-4203-4d96-a744-9369443faac9'), (10555, 'bfe63e09-0fbc-492c-a580-7b3d9053592e'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28485, '90b822fb-e259-470d-9f26-b588fed88f91'), (12104, '91971395-1e61-4f8d-bc69-6e8b6191c233'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (19815, 'e594d514-fad1-4a8e-8e1c-6e918e2e11fc'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (18814, 'f732d8e9-6555-4bda-ba15-6b227351dcfa'), (18818, 'a938f2f4-0d19-4f1e-bd3e-858ce23a0d1d'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (3978, 'd33f16e0-da67-463b-9ea2-f6f1df110c41'), (13201, 'f0b65266-be39-49d4-9757-564e662bb018'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (13217, '1defdd33-80ca-4389-a1ab-e9ed401b4919'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14782, '1cc646cf-ac7b-4712-a786-9905b3365b13'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ---

---
**NOTE 2**

When training a model on large datasets it is recommended to run the data preprocessing 
in a first run in a **non-distributed** mode via `--preprocessing_only` so that 
when running the model in **distributed** mode in a second step the preprocessed data
can easily be loaded on each distributed device.

---

### Demo

In this demo run we pre-train a `""base-sized""` Wav2Vec2 model simply only on the validation
and test data of [librispeech_asr](https://huggingface.co/datasets/librispeech_asr).

The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available 
per device, consider reducing `--batch_size` and/or the `--max_duration_in_seconds`.
### Preprocessing
Graph transformer frameworks usually apply specific preprocessing to their datasets to generate added features and properties which help the underlying learning task (classification in our case).
Here, we use Graphormer's default preprocessing, which generates in/out degree information, the shortest path between node matrices, and other properties of interest for the model. 
 
```python
from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator

dataset_processed = dataset.map(preprocess_item, batched=False)
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
```

## Preprocess the data

To finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model.
[`AutoImageProcessor`] takes care of processing image data to create `pixel_values`, `pixel_mask`, and
`labels` that a DETR model can train with. The image processor has some attributes that you won't have to worry about:

- `image_mean = [0.485, 0.456, 0.406 ]`
- `image_std = [0.229, 0.224, 0.225]`

These are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial
to replicate when doing inference or finetuning a pre-trained image model.

Instantiate the image processor from the same checkpoint as the model you want to finetune.

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = ""facebook/detr-resnet-50""
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

As we can see, the preprocessing added roughly 1,000 features. Our training set is now ready to be used -- let's dig into the preprocessing of the validation set!

### Processing the validation data[[processing-the-validation-data]]

Preprocessing the validation data will be slightly easier as we don't need to generate labels (unless we want to compute a validation loss, but that number won't really help us understand how good the model is). The real joy will be to interpret the predictions of the model into spans of the original context. For this, we will just need to store both the offset mappings and some way to match each created feature to the original example it comes from. Since there is an ID column in the original dataset, we'll use that ID.
```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use ü§ó Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
Since those choices are not always obvious when in doubt feel free to ask on Slack or even better post on the forum, as was 
done, *e.g.* [here](https://discuss.huggingface.co/t/spanish-asr-fine-tuning-wav2vec2/4586).

## Tips and tricks

This section summarizes a couple of tips and tricks across various topics. It will continously be updated during the week.

### How to combine multiple datasets into one

Check out [this](https://discuss.huggingface.co/t/how-to-combine-local-data-files-with-an-official-dataset/4685) post.

### How to effectively preprocess the data


### How to do efficiently load datasets with limited ram and hard drive space

Check out [this](https://discuss.huggingface.co/t/german-asr-fine-tuning-wav2vec2/4558/8?u=patrickvonplaten) post.


### How to do hyperparameter tuning


### How to preprocess and evaluate character based languages


## Further reading material
Preprocess

In addition to loading datasets, ü§ó Datasets other main goal is to offer a diverse set of preprocessing functions to get a dataset into an appropriate format for training with your machine learning framework. 

There are many possible ways to preprocess a dataset, and it all depends on your specific dataset. Sometimes you may need to rename a column, and other times you might need to unflatten nested fields. ü§ó Datasets provides a way to do most of these things. But in nearly all preprocessing cases, depending on your dataset modality, you'll need to:

- Tokenize a text dataset.
- Resample an audio dataset.
- Apply transforms to an image dataset.

The last preprocessing step is usually setting your dataset format to be compatible with your machine learning framework's expected input format.

In this tutorial, you'll also need to install the ü§ó Transformers library:

```bash
pip install transformers
```

- For fine-tuned speech recognition models, you only need to load a `processor`:

    ```py
    >>> from transformers import AutoProcessor

    >>> processor = AutoProcessor.from_pretrained(""facebook/wav2vec2-base-960h"")
    ```

When you use [`~Dataset.map`] with your preprocessing function, include the `audio` column to ensure you're actually resampling the audio data:

```py
>>> def prepare_dataset(batch):
...     audio = batch[""audio""]
...     batch[""input_values""] = processor(audio[""array""], sampling_rate=audio[""sampling_rate""]).input_values[0]
...     batch[""input_length""] = len(batch[""input_values""])
...     with processor.as_target_processor():
...         batch[""labels""] = processor(batch[""sentence""]).input_ids
...     return batch
>>> dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)
```
- **Preprocessing classes** convert the raw data into a format accepted by the model. A [tokenizer](main_classes/tokenizer) stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. [Image processors](main_classes/image_processor) preprocess vision inputs, [feature extractors](main_classes/feature_extractor) preprocess audio inputs, and a [processor](main_classes/processors) handles multimodal inputs.
```

It is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (by setting `on_the_fly_processing` to True): not all datasets are as small as `ogbg-molhiv`, and for large graphs, it might be too costly to store all the preprocessed data beforehand. 

## Model

### Loading
Here, we load an existing pretrained model/checkpoint and fine-tune it on our downstream task, which is a binary classification task (hence `num_classes = 2`). We could also fine-tune our model on regression tasks (`num_classes = 1`) or on multi-task classification.
```python
from transformers import GraphormerForGraphClassification

model = GraphormerForGraphClassification.from_pretrained(
    ""clefourrier/pcqm4mv2_graphormer_base"",
    num_classes=2, # num_classes for the downstream task 
    ignore_mismatched_sizes=True,
)
## Preprocessing the data[[preprocessing-the-data]]

<Youtube id=""8PmhEIXhBvI""/>

For both auto-regressive and masked language modeling, a common preprocessing step is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if they're too long, and that would result in losing information that might be useful for the language modeling task!
```

As usual with image datasets, we need to apply the same image transformations that were used at training time. We use the preprocessing defined in the processor. We also define a data collation function to feed the model batches of properly formatted tensors.
‚Äã
In this notebook, we will give an in-detail explanation of how
Wav2Vec2\'s pretrained checkpoints can be fine-tuned on any English ASR
dataset. Note that in this notebook, we will fine-tune Wav2Vec2 without
making use of a language model. It is much simpler to use Wav2Vec2
without a language model as an end-to-end ASR system and it has been
shown that a standalone Wav2Vec2 acoustic model achieves impressive
results. For demonstration purposes, we fine-tune the \""base\""-sized
[pretrained checkpoint](https://huggingface.co/facebook/wav2vec2-base)
on the rather small [Timit](https://huggingface.co/datasets/timit_asr)
dataset that contains just 5h of training data.

Wav2Vec2 is fine-tuned using Connectionist Temporal Classification
(CTC), which is an algorithm that is used to train neural networks for
sequence-to-sequence problems and mainly in Automatic Speech Recognition
and handwriting recognition.
A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in twice as many data points. Thus,
before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.

A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution. For example, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.
```

Now create a preprocessing function that:

1. Calls the `audio` column to load, and if necessary, resample the audio file.
2. Checks if the sampling rate of the audio file matches the sampling rate of the audio data a model was pretrained with. You can find this information in the Wav2Vec2 [model card](https://huggingface.co/facebook/wav2vec2-base).
3. Set a maximum input length to batch longer inputs without truncating them.

```py
>>> def preprocess_function(examples):
...     audio_arrays = [x[""array""] for x in examples[""audio""]]
...     inputs = feature_extractor(
...         audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True
...     )
...     return inputs
Next, let's talk about preprocessing. Audio data and transcriptions have to be brought into the correct format when 
training the acoustic model (example shown in [How to fine-tune an acoustic model](#how-to-finetune-an-acoustic-model)).
It is recommended that this is done by using ü§ó Datasets `.map()` function as shown 
[here](https://github.com/huggingface/transformers/blob/9a2dabae7002258e41419491c73dd43ad61b5de7/examples/pytorch/speech-recognition/run_speech_recognition_ctc.py#L444). As can be 
see we can pass some characters that will be removed from the transcriptions, *e.g.*: `--chars_to_ignore , ? . ! - \; \: \"" ‚Äú % ‚Äò ‚Äù ÔøΩ \`
on the official [""Single GPU Example""](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition#single-gpu-ctc).
The participants are free to modify this preprocessing by removing more characters or even replacing characters as
performance of the ASR system with a CER of 0 being a perfect score.
---

Here is the question: Which preprocessing strategy ensures robust ASR performance for real-world data?
Here is answer 1:Exclude punctuation and normalize audio without considering alignment
Here is answer 2: Normalize text, align datasets, and preprocess for model-specific requirements
Here is answer3: Use raw data without preprocessing for simplicity
Here is answer 4: Exclude multilingual considerations to simplify processing",Based solely based solely based solely based solely Based solely based solely-Based purely Based purely Based purely  
How can multitask models improve speech recognition systems?,"[(18432, '7458df8c-e69b-43d8-a456-b53c44af4802'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (27145, '79d5186a-9f88-4b85-8e03-efaf80d1bc88'), (18441, 'cfa471d2-0b8d-475e-9d32-c7873eeb92f8'), (9744, 'ee479e38-312b-4563-958c-33ce1e0eedea'), (5139, 'ba7b9456-398d-4fbc-85d5-361dab3a6bdb'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (27180, 'd501116e-8c5c-40b0-b3af-1c259beca630'), (14925, '3646284b-3c56-4876-8332-253857a7160a'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (1624, '9c6c249c-815c-471a-9fbc-0b8c0c71e013'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (25193, '03a63ec7-afd1-45dd-acea-f1087aadab69'), (19075, '131641af-b7be-4b3a-a31a-a76b816e3822'), (19076, 'c3f9c5df-c349-46ea-932a-eead03777122'), (22149, '9e6c02a3-618a-4a76-89f9-9b09c5630680'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (19077, 'ad635793-978a-455b-9c0f-cb60f9dae0e5'), (26255, '30372f00-e006-40e8-9ba2-91e9ccbec1b7'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (20644, '35d54144-39c9-4e31-b7cc-fa6416b43dcb'), (3748, '332644a1-eafc-4d25-b177-40728dd8f84b'), (9901, '3fb89436-7a71-469f-963f-9170d1e95727'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (20660, '36d46a61-da08-4a79-b974-9b8f75c6d8d9'), (9928, '5c999e86-a8bc-40b7-aea5-7bf657e92285'), (21198, '9468cc16-d5f1-4d2f-b79a-15502e9e9fee'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (10450, '3d06676d-85ad-4fa3-965c-e9d2ff2a64d7'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (4839, '8b5fcfe4-422d-41ad-ba2a-435d4859c7ef'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (4848, '76f1fb67-e046-408f-ad66-1ced57c240ba'), (6900, '0ae7974c-5457-40e4-a353-fde028bee870'), (26356, 'bd637992-bac8-476e-a6c7-09fa000bfe99'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (7933, '6347e581-ebbe-416a-b695-d3ee9376acb3'), (18174, 'f2cacc2f-d7e9-4f62-b292-98c6073da18d'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (1811, 'f4e9b143-1453-475d-a8a1-593ae0511e73'), (13591, '418bd9e7-11b8-4f5b-85ea-39f2266eb907'), (18200, 'c90b132b-e7b4-4305-99ea-9c6f13711dd2'), (29468, '17fc845b-a10d-41de-978b-b91257cc0ada'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (18209, '3b3b2f22-3c8a-4356-b74e-c918809e7457'), (290, 'fb7ae342-9f29-473e-9ab5-9270648dd176'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (13617, '5d6f1844-a996-4b2b-9cce-54cc40faf578'), (23859, 'e47fd052-4097-479b-95f3-f65bc65a3c2c'), (23860, 'afa6eb69-b861-463e-94f7-a8cffe60467e'), (23865, '7bccc135-749d-4acc-a179-12c10e7b97b3'), (13626, '1256a71c-2ece-41bf-ba82-dc791b09e6c6'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (316, '3d68da03-3df0-4ea4-8d35-55ca5e6bf70e'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (325, '05618000-706f-40ed-8186-9da01584ee2a'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (13646, '3afe34a0-b03d-44b2-91e9-71e637ccc48c'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13661, '70fe1907-eadd-4d2b-b4b9-aec545b32395'), (22877, '79c5fcb1-22aa-4ce1-be25-33a02d2b8e03'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (17774, 'e13cc263-c072-4a3b-9f64-7b688f1c79bf'), (18802, '4c9381c5-5784-4e2c-acc4-8d6a5ab6aa52'), (18804, '313d5a47-6d9e-49a4-9bd5-4098e9462ef6'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (8065, 'ad6fcc55-b711-476c-9874-0eb440cca8ba'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (18831, '73dccec5-c809-4615-b089-5d0e22f17cb5'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (18844, 'c97c2cdd-44fb-4531-87d5-afef8f19b517'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (18850, '1731352e-cdab-4fa7-8131-b488d0649c92'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (15268, 'c0f4cbd2-9369-47e9-9c6f-ed3053963c1a'), (10151, '3fecdbe5-af09-45b7-a31e-9610a12b9031'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (9148, '9a67e291-a5dd-4957-8d10-703a415fccb6'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (4041, '2284abda-6ee7-4523-bc73-621c8b8df07b'), (7123, '5550e6f5-759d-4aef-9d97-83b53144cc4b'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (29659, '5a15e754-3b25-4dce-88a6-79573d35cf5d'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (15870, 'aa4dffab-9c16-4523-8b50-2bde7016c29f'), (14317, '8c6ac79e-1f72-496b-a40f-fc1c956bf3eb'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (21502, 'c2163ba2-004a-43ab-91b4-058e6123a72f'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: The performance of speech recognition system can often significantly be improved by adding a 
language model for decoding. For more information on how to add a language model, please 
take a look at [How to combine n-gram language models with speech recognition models](#how-to-combine-n-gram-with-model).
*Expanding the language coverage of speech technology has the potential to improve access to information for many more people. 
However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000
languages spoken around the world. 
The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. 
The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging
self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, 
a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models 
for the same number of languages, as well as a language identification model for 4,017 languages. 
Experiments show that our multilingual speech recognition model more than halves the word error rate of
The abstract from the paper is the following:

*We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.*

This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ). The Tensorflow version of this model was contributed by [amyeroberts](https://huggingface.co/amyeroberts).
The original code can be found [here](https://github.com/openai/whisper).

## Usage tips
Streaming mode can take your research to the next level: not only are the biggest datasets accessible to you, but you 
can easily evaluate systems over multiple datasets in one go without worrying about your disk space. Compared 
to evaluating on a single dataset, multi-dataset evaluation gives a better metric for the generalisation 
abilities of a speech recognition system (_c.f._ [End-to-end Speech Benchmark (ESB)](https://arxiv.org/abs/2210.13352)). 
The accompanying [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio_datasets_colab.ipynb) 
provides an example for evaluating the Whisper model on eight English speech recognition datasets in one script using 
streaming mode.
*In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.*
```

Play with an interactive demo for the [speech-to-text task](https://huggingface.co/spaces/Matthijs/speecht5-asr-demo).

## Conclusion

SpeechT5 is an interesting model because ‚Äî unlike most other models ‚Äî it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned.

We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It‚Äôs very versatile!
Experiments show that our multilingual speech recognition model more than halves the word error rate of 
Whisper on 54 languages of the FLEURS benchmark while being trained on a small fraction of the labeled data.*
Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1,100 spoken languages can be identified, transcribed and generated with the various [language identification, speech recognition, and text-to-speech checkpoints released](https://huggingface.co/models?other=mms).

In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning. 

For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
SeamlessM4T-v2 is a collection of models designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text. It is an improvement on the [previous version](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t). For more details on the differences between v1 and v2, refer to section [Difference with SeamlessM4T-v1](#difference-with-seamlessm4t-v1).

SeamlessM4T-v2 enables multiple tasks without relying on separate models:

- Speech-to-speech translation (S2ST)
- Speech-to-text translation (S2TT)
- Text-to-speech translation (T2ST)
- Text-to-text translation (T2TT)
- Automatic speech recognition (ASR)

[`SeamlessM4Tv2Model`] can perform all the above tasks, but each task also has its own dedicated sub-model.

The abstract from the paper is the following:
The work done in **MMS** leverages this idea of adapters for speech recognition across different languages. A small number of adapter weights are fine-tuned to grasp unique phonetic and grammatical traits of each target language. Thereby, MMS enables a single large base model (*e.g.*, the [**`mms-1b-all`**](https://huggingface.co/facebook/mms-1b-all) checkpoint) and 1000+ small adapter layers (2.5M weights each for **`mms-1b-all`**) to comprehend and transcribe multiple languages. This dramatically reduces the computational demand of developing distinct models for each language.

Great! Now that we understood the motivation and theory, let's look into fine-tuning adapter weights for **`mms-1b-all`** üî•

## Notebook Setup
Keywords: NLP, Framework, LLM

## [spaCy](https://github.com/explosion/spaCy)

[spaCy](https://github.com/explosion/spaCy) is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. It offers support for transformers models through its third party package, spacy-transformers.

Keywords: NLP, Framework

## [speechbrain](https://github.com/speechbrain/speechbrain)

SpeechBrain is an open-source and all-in-one conversational AI toolkit based on PyTorch.
The goal is to create a single, flexible, and user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies, including systems for speech recognition, speaker recognition, speech enhancement, speech separation, language identification, multi-microphone signal processing, and many others.

Keywords: Conversational, Speech

## [skorch](https://github.com/skorch-dev/skorch)
- Talk: XLS-R: Large-Scale Cross-lingual Speech Representation Learning on 128 Languages
- Abstract: In this talk, Changhan will present XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. XLS-R has up to 2B parameters and was trained on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. On the CoVoST-2 speech translation benchmark, XLS-R improves the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. The XLS-R team hopes to work together with the open-source community to improve speech processing tasks for many more languages of the world.
N-gram language models can be built on CPU in just a few minutes. *N-gram-boosted* beam search decoding noticeably slows down the 
inference time, but also yields significant word error rates improvements - usually between 10-40 %.

You can find an in-detail blog post on how to build an *n-gram* [here](https://huggingface.co/blog/wav2vec2-with-ngram).
The blog post can be opened in a google colab and by adapting three lines of the example for your use case, one can directly
create an *n-gram* in the google colab.
The blog post gives in-detail instructions on how to build an n-gram and how to add it to your trained speech recognition model.

- why one should add an *n-gram* to her/his speech recognition system,
- how to build an *n-gram*, and,
- how to add the built *n-gram* the speech recognition system for seamless decoding
```

## Multimodal

For tasks involving multimodal inputs, you'll need a [processor](main_classes/processors) to prepare your dataset for the model. A processor couples together two processing objects such as as tokenizer and feature extractor.

Load the [LJ Speech](https://huggingface.co/datasets/lj_speech) dataset (see the ü§ó [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub) for more details on how to load a dataset) to see how you can use a processor for automatic speech recognition (ASR):

```py
>>> from datasets import load_dataset

>>> lj_speech = load_dataset(""lj_speech"", split=""train"")
```

For ASR, you're mainly focused on `audio` and `text` so you can remove the other columns:

```py
>>> lj_speech = lj_speech.map(remove_columns=[""file"", ""id"", ""normalized_text""])
## Connectionist Temporal Classification With Adapters

The script [`run_speech_recognition_ctc_adapter.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py) can be used to fine-tune adapter layers for [Wav2Vec2-like models like MMS](https://huggingface.co/docs/transformers/main/en/model_doc/mms) for automatic speech recognition.

### MMS Model

The [Massive Multilingual Speech (MMS) model](https://huggingface.co/facebook/mms-1b-all) has been pre-trained and fine-tuned
on 1000+ languages. The model makes use of adapter attention layers to fine-tune only a small part 
of the model on a specific language. The model already comes with fine-tuned adapter layers for 1000+ languages and 
can be used for inference for 1000+ languages out of the box.
additional overlapped utterances are created unsupervisely and incorporated during model training. Lastly, we scale up
the training dataset from 60k hours to 94k hours. WavLM Large achieves state-of-the-art performance on the SUPERB
benchmark, and brings significant improvements for various speech processing tasks on their representative benchmarks.*
language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models. *
This enhanced local self-attention is better than the vanilla local self-attention architecture but still has a major drawback in that every input vector can only attend to a local context of predefined size. For NLP tasks that do not require the transformer model to learn long-range dependencies between the input vectors, which include arguably *e.g.* speech recognition, named entity recognition and causal language modeling of short sentences, this might not be a big issue. Many NLP tasks do require the model to learn long-range dependencies, so that local self-attention could lead to significant performance degradation, *e.g.* 
* *Question-answering*: the model has to learn the relationship between the question tokens and relevant answer tokens which will most likely not be in the same local range
* *Multiple-Choice*: the model has to compare multiple answer token segments to each other which are usually separated by a significant length
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
8. LoHa: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098)
9. LoKr: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650) based on [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://arxiv.org/abs/2309.14859) implementation
10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
11. OFT: [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)
------------------------------------------------------------------------

\\({}^1 \\) Some research shows that a model such as
`facebook/wav2vec2-base-100h` - when sufficiently large and trained on
enough data - can learn language modeling dependencies between
intermediate audio representations similar to a language model.

Great, now that you have seen the advantages adding an *n-gram* language
model can bring, let's dive into how to create an *n-gram* and
`Wav2Vec2ProcessorWithLM` from scratch.

## **2. Getting data for your language model**

A language model that is useful for a speech recognition system should
support the acoustic model, *e.g.* Wav2Vec2, in predicting the next word
(or token, letter) and therefore model the following distribution:

Here is the question: How can multitask models improve speech recognition systems?
Here is answer 1:Focus only on single-task tuning to reduce complexity
Here is answer 2: Leverage prompt-based multitask tuning and integrate language models
Here is answer3: Avoid using multitask approaches to reduce overhead
Here is answer 4: Train separate models for each task instead of multitasking","Based upon reviewing all four options provided against context provided:
Answer3 appears incorrect since there seems no strong evidence suggesting avoiding multitasking would necessarily help improve performance because they seem effective enough given examples shown here related topics like enabling streaming mode or dealing successfully too after building combined several items related modes projects demonstrating everything works sufficiently even though challenges been managed during process development multi-processing times till conditions purposes given examples reaching multitude accomplishments impressive impress outcomes altogether applying diverse operative approaches throughout discussions leading possibilities achieving shared value concepts ensuring overall consistency output"
What key advantage does Intel Sapphire Rapids hardware provide for AI model fine-tuning?,"[(4608, '11eba3f7-462f-400d-a1c6-26fbdecddb11'), (10250, '3d60dd94-162e-41c3-b924-86cd1b72a0fc'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (14873, '85d82d32-dcbc-4c5e-ac7b-70e15da6671e'), (1050, '3dd7da70-bcad-4e0f-8472-4305b11553cb'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (28749, '1e60f874-1e0f-4209-8dc6-28b79fc8904c'), (28752, 'a8f3fc54-9b72-46a0-b4ac-29da5374d20e'), (28754, '2f370cf6-442c-48bd-a56c-8301e99110f6'), (2643, 'd3577235-2eda-4e17-a9e6-7cdf05401f5b'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (9824, 'dd81f094-0a63-41f4-9d2d-7fcedd15ebb6'), (2655, '19d73693-6464-4f45-b475-f486597c0c5d'), (5729, '8325f704-8f9e-470a-9d07-777ab1401244'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (5738, '1b3f6207-1eaf-4d30-b7f5-15ce5479c98a'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (16495, '3f606db4-0681-497d-9322-b23accc9a09a'), (3190, '9ac364e4-812c-4816-b2b7-cd39501c3080'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (4217, '1a8e3437-ea12-4e7b-b9d9-6929a9a940d2'), (4225, '8a889917-10a2-425c-8ce5-9a4d7cf16589'), (4226, 'f995a19f-7e5a-4028-a50a-315c452620ee'), (4227, 'd9a4a5bd-4e54-4a56-8292-9967b567d5d8'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (22163, '8d167b0c-14d6-4205-a383-f37c9f717a64'), (21144, '733daaa7-2d91-4c46-b007-fe08a12ee437'), (21145, '8f8f780e-b116-4a49-a443-62c3ed772cb9'), (21146, '52b3b9bb-9ff0-4cbb-85ea-66902b53e6a6'), (21147, 'dd1291ef-03a7-4a6a-91f6-3ea791acefdb'), (9882, '3d63264c-0ee7-49b5-ae7c-5dc424d9d8c7'), (5806, '5054cac8-6958-4df8-9986-f0bb2157877f'), (22205, '2c941caf-5007-4276-8c86-d5eeff359547'), (6356, 'fd9611a0-cae2-4941-8609-f6440c2b0285'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (26855, 'daaa5d9b-d90e-4dcb-b7be-640944232958'), (22775, '92b448c2-17a3-48db-9679-dfae336f0e2f'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (18694, '14ec5ba0-bd18-4aa7-a57f-338402d25194'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (16660, '1dfbc513-b29b-4f26-ac16-6bccdb48a3d4'), (2836, 'dad2f3b4-337f-49e3-ae8a-2714f6bb79e0'), (25882, 'd29c1150-e982-47d6-af8d-81cbe1722e94'), (25883, 'f01a6bd7-35e0-4d59-b1e1-8879b0662972'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (9562, 'b75f1239-64b9-4c25-9106-2b9ff2961471'), (9563, '211904bc-1535-4038-9e7d-c897425b8bcc'), (16221, 'c4f242f2-51f2-4b28-9f44-e725b8705bf6'), (23903, '923c4027-8d23-4edc-9d3e-daf70d4b077b'), (9567, '21f12568-4fc0-4680-b763-b7907ea0ea81'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (20322, '2ac8515a-2656-4500-b62a-859ab1fe9812'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (3941, 'a871cd71-4386-4492-ae52-7dda1294b854'), (3942, 'b94b5ec5-7ed0-4a6a-abb0-7f8bbc3cc4c3'), (3943, '3eb2b5e6-fc2b-4577-8b18-e8633255362d'), (3944, '4951a0e6-56b4-42c8-855e-2698bcf9658f'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (25459, '7e9e55e8-8027-4677-b297-11f6cebd0a17'), (23928, 'fa312533-bb24-4e27-982f-befb97bafd49'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (23930, 'b54bef38-4510-49ab-87a1-dbc20ba90085'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (7062, 'c8c890e1-e7b8-4e8f-9799-111728d14f27'), (3993, '193bb31e-a1b5-4129-bd41-177f89121efd'), (7065, '0fb31b29-d4f9-4207-9934-c683259bbb1a'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (12725, '69c1bca3-0390-4f32-82e9-9812d1bc36a6'), (19383, '44c8fa9d-7167-4105-9431-707875941cb8'), (19384, '75f513c0-3b52-4f83-aec6-7b436161c162'), (29112, 'f5df6a53-0df1-4420-9043-3d9ba923f41b'), (19387, 'fd8aa818-6add-4bbf-a661-88e05a30e489'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (19390, 'c0f4fdad-c5e6-4511-aaf0-4cc870c059c1'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11711, 'f0aab09f-a3dc-496a-9cd8-08e8d39a1306'), (11713, '16213aad-a177-42a5-8be6-a4c1224c45d0'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19398, 'c99ed4d1-a0f8-47fc-9f4d-4867015d3211'), (19399, '7f318cf3-2841-4ac1-9fff-8943985050c9'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (11725, 'c643f493-d651-49b0-8fcb-81b52c34618a'), (11727, 'f0c4c308-23bc-4116-8f86-1ad541066b86'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (31735, '31b3ae48-f7ec-45ee-adb9-0d7d5e3541ca'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (31740, '7f15437c-296d-44c3-89c4-94654b268fc9')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: In a follow-up post, we'll look at inference on Sapphire Rapids CPUs and the performance boost that they bring.

## Why You Should Consider Training On CPUs

Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach, especially when using techniques such as distributed training and fine-tuning on small and medium datasets. 

Xeon CPUs support advanced features such as Advanced Vector Extensions ([AVX-512](https://en.wikipedia.org/wiki/AVX-512)) and Hyper-Threading, which help improve the parallelism and efficiency of DL models. This enables faster training times as well as better utilization of hardware resources.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cluster. We will use [textual inversion](https://huggingface.co/docs/diffusers/training/text_inversion), a technique that only requires a small number of example images. We'll use only five!

Let's get started.

## Setting up the cluster

Our friends at [Intel](https://huggingface.co/intel) provided four servers hosted on the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html) (IDC), a service platform for developing and running workloads in Intel¬Æ-optimized deployment environments with the latest Intel processors and [performance-optimized software stacks](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html). 

Each server is powered by two Intel Sapphire Rapids CPUs with 56 physical cores and 112 threads. Here's the output of `lscpu`:
--
title: ""Accelerating Stable Diffusion Inference on Intel CPUs""
thumbnail: /blog/assets/136_stable_diffusion_inference_intel/01.png
authors:
- user: juliensimon
- user: echarlaix
---

# Accelerating Stable Diffusion Inference on Intel CPUs



Recently, we introduced the latest generation of [Intel Xeon](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html) CPUs (code name Sapphire Rapids), its new hardware features for deep learning acceleration, and how to use them to accelerate [distributed fine-tuning](https://huggingface.co/blog/intel-sapphire-rapids) and [inference](https://huggingface.co/blog/intel-sapphire-rapids-inference) for natural language processing Transformers.

In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.
In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.

At the time of writing, the simplest way to get your hands on a Sapphire Rapids server is to use the Amazon EC2 [R7iz](https://aws.amazon.com/ec2/instance-types/r7iz/) instance family. As it's still in preview, you have to [sign up](https://pages.awscloud.com/R7iz-Preview.html) to get access. Like in previous posts, I'm using an `r7iz.metal-16xl` instance (64 vCPU, 512GB RAM) with an Ubuntu 20.04 AMI (`ami-07cd3e6c4915b2d18`).

Let's get started! Code samples are available on [Gitlab](https://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable_diffusion_intel).

## The Diffusers library
```

The average latency is **32.3 seconds**. As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. 

Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change!

Now, let's accelerate!

## Optimum Intel and OpenVINO

[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code.

Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. 

Optimum Intel and OpenVINO can be installed as follows:

```
pip install optimum[openvino]
Now, let's look at the new instructions in the Sapphire Rapids architecture.


## Advanced Matrix Extensions: New Instructions for Deep Learning

The Sapphire Rapids architecture introduces the Intel Advanced Matrix Extensions ([AMX](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions)) to accelerate DL workloads. Using them is as easy as installing the latest version of IPEX. There is no need to change anything in your Hugging Face code.
In this post, we're going to focus on inference. Working with popular HuggingFace transformers implemented with PyTorch, we'll first measure their performance on an Ice Lake server for short and long NLP token sequences. Then, we'll do the same with a Sapphire Rapids server and the latest version of Hugging Face [Optimum Intel](https://github.com/huggingface/optimum-intel), an open-source library dedicated to hardware acceleration for Intel platforms.

Let's get started!


## Why You Should Consider CPU-based Inference

There are several factors to consider when deciding whether to run deep learning inference on a CPU or GPU. The most important one is certainly the size of the model. In general, larger models may benefit more from the additional computational power provided by a GPU, while smaller models can run efficiently on a CPU.
In this post, you will learn how to accelerate a PyTorch training job with a cluster of Sapphire Rapids servers running on AWS. We will use the [Intel oneAPI Collective Communications Library](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html) (CCL) to distribute the job, and the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) library to automatically put the new CPU instructions to work. As both libraries are already integrated with the Hugging Face transformers library, we will be able to run our sample scripts out of the box without changing a line of code.

In a follow-up post, we'll look at inference on Sapphire Rapids CPUs and the performance boost that they bring.

## Why You Should Consider Training On CPUs
This lets you achieve reach **single-digit prediction latency** even with long text sequences, which was only possible with GPUs so far.

## Conclusion

The fourth generation of Intel Xeon CPUs delivers excellent inference performance, especially when combined with Hugging Face Optimum. This is yet another step on the way to making Deep Learning more accessible and more cost-effective, and we're looking forward to continuing this work with our friends at Intel.

Here are some additional resources to help you get started:

* [Intel IPEX](https://github.com/intel/intel-extension-for-pytorch) on GitHub
* [Hugging Face Optimum](https://github.com/huggingface/optimum) on GitHub


If you have questions or feedback, we'd love to read them on the [Hugging Face forum](https://discuss.huggingface.co/).

Thanks for reading!


## Appendix: full results


<kbd>
  <img src=""assets/129_intel_sapphire_rapids_inference/02.png"">
</kbd>
--
title: ""Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2""
thumbnail: /blog/assets/129_intel_sapphire_rapids_inference/01.png
authors:
- user: juliensimon
---

# Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2


In a [recent post](https://huggingface.co/blog/intel-sapphire-rapids), we introduced you to the fourth generation of Intel Xeon CPUs, code-named [Sapphire Rapids](https://en.wikipedia.org/wiki/Sapphire_Rapids), and its new Advanced Matrix Extensions ([AMX](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions)) instruction set. Combining a cluster of Sapphire Rapids servers running on Amazon EC2 and Intel libraries like the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch), we showed you how to efficiently run distributed training at scale, achieving an 8-fold speedup compared to the previous Xeon generation (Ice Lake) with near-linear scaling.
The AMX instructions accelerate matrix multiplication, an operation central to training DL models on data batches. They support both Brain Floating Point ([BF16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)) and 8-bit integer (INT8) values, enabling acceleration for different training scenarios.
 
AMX introduces new 2-dimensional CPU registers, called tile registers. As these registers need to be saved and restored during context switches, they require kernel support: On Linux, you'll need [v5.16](https://discourse.ubuntu.com/t/kinetic-kudu-release-notes/27976) or newer.
 
Now, let's see how we can build a cluster of Sapphire Rapids CPUs for distributed training.

## Building a Cluster of Sapphire Rapids CPUs
--
title: ""Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 1""
thumbnail: /blog/assets/124_intel_sapphire_rapids/02.png
authors:
- user: juliensimon
---

# Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1

About a year ago, we [showed you](https://huggingface.co/blog/accelerating-pytorch) how to distribute the training of Hugging Face transformers on a cluster or third-generation [Intel Xeon Scalable](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html) CPUs (aka Ice Lake). Recently, Intel has launched the fourth generation of Xeon CPUs, code-named Sapphire Rapids, with exciting new instructions that speed up operations commonly found in deep learning models.
```

With this final version, inference latency is now down to **5.05 seconds**. Compared to our initial Sapphire Rapids baseline (32.3 seconds), this is almost 6.5x faster!

<kbd>
  <img src=""assets/136_stable_diffusion_inference_intel/01.png"">
</kbd>
*Environment: Amazon EC2 r7iz.metal-16xl, Ubuntu 20.04, Linux 5.15.0-1031-aws, libjemalloc-dev 5.2.1-1, intel-mkl 2020.0.166-1, PyTorch 1.13.1, Intel Extension for PyTorch 1.13.1, transformers 4.27.2, diffusers 0.14, accelerate 0.17.1, openvino 2023.0.0.dev20230217, optimum 1.7.1, optimum-intel 1.7*



## Conclusion

The ability to generate high-quality images in seconds should work well for a lot of use cases, such as customer apps, content generation for marketing and media, or synthetic data for dataset augmentation.

Here are some resources to help you get started:
In addition, Xeon CPUs are generally more affordable and widely available compared to specialized hardware such as GPUs, which are typically required for training large deep learning models. Xeon CPUs can also be easily repurposed for other production tasks, from web servers to databases, making them a versatile and flexible choice for your IT infrastructure.

Finally, cloud users can further reduce the cost of training on Xeon CPUs with spot instances. Spot instances are built from spare compute capacities and sold at a discounted price. They can provide significant cost savings compared to using on-demand instances, sometimes up to 90%. Last but not least, CPU spot instances also are generally easier to procure than GPU instances.

Now, let's look at the new instructions in the Sapphire Rapids architecture.


## Advanced Matrix Extensions: New Instructions for Deep Learning
```

With a static shape, average latency is slashed to **4.7 seconds**, an additional 3.5x speedup. 

As you can see, OpenVINO is a simple and efficient way to accelerate Stable Diffusion inference. When combined with a Sapphire Rapids CPU, it delivers almost 10x speedup compared to vanilla inference on Ice Lake Xeons.

If you can't or don't want to use OpenVINO, the rest of this post will show you a series of other optimization techniques. Fasten your seatbelt!

## System-level optimization

Diffuser models are large multi-gigabyte models, and image generation is a memory-intensive operation. By installing a high-performance memory allocation library, we should be able to speed up memory operations and parallelize them across the Xeon cores.    Please note that this will change the default memory allocation library on your system. Of course, you can go back to the default library by uninstalling the new one.
Thanks for reading!


## Appendix: full results


<kbd>
  <img src=""assets/129_intel_sapphire_rapids_inference/02.png"">
</kbd>

*Ubuntu 22.04 with libtcmalloc, Linux 5.15.0 patched for Intel AMX support, PyTorch 1.13 with Intel Extension for PyTorch, Transformers 4.25.1, Optimum 1.6.1, Optimum Intel 1.7.0.dev0*
```

For the sake of brevity, we'll just look at the p99 results for [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased). All times are in milliseconds. You'll find full results at the end of the post. 

<kbd>
  <img src=""assets/129_intel_sapphire_rapids_inference/01.png"">
</kbd>

As you can see in the graph above, single predictions run **60-65%** faster compared to the previous generation of Xeon CPUs. In other words, thanks to the combination of Intel Sapphire Rapids and Hugging Face Optimum, you can accelerate your predictions 3x with only tiny changes to your code.

This lets you achieve reach **single-digit prediction latency** even with long text sequences, which was only possible with GPUs so far.

## Conclusion
Optimum provides an `ORTTrainer` API that extends the `Trainer` in Transformers to use ONNX Runtime as the backend for acceleration. `ORTTrainer` is an easy-to-use API containing feature-complete training loop and evaluation loop. It supports features like hyperparameter search, mixed-precision training and distributed training with multiple GPUs. `ORTTrainer` enables AI developers to compose ONNX Runtime and other third-party acceleration techniques when training Transformers‚Äô models, which helps accelerate the training further and gets the best out of the hardware. For example, developers can combine ONNX Runtime Training with distributed data parallel and mixed-precision training integrated in Transformers‚Äô Trainer. Besides, `ORTTrainer` makes it easy to compose ONNX Runtime Training with DeepSpeed ZeRO-1, which saves memory by partitioning the optimizer states. After the pre-training or the fine-tuning is done, developers can either save the trained PyTorch model or convert it to
## Introduction: Using Intel Software to Optimize AI Efficiency on CPU

As we detailed in our [previous blog post](https://huggingface.co/blog/bert-cpu-scaling-part-1), Intel Xeon CPUs provide a set of features especially designed for AI workloads such as AVX512 or VNNI (Vector Neural Network Instructions) 
for efficient inference using integer quantized neural network for inference along with additional system tools to ensure the work is being done in the most efficient way. 
In this blog post, we will focus on software optimizations and give you a sense of the performances of the new Ice Lake generation of Xeon CPUs from Intel. Our goal is to give you a full picture of what‚Äôs available on the software side to make the most out of your Intel hardware. 
As in the previous blog post, we show the performance with benchmark results and charts, along with new tools to make all these knobs and features easy to use.

Here is the question: What key advantage does Intel Sapphire Rapids hardware provide for AI model fine-tuning?
Here is answer 1:Exclusive support for Transformer models
Here is answer 2: Advanced matrix extensions and hardware acceleration
Here is answer3: GPU-level performance for ASR tasks
Here is answer 4: Preinstalled ASR datasets for multilingual tasks","Based solely off context provided here because there wasn't enough information within just those paragraphs; however based off knowledge about intel sappire rapds cpu based off aritcles etc Answer number two seems like best fit hence I am choosing Answer number two ""**Advanced matrix extensionsappendChildandhardwareacceleration"""
What is the primary benefit of combining n-gram models with acoustic models?,"[(21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (27140, 'e9b6a701-73ea-4220-95c6-b35ffe9aab01'), (9733, 'd8a9b090-d24d-4517-9a0b-03d326562952'), (21508, '191e2549-6667-4c43-839c-7668501e6123'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (9747, '362139c4-a349-4533-8add-38dc8c990f5d'), (6676, '0d231cb5-d381-44c6-9669-18d5df65d10d'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (4164, '4be83d8f-1192-493d-b204-c776cd9a308f'), (4166, 'd62d2dfe-88e9-4269-a3b8-a0c08a9ead60'), (19033, 'b32a6a6b-73de-4fa7-9615-58c2fbfe7015'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (16483, '23532c2b-7b32-489c-8f93-99a40c321cfa'), (16488, '82e6921b-4f0a-43f7-af6b-e88247f9a075'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (19055, '03f15b1f-1ff6-4bc2-90aa-91b49ccf44ca'), (11901, '9b1f45ad-b8f1-42c4-9a6c-4e7607042fc4'), (19074, '2660675c-e9c3-4a2b-97a5-b40971554f7f'), (19075, '131641af-b7be-4b3a-a31a-a76b816e3822'), (19076, 'c3f9c5df-c349-46ea-932a-eead03777122'), (14981, '4b85fc82-f5ca-4436-82ce-5b7b3f15ac05'), (1671, 'b6fe0d87-4506-41f4-86d6-8a5340995e6e'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (16017, '0159b9c3-ea7e-41d0-928e-fce397c3adfe'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (15002, 'da4ee92f-fbca-42a2-9700-0788b253e4d5'), (3748, '332644a1-eafc-4d25-b177-40728dd8f84b'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (15028, '86c91e1c-d5df-4053-b7f8-f475c47edcf1'), (9911, 'df3b7cf0-0e9c-474e-a7b8-0c3edb8eeb91'), (15032, 'a5fd305b-6bf8-4f2d-b724-080da4c84f02'), (16055, '1c40cffb-366c-48a0-b2e8-3d3a31a0feda'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (15041, '043fa5bf-448c-493a-a908-29384cfeee7c'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (9928, '5c999e86-a8bc-40b7-aea5-7bf657e92285'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (17098, '806472d6-0d89-45f3-95ba-14638f4f5a85'), (21198, '9468cc16-d5f1-4d2f-b79a-15502e9e9fee'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (15056, '3aeddfc0-2bd8-44a0-8d5d-def75c612a76'), (720, 'aa7dece3-4d93-444e-a7e0-ff84a5130dd3'), (25307, 'a1dddf5c-6872-4923-9d66-941863c5e2e6'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (15583, '0bd03221-d721-44d6-b845-89e89513d7b5'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (25321, 'b7befe0e-86b5-4603-8086-b358c0606f7c'), (31467, 'd5b8cb99-b5ce-4df9-8ab2-5c8ebaf7db26'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (17646, 'aedaad43-5e00-41ba-9c23-9672db5835b2'), (26356, 'bd637992-bac8-476e-a6c7-09fa000bfe99'), (26363, '7229a8f2-836b-4249-a128-0c3a3a567ef6'), (6907, '1be7a8dc-407e-49e2-b978-d3551d21c091'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (10510, '65932cde-592e-47aa-b229-26bebb986d68'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (26930, '754b4dd4-7bf8-4d66-b302-6b736ac16f61'), (5429, '4cc6156d-79ac-4b12-87d5-ef4b1cc6f55b'), (23864, 'a2c61a52-5e46-47ca-875a-6b4cc5921ba4'), (23865, '7bccc135-749d-4acc-a179-12c10e7b97b3'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (28993, 'e94a1fa2-5915-4591-a20a-5ae790985a23'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (13648, '5af9b7d4-1b8a-436a-bf80-acbd7647fec7'), (13649, 'fe70249f-ab16-426a-9018-41aa8300b852'), (15186, '408a5ea1-a358-4b34-bce8-9381f63fae97'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13661, '70fe1907-eadd-4d2b-b4b9-aec545b32395'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (13669, 'fd5f73cd-31c4-4ed9-a912-115b0d44f3b5'), (13670, '2770a364-f587-45a8-89d3-153b4447c5a3'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (18804, '313d5a47-6d9e-49a4-9bd5-4098e9462ef6'), (10615, '7bf25661-c8bb-4b65-9e16-786afc1051f8'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (13190, '8b1d3ff6-345b-42e3-be4a-562d7b46bb0b'), (8072, '75702fbf-8f70-4667-ade7-37ec084bfc8b'), (13195, 'd9d25c33-f225-4c1f-80aa-eca578fcd3fb'), (18838, '76831eb6-5d71-4001-b374-7a7d4441f632'), (18845, '03dfc9be-b19e-496f-b177-54e25eeb2db3'), (13217, '1defdd33-80ca-4389-a1ab-e9ed401b4919'), (18850, '1731352e-cdab-4fa7-8131-b488d0649c92'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (15268, 'c0f4cbd2-9369-47e9-9c6f-ed3053963c1a'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (27072, '5557dbf2-8acf-4d5d-8381-ece6fc992f43'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (4040, 'a08a5c15-4a59-49e0-83d1-4b285c1643b2'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (14800, 'c2bc4d19-86ff-4079-b5ae-14157e68dea4'), (14801, 'dbcd48d3-a25b-4373-804a-62a5ab0e027e'), (14802, '148eaad9-71ab-478b-b6d7-1e16ddec31d1'), (29659, '5a15e754-3b25-4dce-88a6-79573d35cf5d'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (14317, '8c6ac79e-1f72-496b-a40f-fc1c956bf3eb'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ------------------------------------------------------------------------

\\({}^1 \\) Some research shows that a model such as
`facebook/wav2vec2-base-100h` - when sufficiently large and trained on
enough data - can learn language modeling dependencies between
intermediate audio representations similar to a language model.

Great, now that you have seen the advantages adding an *n-gram* language
model can bring, let's dive into how to create an *n-gram* and
`Wav2Vec2ProcessorWithLM` from scratch.

## **2. Getting data for your language model**

A language model that is useful for a speech recognition system should
support the acoustic model, *e.g.* Wav2Vec2, in predicting the next word
(or token, letter) and therefore model the following distribution:
Looking again at Table 9 of Appendix C of the [official Wav2Vec2 paper](https://arxiv.org/abs/2006.11477), it can be noticed that using a *Transformer*-based LM for decoding clearly yields better results than using an *n-gram* model, but the difference between *n-gram* and *Transformer*-based LM is much less significant than the difference between *n-gram* and no LM. 

*E.g.*, for the large Wav2Vec2 checkpoint that was fine-tuned on 10min only, an *n-gram* reduces the word error rate (WER) compared to no LM by *ca.* 80% while a *Transformer*-based LM *only* reduces the WER by another 23% compared to the *n-gram*. This relative WER reduction becomes less, the more data the acoustic model has been trained on. *E.g.*, for the large checkpoint a *Transformer*-based LM reduces the WER by merely 8% compared to an *n-gram* LM whereas the *n-gram* still yields a 21% WER reduction compared to no language model.
The performance of speech recognition system can often significantly be improved by adding a 
language model for decoding. For more information on how to add a language model, please 
take a look at [How to combine n-gram language models with speech recognition models](#how-to-combine-n-gram-with-model).
**Note**: If you're an experienced Docker user, feel free to create a custom docker image with all of the needed packages 
like the one in step 5. The Dockerfile for it is available here: 
[baaastijn/Dockerimages](https://github.com/baaastijn/Dockerimages/tree/main/Hugginface_challenge_speech).
Once you've built your image, push it to https://hub.docker.com/ and select it during the OVHcloud job creation.

For more quick tutorials about OVHcloud AI products, check out the showcase https://vimeo.com/showcase/8903300

## How to combine n-gram with acoustic model

Having trained a speech recognition model with CTC as shown in the section above, 
one can further improve the model's performance by adding an **n-gram language model**
to the decoding process of the model. By doing so, we are replacing the naive greedy decoding 
with **n-gram-boosted** beam search decoding.
## Table of Contents

- [TLDR;](#tldr)
- [Important dates](#important-dates)
- [How to install pytorch, transformers, datasets](#how-to-install-relevant-libraries)
- [Data and Preprocessing](#data-and-preprocessing)
- [How to fine-tune an acoustic model](#how-to-finetune-an-acoustic-model)
- [How to fine-tune with OVH could](#how-to-finetune-with-ovh-cloud)
- [How to combine n-gram language models with acoustic model](#how-to-combine-n-gram-with-acoustic-model)
- [Evaluation](#evaluation)
- [Prizes](#prizes)
- [Communication and Problems](#communication-and-problems)
- [Talks](#talks)
- [General Tips & Tricks](#general-tips-and-tricks)

## TLDR

Participants are encouraged to leverage pre-trained speech recognition checkpoints,
preferably [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53), 
to train a speech recognition system in a language of their choice.
- why one should add an *n-gram* to her/his speech recognition system,
- how to build an *n-gram*, and,
- how to add the built *n-gram* the speech recognition system for seamless decoding

Our previously trained model - [xls-r-300m-sv](https://huggingface.co/hf-test/xls-r-300m-sv) - enjoys a 30% word error rate reduction after 
having added an n-gram. As shown in the example of the blog post, we strongly advise participants to upload all files required for combining 
the *n-gram* with a trained speech recognition model directly into the same model repository.

## Evaluation
Until recently, the ü§ó Transformers library did not offer a simple user
interface to decode audio files with a fine-tuned Wav2Vec2 **and** a
language model. This has thankfully changed. ü§ó Transformers now offers
an easy-to-use integration with *Kensho Technologies'* [pyctcdecode
library](https://github.com/kensho-technologies/pyctcdecode). This blog
post is a step-by-step **technical** guide to explain how one can create
an **n-gram** language model and combine it with an existing fine-tuned
Wav2Vec2 checkpoint using ü§ó Datasets and ü§ó Transformers.

We start by:

1.  How does decoding audio with an LM differ from decoding audio
    without an LM?
2.  How to get suitable data for a language model?
3.  How to build an *n-gram* with KenLM?
4.  How to combine the *n-gram* with a fine-tuned Wav2Vec2 checkpoint?

For a deep dive into how Wav2Vec2 functions - which is not necessary for
this blog post - the reader is advised to consult the following
material:
N-gram language models can be built on CPU in just a few minutes. *N-gram-boosted* beam search decoding noticeably slows down the 
inference time, but also yields significant word error rates improvements - usually between 10-40 %.

You can find an in-detail blog post on how to build an *n-gram* [here](https://huggingface.co/blog/wav2vec2-with-ngram).
The blog post can be opened in a google colab and by adapting three lines of the example for your use case, one can directly
create an *n-gram* in the google colab.
The blog post gives in-detail instructions on how to build an n-gram and how to add it to your trained speech recognition model.

- why one should add an *n-gram* to her/his speech recognition system,
- how to build an *n-gram*, and,
- how to add the built *n-gram* the speech recognition system for seamless decoding
The reason why an *n-gram* is preferred over a *Transformer*-based LM is that *n-grams* come at a significantly smaller computational cost. For an *n-gram*, retrieving the probability of a word given previous words is almost only as computationally expensive as querying a look-up table or tree-like data storage - *i.e.* it's very fast compared to modern *Transformer*-based language models that would require a full forward pass to retrieve the next word probabilities.

For more information on how *n-grams* function and why they are (still) so useful for speech recognition, the reader is advised to take a look at [this excellent summary](https://web.stanford.edu/~jurafsky/slp3/3.pdf) from Stanford.

Great, let's see step-by-step how to build an *n-gram*. We will use the
popular [KenLM library](https://github.com/kpu/kenlm) to do so. Let's
start by installing the Ubuntu library prerequisites:
In a sequence-to-sequence model, the encoder transforms the audio inputs 
into a set of hidden state representations, extracting important features 
from the spoken speech. The decoder plays the role of a language model, 
processing the hidden state representations and generating the corresponding 
text transcriptions. Incorporating a language model **internally** in the 
system architecture is termed _deep fusion_. This is in contrast to 
_shallow fusion_, where a language model is combined **externally** with 
an encoder, such as with CTC + \\(n\\)-gram (_c.f._ [Internal Language Model Estimation](https://arxiv.org/pdf/2011.01991.pdf)).
With deep fusion, the entire system can be trained end-to-end with the 
same training data and loss function, giving greater flexibility and generally 
superior performance (_c.f._ [ESB Benchmark](https://arxiv.org/abs/2210.13352)).
### What are the benefits of autoGPTQ?
**fast for text generation**: GPTQ quantized models are fast compared to bitsandbytes quantized models for [text generation](https://huggingface.co/docs/transformers/main_classes/text_generation). We will address the speed comparison in an appropriate section. 

**n-bit support**: The GPTQ algorithm makes it possible to quantize models up to 2 bits! However, this might come with severe quality degradation. The recommended number of bits is 4, which seems to be a great tradeoff for GPTQ at this time.

**easily-serializable**: GPTQ models support serialization for any number of bits. Loading models from TheBloke namespace: https://huggingface.co/TheBloke (look for those that end with the `-GPTQ` suffix) is supported out of the box, as long as you have the required packages installed. Bitsandbytes supports 8-bit serialization but does not support 4-bit serialization as of today.
Multivariate models usually perform well when trained on large amounts of data. However, when compared to univariate models, especially on smaller open datasets, the univariate models tend to provide better metrics. By comparing the linear model with equivalent-sized univariate transformers or in fact any other neural univariate model, one will typically get better performance.
One of the main reasons for the importance of increasing `nb_loops` is that the speech lengths generated vary greatly between different iterations, even with a fixed input.

 One consequence of this is that the latency measured by `measure_latency_and_memory_use` may not actually reflect the actual performance of optimization techniques! The benchmark at the end of the blog post reports the results averaged over 100 iterations, which gives a true indication of the performance of the model.

## 1. ü§ó Better Transformer

Better Transformer is an ü§ó Optimum feature that performs kernel fusion under the hood. This means that certain model operations will be better optimized on the GPU and that the model will ultimately be faster.
to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly
translating between non-English directions while performing competitively to the best single systems of WMT. We
open-source our scripts so that others may reproduce the data, evaluation, and final M2M-100 model.*
- A blog post on [boosting Wav2Vec2 with n-grams in ü§ó Transformers](https://huggingface.co/blog/wav2vec2-with-ngram).
- A blog post on how to [finetune Wav2Vec2 for English ASR with ü§ó Transformers](https://huggingface.co/blog/fine-tune-wav2vec2-english).
- A blog post on [finetuning XLS-R for Multi-Lingual ASR with ü§ó Transformers](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
- A notebook on how to [create YouTube captions from any video by transcribing audio with Wav2Vec2](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb). üåé
- [`Wav2Vec2ForCTC`] is supported by a notebook on [how to finetune a speech recognition model in English](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/speech_recognition.ipynb), and [how to finetune a speech recognition model in any language](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multi_lingual_speech_recognition.ipynb).
- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.
- **Flexibility**: At their core, all models are simple PyTorch `nn.Module` or TensorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) frameworks.
- **Simplicity**: Hardly any abstractions are made across the library. The ""All in one file"" is a core concept: a model's forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.

This last feature makes ü§ó Transformers quite different from other ML libraries. The models are not built on modules 
that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.
From the results above, we conclude that there is less degradation in bigger models. More interestingly, the degradation is minimal!

## Conclusion and final words

In this blogpost, we compared bitsandbytes and GPTQ quantization across multiple setups. We saw that bitsandbytes is better suited for fine-tuning while GPTQ is better for generation. From this observation, one way to get better merged models would be to: 

- (1) quantize the base model using bitsandbytes (zero-shot quantization)
- (2) add and fine-tune the adapters
- (3) merge the trained adapters on top of the base model or the [dequantized model](https://github.com/huggingface/peft/pull/851/files) !
- (4) quantize the merged model using GPTQ and use it for deployment 

We hope that this overview will make it easier for everyone to use LLMs in their applications and usecases, and we are looking forward to seeing what you will build with it!

## Acknowledgements
```

In our experience, obtaining satisfactory results from this model can be challenging. The quality of the speaker 
embeddings appears to be a significant factor. Since SpeechT5 was pre-trained with English x-vectors, it performs best 
when using English speaker embeddings. If the synthesized speech sounds poor, try using a different speaker embedding.

Increasing the training duration is also likely to enhance the quality of the results. Even so, the speech clearly is Dutch instead of English, and it does 
capture the voice characteristics of the speaker (compare to the original audio in the example).
Another thing to experiment with is the model's configuration. For example, try using `config.reduction_factor = 1` to 
see if this improves the results.
```

Play with an interactive demo for the [speech-to-text task](https://huggingface.co/spaces/Matthijs/speecht5-asr-demo).

## Conclusion

SpeechT5 is an interesting model because ‚Äî unlike most other models ‚Äî it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned.

We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It‚Äôs very versatile!
employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.*

Here is the question: What is the primary benefit of combining n-gram models with acoustic models?
Here is answer 1:Reduced computational complexity during training
Here is answer 2: Improved decoding accuracy through boosted beam search
Here is answer3: Simplification of data preprocessing workflows
Here is answer 4: Exclusive support for Transformer-based ASR",Based solely off context provided here I believe Answer Choice No.: 'Improved decoding accuracy through boosted beam search'
How can multilingual ASR models be optimized for low-resource languages?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (7688, '157a8a18-f0d3-4b4c-9ca0-8abdff1aa12f'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (3091, 'a48ade52-a88a-48fc-b5a3-a226b83924c0'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (6167, '15f088a9-d05c-4c8f-bd6b-bf5effe648dc'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (30822, '1357395c-e339-49a4-9470-5057472025be'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (30823, '6a1ba3fc-cac8-4ca8-bb2a-f30168b0c3ea'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (2731, '82686c50-994c-41d1-a986-d0c024bd9a91'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (8408, '2183f6dd-6b00-48c0-9ef5-29f2063157e5'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (6903, '807eb0ed-deee-4627-8c1d-d65ce5f2f592'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (11012, '2c7ed7d6-155a-4473-b3f7-232806a7a924'), (22794, '364ec0e7-da72-440c-aef3-1db9aed7f3dc'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (21338, 'f2a1c716-da2e-48e9-a235-2c5f9cf76008'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21495, '90b03904-a64e-420c-95a4-be77d44b1afa'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing
per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.*
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1,100 spoken languages can be identified, transcribed and generated with the various [language identification, speech recognition, and text-to-speech checkpoints released](https://huggingface.co/models?other=mms).

In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning. 

For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
The abstract from the paper is the following:

*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.*

This model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
## Introduction

Whisper is a pre-trained model for automatic speech recognition (ASR) 
published in [September 2022](https://openai.com/blog/whisper/) by the authors 
Alec Radford et al. from OpenAI. Unlike many of its predecessors, such as 
[Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), which are pre-trained 
on un-labelled audio data, Whisper is pre-trained on a vast quantity of 
**labelled** audio-transcription data, 680,000 hours to be precise. 
This is an order of magnitude more data than the un-labelled audio data used 
to train Wav2Vec 2.0 (60,000 hours). What is more, 117,000 hours of this 
pre-training data is multilingual ASR data. This results in checkpoints 
that can be applied to over 96 languages, many of which are considered 
_low-resource_.
## Introduction

Whisper is a pre-trained model for automatic speech recognition (ASR) 
published in [September 2022](https://openai.com/blog/whisper/) by the authors 
Alec Radford et al. from OpenAI. Unlike many of its predecessors, such as 
[Wav2Vec 2.0](https://arxiv.org/abs/2006.11477), which are pre-trained 
on un-labelled audio data, Whisper is pre-trained on a vast quantity of 
**labelled** audio-transcription data, 680,000 hours to be precise. 
This is an order of magnitude more data than the un-labelled audio data used 
to train Wav2Vec 2.0 (60,000 hours). What is more, 117,000 hours of this 
pre-training data is multilingual ASR data. This results in checkpoints 
that can be applied to over 96 languages, many of which are considered 
_low-resource_.
--
title: ""Fine-Tune MMS Adapter Models for low-resource ASR""
thumbnail: /blog/assets/151_mms/mms_map.png
authors:
- user: patrickvonplaten
---

# **Fine-tuning MMS Adapter Models for Multi-Lingual ASR**


<a target=""_blank"" href=""https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_MMS_on_Common_Voice.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>

***New (06/2023)***: *This blog post is strongly inspired by [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)* and can be seen as an improved version of it.
```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use ü§ó Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
*Multilingual pre-trained models are known to suffer from the curse of multilinguality, which causes per-language performance to drop as they cover more languages. We address this issue by introducing language-specific modules, which allows us to grow the total capacity of the model, while keeping the total number of trainable parameters per language constant. In contrast with prior work that learns language-specific components post-hoc, we pre-train the modules of our Cross-lingual Modular (X-MOD) models from the start. Our experiments on natural language inference, named entity recognition and question answering show that our approach not only mitigates the negative interference between languages, but also enables positive transfer, resulting in improved monolingual and cross-lingual performance. Furthermore, our approach enables adding languages post-hoc with no measurable drop in performance, no longer limiting the model usage to the set of pre-trained languages.*
According to the abstract

*Multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one
direction, a pretrained model is finetuned on many directions at the same time. It demonstrates that pretrained models
can be extended to incorporate additional languages without loss of performance. Multilingual finetuning improves on
average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while
improving 9.3 BLEU on average over bilingual baselines from scratch.*


### Training of MBart-50

The text format for MBart-50 is slightly different from mBART. For MBart-50 the language id token is used as a prefix
for both source and target text i.e the text format is `[lang_code] X [eos]`, where `lang_code` is source
language id for source text and target language id for target text, with `X` being the source or target text
respectively.


MBart-50 has its own tokenizer [`MBart50Tokenizer`].
*Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for lowresource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental
*Large multilingual language models typically rely on a single vocabulary shared across 100+ languages.
As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged.
This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R.
In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by
de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity
to achieve sufficient coverage for each individual language. Tokenizations using our vocabulary are typically
more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V,
a multilingual language model with a one million token vocabulary. XLM-V outperforms XLM-R on every task we
tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), and
</Tip>

This guide will show you how to train a [`openai/whisper-large-v2`](https://huggingface.co/openai/whisper-large-v2) model for multilingual automatic speech recognition (ASR) using a combination of `int8` quantization and LoRA. You'll train Whisper for multilingual ASR on Marathi from the [Common Voice 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) dataset.

Before you start, make sure you have all the necessary libraries installed:

```bash
!pip install -q peft transformers datasets accelerate evaluate jiwer bitsandbytes
Great! Now that we understood the motivation and theory, let's look into fine-tuning adapter weights for **`mms-1b-all`** üî•

## Notebook Setup

As done previously in the [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) blog post, we fine-tune the model on the low resource ASR dataset of [Common Voice](https://huggingface.co/datasets/common_voice) that contains only *ca.* 4h of validated training data.

Just like Wav2Vec2 or XLS-R, MMS is fine-tuned using Connectionist Temporal Classification (CTC), which is an algorithm that is used to train neural networks for sequence-to-sequence problems, such as ASR and handwriting recognition.

For more details on the CTC algorithm, I highly recommend reading the well-written blog post [*Sequence Modeling with CTC (2017)*](https://distill.pub/2017/ctc/) by Awni Hannun.
```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
![wav2vec2\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xls_r.png)

XLS-R shows impressive improvements over previous state-of-the-art
results on both speech recognition, speech translation and
speaker/language identification, *cf.* with Table 3-6, Table 7-10, and
Table 11-12 respectively of the official [paper](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages).

Setup
--------------

In this blog, we will give an in-detail explanation of how XLS-R -
more specifically the pre-trained checkpoint
[**Wav2Vec2-XLS-R-300M**](https://huggingface.co/facebook/wav2vec2-xls-r-300m) - can be fine-tuned for ASR.

For demonstration purposes, we fine-tune the model on the low resource
ASR dataset of [Common
Voice](https://huggingface.co/datasets/common_voice) that contains only
*ca.* 4h of validated training data.
*Note*: If you want to fine-tune the base models, you can do so in the exact same way as shown in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).

Similar to [BERT's masked language modeling objective](http://jalammar.github.io/illustrated-bert/), MMS learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network during self-supervised pre-training.

For ASR, the pretrained [`MMS-1B` checkpoint](https://huggingface.co/facebook/mms-1b) was further fine-tuned in a supervised fashion on 1000+ languages with a joint vocabulary output layer. As a final step, the joint vocabulary output layer was thrown away and language-specific adapter layers were kept instead. Each adapter layer contains **just** ~2.5M weights, consisting of small linear projection layers for each attention block as well as a language-specific vocabulary output layer.
```

## Limitations and bias

CER is useful for comparing different models for tasks such as automatic speech recognition (ASR) and optic character recognition (OCR), especially for multilingual datasets where WER is not suitable given the diversity of languages. However, CER provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.

Also, in some cases, instead of reporting the raw CER, a normalized CER is reported where the number of mistakes is divided by the sum of the number of edit operations (`I` + `S` + `D`) and `C` (the number of correct characters), which results in CER values that fall within the range of 0‚Äì100%.


## Citation

Here is the question: How can multilingual ASR models be optimized for low-resource languages?
Here is answer 1:Focus on high-resource datasets to compensate for data scarcity
Here is answer 2: Utilize multilingual fine-tuning and robust preprocessing techniques
Here is answer3: Exclude real-world data for simplicity
Here is answer 4: Train language-specific models without shared resources",Answer : Utilize-multilingal-fine tuning-and robust-preprocessingtechniques
What preprocessing considerations are crucial for robust ASR evaluations?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (29219, 'adf5fa87-721a-4bea-96f4-9491147ce70c'), (21548, 'e7019919-6237-4926-8823-50d944591b8d'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (30259, '39ca0aec-f1a0-42ed-bdac-a95454f5f95c'), (21560, '6de334f4-432b-4f38-86da-f421d030fb94'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (14917, '141af70d-3a35-4041-8e5f-9073db7d21e8'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (25709, 'd4436e38-6093-40ed-b39c-faee6f5bae46'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (4730, '1588743f-8472-4fdd-abe0-3ab3a94511ca'), (22144, '53697ae7-3817-4e34-ac41-c9d6703921e2'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (11905, '36ef56a0-66da-48a3-9fa8-8f6e268d8f94'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22662, 'f32c3a87-abbc-4c09-a640-bf8926fe8342'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (19086, 'd9887724-94c5-432a-9f4c-6823c878e6c0'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (28306, '3c0328c5-3673-42be-bc9a-3fe3d22eaa2d'), (22696, '47eb8a13-1c6a-4060-a807-7eb65a6df466'), (22699, '4f2bf23c-4a1f-42ce-8fcd-57d8b79aa246'), (24235, 'ea71c622-0f52-4f2f-a826-3643289d9388'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16051, '7ef3b4d6-2896-4ff8-8c3e-15ec9ed4f6ec'), (9911, 'df3b7cf0-0e9c-474e-a7b8-0c3edb8eeb91'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (21192, '33a1caa5-4ef6-4cf7-ad0c-79c8a5bca80d'), (9933, '361b3753-2f11-40ad-a8b0-91570e57b5d7'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (26338, 'c3ae9637-8b7a-42b5-af7f-76b6722ab0fc'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (31473, '107a8b8a-6308-41aa-a924-46e82911e1bc'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (8436, 'cd5d3142-6714-4b42-a6e4-e1a1a7ec18f9'), (26374, 'ab6b8f80-6158-4925-b405-c3ffdf036bd1'), (7946, 'dca97276-bfd9-4ea5-8570-0a1d528b911a'), (7952, 'bf82155c-99fa-4935-9c09-5ef3eba820c5'), (26390, '7e9534d9-70b4-497b-9fae-b424e5d8e1a3'), (8476, '570a83e4-cd08-4018-ada2-b19429c538d5'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (10551, '23b0fa1f-8a46-4da7-aac0-1805e14a220b'), (10552, '19f613db-4203-4d96-a744-9369443faac9'), (26937, 'a68f8601-700e-405b-868e-cf20c77cb9db'), (28472, '67899330-cf36-437d-850b-e81e37b4cfca'), (10555, 'bfe63e09-0fbc-492c-a580-7b3d9053592e'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28485, '90b822fb-e259-470d-9f26-b588fed88f91'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (19815, 'e594d514-fad1-4a8e-8e1c-6e918e2e11fc'), (18814, 'f732d8e9-6555-4bda-ba15-6b227351dcfa'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (2432, '73c38322-cc61-4826-9870-4162d473eef8'), (18818, 'a938f2f4-0d19-4f1e-bd3e-858ce23a0d1d'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (13201, 'f0b65266-be39-49d4-9757-564e662bb018'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (13225, '57dc4972-6e1b-45f7-aafc-58752ba3618d'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14782, '1cc646cf-ac7b-4712-a786-9905b3365b13'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (2511, '9d4f9ab1-abb1-483e-84e9-4cd82481acea'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (17914, 'e3f62a9f-6c82-48c3-84c5-d5b2d2cdf56b'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (24062, '17a005d1-952b-439c-bf0d-09520cded7bf')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ---

---
**NOTE 2**

When training a model on large datasets it is recommended to run the data preprocessing 
in a first run in a **non-distributed** mode via `--preprocessing_only` so that 
when running the model in **distributed** mode in a second step the preprocessed data
can easily be loaded on each distributed device.

---

### Demo

In this demo run we pre-train a `""base-sized""` Wav2Vec2 model simply only on the validation
and test data of [librispeech_asr](https://huggingface.co/datasets/librispeech_asr).

The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available 
per device, consider reducing `--batch_size` and/or the `--max_duration_in_seconds`.
Since those choices are not always obvious when in doubt feel free to ask on Slack or even better post on the forum, as was 
done, *e.g.* [here](https://discuss.huggingface.co/t/spanish-asr-fine-tuning-wav2vec2/4586).

## Tips and tricks

This section summarizes a couple of tips and tricks across various topics. It will continously be updated during the week.

### How to combine multiple datasets into one

Check out [this](https://discuss.huggingface.co/t/how-to-combine-local-data-files-with-an-official-dataset/4685) post.

### How to effectively preprocess the data


### How to do efficiently load datasets with limited ram and hard drive space

Check out [this](https://discuss.huggingface.co/t/german-asr-fine-tuning-wav2vec2/4558/8?u=patrickvonplaten) post.


### How to do hyperparameter tuning


### How to preprocess and evaluate character based languages


## Further reading material
A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in twice as many data points. Thus,
before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.

A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution. For example, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution, *e.g.*, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.
```

------------------------------------------------------------------------

\\( {}^1 \\) In the [paper](https://arxiv.org/pdf/2006.13979.pdf), the model
was evaluated using the phoneme error rate (PER), but by far the most
common metric in ASR is the word error rate (WER). To keep this notebook
as general as possible we decided to evaluate the model using WER.

Prepare Data, Tokenizer, Feature Extractor
------------------------------------------

ASR models transcribe speech to text, which means that we both need a
feature extractor that processes the speech signal to the model\'s input
format, *e.g.* a feature vector, and a tokenizer that processes the
model\'s output format to text.
```

```python
data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)
```

Next, the evaluation metric is defined. As mentioned earlier, the
predominant metric in ASR is the word error rate (WER), hence we will use it in this notebook as well.

```python
from evaluate import load

wer_metric = load(""wer"")
```

```python
data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)
```

Next, the evaluation metric is defined. As mentioned earlier, the
predominant metric in ASR is the word error rate (WER), hence we will
use it in this notebook as well.

```python
wer_metric = load_metric(""wer"")
```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use ü§ó Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
In this section, we will quickly go over what data is allowed to be used as training 
data, what kind of data preprocessing is allowed be used, and how the model should be evaluated.

To make it very simple regarding the first point: **All data except the official common voice `test` data set can be used as training data**. For models trained in a language that is not included in Common Voice, the author of the model is responsible to 
leave a reasonable amount of data for evaluation.
```

------------------------------------------------------------------------

\\({}^1\\) Timit is usually evaluated using the phoneme error rate (PER),
but by far the most common metric in ASR is the word error rate (WER).
To keep this notebook as general as possible we decided to evaluate the
model using WER.

Prepare Data, Tokenizer, Feature Extractor
------------------------------------------

ASR models transcribe speech to text, which means that we both need a
feature extractor that processes the speech signal to the model\'s input
format, *e.g.* a feature vector, and a tokenizer that processes the
model\'s output format to text.

In ü§ó Transformers, the Wav2Vec2 model is thus accompanied by both a
tokenizer, called
[Wav2Vec2CTCTokenizer](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2ctctokenizer),
and a feature extractor, called
[Wav2Vec2FeatureExtractor](https://huggingface.co/transformers/master/model_doc/wav2vec2.html#wav2vec2featureextractor).
Real Time Speech Recognition

Tags: ASR, SPEECH, STREAMING

## Introduction

Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).

Using `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.
```

Let's initialize the data collator.

```python
data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)
```

Next, the evaluation metric is defined. As mentioned earlier, the
predominant metric in ASR is the word error rate (WER), hence we will
use it in this notebook as well.

```python
wer_metric = load_metric(""wer"")
```

Let's initialise the data collator we've just defined:

```python
data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)
```

### Evaluation Metrics
Next, we define the evaluation metric we'll use on our evaluation
set. We'll use the Word Error Rate (WER) metric, the 'de-facto' metric for assessing 
ASR systems. For more information, refer to the WER [docs](https://huggingface.co/metrics/wer). 
We'll load the WER metric from ü§ó Evaluate:

```python
import evaluate

metric = evaluate.load(""wer"")
In short, one should prepare the data either in COCO detection or COCO panoptic format, then use
[`~transformers.DetrImageProcessor`] to create `pixel_values`, `pixel_mask` and optional
`labels`, which can then be used to train (or fine-tune) a model. For evaluation, one should first convert the
outputs of the model using one of the postprocessing methods of [`~transformers.DetrImageProcessor`]. These can
be be provided to either `CocoEvaluator` or `PanopticEvaluator`, which allow you to calculate metrics like
mean Average Precision (mAP) and Panoptic Quality (PQ). The latter objects are implemented in the [original repository](https://github.com/facebookresearch/detr). See the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETR) for more info regarding evaluation.

## Resources

A list of official Hugging Face and community (indicated by üåé) resources to help you get started with DETR.

<PipelineTag pipeline=""object-detection""/>
It is crucial that we match the sampling rate of our audio inputs to the sampling
rate expected by our model, as audio signals with different sampling rates have very
different distributions. Audio samples should only ever be processed with the 
correct sampling rate. Failing to do so can lead to unexpected results!
For instance, taking an audio sample with a sampling rate of 16kHz and listening 
to it with a sampling rate of 8kHz will make the audio sound as though it's in half-speed. 
In the same way, passing audio with the wrong sampling rate can falter an ASR model 
that expects one sampling rate and receives another. The Whisper 
feature extractor expects audio inputs with a sampling rate of 16kHz, so we need to 
match our inputs to this value. We don't want to inadvertently train an ASR 
system on slow-motion speech!
Tips:

- All ASR models accept a float array corresponding to the raw waveform of the speech signal. The raw waveform should be pre-processed with [`Wav2Vec2FeatureExtractor`].
- The models were trained using connectionist temporal classification (CTC) so the model output has to be decoded using
  [`Wav2Vec2CTCTokenizer`].
- You can load different language adapter weights for different languages via [`~Wav2Vec2PreTrainedModel.load_adapter`]. Language adapters only consists of roughly 2 million parameters 
  and can therefore be efficiently loaded on the fly when needed.

#### Loading
```

## Preprocess the data

To finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model.
[`AutoImageProcessor`] takes care of processing image data to create `pixel_values`, `pixel_mask`, and
`labels` that a DETR model can train with. The image processor has some attributes that you won't have to worry about:

- `image_mean = [0.485, 0.456, 0.406 ]`
- `image_std = [0.229, 0.224, 0.225]`

These are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial
to replicate when doing inference or finetuning a pre-trained image model.

Instantiate the image processor from the same checkpoint as the model you want to finetune.

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = ""facebook/detr-resnet-50""
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
performance of the ASR system with a CER of 0 being a perfect score.
---
Refer to the [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio_datasets_colab.ipynb) 
for a guide on evaluating a system on all eight English speech recognition datasets in one script.

The following dataset descriptions are largely taken from the [ESB Benchmark](https://arxiv.org/abs/2210.13352) paper.

#### [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr)
LibriSpeech is a standard large-scale dataset for evaluating ASR systems. It consists of approximately 1,000 
hours of narrated audiobooks collected from the [LibriVox](https://librivox.org/) project. LibriSpeech has been 
instrumental in facilitating researchers to leverage a large body of pre-existing transcribed speech data. As such, it 
has become one of the most popular datasets for benchmarking academic speech systems.

```python
librispeech = load_dataset(""librispeech_asr"", ""all"")

Here is the question: What preprocessing considerations are crucial for robust ASR evaluations?
Here is answer 1:Exclude multilingual data for consistency
Here is answer 2: Normalize text, preprocess audio, and align datasets
Here is answer3: Focus only on raw audio quality
Here is answer 4: Avoid additional preprocessing for evaluation datasets","The correct answer11is : Normalize text , preprocess"
How can hardware optimizations improve Transformer model fine-tuning?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (2591, 'e61cc32e-e588-487d-a8be-8a9025bff659'), (27168, 'c58b0c00-3b4b-4ee1-8f01-287cfcf0f690'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (2643, 'd3577235-2eda-4e17-a9e6-7cdf05401f5b'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (27234, '1560c7a6-63b7-40e5-ac9b-76996e3640b5'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (18550, '01e230dd-8782-4f9f-82af-2058aa9bd4d9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (8903, 'dce0bc34-0d40-483c-9870-a80c6a620c0b'), (6355, '9cf0e1e3-ca85-4c73-afc6-a71c99755175'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (20694, '0a712ebe-d559-4eea-b2a2-7181b1e80f23'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (23785, 'd977df81-c972-4b3e-a756-008f08c72b27'), (11497, '91203451-00d3-4187-b728-1b318e5e05d9'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (19187, '3903562f-59a1-4286-92d6-9666ba3e9bd1'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (17166, 'f8edb48c-3037-4068-b3cc-1bc5a12bd5d3'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (25884, '585808ef-bcbd-4bda-9dc1-376342a66a99'), (17188, '088b5e4c-6891-4808-bd38-f2e2d894daab'), (17701, '784d3fc4-515c-4f02-aaf0-53a858680d2d'), (17190, '03819edd-dcd1-4e8c-baa0-f847b307fe77'), (16678, '1e0fefb6-23b9-4fea-8cfe-628c3ca26e11'), (23847, '25175e5a-8fc3-4050-87fd-bf640d60de2f'), (16681, 'fc9a2a86-bdfe-4b9d-8e65-72359a9c1b04'), (15659, '54679008-9ef1-4235-9d99-b2a9666e3d13'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (21345, '721fab0b-7c94-420c-b346-46fdaea2cb58'), (22885, '6f79d12f-3eff-4c84-a3bc-e653b1d7ab70'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (23912, '269f3664-542a-4c1e-b029-6c9a8442db1b'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (23918, '648aab91-0479-46e6-a00a-7d416b53cdd0'), (23920, '918f2d3c-9fd1-41a7-b7bf-db620459fcb1'), (30576, '76618c07-7a1a-4ffb-bc13-6da43eb286d5'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5537, '98a3798c-4c95-4db2-aad9-77037f52f460'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (6065, '4628167e-c8db-40a3-a76a-fc6588f689e0'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (23475, '68a40f24-cf09-4158-a626-b4357e796ceb'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (11713, '16213aad-a177-42a5-8be6-a4c1224c45d0'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (475, 'fc8680f8-2eee-4940-8f49-33e177da211a'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (31735, '31b3ae48-f7ec-45ee-adb9-0d7d5e3541ca'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (31741, '3cb8440b-d4b0-4bd5-930d-1fc4596f3292'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune‚Äôs integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.

And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!
Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.

Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
{/if}

For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results. 

However, there are a few cases where you'll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!
![IGF performance](result_igf.png)

Figure 1: Comparing IGF to Standard Fine-tuning:
IGF with constant (p < 10‚àí3 , t-test) and shifting(p < 10‚àí6 , t-test) thresholding significantly outperform standard fine-tuning. The left-hand figure shows
test-set perplexity after each fine-tuning batch, averaged over 50 runs (error bars denote ¬± one standard error). The right-hand figure shows the perplexity of each
method after 60 batches. IGF with shifting thresholding (red) clearly improves over standard batched fine-tuning with Adam

## How to use this project?

To fine-tune a transformer model with IGF on a language modeling task, use the following script:
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model -- one as close as possible to the task you have at hand -- and fine-tune it.

## General architecture[[general-architecture]]

In this section, we'll go over the general architecture of the Transformer model. Don't worry if you don't understand some of the concepts; there are detailed sections later covering each of the components.

<Youtube id=""H39Z_720T5s"" />

## Introduction[[introduction]]

The model is primarily composed of two blocks:

* **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
* **Decoder (right)**: The decoder uses the encoder's representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.
However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!
However, simple experiments are able to show the benefit of using an advanced tuning technique. Below is [a recent experiment run on a BERT](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b) model from [Hugging Face transformers](https://github.com/huggingface/transformers) on the [RTE dataset](https://aclweb.org/aclwiki/Textual_Entailment_Resource_Pool). Genetic optimization techniques like [PBT](https://docs.ray.io/en/latest/tune/api_docs/schedulers.html#population-based-training-tune-schedulers-populationbasedtraining) can provide large performance improvements compared to standard hyperparameter optimization techniques.
[Optimum](https://github.com/huggingface/optimum) aims to make this work easy, providing performance optimization tools targeting efficient AI hardware,
built in collaboration with our Hardware Partners, and turn Machine Learning Engineers into ML Optimization wizards.

With the [Transformers](https://github.com/huggingface/transformers) library, we made it easy for researchers and engineers to use state-of-the-art models,
abstracting away the complexity of frameworks, architectures and pipelines.

With the [Optimum](https://github.com/huggingface/optimum) library, we are making it easy for engineers to leverage all the available hardware features at their disposal,
abstracting away the complexity of model acceleration on hardware platforms.

## ü§ó Optimum in practice: how to quantize a model for Intel Xeon CPU
### ü§î Why quantization is important but tricky to get right
### üîÆ State-of-the-Art Models

The collaboration with our Hardware Partners will yield hardware-specific optimized model configurations and artifacts,
which we will make available to the AI community via the Hugging Face [Model Hub](https://huggingface.co/models).
We hope that Optimum and hardware-optimized models will accelerate the adoption of efficiency in production workloads,
which represent most of the aggregate energy spent on Machine Learning.
And most of all, we hope that Optimum will accelerate the adoption of Transformers at scale, not just for the biggest tech companies, but for all of us.

### üåü A journey of collaboration: join us, follow our progress

Every journey starts with a first step, and ours was the public release of Optimum.
Join us and make your first step by [giving the library a Star](https://github.com/huggingface/optimum),
so you can follow along as we introduce new supported hardware, acceleration techniques and optimized models.
```

## Prompting vs fine-tuning

You can achieve great results by optimizing your prompts, however, you may still ponder whether fine-tuning a model 
would work better for your case. Here are some scenarios when fine-tuning a smaller model may be a preferred option:

- Your domain is wildly different from what LLMs were pre-trained on and extensive prompt optimization did not yield sufficient results. 
- You need your model to work well in a low-resource language.
- You need the model to be trained on sensitive data that is under strict regulations. 
- You have to use a small model due to cost, privacy, infrastructure or other limitations. 

In all of the above examples, you will need to make sure that you either already have or can easily obtain a large enough 
domain-specific dataset at a reasonable cost to fine-tune a model. You will also need to have enough time and resources 
to fine-tune a model.
## Optimizer choice

The most common optimizer used to train transformer models is Adam or AdamW (Adam with weight decay). Adam achieves 
good convergence by storing the rolling average of the previous gradients; however, it adds an additional memory 
footprint of the order of the number of model parameters. To remedy this, you can use an alternative optimizer. 
For example if you have [NVIDIA/apex](https://github.com/NVIDIA/apex) installed for NVIDIA GPUs, or [ROCmSoftwarePlatform/apex](https://github.com/ROCmSoftwarePlatform/apex) for AMD GPUs, `adamw_apex_fused` will give you the
fastest training experience among all supported AdamW optimizers.

[`Trainer`] integrates a variety of optimizers that can be used out of box: `adamw_hf`, `adamw_torch`, `adamw_torch_fused`, 
`adamw_apex_fused`, `adamw_anyprecision`, `adafactor`, or `adamw_bnb_8bit`. More optimizers can be plugged in via a third-party implementation.
### üè≠ Optimum puts Transformers to work

To get optimal performance training and serving models, the model acceleration techniques need to be specifically compatible with the targeted hardware.
Each hardware platform offers specific software tooling,
[features and knobs that can have a huge impact on performance](https://huggingface.co/blog/bert-cpu-scaling-part-1).
Similarly, to take advantage of advanced model acceleration techniques like sparsity and quantization, optimized kernels need to be compatible with the operators on silicon,
and specific to the neural network graph derived from the model architecture.
Diving into this 3-dimensional compatibility matrix and how to use model acceleration libraries is daunting work,
which few Machine Learning Engineers have experience on.
Luckily, Hugging Face has introduced [Optimum](https://huggingface.co/hardware), an open source library which makes it much easier to reduce the prediction latency of Transformer models on a variety of hardware platforms. In this blog post, you will learn how to accelerate Transformer models for the Graphcore [Intelligence Processing Unit](https://www.graphcore.ai/products/ipu) (IPU), a highly flexible, easy-to-use parallel processor designed from the ground up for AI workloads.

### Optimum Meets Graphcore IPU

Through this partnership between Graphcore and Hugging Face, we are now introducing BERT as the first IPU-optimized model. We will be introducing many more of these IPU-optimized models in the coming months, spanning applications such as vision, speech, translation and text generation.
One of the main reasons for the importance of increasing `nb_loops` is that the speech lengths generated vary greatly between different iterations, even with a fixed input.

 One consequence of this is that the latency measured by `measure_latency_and_memory_use` may not actually reflect the actual performance of optimization techniques! The benchmark at the end of the blog post reports the results averaged over 100 iterations, which gives a true indication of the performance of the model.

## 1. ü§ó Better Transformer

Better Transformer is an ü§ó Optimum feature that performs kernel fusion under the hood. This means that certain model operations will be better optimized on the GPU and that the model will ultimately be faster.
The abstract from the paper is the following:

*Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can
be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of
Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its
complexity from O(L^2) to O(Llog(L)), where L is the length of the sequence. Furthermore, we use reversible residual
layers instead of the standard residuals, which allows storing activations only once in the training process instead of
N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models
while being much more memory-efficient and much faster on long sequences.*
* Compressive Transformer requires a special optimization schedule in which the effective batch size is progressively increased to avoid significant performance degradation for lower learning rates. This effect is not well understood and calls into more analysis.
* The Compressive Transformer has many more hyperparameters compared to a simple model like BERT or GPT2: the compression rate, the compression function and loss, the regular and compressed memory sizes, etc.  It is not clear whether those parameters generalize well across different tasks (other than language modeling) or similar to the learning rate, make the training also very brittle.
We'll keep adding best practices to our tools so you don't have to discover the same issues again.

## Optimization

There's no point in converting models if they don't run fast on your target hardware and respect system resources. The models mentioned in this post are pretty big for local use, and we are consciously using them to stretch the limits of what's possible with current technology and understand where the bottlenecks are.

There are a few key optimization areas we've identified. They are a very important topic for us and the subject of current and upcoming work. Some of them include:
## QLoRA paper, a new way of democratizing quantized large transformer models

In few words, QLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning. This method enables 33B model finetuning on a single 24GB GPU and 65B model finetuning on a single 46GB GPU.

More specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. Read more about LoRA in the [original LoRA paper](https://arxiv.org/abs/2106.09685).
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. ü§ó Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with ü§ó Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>

Here is the question: How can hardware optimizations improve Transformer model fine-tuning?
Here is answer 1:By focusing solely on GPU-specific libraries
Here is answer 2: Utilizing Intel's matrix extensions and pipeline optimizations
Here is answer3: Avoiding hardware-specific features to ensure compatibility
Here is answer 4: By using pretrained models without tuning",Answer : Utilizing Intel's matrix extensions
How can multitask prompt tuning enhance multilingual model performance?,"[(11279, '884c0122-efae-4da0-9f5a-a74765dcd2a2'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (16919, '942238c0-9ba8-45fc-bd5c-b13953074601'), (16920, 'cd0788e0-82b0-4b44-9dd4-8b8f83cf3ab9'), (16921, 'a31bb2a1-bb83-4766-8534-5664f075ae46'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (25150, 'c125fa87-ac9a-4bb7-9926-8dc62c5fb983'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (25151, '15aa2c96-bd80-4f22-bfb0-0ec6a68df5d3'), (30784, '5abe754b-87a4-4fc2-9103-0a0a0aab8e09'), (30785, 'fe7b98b8-14ab-4e54-8af2-68f748159690'), (10830, '63fe604b-e2e2-49c7-bfbb-16c17a3cb8c7'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (30825, '8934d6ff-828c-4bde-8179-5d5a150bf5e1'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (9368, 'a2674ef7-b70d-4243-8f59-ea725770523f'), (30381, 'fb5013aa-10f1-4372-90a9-b341b7365744'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (5815, '7a89c479-9823-4c0f-a0de-e3ecf304e920'), (696, 'b996a5f4-7c16-44f1-aa17-bcf65b15101a'), (22721, '3c6eb39a-0e3f-4234-8cdb-5330689e73b3'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (17622, '18cebb0d-bc71-4646-91ee-08fdc8fb51a7'), (731, 'ad4324cd-9c6c-4555-88d6-faf72cb8a9da'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (4847, 'daa43fb0-dad9-451b-bddf-101880195771'), (7923, '3d12a194-ca8a-4ffe-9e77-8fb01bda0fd8'), (7924, '9e9cce0b-318b-4407-8527-b450af5fc69f'), (7925, '898386e9-41b2-43b2-bf00-f5fe304bcf79'), (7926, 'f97b01a4-b979-42d6-8789-bec7870b5e30'), (7927, 'be9f55e3-25af-4774-95e1-537c53e8e6e5'), (7928, '3af4d2b1-479b-4154-9dfd-f8a6036bc83c'), (17657, '12121e46-a187-42c0-a9ad-22a96e02c8b9'), (7929, '698a3ccc-aa9e-4fff-af7d-792856b4a975'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (20749, 'b2e0610f-aa0c-4852-b1ad-2bf187ecf97e'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (1330, '10f0617c-6b29-4d38-a4b3-c0e4a997e308'), (26421, '2c89420e-e9cb-4967-9268-cef5f028e779'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (30534, '95e21773-3553-4d8f-8b88-fff6315116ba'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (21324, 'c689a247-869d-4ea7-b9f0-170aa19a493b'), (8525, '931d0e8a-fa6c-4b84-9b4a-f7329eb19392'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (17774, 'e13cc263-c072-4a3b-9f64-7b688f1c79bf'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (3441, 'a50198ff-bdce-4045-af16-a7f3e4ed8e3a'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (20860, 'f6cd2e41-5cbe-4525-a0d2-89d48c843903'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (21887, '2fc3be04-7823-41a9-af0e-6c649e006652'), (18309, '77eb1ea6-c7df-40f4-a112-aec1b6d6ae26'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (28555, '3900d0c8-9171-4b7d-bf48-27c131dec37b'), (28556, '6de4578f-f7a2-4271-a425-65baf13f4391'), (10648, '66ffc54d-754b-40e5-b926-93bd59b981cb'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (28579, 'faeb2a24-8c7e-43f7-b3f4-3ae4017753be'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (28585, 'd3223831-24e7-4557-a7a0-28f800f26eb0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (26043, 'a7b91905-1ecc-4b7e-a8c2-bd488f3d3b67'), (29122, '81bc2e07-9a53-43ff-a046-a73c2a5c8858'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (25033, '1414432c-aab4-4c4c-b04a-1e8d688255ed'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (30677, 'a25bef54-3520-4f86-a7af-0451578e6208'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (25565, 'bfb5de19-ac27-4d58-ade9-2cf5e047b9eb'), (482, 'ecfc6092-627b-4285-bb59-322895262902'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (13288, '50cf0dc9-5985-4564-b357-edc6d717de24'), (15871, '23fe4629-d4f8-4f28-85a5-d238feb3371a'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (1522, '28731235-d234-44e1-9e19-7f8f6084cb19'), (16890, 'c8b38db6-3834-4660-9877-40161fc8646f'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (15869, '5e46d120-df1b-4d36-bd63-b7491a1e4eff'), (15870, 'aa4dffab-9c16-4523-8b50-2bde7016c29f'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: - the prompt tokens can be inserted anywhere in the input sequence, and it isn't restricted to only the beginning
- the prompt tokens are only added to the input instead of adding them to every layer of the model
- introducing *anchor* tokens can improve performance because they indicate characteristics of a component in the input sequence

The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables GPT-like models to compete with BERT-like models on NLU tasks.

Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification) for a step-by-step guide on how to train a model with P-tuning.

## Multitask prompt tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png""/>
</div>
<small><a href=""https://hf.co/papers/2103.10385"">Multitask prompt tuning enables parameter-efficient transfer learning</a>.</small>
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
8. LoHa: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098)
9. LoKr: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650) based on [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://arxiv.org/abs/2309.14859) implementation
10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
11. OFT: [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)
The abstract from the paper is:

*Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters*.
[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:

1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Multitask Prompt Tuning

[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.

The abstract from the paper is:
1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)
2. Prefix Tuning: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/), [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
3. P-Tuning: [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
4. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
5. AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)  
6. $(IA)^3$: [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
*We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art
pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to
significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By
reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on
standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that
allocating additional capacity to the output embedding provides benefits to the model that persist through the
fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
*In this work, we explore ""prompt tuning"", a simple yet effective mechanism for learning ""soft prompts"" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ""closes the gap"" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and
- **FLan-UL2** : the UL2 model finetuned using the ""Flan"" prompt tuning and dataset collection.

- **UMT5**: UmT5 is a multilingual T5 model trained on an improved and refreshed mC4 multilingual corpus,  29 trillion characters across 107 language, using a new sampling method, UniMax. Refer to
 the documentation of mT5 which can be found [here](umt5).

## Training
## MultitaskPromptTuningConfig

[[autodoc]] tuners.multitask_prompt_tuning.config.MultitaskPromptTuningConfig

## MultitaskPromptEmbedding

[[autodoc]] tuners.multitask_prompt_tuning.model.MultitaskPromptEmbedding
[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification tasks on T5 models, and all downstream tasks are cast as a text generation task. For example, sequence classification usually assigns a single class label to a sequence of text. By casting it as a text generation task, the tokens that make up the class label are *generated*. Prompts are added to the input as a series of tokens. Typically, the model parameters are fixed which means the prompt tokens are also fixed by the model parameters.

The key idea behind prompt tuning is that prompt tokens have their own parameters that are updated independently. This means you can keep the pretrained model's parameters frozen, and only update the gradients of the prompt token embeddings. The results are comparable to the traditional method of training the entire model, and prompt tuning performance scales as model size increases.
According to the abstract

*Multilingual translation models can be created through multilingual finetuning. Instead of finetuning on one
direction, a pretrained model is finetuned on many directions at the same time. It demonstrates that pretrained models
can be extended to incorporate additional languages without loss of performance. Multilingual finetuning improves on
average 1 BLEU over the strongest baselines (being either multilingual from scratch or bilingual finetuning) while
improving 9.3 BLEU on average over bilingual baselines from scratch.*


### Training of MBart-50

The text format for MBart-50 is slightly different from mBART. For MBart-50 the language id token is used as a prefix
for both source and target text i.e the text format is `[lang_code] X [eos]`, where `lang_code` is source
language id for source text and target language id for target text, with `X` being the source or target text
respectively.


MBart-50 has its own tokenizer [`MBart50Tokenizer`].
Compared to other few-shot learning methods, SetFit has several unique features:

* üó£ **No prompts or verbalizers:** Current techniques for few-shot fine-tuning require handcrafted prompts or verbalizers to convert examples into a format suitable for the underlying language model. SetFit dispenses with prompts altogether by generating rich embeddings directly from text examples.
* üèé **Fast to train:** SetFit doesn't require large-scale models like [T0](https://huggingface.co/bigscience/T0) or GPT-3 to achieve high accuracy. As a result, it is typically an order of magnitude (or more) faster to train and run inference with.
* üåé **Multilingual support**: SetFit can be used with any [Sentence Transformer](https://huggingface.co/models?library=sentence-transformers&sort=downloads) on the Hub, which means you can classify text in multiple languages by simply fine-tuning a multilingual checkpoint.

## Exploring SetFit on the Hub
*Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual
alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining
and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging
entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages
with entity representations and show the model consistently outperforms word-based pretrained models in various
cross-lingual transfer tasks. We also analyze the model and the key insight is that incorporating entity
representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a
multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual
knowledge more likely than using only word representations.*
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
-->

# Prompt tuning for causal language modeling

[[open-in-colab]]

Prompting helps guide language model behavior by adding some input text specific to a task. Prompt tuning is an additive method for only training and updating the newly added prompt tokens to a pretrained model. This way, you can use one pretrained model whose weights are frozen, and train and update a smaller set of prompt parameters for each downstream task instead of fully finetuning a separate model. As models grow larger and larger, prompt tuning can be more efficient, and results are even better as model parameters scale.

<Tip>

üí° Read [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) to learn more about prompt tuning.

</Tip>

This guide will show you how to apply prompt tuning to train a [`bloomz-560m`](https://huggingface.co/bigscience/bloomz-560m) model on the `twitter_complaints` subset of the [RAFT](https://huggingface.co/datasets/ought/raft) dataset.
The main difference is that the prefix parameters are inserted in **all** of the model layers, whereas prompt tuning only adds the prompt parameters to the model input embeddings. The prefix parameters are also optimized by a separate feed-forward network (FFN) instead of training directly on the soft prompts because it causes instability and hurts performance. The FFN is discarded after updating the soft prompts.

As a result, the authors found that prefix tuning demonstrates comparable performance to fully finetuning a model, despite having 1000x fewer parameters, and it performs even better in low-data settings.

Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning) for a step-by-step guide on how to train a model with prefix tuning.

## P-tuning
Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1,100 spoken languages can be identified, transcribed and generated with the various [language identification, speech recognition, and text-to-speech checkpoints released](https://huggingface.co/models?other=mms).

In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning. 

For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
```

## Closing Remarks

In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR 
using ü§ó Datasets, Transformers and the Hugging Face Hub. Refer to the [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb) 
should you wish to try fine-tuning for yourself. If you're interested in fine-tuning other 
Transformers models, both for English and multilingual ASR, be sure to check out the 
examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).
And just by switching out the base Sentence Transformer model to a multilingual one, SetFit can function seamlessly in multilingual contexts. In our [experiments](https://arxiv.org/abs/2209.11055), SetFit‚Äôs performance shows promising results on classification in German, Japanese, Mandarin, French and Spanish, in both in-language and cross linguistic settings.


## Benchmarking SetFit

Here is the question: How can multitask prompt tuning enhance multilingual model performance?
Here is answer 1:By reducing task-specific customization
Here is answer 2: Leveraging shared resources and robust prompt initialization
Here is answer3: Focusing solely on monolingual datasets
Here is answer 4: Avoiding task-specific prompts to simplify tuning",Answer : By reducing.task_specific customization
What is the main challenge when training ASR models for cross-language tasks?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (30211, 'b3191d20-4a96-4fec-acc1-45b0aa492a6e'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (7688, '157a8a18-f0d3-4b4c-9ca0-8abdff1aa12f'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9747, '362139c4-a349-4533-8add-38dc8c990f5d'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (30269, 'ce1b2ec6-16bc-48d4-8e48-ee802e65abe6'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (14430, 'df8628a2-0050-42f4-be97-ef5d21f207ed'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (30823, '6a1ba3fc-cac8-4ca8-bb2a-f30168b0c3ea'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (9371, '7b487395-e96b-4d33-9523-2231254a4ce4'), (9379, '124e807e-b7a7-4f35-89e3-cc8b593f5dc6'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (7341, 'b601a33f-7e1f-42fd-90b5-1a99b5987075'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26334, 'f86f1902-9868-454f-9ef6-f7c11ce29015'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (9452, '59aaeec8-b95f-4c82-89b3-f8905ba521d8'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (11009, 'eb1201bc-1aad-46e8-8ded-a206fbebb020'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11012, '2c7ed7d6-155a-4473-b3f7-232806a7a924'), (22794, '364ec0e7-da72-440c-aef3-1db9aed7f3dc'), (22795, '52c61654-a4db-433c-b73f-a890c21cbc8a'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (23860, 'afa6eb69-b861-463e-94f7-a8cffe60467e'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26946, 'd51e55e0-4a83-4946-a69c-5343acec6844'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (8019, '48aa920d-6f37-4719-a7a4-3bd78af534fe'), (25428, 'd391c525-320b-49a3-83dc-e6a90c266db7'), (8023, '1dec94c2-ffe1-4e8f-a0e8-d220743cc341'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (8034, '6748368f-01a0-48c5-9d0b-812ee69deeff'), (8035, '85373154-23b0-4c8d-826b-c39c88e0e81b'), (17274, '74261f18-6c75-412b-a605-5744fcbd65d1'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (18844, 'c97c2cdd-44fb-4531-87d5-afef8f19b517'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21495, '90b03904-a64e-420c-95a4-be77d44b1afa'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
Our best WER is 32.0% - not bad for 8h of training data! The big question is how this 
compares to other ASR systems. For that, we can view the [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench), 
a leaderboard that categorises models by language and dataset, and subsequently ranks 
them according to their WER.

<figure>
<img src=""assets/111_fine_tune_whisper/hf_speech_bench.jpg"" alt=""Trulli"" style=""width:100%"">
</figure>

Our fine-tuned model significantly improves upon the zero-shot performance of the Whisper 
`small` checkpoint, highlighting the strong transfer learning capabilities of Whisper.

We can automatically submit our checkpoint to the leaderboard when we
push the training results to the Hub - we simply have to set the appropriate key-word 
arguments (kwargs). You can change these values to match your dataset, language and 
model name accordingly:
```

Play with an interactive demo for the [speech-to-text task](https://huggingface.co/spaces/Matthijs/speecht5-asr-demo).

## Conclusion

SpeechT5 is an interesting model because ‚Äî unlike most other models ‚Äî it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned.

We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It‚Äôs very versatile!
**Wav2Vec2** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by *Alexei Baevski, Michael Auli, and Alex Conneau*. Soon after the strong performance of Wav2Vec2 was demonstrated on one of the most popular English datasets for ASR, called [LibriSpeech](https://huggingface.co/datasets/librispeech_asr), *Facebook AI* presented two multi-lingual versions of Wav2Vec2, called [XLSR](https://arxiv.org/abs/2006.13979) and [XLM-R](https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/), capable of recognising speech in up to 128 languages. XLSR stands for *cross-lingual speech representations* and refers to the model's ability to learn speech representations that are useful across multiple languages.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
### 11. Why is it easy to pretrain a language model on lots and lots of texts?

<Question
	choices={[
		{
			text: ""Because there are plenty of texts available on the internet"",
			explain: ""Although true, that doesn't really answer the question. Try again!""
		},
		{
			text: ""Because the pretraining objective does not require humans to label the data"",
			explain: ""That's correct, language modeling is a self-supervised problem."",
			correct: true
		},
		{
			text: ""Because the ü§ó Transformers library only requires a few lines of code to start the training"",
			explain: ""Although true, that doesn't really answer the question asked. Try another answer!""
		}
	]}
/>

### 12. What are the main challenges when preprocessing data for a question answering task?
Tips:

- All ASR models accept a float array corresponding to the raw waveform of the speech signal. The raw waveform should be pre-processed with [`Wav2Vec2FeatureExtractor`].
- The models were trained using connectionist temporal classification (CTC) so the model output has to be decoded using
  [`Wav2Vec2CTCTokenizer`].
- You can load different language adapter weights for different languages via [`~Wav2Vec2PreTrainedModel.load_adapter`]. Language adapters only consists of roughly 2 million parameters 
  and can therefore be efficiently loaded on the fly when needed.

#### Loading
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
***New (11/2021)***: *This blog post has been updated to feature XLSR\'s
successor, called [XLS-R](https://huggingface.co/models?other=xls_r)*.

**Wav2Vec2** is a pretrained model for Automatic Speech Recognition
(ASR) and was released in [September
2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/)
by *Alexei Baevski, Michael Auli, and Alex Conneau*. Soon after the
superior performance of Wav2Vec2 was demonstrated on one of the most
popular English datasets for ASR, called
[LibriSpeech](https://huggingface.co/datasets/librispeech_asr),
*Facebook AI* presented a multi-lingual version of Wav2Vec2, called
[XLSR](https://arxiv.org/abs/2006.13979). XLSR stands for *cross-lingual
speech representations* and refers to model\'s ability to learn speech
representations that are useful across multiple languages.
Using a novel contrastive pretraining objective, Wav2Vec2 learns
powerful speech representations from more than 50.000 hours of unlabeled
speech. Similar, to [BERT\'s masked language
modeling](http://jalammar.github.io/illustrated-bert/), the model learns
contextualized speech representations by randomly masking feature
vectors before passing them to a transformer network.

![wav2vec2\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/wav2vec2.png)

For the first time, it has been shown that pretraining, followed by
fine-tuning on very little labeled speech data achieves competitive
results to state-of-the-art ASR systems. Using as little as 10 minutes
of labeled data, Wav2Vec2 yields a word error rate (WER) of less than 5%
on the clean test set of
[LibriSpeech](https://huggingface.co/datasets/librispeech_asr) - *cf.*
with Table 9 of the [paper](https://arxiv.org/pdf/2006.11477.pdf).
![wav2vec2\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xls_r.png)

XLS-R shows impressive improvements over previous state-of-the-art
results on both speech recognition, speech translation and
speaker/language identification, *cf.* with Table 3-6, Table 7-10, and
Table 11-12 respectively of the official [paper](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages).

Setup
--------------

In this blog, we will give an in-detail explanation of how XLS-R -
more specifically the pre-trained checkpoint
[**Wav2Vec2-XLS-R-300M**](https://huggingface.co/facebook/wav2vec2-xls-r-300m) - can be fine-tuned for ASR.

For demonstration purposes, we fine-tune the model on the low resource
ASR dataset of [Common
Voice](https://huggingface.co/datasets/common_voice) that contains only
*ca.* 4h of validated training data.
```

## Limitations and bias

CER is useful for comparing different models for tasks such as automatic speech recognition (ASR) and optic character recognition (OCR), especially for multilingual datasets where WER is not suitable given the diversity of languages. However, CER provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.

Also, in some cases, instead of reporting the raw CER, a normalized CER is reported where the number of mistakes is divided by the sum of the number of edit operations (`I` + `S` + `D`) and `C` (the number of correct characters), which results in CER values that fall within the range of 0‚Äì100%.


## Citation
```

## Limitations and bias

CER is useful for comparing different models for tasks such as automatic speech recognition (ASR) and optic character recognition (OCR), especially for multilingual datasets where WER is not suitable given the diversity of languages. However, CER provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.

Also, in some cases, instead of reporting the raw CER, a normalized CER is reported where the number of mistakes is divided by the sum of the number of edit operations (`I` + `S` + `D`) and `C` (the number of correct characters), which results in CER values that fall within the range of 0‚Äì100%.


## Citation
*Recent studies have demonstrated that pre-trained cross-lingual models achieve impressive performance in downstream cross-lingual tasks. This improvement benefits from learning a large amount of monolingual and parallel corpora. Although it is generally acknowledged that parallel corpora are critical for improving the model performance, existing methods are often constrained by the size of parallel corpora, especially for lowresource languages. In this paper, we propose ERNIE-M, a new training method that encourages the model to align the representation of multiple languages with monolingual corpora, to overcome the constraint that the parallel corpus size places on the model performance. Our key insight is to integrate back-translation into the pre-training process. We generate pseudo-parallel sentence pairs on a monolingual corpus to enable the learning of semantic alignments between different languages, thereby enhancing the semantic modeling of cross-lingual models. Experimental
-->

# Automatic speech recognition

[[open-in-colab]]

<Youtube id=""TksaY_FDgnk""/>

Automatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa use ASR models to help users everyday, and there are many other useful user-facing applications like live captioning and note-taking during meetings.

This guide will show you how to:

1. Finetune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to transcribe audio to text.
2. Use your finetuned model for inference.

<Tip>
The task illustrated in this tutorial is supported by the following model architectures:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->
*Note*: If you want to fine-tune the base models, you can do so in the exact same way as shown in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).

Similar to [BERT's masked language modeling objective](http://jalammar.github.io/illustrated-bert/), MMS learns contextualized speech representations by randomly masking feature vectors before passing them to a transformer network during self-supervised pre-training.

For ASR, the pretrained [`MMS-1B` checkpoint](https://huggingface.co/facebook/mms-1b) was further fine-tuned in a supervised fashion on 1000+ languages with a joint vocabulary output layer. As a final step, the joint vocabulary output layer was thrown away and language-specific adapter layers were kept instead. Each adapter layer contains **just** ~2.5M weights, consisting of small linear projection layers for each attention block as well as a language-specific vocabulary output layer.
In this notebook, we will give an in-detail explanation of how
Wav2Vec2\'s pretrained checkpoints can be fine-tuned on any English ASR
dataset. Note that in this notebook, we will fine-tune Wav2Vec2 without
making use of a language model. It is much simpler to use Wav2Vec2
without a language model as an end-to-end ASR system and it has been
shown that a standalone Wav2Vec2 acoustic model achieves impressive
results. For demonstration purposes, we fine-tune the \""base\""-sized
[pretrained checkpoint](https://huggingface.co/facebook/wav2vec2-base)
on the rather small [Timit](https://huggingface.co/datasets/timit_asr)
dataset that contains just 5h of training data.

Wav2Vec2 is fine-tuned using Connectionist Temporal Classification
(CTC), which is an algorithm that is used to train neural networks for
sequence-to-sequence problems and mainly in Automatic Speech Recognition
and handwriting recognition.
</Tip>

This guide will show you how to train a [`openai/whisper-large-v2`](https://huggingface.co/openai/whisper-large-v2) model for multilingual automatic speech recognition (ASR) using a combination of `int8` quantization and LoRA. You'll train Whisper for multilingual ASR on Marathi from the [Common Voice 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) dataset.

Before you start, make sure you have all the necessary libraries installed:

```bash
!pip install -q peft transformers datasets accelerate evaluate jiwer bitsandbytes
To adapt to 1000+ different vocabularies, **MMS** uses of Adapters - a training method where only a small fraction of model weights are trained.

Adapter layers act like linguistic bridges, enabling the model to leverage knowledge from one language when deciphering another.

## **Fine-tuning MMS**

**MMS** unsupervised checkpoints were pre-trained on more than **half a million** hours of audio in over **1,400** languages, ranging from 300 million to one billion parameters.

You can find the pretrained-only checkpoints on the ü§ó Hub for model sizes of 300 million parameters (300M) and one billion parameters (1B):

-   [**`mms-300m`**](https://huggingface.co/facebook/mms-300m)
-   [**`mms-1b`**](https://huggingface.co/facebook/mms-1b)

*Note*: If you want to fine-tune the base models, you can do so in the exact same way as shown in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).

Here is the question: What is the main challenge when training ASR models for cross-language tasks?
Here is answer 1:Multilingual datasets are easy to align but difficult to preprocess
Here is answer 2: Dataset alignment and preprocessing require multilingual fine-tuning strategies
Here is answer3: Cross-language tasks do not benefit from multilingual fine-tuning
Here is answer 4: Monolingual models perform better for cross-language tasks",The correct answer4(the others seem incorrect or related but I'm unsure): Because there isn't much labeled multi-language test set available(database or annotations need human labeling Artificial intelligence makes human-level labeling challenging\xe9\x85\xa4\xe7\xbe\xa9\xe7\xb6\x95\xe5\xaa\xb3\xe8\xaa\xbb\xe4\xb9\xa6); It's challenging because artificial intelligence makes human-level labeling challenging since databases or annotations need human labeling.)
How can ASR pipelines be optimized for efficiency on Intel CPUs?,"[(4608, '11eba3f7-462f-400d-a1c6-26fbdecddb11'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (11298, 'e265f736-1fce-44dc-83cd-0e039d2d5f03'), (28707, '1579ae2b-a259-4e21-a4ab-6554888fc9db'), (22052, '1a82c910-0703-4313-a37c-d5ad255b37fb'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (11302, 'a3c2eeca-7cdc-4f91-b078-57ecfbe53731'), (10276, 'debee365-bf66-467b-9618-eea6f12bffa9'), (14378, 'bdbac296-8dd0-4a3f-b93d-26f2653df54e'), (14382, 'f0a3cc11-b2fc-4b21-a76e-c0d46a55834a'), (30770, '964e522f-8c2f-4031-a160-73dccc219846'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (3129, 'eb945bfb-2047-4a55-a268-2f7b9efc42f6'), (10302, '3b9c2808-fcbc-4138-a267-153f19e63291'), (17474, 'b44d6938-b9e8-475d-91fd-ce040ce67390'), (10819, '9fb4e5bf-b2d1-4c28-ac14-c7b7fd6f77ae'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (9803, 'a0b49fbb-8568-4355-8bdb-8c64f1a568ad'), (28750, '909829c2-8f61-4347-b43a-4fbe10ee781f'), (9296, 'b982a595-b413-47c2-a522-d71819d17b20'), (28752, 'a8f3fc54-9b72-46a0-b4ac-29da5374d20e'), (28754, '2f370cf6-442c-48bd-a56c-8301e99110f6'), (9297, 'b1ddc06d-35a1-4ac0-b68f-697de22d55e0'), (15956, 'd03f6960-2286-4540-a440-e6bb93b141f6'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (97, 'c110d4b5-e404-4749-8851-600a26eec5e4'), (5730, '73559eea-5360-4f48-8b5e-01efa4ef8a4f'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (9832, 'bdc6bea1-7f20-4300-aae9-3f7ea03a6783'), (9834, '5c02c88b-44fc-4234-9888-577064cae93c'), (9835, '221255a0-2941-471c-9c13-9a9cbe7621a7'), (5738, '1b3f6207-1eaf-4d30-b7f5-15ce5479c98a'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (5740, '09139d8a-3586-435c-8b0f-40c071bbd0ee'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (14454, 'ce330ae8-8dec-4720-92e6-052305cb40dc'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (4223, '386573d6-bd83-4bd5-824c-7fbdd3122bad'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (24739, 'dcc5e35f-9208-4a2c-b2ef-771fe6332967'), (7335, '8eab8d38-3c38-4253-ac73-f13ceb06a81f'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (20690, 'cbcf4eb2-979a-495d-a8ad-950b3cb6692a'), (28372, '15a0b464-97c2-440b-bdc2-bd235ffd25b0'), (20694, '0a712ebe-d559-4eea-b2a2-7181b1e80f23'), (25816, '0aee80dc-2820-4f8b-a131-2042458d62e0'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (22768, '0aa32bd4-ae8d-4d98-b212-4087c9da1896'), (25334, '33427231-4e65-4935-ba92-1cda98b9c5fa'), (22775, '92b448c2-17a3-48db-9679-dfae336f0e2f'), (30456, '7f75b6b0-ffd7-420b-9078-22030660ddb2'), (22779, '1b8ccb79-44ab-4d6d-b222-a9f1fd2244d4'), (4874, 'e733caa3-5f74-4bf7-b370-8d9bca505b85'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (24343, '1aa794ee-f259-4d0c-89b9-9dba2932d569'), (15648, 'f58ba64c-38eb-47ea-9da1-86ef9f6ad5ed'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (17207, 'fdc3377d-63a0-4bb4-b993-7a30e8b7189f'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (1872, '4ab11bb3-d5e2-4c59-ac01-a9f3d8773989'), (9556, '4ebcb0ec-bfc7-44ca-92cd-96ee50f00f5f'), (10580, '5b2828bb-b3d9-4084-bbf1-9a510e142f1f'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (3941, 'a871cd71-4386-4492-ae52-7dda1294b854'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (14712, '1861bc32-ca30-45b0-bdb8-516bd6dd3448'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (19867, '88ae508e-cedb-469a-b846-e494beb34b6b'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (2474, 'f1912b84-7ebb-463f-b45e-8044c7679524'), (26540, '313630db-d36c-4654-8dfa-112a43b9691f'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (26541, 'bf53285c-42c7-4678-b22d-2dfb988d32b9'), (19383, '44c8fa9d-7167-4105-9431-707875941cb8'), (19384, '75f513c0-3b52-4f83-aec6-7b436161c162'), (29112, 'f5df6a53-0df1-4420-9043-3d9ba923f41b'), (29114, '131eb762-91f6-4098-b8e1-c638fb32b6eb'), (1466, 'c444b35a-68fb-4fae-8309-306d1b03ac87'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (29118, '553e8a5e-a197-4b4c-9e25-dce549489f39'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (11713, '16213aad-a177-42a5-8be6-a4c1224c45d0'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19398, 'c99ed4d1-a0f8-47fc-9f4d-4867015d3211'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (19407, '5f9399cd-785b-459e-bf05-4d7cc41174c0'), (4047, '1a54fabd-203c-4e44-9c9a-efc4a3b0004a'), (17365, '5fff32a6-0c83-4121-b3e6-5237d83d0fde'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (12270, '002549e4-baef-4cb3-baf4-279f524b849b'), (8691, 'c4abf821-f5ee-419a-b75d-9cfb752e2e97'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (31740, '7f15437c-296d-44c3-89c4-94654b268fc9'), (31741, '3cb8440b-d4b0-4bd5-930d-1fc4596f3292')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: -->

# Efficient Training on CPU

This guide focuses on training large models efficiently on CPU.

## Mixed precision with IPEX

IPEX is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16. The usage of BFloat16 is the main focus of the following sections.
## Introduction: Using Intel Software to Optimize AI Efficiency on CPU

As we detailed in our [previous blog post](https://huggingface.co/blog/bert-cpu-scaling-part-1), Intel Xeon CPUs provide a set of features especially designed for AI workloads such as AVX512 or VNNI (Vector Neural Network Instructions) 
for efficient inference using integer quantized neural network for inference along with additional system tools to ensure the work is being done in the most efficient way. 
In this blog post, we will focus on software optimizations and give you a sense of the performances of the new Ice Lake generation of Xeon CPUs from Intel. Our goal is to give you a full picture of what‚Äôs available on the software side to make the most out of your Intel hardware. 
As in the previous blog post, we show the performance with benchmark results and charts, along with new tools to make all these knobs and features easy to use.
[Optimum](https://github.com/huggingface/optimum) aims to make this work easy, providing performance optimization tools targeting efficient AI hardware,
built in collaboration with our Hardware Partners, and turn Machine Learning Engineers into ML Optimization wizards.

With the [Transformers](https://github.com/huggingface/transformers) library, we made it easy for researchers and engineers to use state-of-the-art models,
abstracting away the complexity of frameworks, architectures and pipelines.

With the [Optimum](https://github.com/huggingface/optimum) library, we are making it easy for engineers to leverage all the available hardware features at their disposal,
abstracting away the complexity of model acceleration on hardware platforms.

## ü§ó Optimum in practice: how to quantize a model for Intel Xeon CPU
### ü§î Why quantization is important but tricky to get right
For the results shown in this blog, we used the default number of 50 inference steps. With fewer inference steps, inference speed will be faster, but this has an effect on the quality of the resulting image. How large this effect is depends on the model and the [scheduler](https://huggingface.co/docs/diffusers/using-diffusers/schedulers). We recommend experimenting with different number of steps and schedulers and find what works best for your use case.

Below we show how to perform inference with the final pipeline optimized to run on Intel CPUs:

```python
from optimum.intel import OVStableDiffusionPipeline

# Load and compile the pipeline for performance.
name = ""OpenVINO/stable-diffusion-pokemons-tome-quantized-aggressive""
pipe = OVStableDiffusionPipeline.from_pretrained(name, compile=False)
pipe.reshape(batch_size=1, height=512, width=512, num_images_per_prompt=1)
pipe.compile()
### üî• How to easily quantize Transformers for Intel Xeon CPUs with Optimum

![Automatic quantization code snippet](assets/25_hardware_partners_program/carbon_inc_quantizer.png)

## Follow ü§ó Optimum: a journey to democratize ML production performance
### ‚ö°Ô∏èState of the Art Hardware
Optimum will focus on achieving optimal production performance on dedicated hardware, where software and hardware acceleration techniques can be applied for maximum efficiency.
We will work hand in hand with our Hardware Partners to enable, test and maintain acceleration, and deliver it in an easy and accessible way through Optimum, as we did with Intel and Neural Compressor.
We will soon announce new Hardware Partners who have joined us on our journey toward Machine Learning efficiency.

### üîÆ State-of-the-Art Models
- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.
- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.
- While Diffusers has many built-in performance-enhancing features (see [Memory and Speed](https://huggingface.co/docs/diffusers/optimization/fp16)), models are always loaded with the highest precision and lowest optimization. Therefore, by default diffusion pipelines are always instantiated on CPU with float32 precision if not otherwise defined by the user. This ensures usability across different platforms and accelerators and means that no complex installations are required to run the library.
- Diffusers aims to be a **light-weight** package and therefore has very few required dependencies, but many soft dependencies that can improve performance (such as `accelerate`, `safetensors`, `onnx`, etc...). We strive to keep the library as lightweight as possible so that it can be added without much concern as a dependency on other packages.
Finally, on top of this, one can find some domain specific libraries such as Intel's oneDNN which brings all the most common and essential building blocks required to implement neural network layers. 
Intel MKL and oneDNN are natively integrated within the PyTorch framework, where it can enable some performance speedup for certain operations such as Linear + ReLU or Convolution. 
On the TensorFlow side, oneDNN can be enabled by setting the environment variable `TF_ENABLE_ONEDNN_OPTS=1` (_TensorFlow >= 2.5.0_) to achieve similar machinery under the hood.

## More Efficient AI Processing on latest Intel Ice Lake CPUs
```

CPU offloading works on submodules rather than whole models. This is the best way to minimize memory consumption, but inference is much slower due to the iterative nature of the diffusion process. The UNet component of the pipeline runs several times (as many as `num_inference_steps`); each time, the different UNet submodules are sequentially onloaded and offloaded as needed, resulting in a large number of memory transfers.

<Tip>

Consider using [model offloading](#model-offloading) if you want to optimize for speed because it is much faster. The tradeoff is your memory savings won't be as large.

</Tip>

<Tip warning={true}>

When using [`~StableDiffusionPipeline.enable_sequential_cpu_offload`], don't move the pipeline to CUDA beforehand or else the gain in memory consumption will only be minimal (see this [issue](https://github.com/huggingface/diffusers/issues/1934) for more information).
Back in April, Intel launched its [latest generation of Intel Xeon processors](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html), codename Ice Lake, targeting more efficient and performant AI workloads. 
More precisely, Ice Lake Xeon CPUs can achieve up to 75% faster inference on a variety of NLP tasks when comparing against the previous generation of Cascade Lake Xeon processors. 
This is achieved by a combination of both hardware and software improvements, [such as new instructions](https://en.wikichip.org/wiki/x86/avx512_vnni) and PCIe 4.0 featured on the new Sunny Cove architecture to supports Machine Learning and Deep Learning workloads. 
Last but not least, Intel worked on dedicated optimizations for various frameworks which now come with Intel‚Äôs flavors like 
[Intel‚Äôs Extension for Scikit Learn](https://intel.github.io/scikit-learn-intelex/),
```

Compared to a completely unoptimized SDXL pipeline, using fp16 takes 21.7GB of memory and only 14.8 seconds. You‚Äôre almost speeding up inference by a full minute!

### Memory-efficient attention

The attention blocks used in transformers modules can be a huge bottleneck, because memory increases _quadratically_ as input sequences get longer. This can quickly take up a ton of memory and leave you with an out-of-memory error message. üò¨
```


## Optimizing and quantizing in pipelines

The [`~pipelines.pipeline`] function can not only run inference on vanilla ONNX Runtime checkpoints - you can also use
checkpoints optimized with the [`~optimum.onnxruntime.ORTQuantizer`] and the [`~optimum.onnxruntime.ORTOptimizer`].

Below you can find two examples of how you could use the [`~optimum.onnxruntime.ORTOptimizer`] and the
[`~optimum.onnxruntime.ORTQuantizer`] to optimize/quantize your model and use it for inference afterwards.

### Quantizing with the `ORTQuantizer`

```python
>>> from transformers import AutoTokenizer
>>> from optimum.onnxruntime import (
...     AutoQuantizationConfig,
...     ORTModelForSequenceClassification,
...     ORTQuantizer
... )
>>> from optimum.pipelines import pipeline

>>> # Load the tokenizer and export the model to the ONNX format
>>> model_id = ""distilbert-base-uncased-finetuned-sst-2-english""
>>> save_dir = ""distilbert_quantized""
```

The average latency is **32.3 seconds**. As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. 

Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change!

Now, let's accelerate!

## Optimum Intel and OpenVINO

[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code.

Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. 

Optimum Intel and OpenVINO can be installed as follows:

```
pip install optimum[openvino]
```

The following code compares the performance of the original stable diffusion pipeline with the ipex-optimized pipeline.

```python
import torch
import intel_extension_for_pytorch as ipex
from diffusers import StableDiffusionPipeline
import time

prompt = ""sailing ship in storm by Rembrandt""
model_id = ""runwayml/stable-diffusion-v1-5""
# Helper function for time evaluation
def elapsed_time(pipeline, nb_pass=3, num_inference_steps=20):
    # warmup
    for _ in range(2):
        images = pipeline(prompt, num_inference_steps=num_inference_steps, height=512, width=512).images
    #time evaluation
    start = time.time()
    for _ in range(nb_pass):
        pipeline(prompt, num_inference_steps=num_inference_steps, height=512, width=512)
    end = time.time()
    return (end - start) / nb_pass

##############     bf16 inference performance    ###############
Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.

Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.
<Tip>

Make sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!

If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!

</Tip>

## StableDiffusionUpscalePipeline

[[autodoc]] StableDiffusionUpscalePipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention

## StableDiffusionPipelineOutput

[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput
In a follow-up post, we'll look at inference on Sapphire Rapids CPUs and the performance boost that they bring.

## Why You Should Consider Training On CPUs

Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach, especially when using techniques such as distributed training and fine-tuning on small and medium datasets. 

Xeon CPUs support advanced features such as Advanced Vector Extensions ([AVX-512](https://en.wikipedia.org/wiki/AVX-512)) and Hyper-Threading, which help improve the parallelism and efficiency of DL models. This enables faster training times as well as better utilization of hardware resources.
```

<Tip>

Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.

</Tip>

## StableUnCLIPPipeline

[[autodoc]] StableUnCLIPPipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_vae_slicing
	- disable_vae_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention

## StableUnCLIPImg2ImgPipeline

[[autodoc]] StableUnCLIPImg2ImgPipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_vae_slicing
	- disable_vae_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention

## ImagePipelineOutput
[[autodoc]] pipelines.ImagePipelineOutput
Make sure to check out the Stable Diffusion [Tips](overview#tips) section to learn how to explore the tradeoff between scheduler speed and quality, and how to reuse pipeline components efficiently!

If you're interested in using one of the official checkpoints for a task, explore the [CompVis](https://huggingface.co/CompVis), [Runway](https://huggingface.co/runwayml), and [Stability AI](https://huggingface.co/stabilityai) Hub organizations!

</Tip>

## StableDiffusionInpaintPipeline

[[autodoc]] StableDiffusionInpaintPipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention
	- load_textual_inversion
	- load_lora_weights
	- save_lora_weights

## StableDiffusionPipelineOutput

[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput

## FlaxStableDiffusionInpaintPipeline

[[autodoc]] FlaxStableDiffusionInpaintPipeline
	- all
	- __call__
```

And with that, we can generate four samples, each of 150 seconds in duration, all in one call to the pipeline! Using the 
large AudioLDM 2 checkpoint will result in higher overall memory usage than the base checkpoint, since the UNet is over 
twice the size (750M parameters compared to 350M), so this memory saving trick is particularly beneficial here.

## Conclusion

In this blog post, we showcased four optimisation methods that are available out of the box with üß® Diffusers, taking 
the generation time of AudioLDM 2 from 14 seconds down to less than 1 second. We also highlighted how to employ memory 
saving tricks, such as half-precision and CPU offload, to reduce peak memory usage for long audio samples or large 
checkpoint sizes.

Here is the question: How can ASR pipelines be optimized for efficiency on Intel CPUs?
Here is answer 1:Focus only on audio preprocessing without hardware-specific adjustments
Here is answer 2: Leverage hardware-specific libraries and robust preprocessing workflows
Here is answer3: Exclude multitask tuning to reduce pipeline complexity
Here is answer 4: Ignore hardware-specific optimizations to maintain generality",The correct answer based upon context provided seems like Answer No : Leveragehardware-specific librariesand robust pre-processingworkflows
What key advantage does multitask prompt tuning provide in ASR models?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (16919, '942238c0-9ba8-45fc-bd5c-b13953074601'), (16920, 'cd0788e0-82b0-4b44-9dd4-8b8f83cf3ab9'), (16921, 'a31bb2a1-bb83-4766-8534-5664f075ae46'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (16922, '6ffff410-16b9-4bc9-88db-d6d4c3ff10bd'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (10830, '63fe604b-e2e2-49c7-bfbb-16c17a3cb8c7'), (11859, 'eca711ae-a436-4004-a11f-ed98258cbd62'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (16492, 'e9159689-7c45-42ef-90ba-2efb9da7ad91'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (30333, '463aafaa-1474-444d-8fe3-5bc8c8d95517'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22662, 'f32c3a87-abbc-4c09-a640-bf8926fe8342'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (18065, '70066450-a7ad-4637-83d5-a5f132a8ac63'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9'), (9368, 'a2674ef7-b70d-4243-8f59-ea725770523f'), (9370, 'db1c6e3e-6f2c-4a80-961d-6c6bfe33415e'), (25759, 'b2156363-4f81-4b08-a11e-6e35136835d6'), (30369, '78b085d6-10c1-4309-8a6c-6f457b35b583'), (13501, '2a2b7fc2-3c43-4d05-a703-7d2ed7939397'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (7923, '3d12a194-ca8a-4ffe-9e77-8fb01bda0fd8'), (7924, '9e9cce0b-318b-4407-8527-b450af5fc69f'), (7925, '898386e9-41b2-43b2-bf00-f5fe304bcf79'), (7926, 'f97b01a4-b979-42d6-8789-bec7870b5e30'), (7927, 'be9f55e3-25af-4774-95e1-537c53e8e6e5'), (7928, '3af4d2b1-479b-4154-9dfd-f8a6036bc83c'), (7929, '698a3ccc-aa9e-4fff-af7d-792856b4a975'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (20749, 'b2e0610f-aa0c-4852-b1ad-2bf187ecf97e'), (25883, 'f01a6bd7-35e0-4d59-b1e1-8879b0662972'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (21280, 'bbe915f8-674b-4422-a8c3-9fbc5e7e1858'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (21324, 'c689a247-869d-4ea7-b9f0-170aa19a493b'), (8525, '931d0e8a-fa6c-4b84-9b4a-f7329eb19392'), (27473, '628787b7-dd50-49d4-bbfc-cd5771b01254'), (21331, 'c706e01b-6612-49e1-b624-510376de99e7'), (27481, '670fac96-da41-4462-a2f5-621a35d75778'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (17774, 'e13cc263-c072-4a3b-9f64-7b688f1c79bf'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (21887, '2fc3be04-7823-41a9-af0e-6c649e006652'), (3458, '933bcf19-3ed0-4489-b20d-2fed591675c0'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (18309, '77eb1ea6-c7df-40f4-a112-aec1b6d6ae26'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (28555, '3900d0c8-9171-4b7d-bf48-27c131dec37b'), (28556, '6de4578f-f7a2-4271-a425-65baf13f4391'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (28579, 'faeb2a24-8c7e-43f7-b3f4-3ae4017753be'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (28585, 'd3223831-24e7-4557-a7a0-28f800f26eb0'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14782, '1cc646cf-ac7b-4712-a786-9905b3365b13'), (29638, '090cbe12-3fd1-4622-b456-102ca3ae6d96'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (26067, '61a8f2fe-8e9a-4fd1-994b-c13c84582f4a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (15868, 'defeffab-5c59-48a0-8058-9ff3d5f02504'), (15869, '5e46d120-df1b-4d36-bd63-b7491a1e4eff'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (16885, 'cefd3130-e406-4b58-ab29-142eff3b2561'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (16889, '4e9f1ab4-cab8-40fe-870f-69a3b341a3d7'), (16890, 'c8b38db6-3834-4660-9877-40161fc8646f'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (15870, 'aa4dffab-9c16-4523-8b50-2bde7016c29f'), (15871, '23fe4629-d4f8-4f28-85a5-d238feb3371a')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: 7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
8. LoHa: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098)
9. LoKr: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650) based on [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://arxiv.org/abs/2309.14859) implementation
10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
11. OFT: [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)
- the prompt tokens can be inserted anywhere in the input sequence, and it isn't restricted to only the beginning
- the prompt tokens are only added to the input instead of adding them to every layer of the model
- introducing *anchor* tokens can improve performance because they indicate characteristics of a component in the input sequence

The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables GPT-like models to compete with BERT-like models on NLU tasks.

Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification) for a step-by-step guide on how to train a model with P-tuning.

## Multitask prompt tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png""/>
</div>
<small><a href=""https://hf.co/papers/2103.10385"">Multitask prompt tuning enables parameter-efficient transfer learning</a>.</small>
The abstract from the paper is:

*Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters*.
[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:

1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.
1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)
2. Prefix Tuning: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/), [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
3. P-Tuning: [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
4. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
5. AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)  
6. $(IA)^3$: [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Multitask Prompt Tuning

[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.

The abstract from the paper is:
## MultitaskPromptTuningConfig

[[autodoc]] tuners.multitask_prompt_tuning.config.MultitaskPromptTuningConfig

## MultitaskPromptEmbedding

[[autodoc]] tuners.multitask_prompt_tuning.model.MultitaskPromptEmbedding
*In this work, we explore ""prompt tuning"", a simple yet effective mechanism for learning ""soft prompts"" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ""closes the gap"" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and
```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use ü§ó Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
```

Play with an interactive demo for the [speech-to-text task](https://huggingface.co/spaces/Matthijs/speecht5-asr-demo).

## Conclusion

SpeechT5 is an interesting model because ‚Äî unlike most other models ‚Äî it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned.

We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It‚Äôs very versatile!
and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning*.
The main difference is that the prefix parameters are inserted in **all** of the model layers, whereas prompt tuning only adds the prompt parameters to the model input embeddings. The prefix parameters are also optimized by a separate feed-forward network (FFN) instead of training directly on the soft prompts because it causes instability and hurts performance. The FFN is discarded after updating the soft prompts.

As a result, the authors found that prefix tuning demonstrates comparable performance to fully finetuning a model, despite having 1000x fewer parameters, and it performs even better in low-data settings.

Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning) for a step-by-step guide on how to train a model with prefix tuning.

## P-tuning
[Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification tasks on T5 models, and all downstream tasks are cast as a text generation task. For example, sequence classification usually assigns a single class label to a sequence of text. By casting it as a text generation task, the tokens that make up the class label are *generated*. Prompts are added to the input as a series of tokens. Typically, the model parameters are fixed which means the prompt tokens are also fixed by the model parameters.

The key idea behind prompt tuning is that prompt tokens have their own parameters that are updated independently. This means you can keep the pretrained model's parameters frozen, and only update the gradients of the prompt token embeddings. The results are comparable to the traditional method of training the entire model, and prompt tuning performance scales as model size increases.
There are two categories of prompting methods:

- hard prompts are manually handcrafted text prompts with discrete input tokens; the downside is that it requires a lot of effort to create a good prompt
- soft prompts are learnable tensors concatenated with the input embeddings that can be optimized to a dataset; the downside is that they aren't human readable because you aren't matching these ""virtual tokens"" to the embeddings of a real word

This conceptual guide provides a brief overview of the soft prompt methods included in ü§ó PEFT: prompt tuning, prefix tuning, P-tuning, and multitask prompt tuning.

## Prompt tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prompt-tuning.png""/>
</div>
<small>Only train and store a significantly smaller set of task-specific prompt parameters <a href=""https://hf.co/papers/2104.08691"">(image source)</a>.</small>
--
title: ""Fine-Tune MMS Adapter Models for low-resource ASR""
thumbnail: /blog/assets/151_mms/mms_map.png
authors:
- user: patrickvonplaten
---

# **Fine-tuning MMS Adapter Models for Multi-Lingual ASR**


<a target=""_blank"" href=""https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_MMS_on_Common_Voice.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>

***New (06/2023)***: *This blog post is strongly inspired by [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)* and can be seen as an improved version of it.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
-->

# Prompt tuning for causal language modeling

[[open-in-colab]]

Prompting helps guide language model behavior by adding some input text specific to a task. Prompt tuning is an additive method for only training and updating the newly added prompt tokens to a pretrained model. This way, you can use one pretrained model whose weights are frozen, and train and update a smaller set of prompt parameters for each downstream task instead of fully finetuning a separate model. As models grow larger and larger, prompt tuning can be more efficient, and results are even better as model parameters scale.

<Tip>

üí° Read [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) to learn more about prompt tuning.

</Tip>

This guide will show you how to apply prompt tuning to train a [`bloomz-560m`](https://huggingface.co/bigscience/bloomz-560m) model on the `twitter_complaints` subset of the [RAFT](https://huggingface.co/datasets/ought/raft) dataset.
Great! Now that we understood the motivation and theory, let's look into fine-tuning adapter weights for **`mms-1b-all`** üî•

## Notebook Setup

As done previously in the [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2) blog post, we fine-tune the model on the low resource ASR dataset of [Common Voice](https://huggingface.co/datasets/common_voice) that contains only *ca.* 4h of validated training data.

Just like Wav2Vec2 or XLS-R, MMS is fine-tuned using Connectionist Temporal Classification (CTC), which is an algorithm that is used to train neural networks for sequence-to-sequence problems, such as ASR and handwriting recognition.

For more details on the CTC algorithm, I highly recommend reading the well-written blog post [*Sequence Modeling with CTC (2017)*](https://distill.pub/2017/ctc/) by Awni Hannun.
```

## Closing Remarks

In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR 
using ü§ó Datasets, Transformers and the Hugging Face Hub. For more details on the Whisper model, the Common Voice dataset and the theory behind fine-tuning, refere to the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). If you're interested in fine-tuning other 
Transformers models, both for English and multilingual ASR, be sure to check out the 
examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).

Here is the question: What key advantage does multitask prompt tuning provide in ASR models?
Here is answer 1:It simplifies tasks by focusing only on text-based data
Here is answer 2: It allows models to adapt quickly to multiple tasks with shared prompts
Here is answer3: It eliminates the need for preprocessing
Here is answer 4: It reduces model performance overhead for single-task workflows","Based solely upon provided contextuiltinanswer choicesannual , I would sayanswer number twoIt allows.models.to adapt quickly.to.multiple.tasks.with.shared.prompts"
What is the best approach for aligning multilingual datasets in ASR workflows?,"[(30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (21509, '2e90ee09-c728-44e9-aa15-984bb64553e5'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30220, '683831a6-5b21-4e34-8508-f7affdc57f5a'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (22542, '9f48bc8c-0874-46b9-a37c-65b01da9c5b7'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (23063, '21ce1de6-8b94-480c-b290-80086a22a5ce'), (23068, '17b19121-a9f3-446f-b249-3b1e64bae14f'), (23070, 'dbb25c0a-8cb6-4e62-9b53-c8a6dd06b4c4'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (29256, 'e581785d-f30a-47db-99a4-eccdd13d7903'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (24659, '1ce77664-fd28-405b-b2df-b85564bdba01'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (7269, '2203fa03-15b5-4a03-87c9-1ad09aa9d60a'), (7276, '0d98213c-cf33-4909-bcf6-74ccaf3923d7'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (16031, 'aeac043d-c603-454d-af9a-5ddd34d267ff'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (691, '49245ea8-a50c-409d-b767-70947845114d'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (17088, '17e59b2c-7682-45e9-abb8-a88720517b2c'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (17617, 'd0201a2e-3af0-47a5-98e7-e13defa7db4f'), (5842, '3b64b1f4-3187-44e6-a1b6-2814799742b2'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (9950, '3570da80-429e-4059-b1b0-db0dddb1ae97'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (3320, '3f22941d-5a99-4599-b75a-7aac2efeee6e'), (28414, '0e69ac2d-e27f-4301-b639-f7070dc7a084'), (13064, '589f6f88-a4f1-45d4-863a-1d5eee88fa12'), (22793, '655baf0e-9e9e-45f3-b8a2-e1b4ddd40b91'), (22794, '364ec0e7-da72-440c-aef3-1db9aed7f3dc'), (22795, '52c61654-a4db-433c-b73f-a890c21cbc8a'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (23329, '9afe4111-7b48-4f69-87cf-d1e360f0331a'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (26421, '2c89420e-e9cb-4967-9268-cef5f028e779'), (10551, '23b0fa1f-8a46-4da7-aac0-1805e14a220b'), (15672, '3c536d00-c1fc-410b-a634-96a25e8778ee'), (23353, '209f08ab-4109-44e3-9822-fbb3609f60fd'), (6457, '416d0180-84a0-47ca-816b-e4151270b222'), (25914, 'd343a79e-38fb-4fce-9fcc-0d1e23c73b40'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (6461, 'b1f177f3-e93d-412b-b346-92ad1e300db7'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (6463, 'deb1abed-40b0-4c6f-ab05-30f0375d9840'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (838, 'b88fee3d-341d-4685-8fc6-fd51e1adf587'), (6471, '7303414c-e3c5-4e1b-8155-b0a2132f27a4'), (841, '3ea3ba81-cfdc-4309-824d-56a17beadb1d'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (4438, 'c433e5f5-9651-489c-8a76-f04bf6884777'), (13664, '0d35fe7f-3409-414c-9082-1c9c87a329cf'), (13666, '8b9a35b3-46c1-4f42-8832-2fff472ad908'), (13668, '86808aee-afa4-4657-9f26-d6a8a9ad0e15'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (16759, '1b62ceba-cef9-48f4-9d42-37f27b4c1c9c'), (20860, 'f6cd2e41-5cbe-4525-a0d2-89d48c843903'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (30608, 'b8bd449a-543d-4b0d-afdc-19e36c13014c'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (7110, 'ace1a408-647b-4149-a9c1-b6a9a032d552'), (18886, 'a01aef5d-3c56-4f8d-8854-e68353f4d189'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (18910, 'c8a02332-3a1d-47c0-a882-2f9634f56cde'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (19962, '04858203-03cf-45bb-9b5a-7f506048c00c'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use ü§ó Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
```

## Limitations and bias

CER is useful for comparing different models for tasks such as automatic speech recognition (ASR) and optic character recognition (OCR), especially for multilingual datasets where WER is not suitable given the diversity of languages. However, CER provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.

Also, in some cases, instead of reporting the raw CER, a normalized CER is reported where the number of mistakes is divided by the sum of the number of edit operations (`I` + `S` + `D`) and `C` (the number of correct characters), which results in CER values that fall within the range of 0‚Äì100%.


## Citation
```

## Limitations and bias

CER is useful for comparing different models for tasks such as automatic speech recognition (ASR) and optic character recognition (OCR), especially for multilingual datasets where WER is not suitable given the diversity of languages. However, CER provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.

Also, in some cases, instead of reporting the raw CER, a normalized CER is reported where the number of mistakes is divided by the sum of the number of edit operations (`I` + `S` + `D`) and `C` (the number of correct characters), which results in CER values that fall within the range of 0‚Äì100%.


## Citation
```

Common Voice is but one multilingual ASR dataset that we can download from the Hub - 
there are plenty more available to us! To view the range of datasets available for speech recognition, 
follow the link: [ASR Datasets on the Hub](https://huggingface.co/datasets?task_categories=task_categories:automatic-speech-recognition&sort=downloads).

### Prepare Feature Extractor, Tokenizer and Data

The ASR pipeline can be de-composed into three components: 
1) A feature extractor which pre-processes the raw audio-inputs
2) The model which performs the sequence-to-sequence mapping 
3) A tokenizer which post-processes the model outputs to text format
</Tip>

This guide will show you how to train a [`openai/whisper-large-v2`](https://huggingface.co/openai/whisper-large-v2) model for multilingual automatic speech recognition (ASR) using a combination of `int8` quantization and LoRA. You'll train Whisper for multilingual ASR on Marathi from the [Common Voice 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) dataset.

Before you start, make sure you have all the necessary libraries installed:

```bash
!pip install -q peft transformers datasets accelerate evaluate jiwer bitsandbytes
In this blog, we present a step-by-step guide on fine-tuning Whisper 
for any multilingual ASR dataset using Hugging Face ü§ó Transformers. This blog 
provides in-depth explanations of the Whisper model, the Common Voice dataset and 
the theory behind fine-tuning, with accompanying code cells to execute the data 
preparation and fine-tuning steps. For a more streamlined version of the notebook 
with fewer explanations but all the code, see the accompanying [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb).

## Table of Contents
```

## Closing Remarks

In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR 
using ü§ó Datasets, Transformers and the Hugging Face Hub. For more details on the Whisper model, the Common Voice dataset and the theory behind fine-tuning, refere to the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). If you're interested in fine-tuning other 
Transformers models, both for English and multilingual ASR, be sure to check out the 
examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
```

## Closing Remarks

In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR 
using ü§ó Datasets, Transformers and the Hugging Face Hub. Refer to the [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb) 
should you wish to try fine-tuning for yourself. If you're interested in fine-tuning other 
Transformers models, both for English and multilingual ASR, be sure to check out the 
examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
*Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual
alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining
and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging
entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages
with entity representations and show the model consistently outperforms word-based pretrained models in various
cross-lingual transfer tasks. We also analyze the model and the key insight is that incorporating entity
representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a
multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual
knowledge more likely than using only word representations.*
We hope with the code release it lowers the barrier to entry for you the readers to try out this method of aligning large language models on your own datasets and we cannot wait to see what you build! And if you want to try out the model yourself you can do so here: [trl-lib/stack-llama](https://huggingface.co/spaces/trl-lib/stack-llama).
```

### Multilingual Speech Recognition

Multilingual speech recognition refers to speech recognition (speech-to-text) for all languages except English.

#### [Multilingual LibriSpeech](https://huggingface.co/datasets/facebook/multilingual_librispeech)
Multilingual LibriSpeech is the multilingual equivalent of the [LibriSpeech ASR](https://huggingface.co/datasets/librispeech_asr) corpus. 
It comprises a large corpus of read audiobooks taken from the [LibriVox](https://librivox.org/) project, making 
it a suitable dataset for academic research. It contains data split into eight high-resource languages - English, 
German, Dutch, Spanish, French, Italian, Portuguese and Polish.
```

The **lower** the CER value, the **better** the performance of the ASR system, with a CER of 0 being a perfect score. 

However, CER's output is not always a number between 0 and 1, in particular when there is a high number of insertions (see [Examples](#Examples) below).

### Values from popular papers

This metric is highly dependent on the content and quality of the dataset, and therefore users can expect very different values for the same model but on different datasets.

Multilingual datasets such as [Common Voice](https://huggingface.co/datasets/common_voice) report different CERs depending on the language, ranging from 0.02-0.03 for languages such as French and Italian, to 0.05-0.07 for English (see [here](https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice/ASR/CTC) for more values).

## Examples 

Perfect match between prediction and reference:
```

The **lower** the CER value, the **better** the performance of the ASR system, with a CER of 0 being a perfect score. 

However, CER's output is not always a number between 0 and 1, in particular when there is a high number of insertions (see [Examples](#Examples) below).

### Values from popular papers

This metric is highly dependent on the content and quality of the dataset, and therefore users can expect very different values for the same model but on different datasets.

Multilingual datasets such as [Common Voice](https://huggingface.co/datasets/common_voice) report different CERs depending on the language, ranging from 0.02-0.03 for languages such as French and Italian, to 0.05-0.07 for English (see [here](https://github.com/speechbrain/speechbrain/tree/develop/recipes/CommonVoice/ASR/CTC) for more values).

## Examples 

Perfect match between prediction and reference:
```

## Multimodal

For tasks involving multimodal inputs, you'll need a [processor](main_classes/processors) to prepare your dataset for the model. A processor couples together two processing objects such as as tokenizer and feature extractor.

Load the [LJ Speech](https://huggingface.co/datasets/lj_speech) dataset (see the ü§ó [Datasets tutorial](https://huggingface.co/docs/datasets/load_hub) for more details on how to load a dataset) to see how you can use a processor for automatic speech recognition (ASR):

```py
>>> from datasets import load_dataset

>>> lj_speech = load_dataset(""lj_speech"", split=""train"")
```

For ASR, you're mainly focused on `audio` and `text` so you can remove the other columns:

```py
>>> lj_speech = lj_speech.map(remove_columns=[""file"", ""id"", ""normalized_text""])
a href=""https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>

# Fine-Tune Whisper For Multilingual ASR with ü§ó Transformers

In this Colab, we present a step-by-step guide on how to fine-tune Whisper 
for any multilingual ASR dataset using Hugging Face ü§ó Transformers. This is a 
more ""hands-on"" version of the accompanying [blog post](https://huggingface.co/blog/fine-tune-whisper). 
For a more in-depth explanation of Whisper, the Common Voice dataset and the theory behind fine-tuning, the reader is advised to refer to the blog post.

## Introduction
```

ü§ó Transformers doesn't have a data collator for ASR, so you'll need to adapt the [`DataCollatorWithPadding`] to create a batch of examples. It'll also dynamically pad your text and labels to the length of the longest element in its batch (instead of the entire dataset) so they are a uniform length. While it is possible to pad your text in the `tokenizer` function by setting `padding=True`, dynamic padding is more efficient.

Unlike other data collators, this specific data collator needs to apply a different padding method to `input_values` and `labels`:

```py
>>> import torch

>>> from dataclasses import dataclass, field
>>> from typing import Any, Dict, List, Optional, Union


>>> @dataclass
... class DataCollatorCTCWithPadding:
...     processor: AutoProcessor
...     padding: Union[bool, str] = ""longest""
Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1,100 spoken languages can be identified, transcribed and generated with the various [language identification, speech recognition, and text-to-speech checkpoints released](https://huggingface.co/models?other=mms).

In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning. 

For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
--
title: ""Fine-Tune MMS Adapter Models for low-resource ASR""
thumbnail: /blog/assets/151_mms/mms_map.png
authors:
- user: patrickvonplaten
---

# **Fine-tuning MMS Adapter Models for Multi-Lingual ASR**


<a target=""_blank"" href=""https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_MMS_on_Common_Voice.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>

***New (06/2023)***: *This blog post is strongly inspired by [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)* and can be seen as an improved version of it.

Here is the question: What is the best approach for aligning multilingual datasets in ASR workflows?
Here is answer 1:Focus only on high-resource language datasets
Here is answer 2: Normalize datasets, align audio and text, and apply multilingual strategies
Here is answer3: Avoid multilingual datasets to simplify preprocessing
Here is answer 4: Rely on automatic transcription without alignment",Based solely off context provided I would choose option BË´ãonly): Normalize
How can hardware optimizations improve multitask ASR pipelines?,"[(9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (27167, '9d246d60-330f-44ca-83ab-b6504683772e'), (14382, 'f0a3cc11-b2fc-4b21-a76e-c0d46a55834a'), (30770, '964e522f-8c2f-4031-a160-73dccc219846'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (3129, 'eb945bfb-2047-4a55-a268-2f7b9efc42f6'), (10819, '9fb4e5bf-b2d1-4c28-ac14-c7b7fd6f77ae'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (9286, 'c5343836-1184-4597-bf0e-0d37580c36fc'), (9803, 'a0b49fbb-8568-4355-8bdb-8c64f1a568ad'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (9296, 'b982a595-b413-47c2-a522-d71819d17b20'), (9297, 'b1ddc06d-35a1-4ac0-b68f-697de22d55e0'), (3154, 'cf9d8add-4e29-4a90-b3d6-2679327c71ec'), (15956, 'd03f6960-2286-4540-a440-e6bb93b141f6'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (97, 'c110d4b5-e404-4749-8851-600a26eec5e4'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (9838, '24e469f0-838c-4b10-8de3-82174bdc04da'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (16510, '28558a81-343e-4be7-a632-bbc1180a6dda'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (6799, '7fdaa5a2-96ae-4629-98d8-4011d0f44647'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (21144, '733daaa7-2d91-4c46-b007-fe08a12ee437'), (25759, 'b2156363-4f81-4b08-a11e-6e35136835d6'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (20149, '44c9682e-0ce4-4b25-98a7-519107c43120'), (20160, 'c0c6fd6f-69be-4907-9b87-052041804e6c'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (6851, 'd6a44bed-daac-46f7-9a6d-c0b18d267b90'), (6340, '659fd3d4-1c85-4ad8-a958-c4a96dac7ac4'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (4300, '22ecff5e-c61a-4907-8ccf-fd6dc4c176a0'), (21196, 'ffd5fcb7-a390-425f-ad65-9e97cac08aec'), (20693, '6ceda67f-d532-4fa1-99ca-1fe2e1e1fe8c'), (20694, '0a712ebe-d559-4eea-b2a2-7181b1e80f23'), (25816, '0aee80dc-2820-4f8b-a131-2042458d62e0'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (25334, '33427231-4e65-4935-ba92-1cda98b9c5fa'), (2808, '2bd90dff-3b24-41da-bdc7-5c27e032cba6'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (4354, '23d6e114-d7c6-4b5d-b123-10b9575f7a47'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (16661, 'f6db86f2-9435-45f8-814c-5423a566c4e4'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (15648, 'f58ba64c-38eb-47ea-9da1-86ef9f6ad5ed'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (24383, '4b93009f-15f5-4e1c-bd98-faf19d8913fe'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (9556, '4ebcb0ec-bfc7-44ca-92cd-96ee50f00f5f'), (10580, '5b2828bb-b3d9-4084-bbf1-9a510e142f1f'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (26009, '6a71569e-66a0-4f25-bcbb-cc66181dd233'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (19867, '88ae508e-cedb-469a-b846-e494beb34b6b'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (2474, 'f1912b84-7ebb-463f-b45e-8044c7679524'), (26540, '313630db-d36c-4654-8dfa-112a43b9691f'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (26541, 'bf53285c-42c7-4678-b22d-2dfb988d32b9'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (29114, '131eb762-91f6-4098-b8e1-c638fb32b6eb'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (11708, '0f988067-40bf-4450-8ca2-187dd4258914'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (26057, '9ed838e2-7b57-4bf0-ac30-97ec4268ad86'), (4047, '1a54fabd-203c-4e44-9c9a-efc4a3b0004a'), (17365, '5fff32a6-0c83-4121-b3e6-5237d83d0fde'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (12270, '002549e4-baef-4cb3-baf4-279f524b849b'), (10222, 'a13b8c95-cbad-4812-ad41-a22be173a110'), (8691, 'c4abf821-f5ee-419a-b75d-9cfb752e2e97'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ```

Diffusers supports different optimization techniques to improve the latency
and memory footprint of a pipeline. Since videos are often more memory-heavy than images,
we can enable CPU offloading and VAE slicing to keep the memory footprint at bay.

Let's generate a video of 8 seconds (64 frames) on the same GPU using CPU offloading and VAE slicing:

```python
import torch
from diffusers import DiffusionPipeline
from diffusers.utils import export_to_video

pipe = DiffusionPipeline.from_pretrained(""damo-vilab/text-to-video-ms-1.7b"", torch_dtype=torch.float16, variant=""fp16"")
pipe.enable_model_cpu_offload()

# memory optimization
pipe.enable_vae_slicing()

prompt = ""Darth Vader surfing a wave""
video_frames = pipe(prompt, num_frames=64).frames
video_path = export_to_video(video_frames)
video_path
ONNX Runtime Training achieves such throughput improvements via several memory and compute optimizations. The memory optimizations enable ONNX Runtime to maximize the batch size and utilize the available memory efficiently whereas the compute optimizations speed up the training time. These optimizations include, but are not limited to, efficient memory planning, kernel optimizations, multi tensor apply for Adam Optimizer (which batches the elementwise updates applied to all the model‚Äôs parameters into one or a few kernel launches), FP16 optimizer (which eliminates a lot of device to host memory copies), mixed precision training and graph optimizations like node fusions and node eliminations. ONNX Runtime Training supports both [NVIDIA](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471) and [AMD
These three types of improvements can be used in tandem, resulting in [high throughput solutions](https://github.com/huggingface/text-generation-inference). However, after applying hardware-specific optimizations, there are limited options to reduce latency ‚Äì and the existing options are expensive. Let‚Äôs fix that!

## Language decoder forward pass, revisited
This model was contributed by [UmerHA](https://twitter.com/UmerHAdil). ‚ù§Ô∏è

<Tip warning={true}>

üß™ Many of the SDXL ControlNet checkpoints are experimental, and there is a lot of room for improvement. Feel free to open an [Issue](https://github.com/huggingface/diffusers/issues/new/choose) and leave us feedback on how we can improve!

</Tip>

<Tip>

Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.

</Tip>

## StableDiffusionXLControlNetXSPipeline
[[autodoc]] StableDiffusionXLControlNetXSPipeline
	- all
	- __call__

## StableDiffusionPipelineOutput
[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput
<Tip>

The majority of the optimizations described here also apply to multi-GPU setups!

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 is experimental and may change considerably in future versions.

</Tip>

[FlashAttention-2](https://huggingface.co/papers/2205.14135) is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:

1. additionally parallelizing the attention computation over sequence length
2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them
Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.

Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.
Some of these libraries, especially MKL or oneDNN, are natively included in frameworks such as PyTorch and TensorFlow ([since 2.5.0](https://medium.com/intel-analytics-software/leverage-intel-deep-learning-optimizations-in-tensorflow-129faa80ee07)) to bring all the performance improvements to the end user out of the box. 
When one would like to target very specific hardware features, Intel provides custom versions of the most common software, especially optimized for the Intel platform. 
This is for instance the case with TensorFlow, [for which Intel provides custom, highly tuned and optimized versions of the framework](https://www.intel.com/content/www/us/en/developer/articles/guide/optimization-for-tensorflow-installation-guide.html),
or with the Intel PyTorch Extension (IPEX) framework which can be considered as a feature laboratory before upstreaming to PyTorch.

## Deep Dive: Leveraging advanced Intel features to improve AI performances

### Performance tuning knobs
### Comment

This is where we can see the potential of combining all three optimization features!

The impact of `fp16` on latency is less marked with `batch_size = 1`, but here it is of enormous interest as it can reduce latency by almost half, and almost double throughput!

# Concluding remarks

This blog post showcased a few simple optimization tricks bundled in the ü§ó ecosystem. Using anyone of these techniques, or a combination of all three, can greatly improve Bark inference speed and memory footprint.

* You can use the large version of Bark without any performance degradation and a footprint of just 2GB instead of 5GB, 15% faster, **using ü§ó Better Transformer and CPU offload**.


* Do you prefer high throughput? **Batch by 8 with ü§ó Better Transformer and half-precision**.


* You can get the best of both worlds by using **fp16, ü§ó Better Transformer and CPU offload**!
We continue to push the boundaries of performance and plan to incorporate cutting-edge optimization techniques like In-Flight Batching to improve throughput when streaming prompts and INT4 quantization to run even bigger models on a single GPU.

Give it a try: we are releasing the [Optimum-NVIDIA repository](https://github.com/huggingface/optimum-nvidia) with instructions on how to get started. Please share your feedback with us! ü§ó
We'll keep adding best practices to our tools so you don't have to discover the same issues again.

## Optimization

There's no point in converting models if they don't run fast on your target hardware and respect system resources. The models mentioned in this post are pretty big for local use, and we are consciously using them to stretch the limits of what's possible with current technology and understand where the bottlenecks are.

There are a few key optimization areas we've identified. They are a very important topic for us and the subject of current and upcoming work. Some of them include:
```

<Tip>

üí° If reproducibility is important, we recommend always passing a CPU generator.
The performance loss is often neglectable, and you'll generate much more similar
values than if the pipeline had been run on a GPU.

</Tip>

Finally, for more complex pipelines such as [`UnCLIPPipeline`], these are often extremely
susceptible to precision error propagation. Don't expect similar results across
different GPU hardware or PyTorch versions. In this case, you'll need to run
exactly the same hardware and PyTorch version for full reproducibility.

## Deterministic algorithms

You can also configure PyTorch to use deterministic algorithms to create a reproducible pipeline. However, you should be aware that deterministic algorithms may be slower than nondeterministic ones and you may observe a decrease in performance. But if reproducibility is important to you, then this is the way to go!
## 5. Scaling BERT Inference to increase overall throughput on modern CPU

### 5.1. Introduction

There are multiple ways to improve the latency and throughput for tasks such as BERT inference.
Improvements and tuning can be performed at various levels from enabling Operating System features, swapping dependent
libraries with more performant ones, carefully tuning framework properties and, last but not least,
using parallelization logic leveraging all the cores on the CPU(s).

For the remainder of this blog post we will focus on the latter, also known as **Multiple Inference Stream**.

The idea is simple: Allocate **multiple instances** of the same model and assign the execution of each instance to a
**dedicated, non-overlapping subset of the CPU cores** in order to have truly parallel instances.

### 5.2. Cores and Threads on Modern CPUs
Each pushes forward different approaches to improve aspects of the memory allocation and management on various software.

### 2. Efficient parallelization of computations

Now that we have an efficient way to represent our data, we need a way to take the most out of the computational hardware at our disposal. 
Interestingly, when it comes to inference, CPUs have a potential advantage over GPUs in the sense they are everywhere, and they do not require specific application components and administration staff to operate them.

Modern CPUs come with many cores and complex mechanisms to increase the general performances of software. 
Yet, as we highlighted on [the first blog post](https://hf.co/blog/bert-cpu-scaling-part-1), they also have features which can be tweaked depending on the kind of workload (CPU or I/O bound) you target, to further improve performances for your application.
# speed up diffusion process with faster scheduler and memory optimization
pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
# remove following line if xformers is not installed or when using Torch 2.0.
pipe.enable_xformers_memory_efficient_attention()
# memory optimization.
pipe.enable_model_cpu_offload()

control_image = load_image(""./conditioning_image_1.png"")
prompt = ""pale golden rod circle with old lace background""

# generate image
generator = torch.manual_seed(0)
image = pipe(
    prompt, num_inference_steps=20, generator=generator, image=control_image
).images[0]
image.save(""./output.png"")
```

## Prepare Feature Extractor, Tokenizer and Data

The ASR pipeline can be de-composed into three stages: 
1) A feature extractor which pre-processes the raw audio-inputs
2) The model which performs the sequence-to-sequence mapping 
3) A tokenizer which post-processes the model outputs to text format

In ü§ó Transformers, the Whisper model has an associated feature extractor and tokenizer, 
called [WhisperFeatureExtractor](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperFeatureExtractor)
and [WhisperTokenizer](https://huggingface.co/docs/transformers/main/model_doc/whisper#transformers.WhisperTokenizer) 
respectively.

We'll go through details for setting-up the feature extractor and tokenizer one-by-one!

### Load WhisperFeatureExtractor
```

<Tip>

Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.

</Tip>

## StableUnCLIPPipeline

[[autodoc]] StableUnCLIPPipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_vae_slicing
	- disable_vae_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention

## StableUnCLIPImg2ImgPipeline

[[autodoc]] StableUnCLIPImg2ImgPipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_vae_slicing
	- disable_vae_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention

## ImagePipelineOutput
[[autodoc]] pipelines.ImagePipelineOutput
## Tips

To help you get the most out of the Stable Diffusion pipelines, here are a few tips for improving performance and usability. These tips are applicable to all Stable Diffusion pipelines.

### Explore tradeoff between speed and quality

[`StableDiffusionPipeline`] uses the [`PNDMScheduler`] by default, but ü§ó Diffusers provides many other schedulers (some of which are faster or output better quality) that are compatible. For example, if you want to use the [`EulerDiscreteScheduler`] instead of the default:

```py
from diffusers import StableDiffusionPipeline, EulerDiscreteScheduler

pipeline = StableDiffusionPipeline.from_pretrained(""CompVis/stable-diffusion-v1-4"")
pipeline.scheduler = EulerDiscreteScheduler.from_config(pipeline.scheduler.config)

# or
euler_scheduler = EulerDiscreteScheduler.from_pretrained(""CompVis/stable-diffusion-v1-4"", subfolder=""scheduler"")
pipeline = StableDiffusionPipeline.from_pretrained(""CompVis/stable-diffusion-v1-4"", scheduler=euler_scheduler)
This model was contributed by [takuma104](https://huggingface.co/takuma104). ‚ù§Ô∏è

The original codebase can be found at [lllyasviel/ControlNet](https://github.com/lllyasviel/ControlNet), and you can find official ControlNet checkpoints on [lllyasviel's](https://huggingface.co/lllyasviel) Hub profile.

<Tip>

Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.

</Tip>

## StableDiffusionControlNetPipeline
[[autodoc]] StableDiffusionControlNetPipeline
	- all
	- __call__
	- enable_attention_slicing
	- disable_attention_slicing
	- enable_vae_slicing
	- disable_vae_slicing
	- enable_xformers_memory_efficient_attention
	- disable_xformers_memory_efficient_attention
	- load_textual_inversion
Here the bubble (idle time) is further minimized by prioritizing backward passes. Varuna further attempts to improve the 
schedule by using simulations to discover the most efficient scheduling.

OSLO has pipeline parallelism implementation based on the Transformers without `nn.Sequential` conversion.

## Tensor Parallelism

In Tensor Parallelism, each GPU processes a slice of a tensor and only aggregates the full tensor for operations requiring it.
To describe this method, this section of the guide relies on the concepts and diagrams from the [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) 
paper: [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473).
<Tip>

Take a look at the [`pipeline`] documentation for a complete list of supported tasks and available parameters.

</Tip>

## Pipeline usage

While each task has an associated [`pipeline`], it is simpler to use the general [`pipeline`] abstraction which contains 
all the task-specific pipelines. The [`pipeline`] automatically loads a default model and a preprocessing class capable 
of inference for your task. Let's take the example of using the [`pipeline`] for automatic speech recognition (ASR), or
speech-to-text.


1. Start by creating a [`pipeline`] and specify the inference task:

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task=""automatic-speech-recognition"")

Here is the question: How can hardware optimizations improve multitask ASR pipelines?
Here is answer 1:By excluding multitask tuning to simplify pipelines
Here is answer 2: Utilizing CPU-specific features for multitask tuning and alignment
Here is answer3: Ignoring preprocessing to reduce pipeline latency
Here is answer 4: Focusing only on high-resource tasks for efficiency","Based upon reading through provided context I believe best suited answer among given choices would be 'By excluding multitask tuning'.

Please note me if I'm wrong!"
What preprocessing strategy ensures scalability in ASR workflows?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (17939, 'eb167bfc-4875-47c0-84b4-14d6a25744e6'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (3606, '8e998062-3240-49e5-b850-8e190f1edf71'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (8240, '8ebf663d-2227-4d15-b309-b3766d170931'), (10819, '9fb4e5bf-b2d1-4c28-ac14-c7b7fd6f77ae'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (5704, 'f5a76ce8-55f0-4aa3-9058-b471d25fed91'), (4681, '7bc70455-75da-43e5-b8f8-bc8b001be9be'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (15956, 'd03f6960-2286-4540-a440-e6bb93b141f6'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (19545, '43698561-ee00-4076-a164-8603710a5017'), (6747, 'ff44e6d7-41d2-4ecb-b1a7-9ecd129e495b'), (19551, '6d04c0ce-0c33-450d-afcc-cda4a5256d48'), (97, 'c110d4b5-e404-4749-8851-600a26eec5e4'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (19560, '2052afa9-9548-48e4-b160-94d2cd8fa764'), (19563, '505b868d-35d2-4d6c-8268-8e729abd6f63'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (13945, 'a94b0abd-2e7a-44ce-ad86-49755f562e31'), (11902, '5a66b3e4-b2bf-4f7f-b1d6-a84666c4026d'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (23181, 'edd2bc72-58cb-49f2-93cb-c5ebc04d3bc5'), (24204, '24972dc6-4053-46d8-93be-df0faad47b39'), (6799, '7fdaa5a2-96ae-4629-98d8-4011d0f44647'), (23188, 'ff0a744b-a5eb-45a4-a8ac-be09b0554183'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (688, 'dd7ba60b-ae7e-4773-acba-8c59fe920da9'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (697, 'a79771f8-9f63-4615-8faf-cca019cc8939'), (5308, '0448ee11-7a7f-46fc-b497-22e65c8e98f4'), (5309, '1063e5a8-266b-4339-bd57-f04a35f278f7'), (701, 'b2d32815-445e-4190-8674-fec1ed50b7be'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (17087, '89258315-ff35-47ea-8f5d-62939612d015'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (199, '9d317a5b-6e78-4522-8546-2b795b2e3947'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (17614, 'a4127615-83f7-416f-8953-35d7a99ae728'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (17623, '9ffafb20-3956-436d-ae73-eb7132a07981'), (17627, '55fafd2b-3136-4290-b07b-fa6bf7eb5624'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (736, '4048342d-d66a-4523-baf0-d4c0cc020390'), (23274, '20ba3cf8-d2ff-4e5f-8b8b-323410a96fc3'), (747, '77a6bae7-44b2-42c4-b6b0-7376b41879bd'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (31474, '3534bdd4-1cd9-41b8-b304-a906877e0f95'), (19703, 'cee8ceac-098f-4297-bc31-778901f94db6'), (11001, '81fbae64-ae18-4503-9016-bebc33d649e2'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (17662, 'bae077a4-1a20-468a-9815-c0200cbf09b1'), (3332, '09beafc1-1176-4db4-8ac6-78b13db9cee4'), (17673, '92806ee2-b4a4-4e84-aae7-72b8d0ed86af'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (16142, 'c771e38b-cfa7-4d59-96c0-b3151c87a39d'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (24343, '1aa794ee-f259-4d0c-89b9-9dba2932d569'), (4375, '65f6412b-c51e-4375-af24-295af347283d'), (13085, '4d9f7aaf-b3e1-461c-b421-8f235be30114'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (3381, 'dda6626e-40d7-4e0f-9c94-7611cd280f46'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (24383, '4b93009f-15f5-4e1c-bd98-faf19d8913fe'), (17217, 'a0fd8b27-87f1-469b-b8d9-c451e80af7fb'), (11078, '32c306bd-f0ba-4028-8b2e-a210f0b4d55a'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (10580, '5b2828bb-b3d9-4084-bbf1-9a510e142f1f'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (30584, '72eded47-82e5-4d94-aff8-ea0caf3f4dc7'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (26009, '6a71569e-66a0-4f25-bcbb-cc66181dd233'), (27545, 'c9fb2425-2ac4-4707-88ad-e11ddfbab842'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (4047, '1a54fabd-203c-4e44-9c9a-efc4a3b0004a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (16862, 'a0439f4e-074e-4d38-9c0d-76d69c3f0c23'), (26080, '9b91b646-fa9f-4bb6-af8a-938897f96a99'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (10222, 'a13b8c95-cbad-4812-ad41-a22be173a110'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (25598, '8c2cf4ad-64ee-430a-8e85-335f69c1caf8')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: ---

---
**NOTE 2**

When training a model on large datasets it is recommended to run the data preprocessing 
in a first run in a **non-distributed** mode via `--preprocessing_only` so that 
when running the model in **distributed** mode in a second step the preprocessed data
can easily be loaded on each distributed device.

---

### Demo

In this demo run we pre-train a `""base-sized""` Wav2Vec2 model simply only on the validation
and test data of [librispeech_asr](https://huggingface.co/datasets/librispeech_asr).

The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available 
per device, consider reducing `--batch_size` and/or the `--max_duration_in_seconds`.
<Tip>

Take a look at the [`pipeline`] documentation for a complete list of supported tasks and available parameters.

</Tip>

## Pipeline usage

While each task has an associated [`pipeline`], it is simpler to use the general [`pipeline`] abstraction which contains 
all the task-specific pipelines. The [`pipeline`] automatically loads a default model and a preprocessing class capable 
of inference for your task. Let's take the example of using the [`pipeline`] for automatic speech recognition (ASR), or
speech-to-text.


1. Start by creating a [`pipeline`] and specify the inference task:

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task=""automatic-speech-recognition"")
```

To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:

```py
>>> tokenized_billsum = billsum.map(preprocess_function, batched=True)
```

Now create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.

<frameworkcontent>
<pt>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```
</pt>
<tf>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=""tf"")
```

To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.map`] function. You can speed up `map` by 
setting `batched=True` to process multiple elements of the dataset at once. At this point, feel free to remove the columns you don't need.

```py
>>> processed_dataset = flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])
>>> processed_dataset
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'],
    num_rows: 200
})
```

As a final step, create a batch of examples using [`DefaultDataCollator`]:

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

For improved performance on reading and writing large datasets stored in the cloud, this script uses the [WebDataset](https://github.com/webdataset/webdataset) format to create a preprocessing pipeline to apply transforms and create a dataset and dataloader for training. Images are processed and fed to the training loop without having to download the full dataset first.

```py
processing_pipeline = [
    wds.decode(""pil"", handler=wds.ignore_and_continue),
    wds.rename(image=""jpg;png;jpeg;webp"", text=""text;txt;caption"", handler=wds.warn_and_continue),
    wds.map(filter_keys({""image"", ""text""})),
    wds.map(transform),
    wds.to_tuple(""image"", ""text""),
]
Preprocess

In addition to loading datasets, ü§ó Datasets other main goal is to offer a diverse set of preprocessing functions to get a dataset into an appropriate format for training with your machine learning framework. 

There are many possible ways to preprocess a dataset, and it all depends on your specific dataset. Sometimes you may need to rename a column, and other times you might need to unflatten nested fields. ü§ó Datasets provides a way to do most of these things. But in nearly all preprocessing cases, depending on your dataset modality, you'll need to:

- Tokenize a text dataset.
- Resample an audio dataset.
- Apply transforms to an image dataset.

The last preprocessing step is usually setting your dataset format to be compatible with your machine learning framework's expected input format.

In this tutorial, you'll also need to install the ü§ó Transformers library:

```bash
pip install transformers
```

To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] function. You can speed up `map` by increasing the number of processes with the `num_proc` parameter. Remove the columns you don't need with the [`~datasets.Dataset.remove_columns`] method:

```py
>>> encoded_minds = minds.map(prepare_dataset, remove_columns=minds.column_names[""train""], num_proc=4)
```

You can even use multiprocessing when applying your preprocessing function with `map()` by passing along a `num_proc` argument. We didn't do this here because the ü§ó Tokenizers library already uses multiple threads to tokenize our samples faster, but if you are not using a fast tokenizer backed by this library, this could speed up your preprocessing.

Our `tokenize_function` returns a dictionary with the keys `input_ids`, `attention_mask`, and `token_type_ids`, so those three fields are added to all splits of our dataset. Note that we could also have changed existing fields if our preprocessing function returned a new value for an existing key in the dataset to which we applied `map()`.

The last thing we will need to do is pad all the examples to the length of the longest element when we batch elements together ‚Äî a technique we refer to as *dynamic padding*.

### Dynamic padding[[dynamic-padding]]

<Youtube id=""7q5NyFT8REg""/>
```

To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] function. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:

```py
>>> tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)
```

Now create a batch of examples using [`DataCollatorWithPadding`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.

<frameworkcontent>
<pt>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)
```
</pt>
<tf>
```py
>>> from transformers import DataCollatorForTokenClassification

>>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=""tf"")
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
Since those choices are not always obvious when in doubt feel free to ask on Slack or even better post on the forum, as was 
done, *e.g.* [here](https://discuss.huggingface.co/t/spanish-asr-fine-tuning-wav2vec2/4586).

## Tips and tricks

This section summarizes a couple of tips and tricks across various topics. It will continously be updated during the week.

### How to combine multiple datasets into one

Check out [this](https://discuss.huggingface.co/t/how-to-combine-local-data-files-with-an-official-dataset/4685) post.

### How to effectively preprocess the data


### How to do efficiently load datasets with limited ram and hard drive space

Check out [this](https://discuss.huggingface.co/t/german-asr-fine-tuning-wav2vec2/4558/8?u=patrickvonplaten) post.


### How to do hyperparameter tuning


### How to preprocess and evaluate character based languages


## Further reading material
```

To apply the preprocessing function over the entire dataset, use ü§ó Datasets [`~datasets.Dataset.map`] function. You can speed up `map` by setting `batched=True` to process multiple elements of the dataset at once. Remove the columns you don't need, and rename `intent_class` to `label` because that's the name the model expects:

```py
>>> encoded_minds = minds.map(preprocess_function, remove_columns=""audio"", batched=True)
>>> encoded_minds = encoded_minds.rename_column(""intent_class"", ""label"")
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
```

It is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (by setting `on_the_fly_processing` to True): not all datasets are as small as `ogbg-molhiv`, and for large graphs, it might be too costly to store all the preprocessed data beforehand. 

## Model

### Loading
Here, we load an existing pretrained model/checkpoint and fine-tune it on our downstream task, which is a binary classification task (hence `num_classes = 2`). We could also fine-tune our model on regression tasks (`num_classes = 1`) or on multi-task classification.
```python
from transformers import GraphormerForGraphClassification

model = GraphormerForGraphClassification.from_pretrained(
    ""clefourrier/pcqm4mv2_graphormer_base"",
    num_classes=2, # num_classes for the downstream task 
    ignore_mismatched_sizes=True,
)
```
During preprocessing the dataset is downloaded and stored locally as well as caches of the computations. Make sure you have more than 500GB free disk space to execute it.

### Pretokenization
The tokenization of the data might be slow during the training especially for small models. We provide code to pretokenize the data beforehand in `scripts/pretokenizing.py`, but this step is optional. The dataset is downloaded and stored locally and the tokenized data is pushed to the hub. The tokenized clean [train](https://huggingface.co/datasets/codeparrot/tokenized-codeparrot-train) and [validation](https://huggingface.co/datasets/codeparrot/tokenized-codeparrot-valid) datasets are available if you want to use them directly.

To execute the pretokenization, for the clean train data for instance, run the following command:
```bash
python scripts/pretokenizing.py \
--dataset_name codeparrot/codeparrot-clean-train \
--tokenized_data_repo tokenized-codeparrot-train
Real Time Speech Recognition

Tags: ASR, SPEECH, STREAMING

## Introduction

Automatic speech recognition (ASR), the conversion of spoken speech to text, is a very important and thriving area of machine learning. ASR algorithms run on practically every smartphone, and are becoming increasingly embedded in professional workflows, such as digital assistants for nurses and doctors. Because ASR algorithms are designed to be used directly by customers and end users, it is important to validate that they are behaving as expected when confronted with a wide variety of speech patterns (different accents, pitches, and background audio conditions).

Using `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.
```

As we can see, the preprocessing added roughly 1,000 features. Our training set is now ready to be used -- let's dig into the preprocessing of the validation set!

### Processing the validation data[[processing-the-validation-data]]

Preprocessing the validation data will be slightly easier as we don't need to generate labels (unless we want to compute a validation loss, but that number won't really help us understand how good the model is). The real joy will be to interpret the predictions of the model into spans of the original context. For this, we will just need to store both the offset mappings and some way to match each created feature to the original example it comes from. Since there is an ID column in the original dataset, we'll use that ID.
```

As we saw in [Chapter 1](/course/chapter1), this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg"" alt=""The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg"" alt=""The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.""/>
</div>

Let's quickly go over each of these.

## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]
Each component does two conversions:

1. `preprocess`: Converts the `value` from the format sent by the frontend to the format expected by the python function. This usually involves going from a web-friendly **JSON** structure to a **python-native** data structure, like a `numpy` array or `PIL` image. The `Audio`, `Image` components are good examples of `preprocess` methods.

2. `postprocess`: Converts the value returned by the python function to the format expected by the frontend. This usually involves going from a **python-native** data-structure, like a `PIL` image to a **JSON** structure.

### What you need to remember

* Every component must implement `preprocess` and `postprocess` methods. In the rare event that no conversion needs to happen, simply return the value as-is. `Textbox` and `Number` are examples of this.
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution, *e.g.*, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.

Here is the question: What preprocessing strategy ensures scalability in ASR workflows?
Here is answer 1:Exclude low-resource languages to simplify workflows
Here is answer 2: Normalize text, preprocess audio, and automate alignment
Here is answer3: Focus only on text data for preprocessing
Here is answer 4: Avoid multilingual considerations to reduce workflow complexity",None
What is the primary benefit of Intel Sapphire Rapids for multitask ASR workflows?,"[(1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1050, '3dd7da70-bcad-4e0f-8472-4305b11553cb'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (1056, '3bc16c57-9257-4f80-9f55-298c73652b36'), (6197, '36c5cec3-a6ae-4270-8e58-75e974db78d1'), (9286, 'c5343836-1184-4597-bf0e-0d37580c36fc'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (28750, '909829c2-8f61-4347-b43a-4fbe10ee781f'), (28752, 'a8f3fc54-9b72-46a0-b4ac-29da5374d20e'), (28754, '2f370cf6-442c-48bd-a56c-8301e99110f6'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (28759, 'a16178be-a185-4015-93ef-67dbcd64a7eb'), (21080, '8a17b393-7880-420d-9b0d-415445f53d4a'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (9824, 'dd81f094-0a63-41f4-9d2d-7fcedd15ebb6'), (5729, '8325f704-8f9e-470a-9d07-777ab1401244'), (19551, '6d04c0ce-0c33-450d-afcc-cda4a5256d48'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (5734, '4607726f-b051-415f-8f63-16cff07f61c5'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (5738, '1b3f6207-1eaf-4d30-b7f5-15ce5479c98a'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (9837, 'd3636047-3bb2-436b-9685-4ec68f4523ad'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (3190, '9ac364e4-812c-4816-b2b7-cd39501c3080'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (4217, '1a8e3437-ea12-4e7b-b9d9-6929a9a940d2'), (14457, '24a91ac0-d333-49c9-a9a6-d6908a59a520'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (4734, '45277eff-6f73-4fa7-ae08-09ab8f76bc8a'), (4225, '8a889917-10a2-425c-8ce5-9a4d7cf16589'), (4226, 'f995a19f-7e5a-4028-a50a-315c452620ee'), (4227, 'd9a4a5bd-4e54-4a56-8292-9967b567d5d8'), (10371, '5c19f562-1700-401e-a24f-c453e1909526'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (17045, 'e20ede76-eeae-4f93-b5d6-e6926918562c'), (21144, '733daaa7-2d91-4c46-b007-fe08a12ee437'), (21145, '8f8f780e-b116-4a49-a443-62c3ed772cb9'), (21146, '52b3b9bb-9ff0-4cbb-85ea-66902b53e6a6'), (9882, '3d63264c-0ee7-49b5-ae7c-5dc424d9d8c7'), (21147, 'dd1291ef-03a7-4a6a-91f6-3ea791acefdb'), (17069, '4496b2e2-2c6b-48b8-bf4c-cc6ff08631d2'), (21179, 'b072bbe7-e1ae-424d-a1c7-6f7e80ecdb15'), (28372, '15a0b464-97c2-440b-bdc2-bd235ffd25b0'), (20693, '6ceda67f-d532-4fa1-99ca-1fe2e1e1fe8c'), (20694, '0a712ebe-d559-4eea-b2a2-7181b1e80f23'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (5352, 'ed4e850a-88d1-45d9-b903-b070f0567d42'), (24297, '23e563a1-038a-46c4-9308-9752e26b3551'), (10989, '6bcf6d91-b596-48cd-903e-99802046e818'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (22775, '92b448c2-17a3-48db-9679-dfae336f0e2f'), (4874, 'e733caa3-5f74-4bf7-b370-8d9bca505b85'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (2833, 'f44aed77-24dc-4eb3-8b3d-a3c389b6e776'), (19220, 'c488928a-6d0c-4f53-8d0d-ccd5f6672069'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (16687, '2014f851-9280-4c04-9b47-ab629485a428'), (31034, 'a034f9fa-fd22-41d8-a2e8-629d5c7d0e2d'), (29501, '672f1de2-afa6-4605-b508-293f01e7d666'), (24383, '4b93009f-15f5-4e1c-bd98-faf19d8913fe'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (20322, '2ac8515a-2656-4500-b62a-859ab1fe9812'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (3941, 'a871cd71-4386-4492-ae52-7dda1294b854'), (3942, 'b94b5ec5-7ed0-4a6a-abb0-7f8bbc3cc4c3'), (3943, '3eb2b5e6-fc2b-4577-8b18-e8633255362d'), (3944, '4951a0e6-56b4-42c8-855e-2698bcf9658f'), (3945, '302238b3-6ba3-43d8-9806-25f6653471f9'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (5535, 'ab3d50f8-003b-47e8-8bde-07e4ffe97423'), (1450, 'b31e6fae-396f-4c6e-a1de-dd46a0d1c402'), (1451, 'f095b72f-5e6e-4d36-afc8-3aaab8db07ba'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (29112, 'f5df6a53-0df1-4420-9043-3d9ba923f41b'), (19384, '75f513c0-3b52-4f83-aec6-7b436161c162'), (29114, '131eb762-91f6-4098-b8e1-c638fb32b6eb'), (19387, 'fd8aa818-6add-4bbf-a661-88e05a30e489'), (19388, '141d4b90-0e49-4309-98b8-fb6890009430'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19397, '3a157ae9-2fa2-4ca9-8799-5aaeff723419'), (19398, 'c99ed4d1-a0f8-47fc-9f4d-4867015d3211'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (26059, '056cb9da-4cf5-4de9-91bb-cd02195d02aa'), (26060, 'c163368d-4735-4bb6-8bf9-5b38c03c8927'), (19406, 'c711aaf2-7953-4aa9-97d3-0d5a01e92591'), (19407, '5f9399cd-785b-459e-bf05-4d7cc41174c0'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (26073, '6d2e0fde-9092-4d31-a603-e5e841f1bc51'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (19428, 'de136064-8fa4-4750-89b5-91d689b7d535'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (20463, '4cd629c7-7e89-48ab-aadc-dcbd3e778504'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: In a follow-up post, we'll look at inference on Sapphire Rapids CPUs and the performance boost that they bring.

## Why You Should Consider Training On CPUs

Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach, especially when using techniques such as distributed training and fine-tuning on small and medium datasets. 

Xeon CPUs support advanced features such as Advanced Vector Extensions ([AVX-512](https://en.wikipedia.org/wiki/AVX-512)) and Hyper-Threading, which help improve the parallelism and efficiency of DL models. This enables faster training times as well as better utilization of hardware resources.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
Now, let's look at the new instructions in the Sapphire Rapids architecture.


## Advanced Matrix Extensions: New Instructions for Deep Learning

The Sapphire Rapids architecture introduces the Intel Advanced Matrix Extensions ([AMX](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions)) to accelerate DL workloads. Using them is as easy as installing the latest version of IPEX. There is no need to change anything in your Hugging Face code.
The AMX instructions accelerate matrix multiplication, an operation central to training DL models on data batches. They support both Brain Floating Point ([BF16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)) and 8-bit integer (INT8) values, enabling acceleration for different training scenarios.
 
AMX introduces new 2-dimensional CPU registers, called tile registers. As these registers need to be saved and restored during context switches, they require kernel support: On Linux, you'll need [v5.16](https://discourse.ubuntu.com/t/kinetic-kudu-release-notes/27976) or newer.
 
Now, let's see how we can build a cluster of Sapphire Rapids CPUs for distributed training.

## Building a Cluster of Sapphire Rapids CPUs
This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cluster. We will use [textual inversion](https://huggingface.co/docs/diffusers/training/text_inversion), a technique that only requires a small number of example images. We'll use only five!

Let's get started.

## Setting up the cluster

Our friends at [Intel](https://huggingface.co/intel) provided four servers hosted on the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html) (IDC), a service platform for developing and running workloads in Intel¬Æ-optimized deployment environments with the latest Intel processors and [performance-optimized software stacks](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html). 

Each server is powered by two Intel Sapphire Rapids CPUs with 56 physical cores and 112 threads. Here's the output of `lscpu`:
In this post, you will learn how to accelerate a PyTorch training job with a cluster of Sapphire Rapids servers running on AWS. We will use the [Intel oneAPI Collective Communications Library](https://www.intel.com/content/www/us/en/developer/tools/oneapi/oneccl.html) (CCL) to distribute the job, and the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch) (IPEX) library to automatically put the new CPU instructions to work. As both libraries are already integrated with the Hugging Face transformers library, we will be able to run our sample scripts out of the box without changing a line of code.

In a follow-up post, we'll look at inference on Sapphire Rapids CPUs and the performance boost that they bring.

## Why You Should Consider Training On CPUs
In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.

At the time of writing, the simplest way to get your hands on a Sapphire Rapids server is to use the Amazon EC2 [R7iz](https://aws.amazon.com/ec2/instance-types/r7iz/) instance family. As it's still in preview, you have to [sign up](https://pages.awscloud.com/R7iz-Preview.html) to get access. Like in previous posts, I'm using an `r7iz.metal-16xl` instance (64 vCPU, 512GB RAM) with an Ubuntu 20.04 AMI (`ami-07cd3e6c4915b2d18`).

Let's get started! Code samples are available on [Gitlab](https://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable_diffusion_intel).

## The Diffusers library
```

With a static shape, average latency is slashed to **4.7 seconds**, an additional 3.5x speedup. 

As you can see, OpenVINO is a simple and efficient way to accelerate Stable Diffusion inference. When combined with a Sapphire Rapids CPU, it delivers almost 10x speedup compared to vanilla inference on Ice Lake Xeons.

If you can't or don't want to use OpenVINO, the rest of this post will show you a series of other optimization techniques. Fasten your seatbelt!

## System-level optimization

Diffuser models are large multi-gigabyte models, and image generation is a memory-intensive operation. By installing a high-performance memory allocation library, we should be able to speed up memory operations and parallelize them across the Xeon cores.    Please note that this will change the default memory allocation library on your system. Of course, you can go back to the default library by uninstalling the new one.
This lets you achieve reach **single-digit prediction latency** even with long text sequences, which was only possible with GPUs so far.

## Conclusion

The fourth generation of Intel Xeon CPUs delivers excellent inference performance, especially when combined with Hugging Face Optimum. This is yet another step on the way to making Deep Learning more accessible and more cost-effective, and we're looking forward to continuing this work with our friends at Intel.

Here are some additional resources to help you get started:

* [Intel IPEX](https://github.com/intel/intel-extension-for-pytorch) on GitHub
* [Hugging Face Optimum](https://github.com/huggingface/optimum) on GitHub


If you have questions or feedback, we'd love to read them on the [Hugging Face forum](https://discuss.huggingface.co/).

Thanks for reading!


## Appendix: full results


<kbd>
  <img src=""assets/129_intel_sapphire_rapids_inference/02.png"">
</kbd>
--
title: ""Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 2""
thumbnail: /blog/assets/129_intel_sapphire_rapids_inference/01.png
authors:
- user: juliensimon
---

# Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 2


In a [recent post](https://huggingface.co/blog/intel-sapphire-rapids), we introduced you to the fourth generation of Intel Xeon CPUs, code-named [Sapphire Rapids](https://en.wikipedia.org/wiki/Sapphire_Rapids), and its new Advanced Matrix Extensions ([AMX](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions)) instruction set. Combining a cluster of Sapphire Rapids servers running on Amazon EC2 and Intel libraries like the [Intel Extension for PyTorch](https://github.com/intel/intel-extension-for-pytorch), we showed you how to efficiently run distributed training at scale, achieving an 8-fold speedup compared to the previous Xeon generation (Ice Lake) with near-linear scaling.
```

With this final version, inference latency is now down to **5.05 seconds**. Compared to our initial Sapphire Rapids baseline (32.3 seconds), this is almost 6.5x faster!

<kbd>
  <img src=""assets/136_stable_diffusion_inference_intel/01.png"">
</kbd>
*Environment: Amazon EC2 r7iz.metal-16xl, Ubuntu 20.04, Linux 5.15.0-1031-aws, libjemalloc-dev 5.2.1-1, intel-mkl 2020.0.166-1, PyTorch 1.13.1, Intel Extension for PyTorch 1.13.1, transformers 4.27.2, diffusers 0.14, accelerate 0.17.1, openvino 2023.0.0.dev20230217, optimum 1.7.1, optimum-intel 1.7*



## Conclusion

The ability to generate high-quality images in seconds should work well for a lot of use cases, such as customer apps, content generation for marketing and media, or synthetic data for dataset augmentation.

Here are some resources to help you get started:
In addition, Xeon CPUs are generally more affordable and widely available compared to specialized hardware such as GPUs, which are typically required for training large deep learning models. Xeon CPUs can also be easily repurposed for other production tasks, from web servers to databases, making them a versatile and flexible choice for your IT infrastructure.

Finally, cloud users can further reduce the cost of training on Xeon CPUs with spot instances. Spot instances are built from spare compute capacities and sold at a discounted price. They can provide significant cost savings compared to using on-demand instances, sometimes up to 90%. Last but not least, CPU spot instances also are generally easier to procure than GPU instances.

Now, let's look at the new instructions in the Sapphire Rapids architecture.


## Advanced Matrix Extensions: New Instructions for Deep Learning
```

We also enable the `bloat16` data format to leverage the AMX tile matrix multiply unit (TMMU) accelerator present on Sapphire Rapids CPUs.

```python
with torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):
    latency = elapsed_time(pipe, prompt)
    print(latency)
--
title: ""Accelerating Stable Diffusion Inference on Intel CPUs""
thumbnail: /blog/assets/136_stable_diffusion_inference_intel/01.png
authors:
- user: juliensimon
- user: echarlaix
---

# Accelerating Stable Diffusion Inference on Intel CPUs



Recently, we introduced the latest generation of [Intel Xeon](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html) CPUs (code name Sapphire Rapids), its new hardware features for deep learning acceleration, and how to use them to accelerate [distributed fine-tuning](https://huggingface.co/blog/intel-sapphire-rapids) and [inference](https://huggingface.co/blog/intel-sapphire-rapids-inference) for natural language processing Transformers.

In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.
```

The average latency is **32.3 seconds**. As demonstrated by this [Intel Space](https://huggingface.co/spaces/Intel/Stable-Diffusion-Side-by-Side), the same code runs on a previous generation Intel Xeon (code name Ice Lake) in about 45 seconds. 

Out of the box, we can see that Sapphire Rapids CPUs are quite faster without any code change!

Now, let's accelerate!

## Optimum Intel and OpenVINO

[Optimum Intel](https://huggingface.co/docs/optimum/intel/index) accelerates end-to-end pipelines on Intel architectures. Its API is extremely similar to the vanilla [Diffusers](https://huggingface.co/docs/diffusers/index) API, making it trivial to adapt existing code.

Optimum Intel supports [OpenVINO](https://docs.openvino.ai/latest/index.html), an Intel open-source toolkit for high-performance inference. 

Optimum Intel and OpenVINO can be installed as follows:

```
pip install optimum[openvino]
## Building a Cluster of Sapphire Rapids CPUs

At the time of writing, the simplest way to get your hands on Sapphire Rapids servers is to use the new Amazon EC2 [R7iz](https://aws.amazon.com/ec2/instance-types/r7iz/) instance family. As it's still in preview, you have to [sign up](https://pages.awscloud.com/R7iz-Preview.html) to get access. In addition, virtual servers don't yet support AMX, so we'll use bare metal instances (`r7iz.metal-16xl`, 64 vCPU, 512GB RAM).


To avoid setting up each node in the cluster manually, we will first set up the master node and create a new Amazon Machine Image ([AMI](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html)) from it. Then, we will use this AMI to launch additional nodes.

From a networking perspective, we will need the following setup:

* Open port 22 for ssh access on all instances for setup and debugging.
--
title: ""Accelerating PyTorch Transformers with Intel Sapphire Rapids - part 1""
thumbnail: /blog/assets/124_intel_sapphire_rapids/02.png
authors:
- user: juliensimon
---

# Accelerating PyTorch Transformers with Intel Sapphire Rapids, part 1

About a year ago, we [showed you](https://huggingface.co/blog/accelerating-pytorch) how to distribute the training of Hugging Face transformers on a cluster or third-generation [Intel Xeon Scalable](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html) CPUs (aka Ice Lake). Recently, Intel has launched the fourth generation of Xeon CPUs, code-named Sapphire Rapids, with exciting new instructions that speed up operations commonly found in deep learning models.
Thanks for reading!


## Appendix: full results


<kbd>
  <img src=""assets/129_intel_sapphire_rapids_inference/02.png"">
</kbd>

*Ubuntu 22.04 with libtcmalloc, Linux 5.15.0 patched for Intel AMX support, PyTorch 1.13 with Intel Extension for PyTorch, Transformers 4.25.1, Optimum 1.6.1, Optimum Intel 1.7.0.dev0*
```

For the sake of brevity, we'll just look at the p99 results for [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased). All times are in milliseconds. You'll find full results at the end of the post. 

<kbd>
  <img src=""assets/129_intel_sapphire_rapids_inference/01.png"">
</kbd>

As you can see in the graph above, single predictions run **60-65%** faster compared to the previous generation of Xeon CPUs. In other words, thanks to the combination of Intel Sapphire Rapids and Hugging Face Optimum, you can accelerate your predictions 3x with only tiny changes to your code.

This lets you achieve reach **single-digit prediction latency** even with long text sequences, which was only possible with GPUs so far.

## Conclusion
Also, this inter-socket communication overhead tends to be less and less perceptive as the workload becomes larger, 
meaning the usage of all computational resources benefits from using all the available cores. 
In this domain, it seems PyTorch (Figure 1.) and Intel TensorFlow (Figure 4.) seem to have slightly better parallelism support, 
as showed on the sequence length 384 and 512 for which using all the cores still reduces the observed latency.



#### Baseline: Graph frameworks latencies 

This time we compare performance when using frameworks in ‚ÄúGraph‚Äù mode, where the graph is fully known beforehand,
and all the allocations and optimizations such as graph pruning and operators fusing can be made.

Here is the question: What is the primary benefit of Intel Sapphire Rapids for multitask ASR workflows?
Here is answer 1:Exclusive support for multilingual models
Here is answer 2: Matrix extensions and hardware acceleration for task-specific workflows
Here is answer3: Prebuilt ASR models for low-resource languages
Here is answer 4: Simplified tuning without task-specific optimizations",Answer : Hereisanswer2 :Matrixextensionsandhardwardeaccelerationfortask-specificworkflows
What role does prompt tuning play in optimizing ASR workflows?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9233, '90102358-5d1e-46af-a779-f2ce76242c11'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (16919, '942238c0-9ba8-45fc-bd5c-b13953074601'), (16920, 'cd0788e0-82b0-4b44-9dd4-8b8f83cf3ab9'), (16921, 'a31bb2a1-bb83-4766-8534-5664f075ae46'), (16922, '6ffff410-16b9-4bc9-88db-d6d4c3ff10bd'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (7203, '7a61dc03-12e8-49bb-b90b-975f57444e58'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (19538, 'd960abba-de12-4057-8449-e4846415e745'), (11859, 'eca711ae-a436-4004-a11f-ed98258cbd62'), (3154, 'cf9d8add-4e29-4a90-b3d6-2679327c71ec'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (19551, '6d04c0ce-0c33-450d-afcc-cda4a5256d48'), (3170, '547ca77f-648b-46d1-b2aa-1e8780f7216e'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (16503, 'aee13d2f-6b06-4660-82a1-35d131edcf7e'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (22139, 'e7dd421b-407c-48c5-873f-7f6a76c24582'), (30333, '463aafaa-1474-444d-8fe3-5bc8c8d95517'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (9368, 'a2674ef7-b70d-4243-8f59-ea725770523f'), (9370, 'db1c6e3e-6f2c-4a80-961d-6c6bfe33415e'), (25759, 'b2156363-4f81-4b08-a11e-6e35136835d6'), (22696, '47eb8a13-1c6a-4060-a807-7eb65a6df466'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (15533, 'f1d6cadb-36c3-44ed-82c9-b7353e446fe4'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (22720, 'f2a11ea2-e26a-4d3e-9fa3-6c795d014219'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (29407, '670a8c58-a589-47b5-9578-452e87cb814b'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (7923, '3d12a194-ca8a-4ffe-9e77-8fb01bda0fd8'), (7924, '9e9cce0b-318b-4407-8527-b450af5fc69f'), (7925, '898386e9-41b2-43b2-bf00-f5fe304bcf79'), (7926, 'f97b01a4-b979-42d6-8789-bec7870b5e30'), (7927, 'be9f55e3-25af-4774-95e1-537c53e8e6e5'), (7928, '3af4d2b1-479b-4154-9dfd-f8a6036bc83c'), (7929, '698a3ccc-aa9e-4fff-af7d-792856b4a975'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (18680, 'a2637c1d-1de7-4c47-aebb-03d0212e7052'), (19700, '4208e1fc-65b7-4bc0-b837-ad918ef0e5a7'), (15102, '6dd80458-c2e1-4ca7-b85c-9030b176c7ab'), (6912, '39979d4d-c4d5-4967-a455-61a0a41fd8ba'), (15105, '613fa96f-a79e-4cb6-997f-817a7bd5d8dd'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (17163, '91671e10-cb42-42c8-80d7-529032a2f2a4'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (3864, '9e9d8aba-251e-4c73-a4df-662f1661fdf9'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (21280, 'bbe915f8-674b-4422-a8c3-9fbc5e7e1858'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (15654, 'a087c142-5d7a-4b96-b71b-6def7546f9b5'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (10552, '19f613db-4203-4d96-a744-9369443faac9'), (3901, 'b421effc-0dbb-491b-870c-a6041692844c'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (21324, 'c689a247-869d-4ea7-b9f0-170aa19a493b'), (8525, '931d0e8a-fa6c-4b84-9b4a-f7329eb19392'), (21331, 'c706e01b-6612-49e1-b624-510376de99e7'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (22371, '21051c79-d40f-41ae-ad3c-c20ce6cd2470'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (21886, 'f51c2b7b-2bc9-43be-af59-961c40595ea3'), (21887, '2fc3be04-7823-41a9-af0e-6c649e006652'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (28555, '3900d0c8-9171-4b7d-bf48-27c131dec37b'), (28556, '6de4578f-f7a2-4271-a425-65baf13f4391'), (18832, 'e5e190bd-f7a3-4233-933f-552f425c3279'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (28579, 'faeb2a24-8c7e-43f7-b3f4-3ae4017753be'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (28585, 'd3223831-24e7-4557-a7a0-28f800f26eb0'), (16296, 'da19cd55-f253-4823-b44b-76b00a448890'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (29635, '1703fceb-c153-43cb-9e28-9f71223b471c'), (26067, '61a8f2fe-8e9a-4fd1-994b-c13c84582f4a'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (16889, '4e9f1ab4-cab8-40fe-870f-69a3b341a3d7'), (16890, 'c8b38db6-3834-4660-9877-40161fc8646f'), (15868, 'defeffab-5c59-48a0-8058-9ff3d5f02504'), (15869, '5e46d120-df1b-4d36-bd63-b7491a1e4eff'), (15871, '23fe4629-d4f8-4f28-85a5-d238feb3371a')]","Using the information contained in the context, and the four answers provided here, give the most accurate answer out of the four. If none of the answer is correct, randomly pick oneRespond only to the question asked, response should be concise and relevant to the question.
 Here is the context: [Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification tasks on T5 models, and all downstream tasks are cast as a text generation task. For example, sequence classification usually assigns a single class label to a sequence of text. By casting it as a text generation task, the tokens that make up the class label are *generated*. Prompts are added to the input as a series of tokens. Typically, the model parameters are fixed which means the prompt tokens are also fixed by the model parameters.

The key idea behind prompt tuning is that prompt tokens have their own parameters that are updated independently. This means you can keep the pretrained model's parameters frozen, and only update the gradients of the prompt token embeddings. The results are comparable to the traditional method of training the entire model, and prompt tuning performance scales as model size increases.
The main difference is that the prefix parameters are inserted in **all** of the model layers, whereas prompt tuning only adds the prompt parameters to the model input embeddings. The prefix parameters are also optimized by a separate feed-forward network (FFN) instead of training directly on the soft prompts because it causes instability and hurts performance. The FFN is discarded after updating the soft prompts.

As a result, the authors found that prefix tuning demonstrates comparable performance to fully finetuning a model, despite having 1000x fewer parameters, and it performs even better in low-data settings.

Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning) for a step-by-step guide on how to train a model with prefix tuning.

## P-tuning
*In this work, we explore ""prompt tuning"", a simple yet effective mechanism for learning ""soft prompts"" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ""closes the gap"" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and
```

## Train

P-tuning uses a prompt encoder to optimize the prompt parameters, so you'll need to initialize the [`PromptEncoderConfig`] with several arguments:

- `task_type`: the type of task you're training on, in this case it is sequence classification or `SEQ_CLS`
- `num_virtual_tokens`: the number of virtual tokens to use, or in other words, the prompt
- `encoder_hidden_size`: the hidden size of the encoder used to optimize the prompt parameters

```py
peft_config = PromptEncoderConfig(task_type=""SEQ_CLS"", num_virtual_tokens=20, encoder_hidden_size=128)
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# P-tuning

[P-tuning](https://hf.co/papers/2103.10385) adds trainable prompt embeddings to the input that is optimized by a prompt encoder to find a better prompt, eliminating the need to manually design prompts. The prompt tokens can be added anywhere in the input sequence, and p-tuning also introduces anchor tokens for improving performance.

The abstract from the paper is:
-->

# Prompt tuning for causal language modeling

[[open-in-colab]]

Prompting helps guide language model behavior by adding some input text specific to a task. Prompt tuning is an additive method for only training and updating the newly added prompt tokens to a pretrained model. This way, you can use one pretrained model whose weights are frozen, and train and update a smaller set of prompt parameters for each downstream task instead of fully finetuning a separate model. As models grow larger and larger, prompt tuning can be more efficient, and results are even better as model parameters scale.

<Tip>

üí° Read [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691) to learn more about prompt tuning.

</Tip>

This guide will show you how to apply prompt tuning to train a [`bloomz-560m`](https://huggingface.co/bigscience/bloomz-560m) model on the `twitter_complaints` subset of the [RAFT](https://huggingface.co/datasets/ought/raft) dataset.
[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:

1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.
Take a look at [Prompt tuning for causal language modeling](../task_guides/clm-prompt-tuning) for a step-by-step guide on how to train a model with prompt tuning.

## Prefix tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/prefix-tuning.png""/>
</div>
<small>Optimize the prefix parameters for each task <a href=""https://hf.co/papers/2101.00190"">(image source)</a>.</small>

[Prefix tuning](https://hf.co/papers/2101.00190) was designed for natural language generation (NLG) tasks on GPT models. It is very similar to prompt tuning; prefix tuning also prepends a sequence of task-specific vectors to the input that can be trained and updated while keeping the rest of the pretrained model's parameters frozen.
- the prompt tokens can be inserted anywhere in the input sequence, and it isn't restricted to only the beginning
- the prompt tokens are only added to the input instead of adding them to every layer of the model
- introducing *anchor* tokens can improve performance because they indicate characteristics of a component in the input sequence

The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables GPT-like models to compete with BERT-like models on NLU tasks.

Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification) for a step-by-step guide on how to train a model with P-tuning.

## Multitask prompt tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png""/>
</div>
<small><a href=""https://hf.co/papers/2103.10385"">Multitask prompt tuning enables parameter-efficient transfer learning</a>.</small>
Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning) for a step-by-step guide on how to train a model with prefix tuning.

## P-tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/p-tuning.png""/>
</div>
<small>Prompt tokens can be inserted anywhere in the input sequence, and they are optimized by a prompt encoder <a href=""https://hf.co/papers/2103.10385"">(image source)</a>.</small>

[P-tuning](https://hf.co/papers/2103.10385) is designed for natural language understanding (NLU) tasks and all language models. 
It is another variation of a soft prompt method; P-tuning also adds a trainable embedding tensor that can be optimized to find better prompts, and it uses a prompt encoder (a bidirectional long-short term memory network or LSTM) to optimize the prompt parameters. Unlike prefix tuning though:
!--‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# P-tuning for sequence classification

It is challenging to finetune large language models for downstream tasks because they have so many parameters. To work around this, you can use *prompts* to steer the model toward a particular downstream task without fully finetuning a model. Typically, these prompts are handcrafted, which may be impractical because you need very large validation sets to find the best prompts. *P-tuning* is a method for automatically searching and optimizing for better prompts in a continuous space.

<Tip>

üí° Read [GPT Understands, Too](https://arxiv.org/abs/2103.10385) to learn more about p-tuning.

</Tip>
1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)
2. Prefix Tuning: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/), [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
3. P-Tuning: [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
4. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
5. AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)  
6. $(IA)^3$: [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
8. LoHa: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098)
9. LoKr: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650) based on [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://arxiv.org/abs/2309.14859) implementation
10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
11. OFT: [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)
‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Multitask Prompt Tuning

[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.

The abstract from the paper is:
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution. For example, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.
```

## Prompting vs fine-tuning

You can achieve great results by optimizing your prompts, however, you may still ponder whether fine-tuning a model 
would work better for your case. Here are some scenarios when fine-tuning a smaller model may be a preferred option:

- Your domain is wildly different from what LLMs were pre-trained on and extensive prompt optimization did not yield sufficient results. 
- You need your model to work well in a low-resource language.
- You need the model to be trained on sensitive data that is under strict regulations. 
- You have to use a small model due to cost, privacy, infrastructure or other limitations. 

In all of the above examples, you will need to make sure that you either already have or can easily obtain a large enough 
domain-specific dataset at a reasonable cost to fine-tune a model. You will also need to have enough time and resources 
to fine-tune a model.
A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in twice as many data points. Thus,
before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.

A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution, *e.g.*, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.
1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)
2. Prefix Tuning: [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
3. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf) 
4. P-Tuning: [GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf) 

## Use Cases

We explore many interesting use cases [here](https://github.com/huggingface/peft#use-cases). These are a few of the most interesting ones:
Fine-Tuning week of XLSR-Wav2Vec2 on 60 languages üåç

Welcome to the fine-tuning week! The goal of this week is to have state-of-the-art automatic speech recognition (ASR) models in as many languages as possible. The fine-tuning week ends on Friday, the 26th March at midnight PST time.

Participants are encouraged to fine-tune the pretrained [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53) checkpoint on one or more of the 60 languages of [Common Voice dataset](https://commonvoice.mozilla.org/en/datasets).
Furthermore, it is very much appreciated if participants fine-tune XLSR-Wav2Vec2 on a language that is not included in the Common Voice dataset.

Here is the question: What role does prompt tuning play in optimizing ASR workflows?
Here is answer 1:It reduces task-specific overhead and enhances multilingual performance
Here is answer 2: It eliminates the need for preprocessing
Here is answer3: It simplifies tuning for monolingual models
Here is answer 4: It focuses only on language-specific prompts",Based upon my analysis I believe Answer 1 'It reduces_task_specific_overhead_and_enhances_multilingual_performance' represents an accurate understanding relatedness between given context & question provided
