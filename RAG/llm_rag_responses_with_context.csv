query,ids,prompt,rag_response
"Which of the following statements is most accurate regarding data analysis, machine learning applications, and ethical considerations in the field of AI development?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13314, '2e33b999-becf-4891-af20-291e49782f33'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (30727, '5a7a6803-dbbe-48fe-8ccd-60d7bb61992d'), (30730, 'e011abf6-7dd0-49d9-bdb0-7971bf51aba6'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (28684, 'bb28814f-b957-4f53-9cce-1a717c5375f2'), (28686, '4fc97a06-1117-4160-a5b6-d2609b8c03a7'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (30739, 'fda68c03-224d-4b2c-8385-15822bc392ef'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (30744, '85546f48-12fb-4929-8f5f-4e0317439b79'), (30745, 'c90fe571-cba3-49fa-bdff-89957278007f'), (22555, '08f31da5-7c25-4b77-a7fd-0ca328732229'), (22557, '6e33e444-597f-40e9-895f-c917338f049e'), (22559, '572b5a06-f2cf-43cb-b6ea-11a0090973c2'), (29220, 'abad5408-6510-4345-8051-645600139f3b'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (23111, 'ce25fab8-4a21-467c-acb9-c565c4d93e4e'), (23112, '9b6b98a3-a1a5-41f1-a367-568a0a6438c5'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8839, 'fb867c7b-6a51-4baa-b761-3c649c36615f'), (8844, '2f887bfe-6371-40c0-960b-dfaabe7353ab'), (8845, '03fc6833-c795-47d4-859d-9c07e60d14ba'), (8850, 'f618b39d-a6d6-423b-b81f-c9e4b252f663'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (16576, '02456d8c-7662-44df-894b-be28d7834936'), (22723, '73aca2a4-7e12-4a27-8803-298209ed9076'), (22724, 'c3235445-b119-4ac8-b43c-c7b6031418a1'), (22725, '8999c10f-4058-42ae-9e57-b3f3824c096a'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17106, '4b18b019-9d95-4523-8663-5dee64012fae'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (1274, '1fa4cb29-360a-47e7-a8d4-b3282777ee5a'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (14121, 'c62b84e0-8e2d-4903-80c4-6e2af3aa7531'), (14122, '24b982d1-e49a-4a35-a283-24beb2dbc093'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (14137, 'b5a4d425-f032-4b3b-a16b-de458ace5de1'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28493, '494ebe5d-f3e9-45a3-aaf0-e4922b35e47c'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (28491, '1c342626-bd85-4bb0-ae55-965bf3ae0c93'), (28492, '1c509592-8204-437e-b480-94c5f8b97ba0'), (28499, 'bed81843-b31c-4629-9c10-c55b67932504'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (28501, 'db1e4893-0d81-48b8-8179-b74c365d7559'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (11610, '7abc171e-bc54-496c-a7d6-c25500c846c1'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (28517, '83d1dded-982b-449c-9af4-e5fecc3aac5b'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (28523, '07ad7b32-283d-4263-93de-b4633439fe56'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (21948, 'e1dd1005-94e8-4562-b9c3-6ba611211942'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (24547, 'e0531ff2-0600-4c1f-9fd3-b2346f1a8ed5'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (24560, 'ae4f8022-24f0-469e-9cb5-358e58b5773d'), (28662, 'b0d29c84-6a49-48b5-b84f-b90eae036639'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Furthermore, as the field of AI and machine learning continues to expand, the variety of use cases and applications proliferates. This makes it essential for us to stay up-to-date with the latest research, ethical considerations, and best practices. For this reason, promoting user collaboration is also vital to the sustainability of our platform. Namely, through our community features, such as the Community Tab, we encourage and foster collaborative solutions between repository authors, users, organizations, and our team.

## Consent as a Core Value
Advances in machine learning and other AI-related areas have flourished these past years partly thanks to the ubiquity of the open source culture in the Information and Communication Technologies (ICT) sector, which has permeated into ML research and development dynamics. Notwithstanding the benefits of openness as a core value for innovation in the field, (not so already) recent events related to the ethical and socio-economic concerns of development and use of machine learning models have spread a clear message: Openness is not enough. Closed systems are not the answer though, as the problem persists under the opacity of firms' private AI development processes.

## **Open source licenses do not fit all**
I’ll leave you with a recent quote from Meg in a [Science News article on Ethical AI](https://www.sciencenews.org/article/computer-science-history-ethics-future-robots-ai):

*“The most pressing problem is the diversity and inclusion of who’s at the table from the start. All the other issues fall out from there.” -Meg Mitchell.*

Thank you for listening to Machine Learning Experts!

<a href=""https://huggingface.co/support?utm_source=blog&utm_medium=blog&utm_campaign=ml_experts&utm_content=meg_interview_article""><img src=""/blog/assets/57_meg_mitchell_interview/Meg-cta.png""></a>
How to operationalize ethics in AI is an open research area. Although theory and scholarship on applied ethics and artificial intelligence have existed for decades, applied and tested practices for ethics within AI development have only begun to emerge within the past 10 years. This is partially a response to machine learning models – the building blocks of AI systems – outgrowing the benchmarks used to measure their progress, leading to wide-spread adoption of machine learning systems in a range of practical applications that affect everyday life. For those of us interested in advancing ethics-informed AI, joining a machine learning company founded in part on ethical principles, just as it begins to grow, and just as people across the world are beginning to grapple with ethical AI issues, is an opportunity to fundamentally shape what the AI of the future looks like. It’s a new kind of modern-day AI experiment: What does a technology company with ethics in mind _from the start_ look
This document is a work in progress and reflects a state of reflection as of May 2022. There is no consensus nor official definition of ""ethical AI"" and our considerations are very likely to change over time. In case of updates, we will reflect changes directly in this document while providing the rationale for changes and tracking the history of updates [through GitHub](https://github.com/huggingface/blog/commits/main/ethical-charter-multimodal.md). This document is not intended to be a source of truth about best practices for ethical AI. We believe that even though it is imperfect, thinking about the impact of our research, the potential harms we foresee, and strategies we can take to mitigate these harms is going in the right direction for the machine learning community. Throughout the project, we will document how we operationalize the values described in this document, along with the advantages and limitations we observe in the context of the project.

## Content policy
 
Here is the question: Which of the following statements is most accurate regarding data analysis, machine learning applications, and ethical considerations in the field of AI development?","Based on the context, the most accurate statement is:

""Operationalizing ethics in AI is an open research area, and although theory and scholarship on applied ethics and artificial intelligence have existed for decades, applied and tested practices for ethics within AI development have only begun to emerge within the past 10 years.""

This statement highlights the importance of considering ethical implications in AI development, and acknowledges that while there is a growing awareness of the need for ethical considerations, there is still much to be learned and developed in"
"Which of the following best explains the relationship between model optimization, framework switching, and user interaction in AI tools?","[(28673, '5d042da5-0429-46b6-bc0e-f68cfd69fe52'), (10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (15885, '971fc97e-36dc-4975-85ab-37248b6d6a03'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (29199, '79544ab4-7872-441a-b8d8-6e43de51aefa'), (29201, 'cdbabce2-0910-47ac-93ec-2dc35c8b6b91'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (20041, '0521172d-93c3-47ab-8eb4-57b9a744de8f'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (11341, 'a7a24881-ef8b-4b8d-89e4-aab048edafa8'), (18514, '2b8c705b-aa16-4343-9147-042e810a70a2'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (25178, '9c7f8ead-1eb7-4e8d-a89f-fb64c773bf7a'), (30812, 'f7e1e127-2236-4b1a-b0ad-74e1af99c668'), (30813, '890d4a98-83d6-48f3-8e71-fd5369636463'), (21602, '5ec88213-8b61-4e6e-a1bd-16c55ccd23cb'), (14439, '86de8893-816f-4b63-b9bc-ba2c4c214252'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (30885, '564ff6e5-7fac-4e5d-9ac9-9ba6a043b086'), (26277, '280024a5-5ad7-4fba-bdcd-244cbbbab2ae'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (8892, '91b7caf7-7012-4878-b2e4-e5e45737faa9'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (10954, '92fe4159-8084-4e9c-9be2-4e72a5e11928'), (26315, '04c7f547-b1f0-45bb-97eb-80e2344f4ddb'), (14028, 'f8a3f61d-2e71-49d7-88a9-4d17cdd22680'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (15583, '0bd03221-d721-44d6-b845-89e89513d7b5'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (20747, '5d8502c9-2531-492e-bf41-336d68415645'), (20749, 'b2e0610f-aa0c-4852-b1ad-2bf187ecf97e'), (1297, '0110063d-229c-4fc0-a7c3-f875aa009eb2'), (14100, '6f205204-a42d-4004-969f-8a6ab569e82a'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (15654, 'a087c142-5d7a-4b96-b71b-6def7546f9b5'), (1320, 'd5a5263a-a570-474e-8d13-92d45d7a6e95'), (17705, 'fe0b5515-c6a2-4316-b6a4-97cb643fb598'), (3376, '8d42215b-b037-49a6-bebe-e20500c36ab2'), (3377, '156b12d0-1aa0-4557-a4ca-1e911e59c896'), (24893, 'ba24282b-69bc-4162-bc20-24e64126aaec'), (24895, '94893f68-b23d-4e91-b83c-fa4b89e68d79'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (8563, '46031f7f-3f23-4de5-882a-d8559db6cccb'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (22397, '31d21656-3ce7-4da6-b134-33cbd4345049'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (24959, '5358d410-e403-4ab7-93bd-554c6fcb2c38'), (17792, '5cb382a6-e1ff-47c1-8d18-2b40951e655c'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (17796, 'ef4db9a3-d486-43f2-8c7c-a26573cc88ea'), (24453, 'f15113e9-5f47-4233-a4a3-52de7f36d3d5'), (24969, '4f152556-8bfa-4a38-8030-becaa6e743bc'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (5520, 'e2f816ac-dce2-4a36-9d80-a615b27cd149'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (3993, '193bb31e-a1b5-4129-bd41-177f89121efd'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (23454, 'bb59e1c0-8193-40ce-a450-9efc2ecc6226'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (6561, 'ee35c81c-c680-4dfc-852b-7c11aae2a074'), (6562, '12d6fc12-1e12-401d-abae-cc180802380d'), (23463, '82a0f702-913c-434f-a161-9ca884ba22ce'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (435, '2b50a808-55c4-4657-8cbb-fd2b852b828f'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (13757, '1ca945fe-3ad2-440a-9679-60b9498487d1'), (13758, '99a9e0d3-b693-4445-af36-b42b304b70a2'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (19390, 'c0f4fdad-c5e6-4511-aaf0-4cc870c059c1'), (29634, 'd756cc17-3381-4c73-99de-56f9d281f772'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (23498, '4c725dcf-74c2-45a9-8efb-757e4fd0c5b6'), (19403, 'c0f08689-c366-48f2-a497-c82f608735e2'), (27599, '1a8e11c0-096a-4932-9583-00f0353d1ff8'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (29141, 'd7f4733c-7048-44a3-a055-aed0a75672f9'), (29142, 'c218b361-c6b2-42be-a4d3-ba2a479b97b9'), (15835, '888ea0a4-ad95-4449-9478-48ba7798a983'), (15838, 'a7cc273a-40da-4c92-a3f3-afaa3e71b4c5'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (8176, '2cc6d0c0-3a2d-4ebc-8266-4d2ee6bf3300'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (8188, '2a39c000-13a6-48da-bb73-63e7c4f60311'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: By exposing a graph with standardized operators and data types, ONNX makes it easy to
switch between frameworks. For example, a model trained in PyTorch can be exported to
ONNX format and then imported in TensorFlow (and vice versa).

Once exported to ONNX format, a model can be:
- optimized for inference via techniques such as [graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) and [quantization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization). 
- run with ONNX Runtime via [`ORTModelForXXX` classes](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort),
which follow the same `AutoModel` API as the one you are used to in 🤗 Transformers.
- run with [optimized inference pipelines](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines),
which has the same API as the [`pipeline`] function in 🤗 Transformers.
## How does AI vs. AI works?

AI vs. AI is an open-source tool developed at Hugging Face **to rank the strength of reinforcement learning models in a multi-agent setting**.

The idea is to get a **relative measure of skill rather than an objective one** by making the models play against each other continuously and use the matches results to assess their performance compared to all the other models and consequently get a view of the quality of their policy without requiring classic metrics.

The more agents are submitted for a given task or environment, **the more representative the rating becomes**.

To generate a rating based on match results in a competitive environment, we decided to base the rankings on the [ELO rating system](https://en.wikipedia.org/wiki/Elo_rating_system).
## New Core ML Optimizations

Core ML is a mature framework that allows machine learning models to run efficiently on-device, taking advantage of all the compute hardware in Apple devices: the CPU, the GPU, and the Neural Engine specialized in ML tasks. On-device execution is going through a period of extraordinary interest triggered by the popularity of models such as Stable Diffusion and Large Language Models with chat interfaces. Many people want to run these models on their hardware for a variety of reasons, including convenience, privacy, and API cost savings. Naturally, many developers are exploring ways to run these models efficiently on-device and creating new apps and use cases. Core ML improvements that contribute to achieving that goal are big news for the community!

The Core ML optimization changes encompass two different (but complementary) software packages:
Keywords: Knowledge Extraction, Knowledge Graphs

## [Nebuly](https://github.com/nebuly-ai/nebuly)

Nebuly is the next-generation platform to monitor and optimize your AI costs in one place. The platform connects to all your AI cost sources (compute, API providers, AI software licenses, etc) and centralizes them in one place to give you full visibility on a model basis. The platform also provides optimization recommendations and a co-pilot model that can guide during the optimization process. The platform builds on top of the open-source tools allowing you to optimize the different steps of your AI stack to squeeze out the best possible cost performances.

Keywords: Optimization, Performance, Monitoring

## [imaginAIry](https://github.com/brycedrennan/imaginAIry)
* When choosing the model to work with, the latest and most capable models are likely to perform better. 
* Start with a simple and short prompt, and iterate from there.
* Put the instructions at the beginning of the prompt, or at the very end. When working with large context, models apply various optimizations to prevent Attention complexity from scaling quadratically. This may make a model more attentive to the beginning or end of a prompt than the middle.
* Clearly separate instructions from the text they apply to - more on this in the next section. 
* Be specific and descriptive about the task and the desired outcome - its format, length, style, language, etc.
* Avoid ambiguous descriptions and instructions.
* Favor instructions that say ""what to do"" instead of those that say ""what not to do"".
* ""Lead"" the output in the right direction by writing the first word (or even begin the first sentence for the model).
 
Here is the question: Which of the following best explains the relationship between model optimization, framework switching, and user interaction in AI tools?","Based on the context, the relationship between model optimization, framework switching, and user interaction in AI tools can be explained as follows:

Model optimization (e.g., via techniques such as graph optimization and quantization) enables the efficient deployment of AI models on various devices and frameworks. This, in turn, allows for framework switching (e.g., from PyTorch to TensorFlow) and facilitates the use of optimized inference pipelines. The optimized models can then be used in various AI tools, such as AI vs"
"Which option best outlines the main benefits and applications of using DuckDB, Gradio, and Hugging Face Spaces in conjunction with machine learning ethics?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (528, 'b3428881-1979-4d8f-8456-ef7f9ab8de07'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (8216, 'e4a9d73e-f3dd-4409-b859-83511a07464d'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23073, 'c8719aa6-c675-4f8d-a578-bcb628162ea5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23078, '773ea43e-498d-4d36-8809-1e19c760b774'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (23083, '005603d6-7fc7-4893-8c08-a335d63ca5d9'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (15456, '6fd69e51-654d-49d7-a168-66db7b25fe3a'), (10848, 'e24b0ae6-43f0-498e-bd66-a33c5139c50d'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (23153, '49b4c809-71ff-410a-b32d-acc1467103b4'), (21623, '32bea9e3-44f8-4464-a8dd-d2dcbc3a2102'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (5783, 'bd71387d-d163-487f-b2f3-474c6bafee76'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24793, '6eeb4cd3-a092-447d-9528-a03119bf4a08'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (3439, '0af44d7a-c74c-4934-9280-08873b27666b'), (8578, 'b6835785-bd61-41a6-94d0-a59988d7384f'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8593, '0724f652-10b8-4c86-8dd0-53285edc20d2'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (13728, '91c1a2c4-7e1d-4ddf-8e7c-768b77a28bbc'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (9174, '3767ffbd-cac9-45c5-b684-68ec257368f6'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (25050, '4ea83d3e-bb4a-4ab5-b25a-c79f5c52c46a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## What are Hugging Face Spaces & Gradio?

### Gradio

Gradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.

Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

## Setting up a Gradio Demo for JoJoGAN
![You can now find interactive demos under ArXiv papers](/blog/assets/arxiv/recording.gif)

Since its launch in October 2021, Hugging Face Spaces has been used to build and share over 12,000 open-source machine learning demos crafted by the community. With Spaces, Hugging Face users can share, explore, discuss models, and build interactive applications that enable anyone with a browser to try them out without having to run any code. These demos are built using open-source tools such as the Gradio and Streamlit Python libraries, and leverage models and datasets available on the Hugging Face Hub.
Demos and GUIs built with Gradio give the power of machine learning to more and more people because they allow non-technical users to access, use, and give feedback on models. And our acquisition by Hugging Face is the next step in this ongoing journey of accessibility. Hugging Face has already radically democratized machine learning so that any software engineer can use state-of-the-art models with a few lines of code. By working together with Hugging Face, we're taking this even further so that machine learning is accessible to literally anyone with an internet connection and a browser. With Hugging Face, we are going to keep growing Gradio and make it the best way to share your machine learning model with anyone, anywhere 🚀
--
title: ""Showcase Your Projects in Spaces using Gradio""
thumbnail: /blog/assets/28_gradio-spaces/thumbnail.png
authors:
- user: merve
---

# Showcase Your Projects in Spaces using Gradio


It's so easy to demonstrate a Machine Learning project thanks to [Gradio](https://gradio.app/). 

In this blog post, we'll walk you through:
- the recent Gradio integration that helps you demo models from the Hub seamlessly with few lines of code leveraging the [Inference API](https://huggingface.co/inference-api).
- how to use Hugging Face Spaces to host demos of your own models.



## Hugging Face Hub Integration in Gradio

You can demonstrate your models in the Hub easily. You only need to define the [Interface](https://gradio.app/docs#interface) that includes:

- The repository ID of the model you want to infer with
- A description and title
- Example inputs to guide your audience
Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

### Hugging Face Models

Hugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)

## How did Hugging Face help the ONNX Model Zoo?
 
Here is the question: Which option best outlines the main benefits and applications of using DuckDB, Gradio, and Hugging Face Spaces in conjunction with machine learning ethics?","There is no mention of DuckDB in the provided context. The context only discusses Gradio and Hugging Face Spaces in the context of machine learning demos and hosting. Therefore, it is not possible to outline the main benefits and applications of using DuckDB, Gradio, and Hugging Face Spaces in conjunction with machine learning ethics."
"How do 8-bit models, deep learning workflows, and Hugging Face’s platform contribute to the practical deployment of AI solutions?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (27672, '59487233-b752-42ea-aa5a-8cb853def35f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8269, '63e383a9-dbfa-4c9f-a286-0668a682d313'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (26754, '167102ca-f572-4d47-aa9b-041aeda233f2'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (21151, 'c1f9f41a-2ae3-4bdd-b40e-a54327d158f4'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (5806, '5054cac8-6958-4df8-9986-f0bb2157877f'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (7433, '8b4a851e-9dbc-46a4-a7e6-6b51652c06b6'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (30519, 'e988d4c0-44ec-4a7f-a800-615f96ae0da2'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14664, '10a5d466-9698-40d2-aa8e-c005dc0608d1'), (16716, 'd9f215fb-7aa2-4b50-a40d-d2e690dc5d03'), (9562, 'b75f1239-64b9-4c25-9106-2b9ff2961471'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (9567, '21f12568-4fc0-4680-b763-b7907ea0ea81'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (3985, 'ecedc47d-0616-441d-a202-07ae59c5cb7a'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (6038, '4398309e-75d4-425a-b50a-40a436da5bd0'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (29593, '6a3d74c2-ed43-42ed-8939-0c89a33f7be8'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (24477, '108827d3-9954-417d-a4ca-4f275ac82faf'), (27040, '56f391b1-2a20-4be2-af9f-8f26721aaaf2'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31672, '94b34b38-3568-488b-94eb-579ac5ccf43e'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (11711, 'f0aab09f-a3dc-496a-9cd8-08e8d39a1306'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25050, '4ea83d3e-bb4a-4ab5-b25a-c79f5c52c46a'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10738, 'd25343c4-9005-48e4-a890-49eaa86d0a81'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: There have been significant advances in new Transformer and Diffuser machine learning models that process and generate text, audio, and images. However, most of these popular generative AI models are not publicly available, widening the gap of machine learning capabilities between the largest tech companies and everyone else. To counter this trend, AWS and Hugging Face are partnering to contribute next-generation models to the global AI community and democratize machine learning. Through the strategic partnership, Hugging Face will leverage AWS as a preferred cloud provider so developers in Hugging Face’s community can access AWS’s state-of-the-art tools (e.g., [Amazon SageMaker](https://aws.amazon.com/sagemaker), [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)) to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their
_“With the new Hugging Face Hub model catalog, natively integrated within Azure Machine Learning, we are opening a new page in our partnership with Microsoft, offering a super easy way for enterprise customers to deploy Hugging Face models for real-time inference, all within their secure Azure environment.”_ said Julien Simon, Chief Evangelist at Hugging Face. 

_""The integration of Hugging Face's open-source models into Azure Machine Learning represents our commitment to empowering developers with industry-leading AI tools,""_ said John Montgomery, Corporate Vice President, Azure AI Platform at Microsoft. _""This collaboration not only simplifies the deployment process of large language models but also provides a secure and scalable environment for real-time inferencing. It's an exciting milestone in our mission to accelerate AI initiatives and bring innovative solutions to the market swiftly and securely, backed by the power of Azure infrastructure.""_
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
</div>

## What's the Hugging Face Hub?

We are helping the community work together towards the goal of advancing Machine Learning 🔥.

The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demos in which people can easily collaborate in their ML workflows. The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.

No single company, including the Tech Titans, will be able to “solve AI” by themselves – the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone 🚀.
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
 
Here is the question: How do 8-bit models, deep learning workflows, and Hugging Face’s platform contribute to the practical deployment of AI solutions?","The context does not provide direct information about 8-bit models, deep learning workflows, and their contribution to the practical deployment of AI solutions. The provided information focuses on the partnership between AWS and Hugging Face, and Hugging Face's partnership with Microsoft, to democratize machine learning and make next-generation models available to the global AI community. It also describes the Hugging Face Hub as a platform for collaboration and sharing of open-source machine learning resources. Therefore, it is not possible to provide a comprehensive"
"How does the integration of model optimization, ethical deployment, and large dataset analysis lead to more efficient and scalable AI systems across various platforms?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (10250, '3d60dd94-162e-41c3-b924-86cd1b72a0fc'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (22557, '6e33e444-597f-40e9-895f-c917338f049e'), (2591, 'e61cc32e-e588-487d-a8be-8a9025bff659'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (1570, '2ffb0ee7-0b83-42e3-bfd2-dd6a862b3727'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (23636, '847215c3-b4a6-4e0a-8064-28fd730a92aa'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (22129, '6e12827f-d0c2-45d5-93c5-c456a0f17de3'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26750, '22aa9771-c45d-48a8-b470-7a4e1ab9c4a0'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (7898, 'a0553017-647f-4a6a-ba47-f2e453034fa7'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (18672, '51d56682-3787-49c9-b694-c8ccc94be302'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (14122, '24b982d1-e49a-4a35-a283-24beb2dbc093'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (10067, '5cc89bb1-7995-4eb1-944f-68108dc5ed63'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (28501, 'db1e4893-0d81-48b8-8179-b74c365d7559'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (16225, '8209b394-4270-47c3-af24-bd1f76be97dc'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (3993, '193bb31e-a1b5-4129-bd41-177f89121efd'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (27048, 'e3873222-b7e8-4f6f-9732-ae991af045de'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (21948, 'e1dd1005-94e8-4562-b9c3-6ba611211942'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (19390, 'c0f4fdad-c5e6-4511-aaf0-4cc870c059c1'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (24550, 'f8edec76-5c02-41b0-a9ee-c83e1d736419'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (10738, 'd25343c4-9005-48e4-a890-49eaa86d0a81'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31735, '31b3ae48-f7ec-45ee-adb9-0d7d5e3541ca'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68'), (13310, 'c6400126-c261-4fd7-aa82-0a4a91b16a06')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
In order to become the best platform for users to jump into the world of AI, we’re working hard to provide the best tools to successfully train, tune, and deploy models. Not only will we continue contributing to Hugging Face, but we’ve also started releasing improvements to our other open source projects. A recent [MLflow](https://www.databricks.com/blog/2023/04/18/introducing-mlflow-23-enhanced-native-llm-support-and-new-features.html) release added support for the transformers library, OpenAI integration, and Langchain support. We also announced [AI Functions](https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html) within Databricks SQL that lets users easily integrate OpenAI (or their own deployed models in the future) into their queries. To top it all off, we also released a [PyTorch distributor](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html) for Spark to
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
The AI ecosystem evolves quickly, and more and more specialized hardware along with their own optimizations are emerging every day.
As such, Optimum enables developers to efficiently use any of these platforms with the same ease inherent to Transformers.

🤗 Optimum is distributed as a collection of packages - check out the links below for an in-depth look at each one.
We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [BigScience](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [world's largest open source multilingual language model](https://twitter.com/BigScienceLLM) 🌸

⚠️ But there’s still a huge amount of work left to do.
 
Here is the question: How does the integration of model optimization, ethical deployment, and large dataset analysis lead to more efficient and scalable AI systems across various platforms?","The integration of model optimization, ethical deployment, and large dataset analysis leads to more efficient and scalable AI systems across various platforms by:

1. Enabling efficient use of large datasets through data transformations, which Spark is designed to handle, to augment AI models and achieve the best performance within specific domains.
2. Providing cost-effectiveness and performance through the integration of Spark with Hugging Face, while retaining pipeline integration.
3. Facilitating open-source support and contributions to the community, ensuring transparency, reproduc"
"How does the integration of Gradio’s user interface, DuckDB’s large dataset analysis, and ethical considerations in AI deployment, particularly in Hugging Face Spaces, lead to a more sustainable machine learning workflow?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (528, 'b3428881-1979-4d8f-8456-ef7f9ab8de07'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (16416, '1b6290e2-d918-4435-af5a-004a7c4401ca'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (22559, '572b5a06-f2cf-43cb-b6ea-11a0090973c2'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (23083, '005603d6-7fc7-4893-8c08-a335d63ca5d9'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (23153, '49b4c809-71ff-410a-b32d-acc1467103b4'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (5783, 'bd71387d-d163-487f-b2f3-474c6bafee76'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (17068, 'e8d4188d-34a3-4d81-96d0-85488e9eb25a'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24793, '6eeb4cd3-a092-447d-9528-a03119bf4a08'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (17119, '3362d998-e1dd-4fe9-838e-5136a991278a'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (14056, 'e4acfaa9-c866-4fa3-b263-ebd8b7794cf3'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (1297, '0110063d-229c-4fc0-a7c3-f875aa009eb2'), (14103, 'c1b29651-dd3f-4521-998a-070409c9ecf7'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (6968, '7afe2662-ca68-4a45-bf15-119a7ec97e18'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (16715, 'e3eb35de-68db-4c66-b9da-4c622cd55f2a'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (11124, '7b9292cb-0cb7-4972-91b4-f4ea38ac87e7'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (16762, '0a98d923-bf03-4726-a7c3-4f2c56457e83'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8593, '0724f652-10b8-4c86-8dd0-53285edc20d2'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (29079, '7d321d5b-7be3-4422-91d3-0c7a768c839f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (11175, 'afd12b90-9db3-41b9-b024-9541936ea7dc'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (6062, '455e25d7-6c17-42c3-8b23-2713e2cfb070'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (25050, '4ea83d3e-bb4a-4ab5-b25a-c79f5c52c46a'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (24040, 'abf451a7-7b6d-48cf-ba04-368d0a6f7410'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (14320, '18b2a378-fe08-4e21-bc4a-6b7b9c5ad71d'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Using Hugging Face Integrations

Related spaces: https://huggingface.co/spaces/gradio/en2es
Tags: HUB, SPACES, EMBED

Contributed by <a href=""https://huggingface.co/osanseviero"">Omar Sanseviero</a> 🦙

## Introduction

The Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). 

Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API
Gradio and W&B Integration

Related spaces: https://huggingface.co/spaces/akhaliq/JoJoGAN
Tags: WANDB, SPACES
Contributed by Gradio team

## Introduction

In this Guide, we'll walk you through:

- Introduction of Gradio, and Hugging Face Spaces, and Wandb
- How to setup a Gradio demo using the Wandb integration for JoJoGAN
- How to contribute your own Gradio demos after tracking your experiments on wandb to the Wandb organization on Hugging Face


## What is Wandb?

Weights and Biases (W&B) allows data scientists and machine learning scientists to track their machine learning experiments at every stage, from training to production. Any metric can be aggregated over samples and shown in panels in a customizable and searchable dashboard, like below:

<img alt=""Screen Shot 2022-08-01 at 5 54 59 PM"" src=""https://user-images.githubusercontent.com/81195143/182252755-4a0e1ca8-fd25-40ff-8c91-c9da38aaa9ec.png"">

## What are Hugging Face Spaces & Gradio?

### Gradio
Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API

Hugging Face has a free service called the [Inference API](https://huggingface.co/inference-api), which allows you to send HTTP requests to models in the Hub. For transformers or diffusers-based models, the API can be 2 to 10 times faster than running the inference yourself. The API is free (rate limited), and you can switch to dedicated [Inference Endpoints](https://huggingface.co/pricing) when you want to use it in production. Gradio integrates directly with the Hugging Face Inference API so that you can create a demo simply by specifying a model's name (e.g. `Helsinki-NLP/opus-mt-en-es`), like this:

```python
import gradio as gr

demo = gr.load(""Helsinki-NLP/opus-mt-en-es"", src=""models"")

demo.launch()
--
title: ""Showcase Your Projects in Spaces using Gradio""
thumbnail: /blog/assets/28_gradio-spaces/thumbnail.png
authors:
- user: merve
---

# Showcase Your Projects in Spaces using Gradio


It's so easy to demonstrate a Machine Learning project thanks to [Gradio](https://gradio.app/). 

In this blog post, we'll walk you through:
- the recent Gradio integration that helps you demo models from the Hub seamlessly with few lines of code leveraging the [Inference API](https://huggingface.co/inference-api).
- how to use Hugging Face Spaces to host demos of your own models.



## Hugging Face Hub Integration in Gradio

You can demonstrate your models in the Hub easily. You only need to define the [Interface](https://gradio.app/docs#interface) that includes:

- The repository ID of the model you want to infer with
- A description and title
- Example inputs to guide your audience
## What are Hugging Face Spaces & Gradio?

### Gradio

Gradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.

Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

## Setting up a Gradio Demo for JoJoGAN
 
Here is the question: How does the integration of Gradio’s user interface, DuckDB’s large dataset analysis, and ethical considerations in AI deployment, particularly in Hugging Face Spaces, lead to a more sustainable machine learning workflow?","The context does not provide a direct answer to this question. The question seems to be asking about the integration of Gradio, DuckDB, and ethical considerations in AI deployment, particularly in Hugging Face Spaces, and how it leads to a more sustainable machine learning workflow. However, the context does not mention DuckDB or ethical considerations in AI deployment. It only discusses the integration of Gradio with Hugging Face Spaces and the Hugging Face Inference API. Therefore, it is not possible to provide"
"What is the primary benefit of utilizing 8-bit models, Hugging Face Spaces, and Gradio when training and deploying AI models, particularly in terms of resource efficiency and user-centered design?","[(17413, '5c272bf9-ff61-4ced-9930-7c174b92dc98'), (10248, 'ff896a10-9c03-49b8-9f92-67794ee39f38'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (528, 'b3428881-1979-4d8f-8456-ef7f9ab8de07'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (16416, '1b6290e2-d918-4435-af5a-004a7c4401ca'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (15456, '6fd69e51-654d-49d7-a168-66db7b25fe3a'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (21111, 'cba3516c-54a9-4577-bc35-4b9589b89052'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (5783, 'bd71387d-d163-487f-b2f3-474c6bafee76'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (20634, 'b26ceca9-16f3-491c-a4e2-ce69a77a514c'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (6358, 'c5b8b78f-d9a9-483d-8f87-70642fe0c191'), (6359, 'd3604ded-fdc8-4b98-8dfc-e74be70af11d'), (4829, '2327c019-21d7-4362-844c-4ec05792b517'), (15070, 'b79c805d-f30e-4344-a16b-e77f25c0719f'), (14056, 'e4acfaa9-c866-4fa3-b263-ebd8b7794cf3'), (21229, '0daeed9a-1966-4934-bab7-9c9eb4e3872c'), (25851, '2ad08d82-5f5e-43ff-81e5-837f3ec89586'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (23813, 'e529e90a-7751-49cc-8418-262a566b79d9'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (6416, 'ede4908b-9f07-4116-8fe3-4094434bf791'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (16683, '8816fbb3-bf82-4453-886f-29f192a503f6'), (30510, 'c72a7174-000b-4d01-9d21-56dd6dc3d5ab'), (1326, 'fd43338d-6799-4283-b2c7-e3b2b1bc1ed3'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (30519, 'e988d4c0-44ec-4a7f-a800-615f96ae0da2'), (6967, '2116c64e-45cb-4613-a3b8-401a488e255f'), (28994, 'b2fdd130-8cb1-4172-828f-ccbf88b499bd'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14664, '10a5d466-9698-40d2-aa8e-c005dc0608d1'), (1864, '9ba52445-c296-42f0-8c37-a67e6f6728e9'), (16715, 'e3eb35de-68db-4c66-b9da-4c622cd55f2a'), (16716, 'd9f215fb-7aa2-4b50-a40d-d2e690dc5d03'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (30545, 'cd0b21b5-7027-43c2-846c-b2ed65d53de5'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (9553, '7bf59ff6-d752-4eb0-9dc8-8aacda143835'), (5474, '1158d6f0-fad9-4bbb-babf-d6178165afc7'), (29037, 'edcf8e80-a695-425f-8db8-bdf35cd8beea'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (25459, '7e9e55e8-8027-4677-b297-11f6cebd0a17'), (15736, 'c73ef376-0e72-4203-88c0-970db6b66c69'), (5508, '18f70486-80e2-4c2d-a2f1-f9e6174711dd'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (5520, 'e2f816ac-dce2-4a36-9d80-a615b27cd149'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (24477, '108827d3-9954-417d-a4ca-4f275ac82faf'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (30629, '7cf3310b-91ee-4c5f-a7ea-66073b3a9197'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (7595, 'e9a0cc5c-98d6-4e8d-8b64-1c28f8f15d45'), (7596, '61cfe7c8-b21a-4aa4-b4c3-9612db7eabb3'), (7599, 'c64e7824-8c85-482f-a5a9-75513d131cd6'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (7604, '76a18b88-7a2a-4930-8f66-1b41521667f6'), (4540, '9075baf3-0984-4207-aedf-b57c9ef7446c'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (11727, 'f0c4c308-23bc-4116-8f86-1ad541066b86'), (2513, '46aef253-eecf-48f6-8c8a-26e8d72a48f7'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (31728, '5ccf1245-717b-44e0-837a-e164da6b012a'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
The obvious benefit of working with smaller models is a significant reduction in inference latency. Here’s a [video](https://drive.google.com/file/d/1Iv5_aV8mKrropr9HeOLIBT_7_oYPmgNl/view?usp=sharing) demonstrating real-time text generation with the MPT-7B-chat model on a single socket Intel Sapphire Rapids CPU with 32 cores and a batch size of 1.

In this example, we ask the model: “*What is the role of Hugging Face in democratizing NLP?*”. This sends the following prompt to the model:
""*A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: What is the role of Hugging Face in democratizing NLP? ASSISTANT:*""
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
## Some Thoughts

Sentence Transformers and Gradio are great choices for this kind of project! ST has the utility functions that we need for quickly generating embeddings, as well as for running semantic search with minimal code. Having access to a large collection of pre-trained models is also extremely helpful, since we don’t need to create and train our own models for this kind of stuff. Building our demo in Gradio means we only have to focus on coding in Python, and [deploying Gradio projects to Hugging Face Spaces is also super simple](https://huggingface.co/docs/hub/spaces-sdks-gradio)!

There’s a ton of other stuff I wish I’d had the time to build into this project, such as these ideas that I might explore in the future:
 
Here is the question: What is the primary benefit of utilizing 8-bit models, Hugging Face Spaces, and Gradio when training and deploying AI models, particularly in terms of resource efficiency and user-centered design?","The primary benefit of utilizing 8-bit models, Hugging Face Spaces, and Gradio when training and deploying AI models is the significant reduction in resource requirements and latency, leading to improved resource efficiency and user-centered design. This is achieved through serverless deployment, low-latency interactions, and the ability to run models without internet access, resulting in faster responses and a smoother user experience."
"How do the integration of frameworks for model deployment, legal considerations, performance optimization, and ethics across various platforms ensure that AI solutions are both efficient and aligned with societal needs?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (14875, '863f500b-8805-487f-a9d0-1a5268b9eb36'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29231, '1e0d8d33-ae9b-45aa-93f5-0c23e829f89b'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26750, '22aa9771-c45d-48a8-b470-7a4e1ab9c4a0'), (25215, '1583a17d-46d2-4646-b011-3599d120d3a9'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (7898, 'a0553017-647f-4a6a-ba47-f2e453034fa7'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (28501, 'db1e4893-0d81-48b8-8179-b74c365d7559'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (11614, 'b2f56d1f-d474-4e31-ac9b-426853bda354'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (16225, '8209b394-4270-47c3-af24-bd1f76be97dc'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (19390, 'c0f4fdad-c5e6-4511-aaf0-4cc870c059c1'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (13778, '8411db58-e2ed-429e-a933-34aeaf22efb7'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (29141, 'd7f4733c-7048-44a3-a055-aed0a75672f9'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (10735, '1815b561-b04c-4411-a95c-7b9cdf9048e4'), (10738, 'd25343c4-9005-48e4-a890-49eaa86d0a81'), (5622, 'ccda777f-15b6-4a2a-9ba6-f00ffacce0d2'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The #1 comment we get from customers is that no single model will rule them all. They understand the value of building the best model for each use case to maximize its relevance on company data while optimizing the compute budget. Of course, privacy and intellectual property are also top concerns, and customers want to ensure they maintain complete control.

As AI finds its way into every department and business unit, customers also realize the need to train and deploy many different models. In a large multinational organization, this could mean running hundreds, even thousands, of models at any time. Given the pace of AI innovation, newer and higher-performance model architectures will also lead customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.
* Accountability mechanisms should **focus on all stages of the ML development process**. The societal impact of a full AI-enabled system depends on choices made at every stage of the development in ways that are impossible to fully predict, and assessments that only focus on the deployment stage risk incentivizing surface-level compliance that fails to address deeper issues until they have caused significant harm.
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
One approach is to develop new models that represent society as we wish it to be. This suggests creating AI systems that don't just mimic the patterns in our data, but actively promote more equitable and fair perspectives. However, this approach raises a crucial question: whose values are we programming into these models? Values differ across cultures, societies, and individuals, making it a complex task to define what an ""ideal"" society should look like within an AI model. The question is indeed complex and multifaceted. If we avoid reproducing existing societal biases in our AI models, we're faced with the challenge of defining an ""ideal"" representation of society. Society is not a static entity, but a dynamic and ever-changing construct. Should AI models, then, adapt to the changes in societal norms and values over time? If so, how do we ensure that these shifts genuinely represent all groups within society, especially those often underrepresented?
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
 
Here is the question: How do the integration of frameworks for model deployment, legal considerations, performance optimization, and ethics across various platforms ensure that AI solutions are both efficient and aligned with societal needs?","The integration of frameworks for model deployment, legal considerations, performance optimization, and ethics across various platforms ensures that AI solutions are both efficient and aligned with societal needs by:

* Enabling the deployment of multiple models, each optimized for specific use cases, while maintaining control over privacy and intellectual property.
* Fostering accountability mechanisms that focus on all stages of the ML development process, rather than just deployment, to address deeper issues and promote fair perspectives.
* Leveraging open-source projects and collaborations to provide flexibility"
"What role do legal and ethical considerations play in the deployment of AI models using platforms like Gradio, DuckDB, and Hugging Face Spaces, especially when considering the impact of 8-bit model optimization on AI performance?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (13324, 'e927c83d-2a0f-4409-a00f-c0599d0ce5aa'), (13325, '16d0f9ea-2690-41bb-b5b7-ea15fa1b01e6'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (24635, 'ec91ea33-772d-48d1-bd05-a8519413a1c3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (23111, 'ce25fab8-4a21-467c-acb9-c565c4d93e4e'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (23153, '49b4c809-71ff-410a-b32d-acc1467103b4'), (21623, '32bea9e3-44f8-4464-a8dd-d2dcbc3a2102'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (8393, '1b5f1552-b0c9-4deb-bb05-011e041936e9'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (4442, '274884d8-70a8-4a04-900c-0d6dc0e00410'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (11614, 'b2f56d1f-d474-4e31-ac9b-426853bda354'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (4444, '309a3203-b151-498e-b083-5af2cd6c30ce'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (4452, '0aadb3d5-551b-4a7a-bbb5-fdb162e65d74'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (6062, '455e25d7-6c17-42c3-8b23-2713e2cfb070'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27112, '831fcd7a-7990-4660-a592-546aa8d108bf'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addressing-bias-throughout-the-ml-development-cycle), there is no one single way to develop machine learning systems, and any of the steps in the development and deployment process can present opportunities to tackle bias, from who is included at the start, to defining the task, to curating the dataset, training the model, and more. This also applies to multimodal models and the ways in which they are ultimately deployed or productionized in society, since the consequences of bias in multimodal models will depend on their downstream use. For instance, if a model is used in a human-in-the-loop setting for graphic design (such as those created by [RunwayML](https://runwayml.com/ai-magic-tools/text-to-image/)), the user has numerous occasions to detect and correct bias, for instance by changing the prompt or the generation options. However, if a model is used as part of a [tool to help forensic
Using Hugging Face Integrations

Related spaces: https://huggingface.co/spaces/gradio/en2es
Tags: HUB, SPACES, EMBED

Contributed by <a href=""https://huggingface.co/osanseviero"">Omar Sanseviero</a> 🦙

## Introduction

The Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). 

Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API
 
Here is the question: What role do legal and ethical considerations play in the deployment of AI models using platforms like Gradio, DuckDB, and Hugging Face Spaces, especially when considering the impact of 8-bit model optimization on AI performance?","Based on the context, legal and ethical considerations play a crucial role in the deployment of AI models using platforms like Gradio, DuckDB, and Hugging Face Spaces. The context highlights the importance of transparency, bias mitigation, and responsible AI development. Specifically, it mentions:

1. Transparency: The development of state-of-the-art AI models should be more transparent, and efforts like IDEFICS aim to reproduce and provide the AI community with systems that match the capabilities of large proprietary models.
2."
"How do the integration of 8-bit models, user interfaces for interactivity, large-scale data analysis, and ethical AI deployment contribute to creating AI systems that are both efficient and socially responsible?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13318, '32e34acb-80e7-4631-b573-ad387764d708'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (29213, 'ecfbdc64-5f35-4f83-8641-32ac46207dca'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29229, 'bba3c00e-cd0b-4f15-b534-103a5596462e'), (29228, '318fac81-dc01-4501-a870-7e820e6f37d0'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26750, '22aa9771-c45d-48a8-b470-7a4e1ab9c4a0'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (8836, 'c289aa2c-ea0e-4e05-b457-9786a0c345e3'), (8845, '03fc6833-c795-47d4-859d-9c07e60d14ba'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (14122, '24b982d1-e49a-4a35-a283-24beb2dbc093'), (14124, 'f4448196-d767-48e0-8f34-d840dce4e29b'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (14131, 'e5c14edb-a4ef-4578-98c2-52b276c71434'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14149, 'e59bf5b6-e780-4c3f-91ef-15a3634e0300'), (14151, '43b41e82-11b5-4653-847a-a40040419d1b'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (5621, 'f5939905-cf0b-4ff5-be82-731d069c0b91'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
How to operationalize ethics in AI is an open research area. Although theory and scholarship on applied ethics and artificial intelligence have existed for decades, applied and tested practices for ethics within AI development have only begun to emerge within the past 10 years. This is partially a response to machine learning models – the building blocks of AI systems – outgrowing the benchmarks used to measure their progress, leading to wide-spread adoption of machine learning systems in a range of practical applications that affect everyday life. For those of us interested in advancing ethics-informed AI, joining a machine learning company founded in part on ethical principles, just as it begins to grow, and just as people across the world are beginning to grapple with ethical AI issues, is an opportunity to fundamentally shape what the AI of the future looks like. It’s a new kind of modern-day AI experiment: What does a technology company with ethics in mind _from the start_ look
We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [BigScience](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [world's largest open source multilingual language model](https://twitter.com/BigScienceLLM) 🌸

⚠️ But there’s still a huge amount of work left to do.
One approach is to develop new models that represent society as we wish it to be. This suggests creating AI systems that don't just mimic the patterns in our data, but actively promote more equitable and fair perspectives. However, this approach raises a crucial question: whose values are we programming into these models? Values differ across cultures, societies, and individuals, making it a complex task to define what an ""ideal"" society should look like within an AI model. The question is indeed complex and multifaceted. If we avoid reproducing existing societal biases in our AI models, we're faced with the challenge of defining an ""ideal"" representation of society. Society is not a static entity, but a dynamic and ever-changing construct. Should AI models, then, adapt to the changes in societal norms and values over time? If so, how do we ensure that these shifts genuinely represent all groups within society, especially those often underrepresented?
After a few years at Microsoft, I left to work at Google to focus on big data problems inherent in deep learning. That’s where I started focusing on things like fairness, rigorous evaluation for different kinds of issues, and bias. While at Google, I founded and co-led the Ethical AI Team which focuses on inclusion and transparency.

After four years at Google, I came over to Hugging Face where I was able to jump in and focus on coding.
I’m helping to create protocols for ethical AI research, inclusive hiring, systems, and setting up a good culture here at Hugging Face.

### When did you recognize the importance of Ethical AI?

**Meg:** This occurred when I was working at Microsoft while I was working on the assistance technology, Seeing AI. In general, I was working on generating language from images and I started to see was how lopsided data was. Data represents a subset of the world and it influences what a model will say.
 
Here is the question: How do the integration of 8-bit models, user interfaces for interactivity, large-scale data analysis, and ethical AI deployment contribute to creating AI systems that are both efficient and socially responsible?","The context does not explicitly discuss the integration of 8-bit models, user interfaces for interactivity, large-scale data analysis, and ethical AI deployment. However, it does provide insights into the importance of ethical AI, transparency, and responsible AI development.

Based on the context, it can be inferred that creating AI systems that are both efficient and socially responsible requires a multifaceted approach. This includes:

1. Transparency: Developing AI models that are transparent, reproducible, and open-source, like I"
"In creating AI models, how do user-centered design, large-scale data handling, performance optimization, and ethical deployment work together to ensure that AI systems meet technical and societal requirements?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (22555, '08f31da5-7c25-4b77-a7fd-0ca328732229'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29229, 'bba3c00e-cd0b-4f15-b534-103a5596462e'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (9341, 'ffa4b5af-91fc-49f6-9a9c-c7b9a15daf55'), (26750, '22aa9771-c45d-48a8-b470-7a4e1ab9c4a0'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29395, 'c107c3ee-1edc-4cf8-a93e-49dc53fb2dc1'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (7898, 'a0553017-647f-4a6a-ba47-f2e453034fa7'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (14122, '24b982d1-e49a-4a35-a283-24beb2dbc093'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14149, 'e59bf5b6-e780-4c3f-91ef-15a3634e0300'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (28501, 'db1e4893-0d81-48b8-8179-b74c365d7559'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (11614, 'b2f56d1f-d474-4e31-ac9b-426853bda354'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29129, '4d2c0131-4d5e-4be6-9b9a-86815ba6a3bc'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (24550, 'f8edec76-5c02-41b0-a9ee-c83e1d736419'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10735, '1815b561-b04c-4411-a95c-7b9cdf9048e4'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (10738, 'd25343c4-9005-48e4-a890-49eaa86d0a81'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
The #1 comment we get from customers is that no single model will rule them all. They understand the value of building the best model for each use case to maximize its relevance on company data while optimizing the compute budget. Of course, privacy and intellectual property are also top concerns, and customers want to ensure they maintain complete control.

As AI finds its way into every department and business unit, customers also realize the need to train and deploy many different models. In a large multinational organization, this could mean running hundreds, even thousands, of models at any time. Given the pace of AI innovation, newer and higher-performance model architectures will also lead customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [BigScience](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [world's largest open source multilingual language model](https://twitter.com/BigScienceLLM) 🌸

⚠️ But there’s still a huge amount of work left to do.
In addition, our cloud partnerships let customers use Hugging Face models and libraries at any scale without worrying about provisioning infrastructure and building technical environments. This makes it much easier to get high-quality models out the door at a rapid pace without having to reinvent the wheel.

Following up on our collaboration with AWS on Amazon SageMaker and Microsoft on Azure Machine Learning, we're thrilled to work with none other than IBM on their new AI studio, [watsonx.ai](https://www.ibm.com/products/watsonx-ai). [watsonx.ai](http://watsonx.ai) is the next-generation enterprise studio for AI builders to train, validate, tune, and deploy both traditional ML and new generative AI capabilities, powered by foundation models.
 
Here is the question: In creating AI models, how do user-centered design, large-scale data handling, performance optimization, and ethical deployment work together to ensure that AI systems meet technical and societal requirements?","Based on the context, here is a comprehensive answer to the question:

In creating AI models, user-centered design, large-scale data handling, performance optimization, and ethical deployment work together to ensure that AI systems meet technical and societal requirements by:

* Building the best model for each use case to maximize relevance on company data while optimizing the compute budget (user-centered design)
* Training and deploying many different models, potentially hundreds or thousands, to cater to various departments and business units (large-scale data handling)
"
"How do the combination of performance optimization, ethical deployment, user interactivity, and scalable data handling contribute to the responsible development and deployment of AI models?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (22555, '08f31da5-7c25-4b77-a7fd-0ca328732229'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29395, 'c107c3ee-1edc-4cf8-a93e-49dc53fb2dc1'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (10470, '9941a43c-cebc-4f71-b0ad-010daaa6b368'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (11610, '7abc171e-bc54-496c-a7d6-c25500c846c1'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (23390, 'a0d6b001-969e-45ef-9770-61309e5be4e1'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29129, '4d2c0131-4d5e-4be6-9b9a-86815ba6a3bc'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10735, '1815b561-b04c-4411-a95c-7b9cdf9048e4'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [BigScience](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [world's largest open source multilingual language model](https://twitter.com/BigScienceLLM) 🌸

⚠️ But there’s still a huge amount of work left to do.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addressing-bias-throughout-the-ml-development-cycle), there is no one single way to develop machine learning systems, and any of the steps in the development and deployment process can present opportunities to tackle bias, from who is included at the start, to defining the task, to curating the dataset, training the model, and more. This also applies to multimodal models and the ways in which they are ultimately deployed or productionized in society, since the consequences of bias in multimodal models will depend on their downstream use. For instance, if a model is used in a human-in-the-loop setting for graphic design (such as those created by [RunwayML](https://runwayml.com/ai-magic-tools/text-to-image/)), the user has numerous occasions to detect and correct bias, for instance by changing the prompt or the generation options. However, if a model is used as part of a [tool to help forensic
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
 
Here is the question: How do the combination of performance optimization, ethical deployment, user interactivity, and scalable data handling contribute to the responsible development and deployment of AI models?","The combination of performance optimization, ethical deployment, user interactivity, and scalable data handling contribute to the responsible development and deployment of AI models by:

* Enabling the optimization of model performance for specific use cases while lowering costs, making AI more accessible and efficient.
* Ensuring that AI models are deployed in a way that tackles bias and promotes transparency, accountability, and fairness throughout the development and deployment process.
* Providing users with opportunities to detect and correct bias through interactive tools and human-in-the-loop settings"
"What is the role of performance optimization, data management, ethical deployment, and user interactivity in building AI models that are both efficient and socially responsible?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13316, '3a0f93a2-ae6d-487d-a9a7-9469be424875'), (13318, '32e34acb-80e7-4631-b573-ad387764d708'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (29213, 'ecfbdc64-5f35-4f83-8641-32ac46207dca'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (12335, '89a7ab1d-22fd-416d-890f-d08e63ca9cd5'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (30812, 'f7e1e127-2236-4b1a-b0ad-74e1af99c668'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (8839, 'fb867c7b-6a51-4baa-b761-3c649c36615f'), (8845, '03fc6833-c795-47d4-859d-9c07e60d14ba'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (14095, '8b96365b-5328-46c0-8348-61ce83609bab'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (14122, '24b982d1-e49a-4a35-a283-24beb2dbc093'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (14131, 'e5c14edb-a4ef-4578-98c2-52b276c71434'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14149, 'e59bf5b6-e780-4c3f-91ef-15a3634e0300'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (11124, '7b9292cb-0cb7-4972-91b4-f4ea38ac87e7'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8624, '339598bc-e183-4af9-9283-072e1969ff78'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29129, '4d2c0131-4d5e-4be6-9b9a-86815ba6a3bc'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (24550, 'f8edec76-5c02-41b0-a9ee-c83e1d736419'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (5622, 'ccda777f-15b6-4a2a-9ba6-f00ffacce0d2'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## _Machine Bias:_ from ML Systems to Personal and Social Risks

ML systems allow us to automate complex tasks at a scale never seen before as they are deployed in more sectors and use cases. When the technology works at its best, it can help smooth interactions between people and technical systems, remove the need for highly repetitive work, or unlock new ways of processing information to support research.

These same systems are also likely to reproduce discriminatory and abusive behaviors represented in their training data, especially when the data encodes human behaviors.
The technology then has the potential to make these issues significantly worse. Automation and deployment at scale can indeed:
Benchmarks are saturating, meaning that machines outperform humans on certain test sets, almost faster than we can come up with new ones. Yet, AI systems are known to be brittle and suffer from, or even worse amplify, severe malicious biases. Reproducibility is lacking. Openness is an afterthought. While people fixate on leaderboards, practical considerations for deploying models, such as efficiency and fairness, are often glossed over. The hugely important role data plays in model development is still not taken seriously enough. What is more, the practices of pretraining and prompt-based in-context learning have blurred what it means to be “in distribution” in the first place. Machine learning is slowly catching up to these things, and we hope to help the field move forward with our work.

## Next Steps
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
The #1 comment we get from customers is that no single model will rule them all. They understand the value of building the best model for each use case to maximize its relevance on company data while optimizing the compute budget. Of course, privacy and intellectual property are also top concerns, and customers want to ensure they maintain complete control.

As AI finds its way into every department and business unit, customers also realize the need to train and deploy many different models. In a large multinational organization, this could mean running hundreds, even thousands, of models at any time. Given the pace of AI innovation, newer and higher-performance model architectures will also lead customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.
Furthermore, as the field of AI and machine learning continues to expand, the variety of use cases and applications proliferates. This makes it essential for us to stay up-to-date with the latest research, ethical considerations, and best practices. For this reason, promoting user collaboration is also vital to the sustainability of our platform. Namely, through our community features, such as the Community Tab, we encourage and foster collaborative solutions between repository authors, users, organizations, and our team.

## Consent as a Core Value
 
Here is the question: What is the role of performance optimization, data management, ethical deployment, and user interactivity in building AI models that are both efficient and socially responsible?","According to the context, the role of performance optimization, data management, ethical deployment, and user interactivity in building AI models that are both efficient and socially responsible is crucial. Performance optimization is essential to ensure that models are efficient and can be deployed at scale without trade-offs in total cost of ownership or performance. Data management is vital as the hugely important role data plays in model development is still not taken seriously enough, and organizations need to maintain flexibility to take advantage of the latest and greatest in open-source"
"How do performance optimization through 8-bit models, ethical deployment in Hugging Face Spaces, and large-scale data handling with DuckDB ensure that AI systems are both effective and aligned with legal, ethical, and societal standards?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13320, '8272c7ae-a9bb-4f00-90df-c471493cb8aa'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (527, '1dd56bec-7545-4166-902a-d654c3c33c0f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (11342, 'ccdb1919-8a99-4eeb-a251-379637ea1b80'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (8845, '03fc6833-c795-47d4-859d-9c07e60d14ba'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (11614, 'b2f56d1f-d474-4e31-ac9b-426853bda354'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it 🤩.

The new ""Community"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.

Some example use cases for discussions and pull requests:

- Propose suggestions in model cards to improve disclosures of ethical biases.
- Let users flag concerning generations of a given Space demo.
- Provide a venue through which model and dataset authors can have a direct discussion with community members.
- Allow others to improve your repositories! For example, users might want to provide TensorFlow weights!

## Discussions

![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
to analyze [ML datasets](https://huggingface.co/spaces/huggingface/data-measurements-tool) and [models](https://huggingface.co/spaces/society-ethics/StableBias). We believe this helps everyone interested to better understand [the limitations of ML systems](https://huggingface.co/blog/ethics-soc-2) and how they can safely be leveraged to best serve users and those affected by these systems. These approaches have already proven their utility in promoting accountability, especially in the larger multidisciplinary research endeavors we’ve helped organize, including [BigScience](https://huggingface.co/bigscience) (see our blog series [on the social stakes of the project](https://montrealethics.ai/category/columns/social-context-in-llm-research/)), and the more recent [BigCode project](https://huggingface.co/bigcode) (whose governance is [described in more details here](https://huggingface.co/datasets/bigcode/governance-card)).
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
“The future of AI is here, but it’s not evenly distributed,” said Clement Delangue, CEO of Hugging Face. “Accessibility and transparency are the keys to sharing progress and creating tools to use these new capabilities wisely and responsibly. Amazon SageMaker and AWS-designed chips will enable our team and the larger machine learning community to convert the latest research into openly reproducible models that anyone can build on.”

## Collaborating to scale AI in the cloud

This expanded strategic partnership enables Hugging Face and AWS to accelerate machine learning adoption using the latest models hosted on Hugging Face with the industry-leading capabilities of Amazon SageMaker. Customers can now easily fine-tune and deploy state-of-the-art Hugging Face models in just a few clicks on Amazon SageMaker and Amazon Elastic Computing Cloud (EC2), taking advantage of purpose-built machine learning accelerators including AWS Trainium and AWS Inferentia.
 
Here is the question: How do performance optimization through 8-bit models, ethical deployment in Hugging Face Spaces, and large-scale data handling with DuckDB ensure that AI systems are both effective and aligned with legal, ethical, and societal standards?","This question cannot be answered based on the provided context. The context does not mention performance optimization through 8-bit models, ethical deployment in Hugging Face Spaces, and large-scale data handling with DuckDB. It discusses the new ""Community"" tab, its alignment with proposals in ethical ML, and its potential to unlock new kinds of positive patterns in ML, collaborations, and progress. It also mentions the development of state-of-the-art AI models, transparency, and accountability in ML systems, but it does"
"How does the integration of model optimization, user interactivity, large-scale data management, and ethical AI deployment ensure the development of responsible and efficient AI models?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (30727, '5a7a6803-dbbe-48fe-8ccd-60d7bb61992d'), (10250, '3d60dd94-162e-41c3-b924-86cd1b72a0fc'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (22555, '08f31da5-7c25-4b77-a7fd-0ca328732229'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (29231, '1e0d8d33-ae9b-45aa-93f5-0c23e829f89b'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (5821, 'c05970b5-a03b-4bcd-8ee9-1dc9884a9266'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (10954, '92fe4159-8084-4e9c-9be2-4e72a5e11928'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (13760, '493c8315-cad4-4414-821b-2e0f5cee9350'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10735, '1815b561-b04c-4411-a95c-7b9cdf9048e4'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: How to operationalize ethics in AI is an open research area. Although theory and scholarship on applied ethics and artificial intelligence have existed for decades, applied and tested practices for ethics within AI development have only begun to emerge within the past 10 years. This is partially a response to machine learning models – the building blocks of AI systems – outgrowing the benchmarks used to measure their progress, leading to wide-spread adoption of machine learning systems in a range of practical applications that affect everyday life. For those of us interested in advancing ethics-informed AI, joining a machine learning company founded in part on ethical principles, just as it begins to grow, and just as people across the world are beginning to grapple with ethical AI issues, is an opportunity to fundamentally shape what the AI of the future looks like. It’s a new kind of modern-day AI experiment: What does a technology company with ethics in mind _from the start_ look
The #1 comment we get from customers is that no single model will rule them all. They understand the value of building the best model for each use case to maximize its relevance on company data while optimizing the compute budget. Of course, privacy and intellectual property are also top concerns, and customers want to ensure they maintain complete control.

As AI finds its way into every department and business unit, customers also realize the need to train and deploy many different models. In a large multinational organization, this could mean running hundreds, even thousands, of models at any time. Given the pace of AI innovation, newer and higher-performance model architectures will also lead customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.
We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [BigScience](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [world's largest open source multilingual language model](https://twitter.com/BigScienceLLM) 🌸

⚠️ But there’s still a huge amount of work left to do.
The integration of use-based restrictions clauses into open AI licenses brings up the ability to better control the use of AI artifacts and the capacity of enforcement to the licensor of the ML model, standing up for a responsible use of the released AI artifact, in case a misuse of the model is identified. If behavioral-use restrictions were not present in open AI licenses, how would licensors even begin to think about responsible use-related legal tools when openly releasing their AI artifacts? OpenRAILs and RAILs are the first step towards enabling ethics-informed behavioral restrictions.
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
 
Here is the question: How does the integration of model optimization, user interactivity, large-scale data management, and ethical AI deployment ensure the development of responsible and efficient AI models?","Based on the context, the integration of model optimization, user interactivity, large-scale data management, and ethical AI deployment ensures the development of responsible and efficient AI models in the following ways:

* Model optimization allows for the development of models that are tailored to specific use cases, maximizing their relevance and optimizing compute budget, while also considering ethical principles from the start.
* User interactivity enables customers to have control over their models, ensuring that they maintain complete control over their data and intellectual property, and can"
"In creating AI models, how do user-centered design, large-scale data handling, performance optimization, and ethical deployment work together to ensure that AI systems meet technical and societal requirements?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (22555, '08f31da5-7c25-4b77-a7fd-0ca328732229'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29229, 'bba3c00e-cd0b-4f15-b534-103a5596462e'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (9341, 'ffa4b5af-91fc-49f6-9a9c-c7b9a15daf55'), (26750, '22aa9771-c45d-48a8-b470-7a4e1ab9c4a0'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29395, 'c107c3ee-1edc-4cf8-a93e-49dc53fb2dc1'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (7898, 'a0553017-647f-4a6a-ba47-f2e453034fa7'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (14122, '24b982d1-e49a-4a35-a283-24beb2dbc093'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14149, 'e59bf5b6-e780-4c3f-91ef-15a3634e0300'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (28501, 'db1e4893-0d81-48b8-8179-b74c365d7559'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (11614, 'b2f56d1f-d474-4e31-ac9b-426853bda354'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29129, '4d2c0131-4d5e-4be6-9b9a-86815ba6a3bc'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (24550, 'f8edec76-5c02-41b0-a9ee-c83e1d736419'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10735, '1815b561-b04c-4411-a95c-7b9cdf9048e4'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (10738, 'd25343c4-9005-48e4-a890-49eaa86d0a81'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
The #1 comment we get from customers is that no single model will rule them all. They understand the value of building the best model for each use case to maximize its relevance on company data while optimizing the compute budget. Of course, privacy and intellectual property are also top concerns, and customers want to ensure they maintain complete control.

As AI finds its way into every department and business unit, customers also realize the need to train and deploy many different models. In a large multinational organization, this could mean running hundreds, even thousands, of models at any time. Given the pace of AI innovation, newer and higher-performance model architectures will also lead customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [BigScience](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [world's largest open source multilingual language model](https://twitter.com/BigScienceLLM) 🌸

⚠️ But there’s still a huge amount of work left to do.
In addition, our cloud partnerships let customers use Hugging Face models and libraries at any scale without worrying about provisioning infrastructure and building technical environments. This makes it much easier to get high-quality models out the door at a rapid pace without having to reinvent the wheel.

Following up on our collaboration with AWS on Amazon SageMaker and Microsoft on Azure Machine Learning, we're thrilled to work with none other than IBM on their new AI studio, [watsonx.ai](https://www.ibm.com/products/watsonx-ai). [watsonx.ai](http://watsonx.ai) is the next-generation enterprise studio for AI builders to train, validate, tune, and deploy both traditional ML and new generative AI capabilities, powered by foundation models.
 
Here is the question: In creating AI models, how do user-centered design, large-scale data handling, performance optimization, and ethical deployment work together to ensure that AI systems meet technical and societal requirements?","Based on the provided context, here is a comprehensive answer to the question:

In creating AI models, user-centered design, large-scale data handling, performance optimization, and ethical deployment work together to ensure that AI systems meet technical and societal requirements by:

1. **User-centered design**: Understanding the value of building the best model for each use case to maximize its relevance on company data while optimizing the compute budget, ensuring that AI models are tailored to specific needs and use cases.

2. **Large-scale data"
"How do the combination of performance optimization, ethical deployment, user interactivity, and scalable data handling contribute to the responsible development and deployment of AI models?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (22555, '08f31da5-7c25-4b77-a7fd-0ca328732229'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29395, 'c107c3ee-1edc-4cf8-a93e-49dc53fb2dc1'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (10470, '9941a43c-cebc-4f71-b0ad-010daaa6b368'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (11610, '7abc171e-bc54-496c-a7d6-c25500c846c1'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (23390, 'a0d6b001-969e-45ef-9770-61309e5be4e1'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29129, '4d2c0131-4d5e-4be6-9b9a-86815ba6a3bc'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10735, '1815b561-b04c-4411-a95c-7b9cdf9048e4'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
We want to have a positive impact on the AI field. We think the direction of more responsible AI is through openly sharing models, datasets, training procedures, evaluation metrics and working together to solve issues. We believe open source and open science bring trust, robustness, reproducibility, and continuous innovation. With this in mind, we are leading [BigScience](https://bigscience.huggingface.co/), a collaborative workshop around the study and creation of very large language models gathering more than 1,000 researchers of all backgrounds and disciplines. We are now training the [world's largest open source multilingual language model](https://twitter.com/BigScienceLLM) 🌸

⚠️ But there’s still a huge amount of work left to do.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addressing-bias-throughout-the-ml-development-cycle), there is no one single way to develop machine learning systems, and any of the steps in the development and deployment process can present opportunities to tackle bias, from who is included at the start, to defining the task, to curating the dataset, training the model, and more. This also applies to multimodal models and the ways in which they are ultimately deployed or productionized in society, since the consequences of bias in multimodal models will depend on their downstream use. For instance, if a model is used in a human-in-the-loop setting for graphic design (such as those created by [RunwayML](https://runwayml.com/ai-magic-tools/text-to-image/)), the user has numerous occasions to detect and correct bias, for instance by changing the prompt or the generation options. However, if a model is used as part of a [tool to help forensic
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
 
Here is the question: How do the combination of performance optimization, ethical deployment, user interactivity, and scalable data handling contribute to the responsible development and deployment of AI models?","The combination of performance optimization, ethical deployment, user interactivity, and scalable data handling contribute to the responsible development and deployment of AI models by:

* Enabling the optimization of model performance for specific use cases while lowering costs, making AI more accessible and efficient.
* Ensuring that AI models are deployed in a way that tackles bias and promotes transparency, accountability, and fairness throughout the development and deployment process.
* Providing users with opportunities to detect and correct bias through interactive tools and human-in-the-loop settings"
"What is the role of performance optimization, data management, ethical deployment, and user interactivity in building AI models that are both efficient and socially responsible?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13316, '3a0f93a2-ae6d-487d-a9a7-9469be424875'), (13318, '32e34acb-80e7-4631-b573-ad387764d708'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (29213, 'ecfbdc64-5f35-4f83-8641-32ac46207dca'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (12335, '89a7ab1d-22fd-416d-890f-d08e63ca9cd5'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (30812, 'f7e1e127-2236-4b1a-b0ad-74e1af99c668'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (8839, 'fb867c7b-6a51-4baa-b761-3c649c36615f'), (8845, '03fc6833-c795-47d4-859d-9c07e60d14ba'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (1284, '2ee50156-2487-48e6-aa26-b091ff8aecbd'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (14095, '8b96365b-5328-46c0-8348-61ce83609bab'), (14111, '5bbc648e-a10a-4780-a837-a0e579cee3bb'), (14122, '24b982d1-e49a-4a35-a283-24beb2dbc093'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (14131, 'e5c14edb-a4ef-4578-98c2-52b276c71434'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14149, 'e59bf5b6-e780-4c3f-91ef-15a3634e0300'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (28500, 'c5d0eafc-1c28-4a3c-818e-9ebb92b15492'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (11124, '7b9292cb-0cb7-4972-91b4-f4ea38ac87e7'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8624, '339598bc-e183-4af9-9283-072e1969ff78'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (29129, '4d2c0131-4d5e-4be6-9b9a-86815ba6a3bc'), (29130, '1a135feb-37c7-47e9-9e6d-23e06815e472'), (15311, '11864a17-4f60-4628-a496-55580064c205'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (24550, 'f8edec76-5c02-41b0-a9ee-c83e1d736419'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (5622, 'ccda777f-15b6-4a2a-9ba6-f00ffacce0d2'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (28669, 'ede31705-2661-4e45-bb6e-8c8cc7fa7a68'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## _Machine Bias:_ from ML Systems to Personal and Social Risks

ML systems allow us to automate complex tasks at a scale never seen before as they are deployed in more sectors and use cases. When the technology works at its best, it can help smooth interactions between people and technical systems, remove the need for highly repetitive work, or unlock new ways of processing information to support research.

These same systems are also likely to reproduce discriminatory and abusive behaviors represented in their training data, especially when the data encodes human behaviors.
The technology then has the potential to make these issues significantly worse. Automation and deployment at scale can indeed:
Benchmarks are saturating, meaning that machines outperform humans on certain test sets, almost faster than we can come up with new ones. Yet, AI systems are known to be brittle and suffer from, or even worse amplify, severe malicious biases. Reproducibility is lacking. Openness is an afterthought. While people fixate on leaderboards, practical considerations for deploying models, such as efficiency and fairness, are often glossed over. The hugely important role data plays in model development is still not taken seriously enough. What is more, the practices of pretraining and prompt-based in-context learning have blurred what it means to be “in distribution” in the first place. Machine learning is slowly catching up to these things, and we hope to help the field move forward with our work.

## Next Steps
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
The #1 comment we get from customers is that no single model will rule them all. They understand the value of building the best model for each use case to maximize its relevance on company data while optimizing the compute budget. Of course, privacy and intellectual property are also top concerns, and customers want to ensure they maintain complete control.

As AI finds its way into every department and business unit, customers also realize the need to train and deploy many different models. In a large multinational organization, this could mean running hundreds, even thousands, of models at any time. Given the pace of AI innovation, newer and higher-performance model architectures will also lead customers to replace their models quicker than expected, reinforcing the need to train and deploy new models in production quickly and seamlessly.
Furthermore, as the field of AI and machine learning continues to expand, the variety of use cases and applications proliferates. This makes it essential for us to stay up-to-date with the latest research, ethical considerations, and best practices. For this reason, promoting user collaboration is also vital to the sustainability of our platform. Namely, through our community features, such as the Community Tab, we encourage and foster collaborative solutions between repository authors, users, organizations, and our team.

## Consent as a Core Value
 
Here is the question: What is the role of performance optimization, data management, ethical deployment, and user interactivity in building AI models that are both efficient and socially responsible?","According to the context, the role of performance optimization, data management, ethical deployment, and user interactivity in building AI models that are both efficient and socially responsible is crucial. Performance optimization is essential to ensure that models are efficient and can be deployed at scale. Data management is critical to address issues of bias and discrimination in the training data, which can be amplified by AI systems. Ethical deployment is necessary to prevent the reproduction of discriminatory and abusive behaviors. User interactivity is vital to promote collaboration and"
"How do performance optimization through 8-bit models, ethical deployment in Hugging Face Spaces, and large-scale data handling with DuckDB ensure that AI systems are both effective and aligned with legal, ethical, and societal standards?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13320, '8272c7ae-a9bb-4f00-90df-c471493cb8aa'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (527, '1dd56bec-7545-4166-902a-d654c3c33c0f'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (28689, '23863e7c-cdf1-432d-acf4-43491a71af5b'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (11342, 'ccdb1919-8a99-4eeb-a251-379637ea1b80'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (8834, '30dd74ba-3880-49d3-8eb4-00205ac75d55'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (8845, '03fc6833-c795-47d4-859d-9c07e60d14ba'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (11614, 'b2f56d1f-d474-4e31-ac9b-426853bda354'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (4457, '0ec2554b-28dd-4d0f-aa4b-f12930b68460'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it 🤩.

The new ""Community"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.

Some example use cases for discussions and pull requests:

- Propose suggestions in model cards to improve disclosures of ethical biases.
- Let users flag concerning generations of a given Space demo.
- Provide a venue through which model and dataset authors can have a direct discussion with community members.
- Allow others to improve your repositories! For example, users might want to provide TensorFlow weights!

## Discussions

![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
to analyze [ML datasets](https://huggingface.co/spaces/huggingface/data-measurements-tool) and [models](https://huggingface.co/spaces/society-ethics/StableBias). We believe this helps everyone interested to better understand [the limitations of ML systems](https://huggingface.co/blog/ethics-soc-2) and how they can safely be leveraged to best serve users and those affected by these systems. These approaches have already proven their utility in promoting accountability, especially in the larger multidisciplinary research endeavors we’ve helped organize, including [BigScience](https://huggingface.co/bigscience) (see our blog series [on the social stakes of the project](https://montrealethics.ai/category/columns/social-context-in-llm-research/)), and the more recent [BigCode project](https://huggingface.co/bigcode) (whose governance is [described in more details here](https://huggingface.co/datasets/bigcode/governance-card)).
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
“The future of AI is here, but it’s not evenly distributed,” said Clement Delangue, CEO of Hugging Face. “Accessibility and transparency are the keys to sharing progress and creating tools to use these new capabilities wisely and responsibly. Amazon SageMaker and AWS-designed chips will enable our team and the larger machine learning community to convert the latest research into openly reproducible models that anyone can build on.”

## Collaborating to scale AI in the cloud

This expanded strategic partnership enables Hugging Face and AWS to accelerate machine learning adoption using the latest models hosted on Hugging Face with the industry-leading capabilities of Amazon SageMaker. Customers can now easily fine-tune and deploy state-of-the-art Hugging Face models in just a few clicks on Amazon SageMaker and Amazon Elastic Computing Cloud (EC2), taking advantage of purpose-built machine learning accelerators including AWS Trainium and AWS Inferentia.
 
Here is the question: How do performance optimization through 8-bit models, ethical deployment in Hugging Face Spaces, and large-scale data handling with DuckDB ensure that AI systems are both effective and aligned with legal, ethical, and societal standards?","This question cannot be answered based on the provided context. The context does not mention performance optimization through 8-bit models, ethical deployment in Hugging Face Spaces, and large-scale data handling with DuckDB. It discusses the new ""Community"" tab, its alignment with proposals in ethical ML, and its potential use cases, as well as collaborations and partnerships to promote transparency, accountability, and responsible AI development."
"How do performance optimization through 8-bit models, large-scale data handling with DuckDB, user interactivity via Gradio, and ethical AI deployment in Hugging Face Spaces contribute to creating AI models that are both scalable, efficient, and aligned with societal values?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (2586, '94d756e9-d679-4390-8d21-2b6d32984ba8'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (1570, '2ffb0ee7-0b83-42e3-bfd2-dd6a862b3727'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11339, 'aeb98c9b-94d7-4d76-894d-f6e3016f3f6e'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (84, '8f5cfcc1-5f99-4ba5-93e4-bee42367e909'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (23151, '00d8db8d-bd63-45b4-b2be-91025c5f85fa'), (23153, '49b4c809-71ff-410a-b32d-acc1467103b4'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (29349, 'f9a62426-7040-4f1b-8771-11b8792ec32f'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (5806, '5054cac8-6958-4df8-9986-f0bb2157877f'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (9567, '21f12568-4fc0-4680-b763-b7907ea0ea81'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2922, 'ed501c5c-b523-4904-84f8-90ac5545623d'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (3985, 'ecedc47d-0616-441d-a202-07ae59c5cb7a'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (27040, '56f391b1-2a20-4be2-af9f-8f26721aaaf2'), (10657, '9bf131aa-a412-4433-ad0c-db172adb5ff6'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: There have been significant advances in new Transformer and Diffuser machine learning models that process and generate text, audio, and images. However, most of these popular generative AI models are not publicly available, widening the gap of machine learning capabilities between the largest tech companies and everyone else. To counter this trend, AWS and Hugging Face are partnering to contribute next-generation models to the global AI community and democratize machine learning. Through the strategic partnership, Hugging Face will leverage AWS as a preferred cloud provider so developers in Hugging Face’s community can access AWS’s state-of-the-art tools (e.g., [Amazon SageMaker](https://aws.amazon.com/sagemaker), [AWS Trainium](https://aws.amazon.com/machine-learning/trainium/), [AWS Inferentia](https://aws.amazon.com/machine-learning/inferentia/)) to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their
The development of state-of-the-art AI models should be more transparent. Our goal with IDEFICS is to reproduce and provide the AI community with systems that match the capabilities of large proprietary models like Flamingo. As such, we took important steps contributing to bringing transparency to these AI systems: we used only publicly available data, we provided tooling to explore training datasets, we shared [technical lessons and mistakes](https://github.com/huggingface/m4-logs/blob/master/memos/README.md) of building such artifacts and assessed the model’s harmfulness by adversarially prompting it before releasing it. We are hopeful that IDEFICS will serve as a solid foundation for more open research in multimodal AI systems, alongside models like [OpenFlamingo](https://huggingface.co/openflamingo)-another open reproduction of Flamingo at the 9 billion parameter scale.
to train, fine-tune, and deploy models on AWS. This will allow developers to further optimize the performance of their models for their specific use cases while lowering costs. Hugging Face will apply the latest in innovative research findings using Amazon SageMaker to build next-generation AI models. Together, Hugging Face and AWS are bridging the gap so the global AI community can benefit from the latest advancements in machine learning to accelerate the creation of generative AI applications.
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.

Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.
 
Here is the question: How do performance optimization through 8-bit models, large-scale data handling with DuckDB, user interactivity via Gradio, and ethical AI deployment in Hugging Face Spaces contribute to creating AI models that are both scalable, efficient, and aligned with societal values?","The question cannot be directly answered based on the provided context. The context discusses the partnership between AWS and Hugging Face to democratize machine learning, the development of transparent AI models, and the integration of Spark with Hugging Face for efficient data handling. It also mentions the collaboration between Hugging Face and Intel for building model optimization tools. However, it does not mention performance optimization through 8-bit models, large-scale data handling with DuckDB, user interactivity via Gradio, and ethical AI deployment"
Which factor is most critical for improving the efficiency of speech synthesis and recognition systems?,"[(18432, '7458df8c-e69b-43d8-a456-b53c44af4802'), (9728, '1cf55a61-9ed1-4a59-9f5b-74b10ffbbebe'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (22538, 'af80992b-9150-4eb5-8fbb-cf860fff6c84'), (9744, 'ee479e38-312b-4563-958c-33ce1e0eedea'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (25107, '5f0f249d-5961-474b-a1f3-beded0fc48ae'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (24602, '414daaba-d8cd-4a25-956e-0e453d08de5f'), (24605, 'bf3a796e-9d27-4aa4-8697-5f30933d5d01'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (10817, '4757d89a-9af0-47c4-a50b-3fb66c962f69'), (4166, 'd62d2dfe-88e9-4269-a3b8-a0c08a9ead60'), (9804, '10c0789a-3617-4e39-810d-705c30c23cca'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (20581, '7871ca5f-2484-4a07-b596-f516a21ea738'), (16488, '82e6921b-4f0a-43f7-af6b-e88247f9a075'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (16492, 'e9159689-7c45-42ef-90ba-2efb9da7ad91'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (19075, '131641af-b7be-4b3a-a31a-a76b816e3822'), (19076, 'c3f9c5df-c349-46ea-932a-eead03777122'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (20644, '35d54144-39c9-4e31-b7cc-fa6416b43dcb'), (4776, 'e8b7f61e-f9e4-46b0-952a-cb86d76a7ab7'), (20652, '68f78608-9c1c-41a7-9309-a792d5069928'), (20653, 'cbd04324-f952-43fe-9755-920b4b485668'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (4787, 'e0ec6c60-dd04-4ade-96c4-c47997cde848'), (20660, '36d46a61-da08-4a79-b974-9b8f75c6d8d9'), (15032, 'a5fd305b-6bf8-4f2d-b724-080da4c84f02'), (21191, '9cfe40a8-87c6-4a55-9f19-3458ca24d801'), (9928, '5c999e86-a8bc-40b7-aea5-7bf657e92285'), (8903, 'dce0bc34-0d40-483c-9870-a80c6a620c0b'), (5839, 'd97b59e5-561b-4a61-bcff-51b7f943fa47'), (15056, '3aeddfc0-2bd8-44a0-8d5d-def75c612a76'), (20689, 'ac2a712e-317a-4369-9bbb-ab3b1744bd16'), (15062, 'febd9689-aff0-4211-8e3c-df4188157724'), (20695, 'c3a7b7e1-6a92-4244-92f3-2ff229410ce9'), (22239, 'f48147dc-d38a-4d1a-b0c9-76db7e13e56c'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (22241, '5b8e4b4b-9d05-4faa-bb22-3b469591c916'), (22242, 'eca67578-c065-4375-ae51-0630a93e01cb'), (4839, '8b5fcfe4-422d-41ad-ba2a-435d4859c7ef'), (25836, '2bd1757a-9559-4170-8a84-7a157abf9fd3'), (4848, '76f1fb67-e046-408f-ad66-1ced57c240ba'), (26356, 'bd637992-bac8-476e-a6c7-09fa000bfe99'), (6907, '1be7a8dc-407e-49e2-b978-d3551d21c091'), (13567, 'a86f0554-9dab-4c51-8b3e-f56d256c6804'), (1811, 'f4e9b143-1453-475d-a8a1-593ae0511e73'), (18200, 'c90b132b-e7b4-4305-99ea-9c6f13711dd2'), (26396, 'edcdcf70-06e6-40bd-88b9-5337b2e19d39'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (13617, '5d6f1844-a996-4b2b-9cce-54cc40faf578'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (316, '3d68da03-3df0-4ea4-8d35-55ca5e6bf70e'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (13661, '70fe1907-eadd-4d2b-b4b9-aec545b32395'), (14178, 'cb2f9dd9-481d-4c77-9f13-59ec50223029'), (13669, 'fd5f73cd-31c4-4ed9-a912-115b0d44f3b5'), (13670, '2770a364-f587-45a8-89d3-153b4447c5a3'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (9073, '814aa859-5cdc-4add-85c5-eefdb37f0d28'), (18804, '313d5a47-6d9e-49a4-9bd5-4098e9462ef6'), (10615, '7bf25661-c8bb-4b65-9e16-786afc1051f8'), (18813, '7358b64b-5096-427f-9e28-1c7ee75f7806'), (9085, 'b637f259-3426-45e9-95da-6b478b336443'), (10112, '0695acc0-fd3b-451b-b774-cb77151cf1eb'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (18844, 'c97c2cdd-44fb-4531-87d5-afef8f19b517'), (18850, '1731352e-cdab-4fa7-8131-b488d0649c92'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (18367, '6a784930-de82-4a31-a463-40d44eda4423'), (10179, 'e27c098e-e4a7-4970-a787-484d25cb713b'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (5064, '442cd4f9-dd4a-42f0-bdbd-31fcb1c45bd1'), (18378, 'f444b8fb-5669-49ee-b560-935ec809bd1a'), (14798, '192f0986-4830-4336-b8ee-124dd7bb6baf'), (14799, '009fb2fe-6429-4c45-a823-a4b71b946f4d'), (14803, '989cb40f-0834-4dc5-9f2e-8f008a7490ae'), (7123, '5550e6f5-759d-4aef-9d97-83b53144cc4b'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (29658, 'f84a2ed0-baa7-4a47-9fd6-bd1962e44064'), (29659, '5a15e754-3b25-4dce-88a6-79573d35cf5d'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (24551, 'ab3cb15d-5aba-47cb-b554-8dde3831a847'), (24552, '385591aa-c2fa-4a51-befd-56b64068611b'), (14316, '61d7bed0-b6a1-4357-8d28-f95b0a73b244'), (14317, '8c6ac79e-1f72-496b-a40f-fc1c956bf3eb'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (21502, 'c2163ba2-004a-43ab-91b4-058e6123a72f'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The performance of speech recognition system can often significantly be improved by adding a 
language model for decoding. For more information on how to add a language model, please 
take a look at [How to combine n-gram language models with speech recognition models](#how-to-combine-n-gram-with-model).
```

## Limitations and bias

WER is a valuable tool for comparing different systems as well as for evaluating improvements within one system. This kind of measurement, however, provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort. 

## Citation

```bibtex
@inproceedings{woodard1982,
author = {Woodard, J.P. and Nelson, J.T.,
year = {1982},
journal = Ẅorkshop on standardisation for speech I/O technology, Naval Air Development Center, Warminster, PA},
title = {An information theoretic measure of speech recognition performance}
}
```

```bibtex
@inproceedings{morris2004,
author = {Morris, Andrew and Maier, Viktoria and Green, Phil},
year = {2004},
month = {01},
pages = {},
title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}
}
```

## Limitations and bias

WER is a valuable tool for comparing different systems as well as for evaluating improvements within one system. This kind of measurement, however, provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort. 

## Citation

```bibtex
@inproceedings{woodard1982,
author = {Woodard, J.P. and Nelson, J.T.,
year = {1982},
journal = {Workshop on standardisation for speech I/O technology, Naval Air Development Center, Warminster, PA},
title = {An information theoretic measure of speech recognition performance}
}
```

```bibtex
@inproceedings{morris2004,
author = {Morris, Andrew and Maier, Viktoria and Green, Phil},
year = {2004},
month = {01},
pages = {},
title = {From WER and RIL to MER and WIL: improved evaluation measures for connected speech recognition.}
}
The abstract from the paper is the following:

*This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition
(ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance
and its efficiency. Putting together all our observations, we introduce SEW (Squeezed and Efficient Wav2vec), a
pre-trained model architecture with significant improvements along both performance and efficiency dimensions across a
variety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x
inference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a similar inference
time, SEW reduces word error rate by 25-50% across different model sizes.*

This model was contributed by [anton-l](https://huggingface.co/anton-l).

## Usage tips
The abstract from the paper is the following:

*This paper is a study of performance-efficiency trade-offs in pre-trained models for automatic speech recognition
(ASR). We focus on wav2vec 2.0, and formalize several architecture designs that influence both the model performance
and its efficiency. Putting together all our observations, we introduce SEW (Squeezed and Efficient Wav2vec), a
pre-trained model architecture with significant improvements along both performance and efficiency dimensions across a
variety of training setups. For example, under the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x
inference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in word error rate. With a similar inference
time, SEW reduces word error rate by 25-50% across different model sizes.*

This model was contributed by [anton-l](https://huggingface.co/anton-l).

## Usage tips
 
Here is the question: Which factor is most critical for improving the efficiency of speech synthesis and recognition systems?","Based on the provided context, the answer cannot be deduced directly. The context discusses the performance of speech recognition systems, limitations of WER, and a specific model architecture (SEW) that improves performance and efficiency. However, it does not explicitly mention the most critical factor for improving the efficiency of speech synthesis and recognition systems."
What is the most effective method for enhancing the model's ability to learn new speech patterns through user interaction?,"[(18432, '7458df8c-e69b-43d8-a456-b53c44af4802'), (18433, 'd6dc1e38-7409-447e-8ac4-ffe543e7c17a'), (9730, 'fbb81594-08b4-4f1d-a9eb-56be92d597d8'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (18441, 'cfa471d2-0b8d-475e-9d32-c7873eeb92f8'), (5130, '0402365e-06b9-415e-9f92-dfe339eb3334'), (22538, 'af80992b-9150-4eb5-8fbb-cf860fff6c84'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (5139, 'ba7b9456-398d-4fbc-85d5-361dab3a6bdb'), (18453, 'c48fe548-242c-451b-9691-c7cea36dd070'), (5151, '4f771382-07e0-40d1-9cc9-a4a89d956e92'), (21543, 'f92a8c47-cb21-4eca-b72d-d106f9a81687'), (9767, '3d8f899f-4403-4a4e-b5ae-f298a872a576'), (4166, 'd62d2dfe-88e9-4269-a3b8-a0c08a9ead60'), (3665, 'b726f74a-c646-4050-9e74-cdaa45984963'), (82, '9b1467ac-93fd-4e92-ae66-0c561a9fe744'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (30817, 'ce6db1b0-14b2-4588-be54-92b24764e498'), (30818, '6eadeeb7-ee4f-4a43-b9f9-36f393a64f85'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (20581, '7871ca5f-2484-4a07-b596-f516a21ea738'), (19076, 'c3f9c5df-c349-46ea-932a-eead03777122'), (19085, '95ca3f5d-7594-4ebe-ae6d-1626686a69fc'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (3222, 'fa31abbe-1fb2-4f53-9685-0eb148e419de'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (3748, '332644a1-eafc-4d25-b177-40728dd8f84b'), (20644, '35d54144-39c9-4e31-b7cc-fa6416b43dcb'), (4776, 'e8b7f61e-f9e4-46b0-952a-cb86d76a7ab7'), (20652, '68f78608-9c1c-41a7-9309-a792d5069928'), (20653, 'cbd04324-f952-43fe-9755-920b4b485668'), (20660, '36d46a61-da08-4a79-b974-9b8f75c6d8d9'), (15028, '86c91e1c-d5df-4053-b7f8-f475c47edcf1'), (20665, '516b7396-a362-4980-97c5-7d58ef8db77b'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (9928, '5c999e86-a8bc-40b7-aea5-7bf657e92285'), (15056, '3aeddfc0-2bd8-44a0-8d5d-def75c612a76'), (20689, 'ac2a712e-317a-4369-9bbb-ab3b1744bd16'), (22241, '5b8e4b4b-9d05-4faa-bb22-3b469591c916'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (4839, '8b5fcfe4-422d-41ad-ba2a-435d4859c7ef'), (4840, '1f740da6-f945-461a-a729-8cad3368c6cd'), (28906, 'e6b32809-15f9-429a-995b-26d8b76dae17'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (4848, '76f1fb67-e046-408f-ad66-1ced57c240ba'), (6899, '5fc72f0c-9870-4f07-bc4f-c8e16dc0b4f2'), (26363, '7229a8f2-836b-4249-a128-0c3a3a567ef6'), (4860, '6904c0aa-f78a-40e3-814c-cc959f84f12b'), (18181, '4e4ca1f0-bd3f-4257-8fef-b758de7364dd'), (1811, 'f4e9b143-1453-475d-a8a1-593ae0511e73'), (1812, 'abbeae44-35ad-4ce3-bc93-b8a186be4830'), (18200, 'c90b132b-e7b4-4305-99ea-9c6f13711dd2'), (18201, '2d381f7c-31b3-4fc8-a6f2-019f577e2bd0'), (1820, 'b075e576-65c5-463f-9d0f-6bb28358cc07'), (13598, '297e738c-2640-4fbc-ab2a-2b1a32905c36'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (18209, '3b3b2f22-3c8a-4356-b74e-c918809e7457'), (18215, '1bfed648-728f-43d6-8081-aed14ce796ed'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (16680, 'e5b38075-7df1-4e23-b005-9664ca802320'), (18221, '774ea25b-9e09-40b1-aaae-fa09c1066b33'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (13617, '5d6f1844-a996-4b2b-9cce-54cc40faf578'), (13618, 'c26d0aa6-d937-42a4-80b1-3ebe7f3941a8'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (13626, '1256a71c-2ece-41bf-ba82-dc791b09e6c6'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (316, '3d68da03-3df0-4ea4-8d35-55ca5e6bf70e'), (317, 'ef8b2514-cf01-4f03-896e-2692824e2715'), (13632, '0e477806-a868-45ab-9ae7-a2536f990931'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (325, '05618000-706f-40ed-8186-9da01584ee2a'), (13638, 'be7b188f-e437-4d0f-8a77-b6eacf7572c1'), (331, '87477336-185d-4aba-8657-20eff34097e5'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (337, '40abecfb-971b-4c06-9118-1202cad8c9b0'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13661, '70fe1907-eadd-4d2b-b4b9-aec545b32395'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (10615, '7bf25661-c8bb-4b65-9e16-786afc1051f8'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (23941, '4552e7c6-32c1-47d7-9c48-1cd66703f9a7'), (23450, '1dea3594-bab7-49d3-b49c-dac62d5a6b39'), (23453, '24bbe86e-9ba8-4256-b0fc-6707f0f6b1c0'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (18850, '1731352e-cdab-4fa7-8131-b488d0649c92'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (15268, 'c0f4cbd2-9369-47e9-9c6f-ed3053963c1a'), (15267, 'c893dfbc-198a-4578-9809-345571353a75'), (15271, 'e9a1746a-2cf7-4b72-a736-ca2f77bf86c8'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (9139, '2af98149-42ee-4660-988d-a8696d5547ac'), (31161, 'e6392f6d-29c2-4b5e-99d0-0af8c82a0670'), (9148, '9a67e291-a5dd-4957-8d10-703a415fccb6'), (13757, '1ca945fe-3ad2-440a-9679-60b9498487d1'), (29631, '05e259fc-f013-438c-92a0-526db3b1673f'), (10179, 'e27c098e-e4a7-4970-a787-484d25cb713b'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (29639, 'ad7bf7d2-d8c6-47bf-9e9c-6c68383ad368'), (9160, 'afce294c-f87e-4fcd-8a56-e3e569555c72'), (4040, 'a08a5c15-4a59-49e0-83d1-4b285c1643b2'), (13787, '3d112122-a48e-416f-9e0d-1d80749c9c5a'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13788, '7c8c4bb0-1e5c-44c3-88c8-6569c468f867'), (24549, 'e159577d-48a3-4a97-b848-68e78c570951'), (24552, '385591aa-c2fa-4a51-befd-56b64068611b'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (21502, 'c2163ba2-004a-43ab-91b4-058e6123a72f')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Learning from Human Feedback (RLHF); use methods from reinforcement learning to directly optimize a language model with human feedback. RLHF has enabled language models to begin to align a model trained on a general corpus of text data to that of complex human values.
Both these methods are relatively easy to implement: you just need to find or generate related datasets and then fine-tune your model using the same technique as when training. A great number of instruct datasets were published last year, which improved model performance in dialogue-like setups. For more information on this topic, you can read an intro blog [here](https://huggingface.co/blog/dialog-agents). However, the models, though better, can still not match what humans expect.
- Data Processing. You should adapt the data processing to your specific language. In data processing, you should make the data more uniform so that it will be easier for the model to learn how to classify speech in your data. Here it can be really helpful to be proficient in the language to know what can be done to simplify the language without changing the meaning. 
Data processing methods include, but are not limited to:
	- Normalizing your data. Make sure all characters are lower-cased.
employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.*
the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.*
 
Here is the question: What is the most effective method for enhancing the model's ability to learn new speech patterns through user interaction?","Based on the context, the most effective method for enhancing the model's ability to learn new speech patterns through user interaction is Learning from Human Feedback (RLHF), which uses methods from reinforcement learning to directly optimize a language model with human feedback. This method has enabled language models to begin to align with complex human values and has shown promising results in dialogue-like setups."
Which solution would best ensure that a deployed image generation model maintains high accuracy while scaling to large datasets in production environments?,"[(22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (22024, 'fc70b5af-54c7-472b-b9b8-02356be01425'), (12305, '2e56823c-16a9-4e2a-81cb-823ada1fe8fe'), (8724, '1421cd58-3d8a-428f-9655-41198f21c799'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (15898, 'b6f9af12-8c93-4907-8584-6063e7adc079'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (7202, '8fe27cdc-8842-4f62-a856-52d8c3064ba6'), (26661, 'ba608cf2-2d94-47fd-801e-7393ff76f4e5'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (21049, '8462308a-79bd-461f-87fd-f98e7b1aa3d5'), (21060, '263973c8-fcb1-456c-9b1d-efbe928972b2'), (11338, '30d2c084-1101-4560-b412-45d9eae8ef69'), (26699, 'e8941aa3-4fe0-4334-997d-3b5eee50c684'), (590, '6523fa0b-894c-49e0-bfb1-fb3213623bf3'), (30802, '29278ad4-3abc-4d93-9603-14c1b27a60e5'), (30803, '04651842-da8f-4f4b-9076-9d74a4b634b6'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (26206, '82193bdb-533e-4c8b-93d8-bc2aa8c46fa6'), (20068, 'd2d9fdee-7033-4cda-a724-d9c2e219656c'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (14448, '137a6dbc-1a8d-494d-9bb2-9df4702ab8c9'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (4728, '16e3aca0-e5d5-4be6-946d-90258a1ebb5a'), (26233, 'b0309052-b6ba-4c28-a638-fdfcd3dd8388'), (13955, 'b43f9f53-919a-4395-ad06-ca2a06b73c23'), (6797, '3160d312-6e44-41aa-9a17-ee3fbb2b0aa2'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (23191, '8c2efd68-2cbd-49b2-b5b0-9b40d0c49323'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (25756, '2b4f6289-a965-4dc0-8640-38b0d7ce311e'), (25757, '24c0f4d2-56ee-43a1-8911-bb06cd2c5c08'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (4260, '2cb74157-c70f-4bdc-9b8c-4d4b2de30994'), (29365, '27957276-e371-47f5-bfad-8d254469d746'), (25785, 'cc8b2b20-c91a-4342-b3f7-0f6232628ce7'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (8384, '631e3d73-c7ec-48e1-8ad8-2c3de8269b8b'), (8385, '209ccc61-402b-4a36-9fef-bedb43f80635'), (22209, 'd7ca43f5-90c6-448e-8d33-58c7acba90c4'), (16577, '65b23733-c969-4f5c-815e-b4d8c66dc8a9'), (31430, 'a0ae72d3-facd-4eed-a140-89022df4536b'), (31432, '86a2c216-1bce-4096-a3d1-5231869d83c6'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (14542, 'adc6532e-0c1e-4f00-a1f5-78b79796dba2'), (14545, 'b793424a-2b53-421d-b2ae-475afd81fc67'), (22740, '42178967-045d-4482-95a3-1453ae35af86'), (25814, '1938526d-56ae-43fe-850c-1f0af6bb61fe'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (29416, 'cc26e971-c084-4571-a0b9-7d98659720f5'), (16105, 'cfa816d5-09be-446f-a412-2b54cfa49648'), (10986, '43de7f8d-6eb7-420a-b184-aa48eb3517b8'), (11502, '330fe3ba-ad53-40c9-bac9-59cda30571b5'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (9459, '50af37d9-9d34-4347-9602-65d90571a951'), (16116, '6b7384a5-48d5-4762-b8b7-e12f2fa93c94'), (4341, 'd6b7f7ab-fb13-44d5-a3d1-5dbcb6ad0ddf'), (7416, '603d0497-9dca-491b-990e-1d3feded2210'), (18685, '52fdc5bf-09c1-40d7-85f6-9c586ebe5ef0'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (6419, '42aef9bd-e9bd-469a-ac13-036843c85c48'), (10534, '0b1a3ae1-395d-469d-93fb-f855d838a751'), (10539, 'f28af08a-cfce-44b3-988c-a51f3824de1f'), (10542, '6cf5c516-c66b-4848-a5f4-b8b0ae2bab1c'), (8503, 'a035c2be-ea22-4ac9-8f47-6603afbcf025'), (8504, '143ac2fe-2f79-4c07-bd2e-996611e74123'), (19262, 'cb4dea97-cfcf-4d5d-827a-c683ab6dcaf2'), (4940, '2177225e-79e0-4f93-abcb-4a22320bfe54'), (19792, 'b40ae752-d09a-4c60-89ae-4a4e5b46513e'), (22353, '466fdc29-f7dd-49cc-97b0-8676ca1a4540'), (14684, '5ae632de-fdb4-46f2-8f90-67cc8c042104'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (17246, '05cc2b18-009e-42a6-9b7d-6ef5c104c74b'), (21343, '08be7e91-b4c4-40b2-8817-a2af8d306009'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (24930, '1197e883-982a-43e8-9907-3861ab237c3e'), (15720, 'c3517980-a237-4b69-bf3b-15b281810859'), (31596, 'dffc937e-55b3-44ec-9044-5bcda5a6f429'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (29045, 'a1f71cbb-556e-4015-8e63-51461e02bf4e'), (21386, '0da28613-9bbd-48d8-a129-d3c4d7cf98df'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (19858, 'faa94907-c6f5-412b-aea1-d75f8b7fe3ce'), (18324, '0d9b0209-f2d7-4d15-b686-619e3d6fe098'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (24485, '4de8b02f-cf9b-40a4-9212-1d8e11cd76f5'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (19374, '0285c640-ebf8-4840-a8c1-1dcac4b4423f'), (22452, '3b2f2626-17d7-4664-ab7f-fb50dda63fdc'), (16315, '670f661d-5822-4779-aa29-436258e97b09'), (30651, 'c706c2d9-b3fb-4077-b6ae-aab8163627f5'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (26578, '05a08a96-8354-44f0-956c-62bc5f6ac12c'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (26080, '9b91b646-fa9f-4bb6-af8a-938897f96a99'), (17378, '2275b94a-b644-42e1-8ec3-e89326e8993f'), (487, '16dd9ded-8590-4746-89e1-353d0f5bd152'), (6120, '7f0e3f1a-b918-45b3-9c9a-40f0e58fcba6'), (8679, '181573b3-1316-4139-9198-4f5ce4c08cf6'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (17386, 'c7266810-ae8b-4c24-bbe4-90907c308678'), (9709, '9da5982b-6242-466f-a0c7-730444cb5016'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (26099, '90ce0b25-3f8a-4fd7-8118-d75557218a1c'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (31228, '6a3de332-01d3-429a-8f97-b4f5969219dd'), (13310, 'c6400126-c261-4fd7-aa82-0a4a91b16a06')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity
models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream
tasks outperforms supervised pre-training and shows promising scaling behavior.*
1. [Easy Deployment](https://huggingface.co/docs/inference-endpoints/index): Deploy models as production-ready APIs with just a few clicks, eliminating the need to handle infrastructure or MLOps.
2. [Cost Efficiency](https://huggingface.co/docs/inference-endpoints/autoscaling): Benefit from automatic scale to zero capability, reducing costs by scaling down the infrastructure when the endpoint is not in use, while paying based on the uptime of the endpoint, ensuring cost-effectiveness.
3. [Enterprise Security](https://huggingface.co/docs/inference-endpoints/security): Deploy models in secure offline endpoints accessible only through direct VPC connections, backed by SOC2 Type 2 certification, and offering BAA and GDPR data processing agreements for enhanced data security and compliance.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
### Controlling Image Generation

If you want finer-grained control over images generated with the SDXL endpoint, you can use the following parameters:

- `negative_prompt`: A text describing content that you want the model to steer _away_ from.
- `guidance_scale`: How closely you want the model to match the prompt. Lower numbers are less accurate, very high numbers might decrease image quality or generate artifacts.
- `width` and `height`: The desired image dimensions. SDXL works best for sizes between 768 and 1024.
- `num_inference_steps`: The number of denoising steps to run. Larger numbers may produce better quality but will be slower. Typical values are between 20 and 50 steps.

For additional details on text-to-image generation, we recommend you check the [diffusers library documentation](https://huggingface.co/docs/diffusers/using-diffusers/sdxl).

### Caching
Keywords: Data-Centric AI, Data Quality, Noisy Labels, Outlier Detection, Active Learning  

## [BentoML](https://github.com/bentoml/BentoML)

[BentoML](https://github.com/bentoml) is the unified framework for for building, shipping, and scaling production-ready AI applications incorporating traditional ML, pre-trained AI models, Generative and Large Language Models. 
All Hugging Face models and pipelines can be seamlessly integrated into BentoML applications, enabling the running of models on the most suitable hardware and independent scaling based on usage.

Keywords: BentoML, Framework, Deployment, AI Applications

## [LLaMA-Efficient-Tuning](https://github.com/hiyouga/LLaMA-Efficient-Tuning)
 
Here is the question: Which solution would best ensure that a deployed image generation model maintains high accuracy while scaling to large datasets in production environments?","Based on the context, the solution that would best ensure that a deployed image generation model maintains high accuracy while scaling to large datasets in production environments is the scalable approach mentioned in the first paragraph, which allows for learning high-capacity models that generalize well. This approach has been shown to achieve the best accuracy (87.8%) among methods that use only ImageNet-1K data. Additionally, the use of BentoML, a unified framework for building, shipping, and scaling production-ready AI applications"
Which combination of strategies is most likely to improve the performance of a speech-to-text model that can generalize well across different dialects and accents?,"[(18432, '7458df8c-e69b-43d8-a456-b53c44af4802'), (9728, '1cf55a61-9ed1-4a59-9f5b-74b10ffbbebe'), (9729, 'adf67345-2f5a-4658-ac51-c3f2ca2c0916'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (9733, 'd8a9b090-d24d-4517-9a0b-03d326562952'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (5130, '0402365e-06b9-415e-9f92-dfe339eb3334'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (4166, 'd62d2dfe-88e9-4269-a3b8-a0c08a9ead60'), (7238, 'eef72ab8-ee77-42ff-8c37-2520ee38ae8b'), (8267, '5870b739-749d-40fc-b471-e8b3aac03259'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (20652, '68f78608-9c1c-41a7-9309-a792d5069928'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (15028, '86c91e1c-d5df-4053-b7f8-f475c47edcf1'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (21191, '9cfe40a8-87c6-4a55-9f19-3458ca24d801'), (24267, '17a4733e-95f5-4a59-bf83-0455673132e8'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (15056, '3aeddfc0-2bd8-44a0-8d5d-def75c612a76'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (22234, '349be3ea-78f4-4a5c-9817-fba623b6c311'), (22235, 'c7600fbe-2a65-4774-a4a3-36b107198900'), (22236, '7f5de617-4a0c-4797-b576-11a06835ca5c'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (4839, '8b5fcfe4-422d-41ad-ba2a-435d4859c7ef'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (1793, 'ee32d640-6d91-4eca-a8d5-149f0e93d946'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (1812, 'abbeae44-35ad-4ce3-bc93-b8a186be4830'), (18200, 'c90b132b-e7b4-4305-99ea-9c6f13711dd2'), (18201, '2d381f7c-31b3-4fc8-a6f2-019f577e2bd0'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (296, 'da61bbf7-b3d9-4a4d-aeb4-69794ca76f5c'), (16173, '021868a6-6880-43b4-aba8-34b48a2ffff3'), (13617, '5d6f1844-a996-4b2b-9cce-54cc40faf578'), (13618, 'c26d0aa6-d937-42a4-80b1-3ebe7f3941a8'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (316, '3d68da03-3df0-4ea4-8d35-55ca5e6bf70e'), (317, 'ef8b2514-cf01-4f03-896e-2692824e2715'), (28993, 'e94a1fa2-5915-4591-a20a-5ae790985a23'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (6993, '4243bc5b-c7a6-4a08-8545-448007bdb844'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (11094, '305b7aba-9247-4c3f-9e13-4bafb67064b0'), (8023, '1dec94c2-ffe1-4e8f-a0e8-d220743cc341'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (1888, 'f359827c-9d02-4d84-a5c6-f5f2f4a48169'), (8034, '6748368f-01a0-48c5-9d0b-812ee69deeff'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (10615, '7bf25661-c8bb-4b65-9e16-786afc1051f8'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (15740, '1ff3f9f4-82bb-4766-8c01-807ece2b0afb'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (8073, '180fe363-498f-4421-9240-3ab8a0e413a0'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (23453, '24bbe86e-9ba8-4256-b0fc-6707f0f6b1c0'), (9117, '74400b1c-c3d4-4221-bf3d-d615c6862e50'), (9139, '2af98149-42ee-4660-988d-a8696d5547ac'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14788, '0d5f36b7-1369-4c9a-bf7c-f809d9bf9650'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (14802, '148eaad9-71ab-478b-b6d7-1e16ddec31d1'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (18411, '4be73c5f-f784-4448-8d45-1061cddc185a'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (5108, '0e1c7ec4-b9e7-4964-bac6-22f4455d6568'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (9723, 'a99c19cf-b2cf-4fdf-a29a-7f4b9b2d9329'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (9726, 'ef486ead-c143-4c25-a64a-bf67a128b117'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel sampling have been proposed, but their sample quality does not match that of two-stage TTS systems. In this work, we present a parallel end-to-end TTS method that generates more natural sounding audio than current two-stage models. Our method adopts variational inference augmented with normalizing flows and an adversarial training process, which improves the expressive power of generative modeling. We also propose a stochastic duration predictor to synthesize speech with diverse rhythms from input text. With the uncertainty modeling over latent variables and the stochastic duration predictor, our method expresses the natural one-to-many relationship in which a text input can be spoken in multiple ways with different pitches and rhythms. A subjective human evaluation (mean opinion score, or MOS) on the LJ Speech, a single speaker dataset, shows that our method outperforms the best
audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all
The performance of speech recognition system can often significantly be improved by adding a 
language model for decoding. For more information on how to add a language model, please 
take a look at [How to combine n-gram language models with speech recognition models](#how-to-combine-n-gram-with-model).
```

In our experience, obtaining satisfactory results from this model can be challenging. The quality of the speaker 
embeddings appears to be a significant factor. Since SpeechT5 was pre-trained with English x-vectors, it performs best 
when using English speaker embeddings. If the synthesized speech sounds poor, try using a different speaker embedding.

Increasing the training duration is also likely to enhance the quality of the results. Even so, the speech clearly is Dutch instead of English, and it does 
capture the voice characteristics of the speaker (compare to the original audio in the example).
Another thing to experiment with is the model's configuration. For example, try using `config.reduction_factor = 1` to 
see if this improves the results.
language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform
English-only pretraining when translating English speech into other languages, a setting which favors monolingual
pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.*
 
Here is the question: Which combination of strategies is most likely to improve the performance of a speech-to-text model that can generalize well across different dialects and accents?","Based on the context, the combination of strategies that is most likely to improve the performance of a speech-to-text model that can generalize well across different dialects and accents is:

1. Adding a language model for decoding
2. Using a large and diverse set of speaker embeddings, including those from different languages and dialects
3. Increasing the training duration
4. Experimenting with different model configurations, such as adjusting the reduction factor
5. Using cross-lingual pretraining, which can"
"What method would best increase the speed and accuracy of a machine translation system handling multiple languages, including low-resource languages?","[(23043, 'e61d83b5-1f81-4bac-a218-ef6fdbd96486'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (18998, '989e75dd-dc43-4eb5-91cb-215c4e4adc17'), (21561, '68fa59f6-381e-4dee-ae59-873812588c22'), (19004, '112963e4-e074-4f17-aa4e-d0a1114feec3'), (19016, '02c05bd4-cf3d-4ffc-a737-bb651f95a396'), (28745, 'be778586-e097-4669-897c-e0f7ae5da8bb'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (10831, '9a5c4c34-d833-48bc-b903-b7202a7ae3c1'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (7293, 'f67a56aa-a60f-4938-b212-3d7d37f3497f'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (8855, 'd6b42baf-f1da-407c-9f6e-bede792b1cd2'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (9374, 'd670622a-7d07-43ea-be13-2a901b9951f9'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (19619, '06d68a4b-ca65-47bc-baaa-c8d78d5f9136'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8879, 'b6bcb9c3-6460-46ff-a441-44af5882b728'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (19643, '11f17d0a-4f66-4c28-9733-118d5b0060ae'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (23039, '4450f9a2-9c97-4909-80aa-9e8a8dd26457'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (8417, 'f2f562af-44ee-4093-ae3b-2d887cb90a71'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (11010, '08c44491-b715-4d4f-9e79-ac87d9c5c9a2'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11012, '2c7ed7d6-155a-4473-b3f7-232806a7a924'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (12095, 'c4744c46-a374-4937-a8a6-60a1818dbaae'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (3415, '876b5010-859a-4a05-a141-541630e6217c'), (22360, '62b0534c-a8bc-4b96-92f2-012032e87932'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (24422, 'd216e4c9-d54b-4244-a3ee-e2c1a8544b1d'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (28567, '90c05a6b-2b62-42a0-a6a4-240253ef999e'), (10648, '66ffc54d-754b-40e5-b926-93bd59b981cb'), (27545, 'c9fb2425-2ac4-4707-88ad-e11ddfbab842'), (27546, '837105a9-01c9-4d5a-95d6-0812b7fb9c20'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (7601, '3ecbf4ed-0cd3-483b-88d7-128fe443adff'), (29617, 'ff9c2445-4c03-4813-a426-7902846a33df'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (9722, 'd6ba2c4b-251f-4927-8327-3ec8ab6122d3'), (9723, 'a99c19cf-b2cf-4fdf-a29a-7f4b9b2d9329'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive
*Existing work in translation demonstrated the potential of massively multilingual machine translation by training a
single model able to translate between any pair of languages. However, much of this work is English-Centric by training
only on data which was translated from or to English. While this is supported by large sources of training data, it
does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation
model that can translate directly between any pair of 100 languages. We build and open source a training dataset that
covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how
to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters
to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
 
Here is the question: What method would best increase the speed and accuracy of a machine translation system handling multiple languages, including low-resource languages?","Based on the context, the method that would best increase the speed and accuracy of a machine translation system handling multiple languages, including low-resource languages, is a combination of:

1. Using a conditional compute model based on Sparsely Gated Mixture of Experts, which is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages.
2. Implementing multiple architectural and training improvements to counteract overfitting while training on thousands of tasks.
3. Utilizing a massively"
Which combination of approaches is most effective for optimizing the training of large models for machine translation across multiple languages while managing computational resources efficiently?,"[(12288, '52b18bc8-0a3d-4c11-80da-5eb6fc2e2137'), (7688, '157a8a18-f0d3-4b4c-9ca0-8abdff1aa12f'), (3597, '837a4f94-d203-4386-b6fe-7eca8f6530c4'), (3609, '9c9d57b3-97d3-4876-a264-de89cd01917f'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (18998, '989e75dd-dc43-4eb5-91cb-215c4e4adc17'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (5690, '85c878e3-f8a0-4571-85d1-e2953c33312b'), (19004, '112963e4-e074-4f17-aa4e-d0a1114feec3'), (19008, '3924cd7e-cd8d-4514-a715-97cc3e28b436'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (19014, '28a449dd-bfb5-4851-90ff-3b2cd97264cc'), (7238, 'eef72ab8-ee77-42ff-8c37-2520ee38ae8b'), (19016, '02c05bd4-cf3d-4ffc-a737-bb651f95a396'), (28745, 'be778586-e097-4669-897c-e0f7ae5da8bb'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (10831, '9a5c4c34-d833-48bc-b903-b7202a7ae3c1'), (82, '9b1467ac-93fd-4e92-ae66-0c561a9fe744'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (5716, 'a521babf-2a86-410a-9a2c-16cd35ecd60f'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (30304, '69655f7c-2296-4da4-9562-97dbe94a3c0f'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (6758, '3d29e927-9a4f-4916-9290-e53743ad7e85'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (9374, 'd670622a-7d07-43ea-be13-2a901b9951f9'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (24267, '17a4733e-95f5-4a59-bf83-0455673132e8'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (22234, '349be3ea-78f4-4a5c-9817-fba623b6c311'), (8924, '28be2b30-bdcc-4a8c-a385-026b57358177'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (24289, 'd4f82980-e65c-4a7e-9b89-0bfcddd019cb'), (6899, '5fc72f0c-9870-4f07-bc4f-c8e16dc0b4f2'), (6903, '807eb0ed-deee-4627-8c1d-d65ce5f2f592'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (1791, '6ece2b29-05a2-4bf8-b063-f7da0cbec2ff'), (11009, 'eb1201bc-1aad-46e8-8ded-a206fbebb020'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11012, '2c7ed7d6-155a-4473-b3f7-232806a7a924'), (22794, '364ec0e7-da72-440c-aef3-1db9aed7f3dc'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (10510, '65932cde-592e-47aa-b229-26bebb986d68'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (6427, '5d10a553-2531-49b7-a3a4-1894936c7a07'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (11050, '72a28810-35cc-4b6e-8d80-a3fa88dce093'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (30534, '95e21773-3553-4d8f-8b88-fff6315116ba'), (26960, '0d8543ba-1ac5-42b6-8a07-6f3216b29f08'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (16227, '052f6047-255b-488b-a997-488387ae3155'), (26979, 'eb432651-3843-474d-a2df-74d722a2c569'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (15740, '1ff3f9f4-82bb-4766-8c01-807ece2b0afb'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (8065, 'ad6fcc55-b711-476c-9874-0eb440cca8ba'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (23941, '4552e7c6-32c1-47d7-9c48-1cd66703f9a7'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (8073, '180fe363-498f-4421-9240-3ab8a0e413a0'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (8077, 'f3bb984f-ab7c-46f2-ad0b-1672833fcea5'), (10648, '66ffc54d-754b-40e5-b926-93bd59b981cb'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (13759, 'e3f7ce62-98c1-4212-90c1-fc43513f4104'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (9722, 'd6ba2c4b-251f-4927-8327-3ec8ab6122d3'), (12287, '2495e672-afff-44de-9495-f9352da75fa6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Existing work in translation demonstrated the potential of massively multilingual machine translation by training a
single model able to translate between any pair of languages. However, much of this work is English-Centric by training
only on data which was translated from or to English. While this is supported by large sources of training data, it
does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation
model that can translate directly between any pair of 100 languages. We build and open source a training dataset that
covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how
to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters
to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly
*Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding.
In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We
propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual
data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain
state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our
approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we
obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised
machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the
*Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language 
tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, 
their training data is dominated by English, potentially limiting their cross-lingual generalization. 
In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, 
and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters 
sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size 
in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) 
and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark,
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
 
Here is the question: Which combination of approaches is most effective for optimizing the training of large models for machine translation across multiple languages while managing computational resources efficiently?","Based on the context, the most effective combination of approaches for optimizing the training of large models for machine translation across multiple languages while managing computational resources efficiently is:

1. **Dense scaling and language-specific sparse parameters**: This approach, mentioned in the first paragraph, allows for increasing model capacity while managing computational resources efficiently.
2. **Cross-lingual pretraining**: This approach, mentioned in the second paragraph, has been shown to be effective in improving machine translation performance across multiple languages.
3. **"
What is the most effective method to improve both the speed and accuracy of a machine translation system for diverse languages while scaling to handle large datasets?,"[(23040, 'f17c1799-6922-4207-906f-59ecf4ef6c33'), (23043, 'e61d83b5-1f81-4bac-a218-ef6fdbd96486'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (7, 'bb62d85a-4224-492b-9ffe-70d571ed09ab'), (3597, '837a4f94-d203-4386-b6fe-7eca8f6530c4'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (21542, '387734b4-81ba-4aec-8bce-c7d8cf647f9f'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (18998, '989e75dd-dc43-4eb5-91cb-215c4e4adc17'), (19004, '112963e4-e074-4f17-aa4e-d0a1114feec3'), (19016, '02c05bd4-cf3d-4ffc-a737-bb651f95a396'), (28745, 'be778586-e097-4669-897c-e0f7ae5da8bb'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (8266, 'c6c13dde-7bcb-40a7-b9d7-2832d1279dc1'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (82, '9b1467ac-93fd-4e92-ae66-0c561a9fe744'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19962, '04858203-03cf-45bb-9b5a-7f506048c00c'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (26732, 'c9cde91d-622f-493d-badd-ada7439748d9'), (26739, '0b12c644-853c-4e0f-a818-938083349460'), (26740, '1717231a-ec52-48aa-a32c-ff19191abfea'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (7293, 'f67a56aa-a60f-4938-b212-3d7d37f3497f'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (7296, 'b2992b22-61a5-427a-bfce-ccccf61a44ee'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (8855, 'd6b42baf-f1da-407c-9f6e-bede792b1cd2'), (9374, 'd670622a-7d07-43ea-be13-2a901b9951f9'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8879, 'b6bcb9c3-6460-46ff-a441-44af5882b728'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (8882, 'b6f92498-3fe6-4e9b-92b8-c1aeac3e22aa'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (9947, '0c45b6f2-397b-4c85-8986-5fdde05232a8'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (24295, '9c24b89c-d484-4cb0-b384-1bbfa5bbd033'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (14582, '6313f6ff-1119-4270-be6e-af65636c95ce'), (3320, '3f22941d-5a99-4599-b75a-7aac2efeee6e'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (11010, '08c44491-b715-4d4f-9e79-ac87d9c5c9a2'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (26960, '0d8543ba-1ac5-42b6-8a07-6f3216b29f08'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (3415, '876b5010-859a-4a05-a141-541630e6217c'), (22360, '62b0534c-a8bc-4b96-92f2-012032e87932'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (24943, '5d16e6db-2729-4b64-89c7-72d8b0f88da1'), (24949, '83efc35a-a86a-47fb-9f8c-04d42a95a7a8'), (12287, '2495e672-afff-44de-9495-f9352da75fa6'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (1448, '58f14425-82d6-4130-83df-faf5f82799d4'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (4534, '3ff4dd5c-528a-4039-b0d8-9c5fd34866bb'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (26057, '9ed838e2-7b57-4bf0-ac30-97ec4268ad86'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (9722, 'd6ba2c4b-251f-4927-8327-3ec8ab6122d3'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (23039, '4450f9a2-9c97-4909-80aa-9e8a8dd26457')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Existing work in translation demonstrated the potential of massively multilingual machine translation by training a
single model able to translate between any pair of languages. However, much of this work is English-Centric by training
only on data which was translated from or to English. While this is supported by large sources of training data, it
does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation
model that can translate directly between any pair of 100 languages. We build and open source a training dataset that
covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how
to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters
to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly
*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
*Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language 
tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, 
their training data is dominated by English, potentially limiting their cross-lingual generalization. 
In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, 
and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters 
sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size 
in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) 
and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark,
 
Here is the question: What is the most effective method to improve both the speed and accuracy of a machine translation system for diverse languages while scaling to handle large datasets?","Based on the context, the most effective method to improve both the speed and accuracy of a machine translation system for diverse languages while scaling to handle large datasets is through a combination of dense scaling and language-specific sparse parameters, as well as using novel and effective data mining techniques tailored for low-resource languages. Additionally, using a conditional compute model based on Sparsely Gated Mixture of Experts and training on a balanced corpus covering a diverse set of languages can also improve the performance of the system. Furthermore,"
Which combination of techniques would best enhance the speed and accuracy of a machine translation system while deploying it globally and handling diverse datasets?,"[(22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (18998, '989e75dd-dc43-4eb5-91cb-215c4e4adc17'), (19004, '112963e4-e074-4f17-aa4e-d0a1114feec3'), (19016, '02c05bd4-cf3d-4ffc-a737-bb651f95a396'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (26732, 'c9cde91d-622f-493d-badd-ada7439748d9'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (26739, '0b12c644-853c-4e0f-a818-938083349460'), (26740, '1717231a-ec52-48aa-a32c-ff19191abfea'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19604, '97b911ed-5d6d-4a6b-b2b1-60cd3abef208'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (8855, 'd6b42baf-f1da-407c-9f6e-bede792b1cd2'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (19619, '06d68a4b-ca65-47bc-baaa-c8d78d5f9136'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8879, 'b6bcb9c3-6460-46ff-a441-44af5882b728'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (19643, '11f17d0a-4f66-4c28-9733-118d5b0060ae'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (700, 'b300514d-f85d-4fdc-b535-980caa8a7eb6'), (11452, '7e2fd91b-ca25-40e4-9aea-47ac22782115'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (17626, '7f59372b-cf68-4fbe-8aad-444ff614a6e7'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (735, 'c25f03c5-5326-4c07-ab56-3fb43f185d39'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (24295, '9c24b89c-d484-4cb0-b384-1bbfa5bbd033'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (17661, '64a5806b-66d6-44ce-ae4d-3cac3d217bde'), (11010, '08c44491-b715-4d4f-9e79-ac87d9c5c9a2'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (23303, 'c8340867-9c9b-4274-adb6-873aeaef3797'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (2836, 'dad2f3b4-337f-49e3-ae8a-2714f6bb79e0'), (6427, '5d10a553-2531-49b7-a3a4-1894936c7a07'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (6454, '50a82ca7-e3f2-482c-bb94-25841dba76f3'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (29002, '8afe6522-4963-48c6-91de-5a7e7715c4bb'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (3415, '876b5010-859a-4a05-a141-541630e6217c'), (22360, '62b0534c-a8bc-4b96-92f2-012032e87932'), (21342, '43c34d2b-8a1a-4a72-8117-9d57107098c8'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (21362, '651a9fca-5d13-4c33-9dd8-27cca38b640b'), (15740, '1ff3f9f4-82bb-4766-8c01-807ece2b0afb'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (1448, '58f14425-82d6-4130-83df-faf5f82799d4'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (4534, '3ff4dd5c-528a-4039-b0d8-9c5fd34866bb'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (19962, '04858203-03cf-45bb-9b5a-7f506048c00c'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Existing work in translation demonstrated the potential of massively multilingual machine translation by training a
single model able to translate between any pair of languages. However, much of this work is English-Centric by training
only on data which was translated from or to English. While this is supported by large sources of training data, it
does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation
model that can translate directly between any pair of 100 languages. We build and open source a training dataset that
covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how
to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters
to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly
The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.

In this guide, we will go over the effective techniques for efficient LLM deployment:

1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve computational advantages without a considerable decline in model performance.

2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a
 
Here is the question: Which combination of techniques would best enhance the speed and accuracy of a machine translation system while deploying it globally and handling diverse datasets?","Based on the context, the combination of techniques that would best enhance the speed and accuracy of a machine translation system while deploying it globally and handling diverse datasets is:

1. **Lower Precision**: Operating at reduced numerical precision (8-bit and 4-bit) to achieve computational advantages without a considerable decline in model performance.
2. **Flash Attention**: Using a variation of the attention algorithm that provides a more memory-efficient approach and realizes increased efficiency due to optimized GPU memory utilization.
3. **Dense Scaling"
Which approach would most effectively optimize the deployment of an image generation model that must handle diverse data types and real-time user interactions?,"[(22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (12305, '2e56823c-16a9-4e2a-81cb-823ada1fe8fe'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (15898, 'b6f9af12-8c93-4907-8584-6063e7adc079'), (14875, '863f500b-8805-487f-a9d0-1a5268b9eb36'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (5158, '2a7fe1df-7c54-4ebf-9da2-15523efb3fd1'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (9284, 'b612daf8-a308-4ac1-a05d-d1af0da0a26b'), (30792, '43ed9e13-950f-4202-8101-cfcff37a492b'), (11338, '30d2c084-1101-4560-b412-45d9eae8ef69'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (590, '6523fa0b-894c-49e0-bfb1-fb3213623bf3'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (31326, '88d3c0ac-55f8-46d8-9da4-9a3c9f1f4db9'), (31327, '2b86ceba-d1b2-479e-a3b5-8a44be247b66'), (14434, '88961ae2-f1af-4f1b-a627-43082e2f0710'), (2663, 'efa7e63d-051b-4d0d-b130-483c5160dd59'), (14439, '86de8893-816f-4b63-b9bc-ba2c4c214252'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (14448, '137a6dbc-1a8d-494d-9bb2-9df4702ab8c9'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (11378, '3ec82f42-0790-49d3-b5aa-eb72bd30cc34'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (25757, '24c0f4d2-56ee-43a1-8911-bb06cd2c5c08'), (25764, 'e6b7b172-a955-4460-a174-888528a760ca'), (2733, '93647d71-2542-44d6-8e19-fe6737bb6b5c'), (8366, '802fb177-2a03-456c-b39c-e9d1d7f566ed'), (2743, '4dec49b3-4714-4c18-a903-59f4b55e455c'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (8384, '631e3d73-c7ec-48e1-8ad8-2c3de8269b8b'), (8385, '209ccc61-402b-4a36-9fef-bedb43f80635'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (709, '68b25497-c5e6-494d-aebf-9f31f7c98cc0'), (31432, '86a2c216-1bce-4096-a3d1-5231869d83c6'), (21706, '4021901a-6386-4fb0-a6a5-6a38d5a2ef66'), (19155, '1e812ea9-3a09-4467-be78-c43f09a00fc7'), (22739, '83649c52-3276-4a79-b854-4166d306a52a'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (28378, 'f04df4ea-ea84-4c3a-ae6d-00f74ddce1d2'), (17635, 'cb4c9589-721d-4162-839c-0a37be083184'), (10986, '43de7f8d-6eb7-420a-b184-aa48eb3517b8'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (10988, 'dcb73d03-986e-4472-8d2c-652914966a8d'), (19697, '2bb5d1ca-7bcf-4074-b6a2-32572672320c'), (9458, '186c90ec-1442-4fa7-81ee-4f33e0ca3e17'), (18676, '19268cbe-5f8f-44b7-9603-fd3d64a79dc2'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (5898, 'a084a22d-2a09-49ca-b08f-541f0667cadf'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (15646, '6f1d3cc0-3c8a-4c86-abf4-65e1bf901355'), (10534, '0b1a3ae1-395d-469d-93fb-f855d838a751'), (10535, 'a92216b1-3959-48ee-80b2-f50cec8a3460'), (10539, 'f28af08a-cfce-44b3-988c-a51f3824de1f'), (10542, '6cf5c516-c66b-4848-a5f4-b8b0ae2bab1c'), (10545, '96a8e973-9ee0-4b63-98cd-8bf3f5b5693e'), (30514, '074fc512-721a-407f-8b23-13283273c2c0'), (6975, 'd66e5cdd-564c-41fe-a9d8-4ba2c2370002'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (19797, '73b4b557-9cbb-451f-9730-463908061bb1'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (5477, '0a7e9332-7b2a-4aa6-86a1-427dd6f19f03'), (15720, 'c3517980-a237-4b69-bf3b-15b281810859'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (7026, 'a54a170b-414e-4326-bff0-925d0cf07fab'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (21876, '19105ac4-8965-4312-b78a-ae73f24b4835'), (892, 'cfc82097-8eff-4f3e-bb63-cb685a59f4a9'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (29061, '151f0f63-15a4-4d08-ae12-ab05df74ddea'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (17811, '525ac724-ad6c-419a-a3a2-1715aa66fbd7'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (10652, '67b22c05-7dd6-4e0b-a38b-0dad7c44ec83'), (10654, '047dde45-f90b-4366-b2bf-77bde03d199c'), (10655, 'b65f54ab-b114-433c-b9d8-c631d2973538'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (24485, '4de8b02f-cf9b-40a4-9212-1d8e11cd76f5'), (10667, '99592116-e7df-436c-aefb-86283541f025'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (30651, 'c706c2d9-b3fb-4077-b6ae-aab8163627f5'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (1477, '608aa8f3-b53c-4fa8-ab06-0d7cf3dc4069'), (1480, 'bcc4845e-a833-4f85-ae18-33311f77a2d3'), (1482, '3cc971d3-2bbf-4e80-aab1-f56f3db2a358'), (4045, 'd183454f-b6ee-4379-8bbb-674c8b2b0384'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (19927, '0fb8b493-49ef-4bd7-84fe-d2f87486e82d'), (17367, '96ac68a8-96d9-460e-98ee-59b8f1dddccd'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (29161, '5c1a0615-2746-4f25-8cae-56a17b9d371d'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13296, '1deb15a7-660c-4670-836e-ea98398cf41b'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Recent advances in text-to-image generation with diffusion models present transformative capabilities in image quality. However, user controllability of the generated image, and fast adaptation to new tasks still remains an open challenge, currently mostly addressed by costly and long re-training and fine-tuning or ad-hoc adaptations to specific image generation tasks. In this work, we present MultiDiffusion, a unified framework that enables versatile and controllable image generation, using a pre-trained text-to-image diffusion model, without any further training or finetuning. At the center of our approach is a new generation process, based on an optimization task that binds together multiple diffusion generation processes with a shared set of parameters or constraints. We show that MultiDiffusion can be readily applied to generate high quality and diverse images that adhere to user-provided controls, such as desired aspect ratio (e.g., panorama), and spatial guiding signals,
models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation).*
This model is exceptionally versatile and can be used for a wide range of image and multimodal tasks. However, 
being a large model means it requires significant computational resources and infrastructure. It is up to you to decide whether 
this approach suits your use case better than fine-tuning specialized models for each individual task.
- Most Cost-Effective Deployment: For users looking for good performance at low cost
- Best Latency Deployment: Minimizing latency for real-time services
- Best Throughput Deployment: Maximizing tokens processed per second

To keep this benchmark fair, transparent, and reproducible, we share all of the assets, code, and data we used and collected: 

- [GitHub Repository](https://github.com/philschmid/text-generation-inference-tests/tree/master/sagemaker_llm_container)
- [Raw Data](https://github.com/philschmid/text-generation-inference-tests/tree/master/results/sagemaker)
- [Spreadsheet with processed data](https://docs.google.com/spreadsheets/d/1PBjw6aG3gPaoxd53vp7ZtCdPngExi2vWPC0kPZXaKlw/edit?usp=sharing)

We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Before we get into the benchmark and data, let's look at the technologies and methods we used.
--
title: Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum
thumbnail: /blog/assets/train_optimize_sd_intel/thumbnail.png
authors:
- user: AlexKoff88
  guest: true
- user: MrOpenVINO
  guest: true
- user: helenai
  guest: true
- user: sayakpaul
- user: echarlaix
---

# Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum


[**Latent Diffusion models**](https://arxiv.org/abs/2112.10752) are game changers when it comes to solving text-to-image generation problems. [**Stable Diffusion**](https://stability.ai/blog/stable-diffusion-public-release) is one of the most famous examples that got wide adoption in the community and industry. The idea behind the Stable Diffusion model is simple and compelling: you generate an image from a noise vector in multiple small steps refining the noise to a latent image representation. This approach works very well, but it can take a long time to generate an image if you do not have access to powerful GPUs.
 
Here is the question: Which approach would most effectively optimize the deployment of an image generation model that must handle diverse data types and real-time user interactions?","Based on the context, the approach that would most effectively optimize the deployment of an image generation model that must handle diverse data types and real-time user interactions is the MultiDiffusion framework. This framework enables versatile and controllable image generation using a pre-trained text-to-image diffusion model, without any further training or fine-tuning, and can handle diverse data types and real-time user interactions."
What combination of techniques would be most effective for accelerating the training and deployment of a large-scale ASR system capable of handling diverse accents and noisy environments?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (21541, '31ad33fe-d4bd-41c7-87fe-097b9a37929d'), (21542, '387734b4-81ba-4aec-8bce-c7d8cf647f9f'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (30269, 'ce1b2ec6-16bc-48d4-8e48-ee802e65abe6'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (4166, 'd62d2dfe-88e9-4269-a3b8-a0c08a9ead60'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19033, 'b32a6a6b-73de-4fa7-9615-58c2fbfe7015'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (19044, '59dbc5e3-e3e8-4301-b9da-90a44c2cec46'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (25708, '0a161716-5f3c-45c5-aac2-4eb719f697f4'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (26255, '30372f00-e006-40e8-9ba2-91e9ccbec1b7'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708'), (3748, '332644a1-eafc-4d25-b177-40728dd8f84b'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (21191, '9cfe40a8-87c6-4a55-9f19-3458ca24d801'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (20696, '9326d027-ee93-4e0a-805c-6c36950bf184'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26334, 'f86f1902-9868-454f-9ef6-f7c11ce29015'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (26945, '285ce1b6-c5b2-4819-bdd9-565f712e1d18'), (26946, 'd51e55e0-4a83-4946-a69c-5343acec6844'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (28485, '90b822fb-e259-470d-9f26-b588fed88f91'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (19290, 'bdaea1c7-fd99-40b6-873e-6221088c435a'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13186, '8a21de92-498b-4425-8685-3721d3a656f7'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (31622, 'c012b457-a421-4af8-92d1-38172f9768b8'), (18823, 'ab017a78-5d1e-4298-b7ee-8e304879d848'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (13195, 'd9d25c33-f225-4c1f-80aa-eca578fcd3fb'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (18839, 'ceaa5ae9-6be8-42c5-8a11-0e0d7e0c8e81'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (18844, 'c97c2cdd-44fb-4531-87d5-afef8f19b517'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (4511, 'af6fa441-8f03-460f-ad80-c0b9214b289c'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (13220, 'f005be6c-563f-4211-a293-7269de39512a'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (27072, '5557dbf2-8acf-4d5d-8381-ece6fc992f43'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: </Tip>

This guide will show you how to train a [`openai/whisper-large-v2`](https://huggingface.co/openai/whisper-large-v2) model for multilingual automatic speech recognition (ASR) using a combination of `int8` quantization and LoRA. You'll train Whisper for multilingual ASR on Marathi from the [Common Voice 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) dataset.

Before you start, make sure you have all the necessary libraries installed:

```bash
!pip install -q peft transformers datasets accelerate evaluate jiwer bitsandbytes
```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use 🤗 Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
To adapt to 1000+ different vocabularies, **MMS** uses of Adapters - a training method where only a small fraction of model weights are trained.

Adapter layers act like linguistic bridges, enabling the model to leverage knowledge from one language when deciphering another.

## **Fine-tuning MMS**

**MMS** unsupervised checkpoints were pre-trained on more than **half a million** hours of audio in over **1,400** languages, ranging from 300 million to one billion parameters.

You can find the pretrained-only checkpoints on the 🤗 Hub for model sizes of 300 million parameters (300M) and one billion parameters (1B):

-   [**`mms-300m`**](https://huggingface.co/facebook/mms-300m)
-   [**`mms-1b`**](https://huggingface.co/facebook/mms-1b)

*Note*: If you want to fine-tune the base models, you can do so in the exact same way as shown in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
 
Here is the question: What combination of techniques would be most effective for accelerating the training and deployment of a large-scale ASR system capable of handling diverse accents and noisy environments?","Based on the context, the most effective combination of techniques for accelerating the training and deployment of a large-scale ASR system capable of handling diverse accents and noisy environments would be:

1. **Quantization**: Using `int8` quantization to reduce the model size and accelerate inference.
2. **LoRA**: Leveraging LoRA (Low-Rank Adaptation) to fine-tune the pre-trained model on specific datasets and languages, which can improve the model's performance on low-resource languages.
"
Which strategy would best enable efficient large-scale deployment of a vision model while maintaining accuracy for real-time image generation tasks?,"[(11264, 'f1b68b88-f347-4a90-9d24-0cae81ae704e'), (22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (31238, '4cfd0ce4-53bc-4ba0-88d1-f313d73cc529'), (11275, 'ac9902bd-95f9-498d-9481-a3722edfd549'), (12305, '2e56823c-16a9-4e2a-81cb-823ada1fe8fe'), (26135, '11058a27-6551-4d3e-bfc6-3bb4a8a0a224'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (26139, '6a2a97d3-a79f-4345-8ae5-db32b0108d69'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (4646, '13611a54-20bb-45b0-89e7-31b0bb22b402'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (21049, '8462308a-79bd-461f-87fd-f98e7b1aa3d5'), (11338, '30d2c084-1101-4560-b412-45d9eae8ef69'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (3158, '1eb57504-ed08-48bb-8092-c53d5c64b0c1'), (22102, 'aff80ee3-6694-4bab-acd1-453fcb62fc3c'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (31326, '88d3c0ac-55f8-46d8-9da4-9a3c9f1f4db9'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (13921, '901f7fe0-f8b1-43a0-9a5c-2cdbe80ba0d2'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (20068, 'd2d9fdee-7033-4cda-a724-d9c2e219656c'), (5733, 'd7b94280-d5e4-4cbe-8307-82acc7c174e7'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (14442, '63643091-647a-4c7f-a8e1-f89173754c81'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (14448, '137a6dbc-1a8d-494d-9bb2-9df4702ab8c9'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (23191, '8c2efd68-2cbd-49b2-b5b0-9b40d0c49323'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (25757, '24c0f4d2-56ee-43a1-8911-bb06cd2c5c08'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (14008, 'c13cab4e-22a7-4e41-acbb-b7ad3a5fcf37'), (25785, 'cc8b2b20-c91a-4342-b3f7-0f6232628ce7'), (10425, 'd741b05e-1f43-419b-bc9e-9e1d9fa9cfbf'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (28868, '9e9e8ba0-7dc0-4aa1-b5c7-5114ad8fc24c'), (31432, '86a2c216-1bce-4096-a3d1-5231869d83c6'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (29416, 'cc26e971-c084-4571-a0b9-7d98659720f5'), (16105, 'cfa816d5-09be-446f-a412-2b54cfa49648'), (10986, '43de7f8d-6eb7-420a-b184-aa48eb3517b8'), (10988, 'dcb73d03-986e-4472-8d2c-652914966a8d'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (4341, 'd6b7f7ab-fb13-44d5-a3d1-5dbcb6ad0ddf'), (7413, '84ce2e0f-b4a5-457a-9916-37f1b7abd93d'), (8438, '38f99541-7669-40e7-9a5c-5d2a3979c084'), (8442, 'a3ef01dd-4670-4ea9-b3a2-23c8d2beb69c'), (5884, '8b1efd71-fa8f-4e3a-85c9-811e0d3446a7'), (18685, '52fdc5bf-09c1-40d7-85f6-9c586ebe5ef0'), (8446, '2d6a4ff5-b300-4ca1-a02e-415dac3cc0fd'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (21768, '61ac4e62-e904-4c53-9623-9416d918cb0e'), (7433, '8b4a851e-9dbc-46a4-a7e6-6b51652c06b6'), (12045, '7efab0f1-50ed-4755-813f-824ba7849ab5'), (12047, '129e9113-1c4c-482f-8100-332f2ee98acd'), (17680, '3419d462-493a-4505-8f11-5ebae03c56fb'), (12049, '9ebd4171-a0ce-40d1-9883-c37ead9490d1'), (8478, 'f1105a01-3245-4970-a80e-abc6d46a47eb'), (19238, '58a1efef-2251-4429-83a5-78aac9e46613'), (10542, '6cf5c516-c66b-4848-a5f4-b8b0ae2bab1c'), (4418, '9410b13d-3f78-4fa7-aeb6-8361c7619510'), (4419, '54f34e71-e4b0-49b3-9756-3378458d26e3'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (15715, 'b6b83e1b-e049-44a5-bc43-dca8acd67993'), (5477, '0a7e9332-7b2a-4aa6-86a1-427dd6f19f03'), (27500, '599c9eb6-cf17-4386-bfea-2e4fd61ed9e7'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (29045, 'a1f71cbb-556e-4015-8e63-51461e02bf4e'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (13173, '944071de-f871-424d-9d14-424c81a28966'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (17825, '25ff9fc5-3959-4e46-bc23-de61ba1996b6'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (24484, '988913b9-fdd8-4eac-81ab-b81d33deb3fc'), (24485, '4de8b02f-cf9b-40a4-9212-1d8e11cd76f5'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (24488, '00dc5bd3-aa5f-4042-96c2-9ccbc96219e6'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (30651, 'c706c2d9-b3fb-4077-b6ae-aab8163627f5'), (16315, '670f661d-5822-4779-aa29-436258e97b09'), (31681, 'cd70c10d-6d2b-404f-842b-5ea26dc18134'), (24514, '9387e120-9dc1-47c1-b901-5e0ee5bccb3e'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (17349, 'b234fb6f-a453-4314-910c-4c6ab029a1a0'), (30672, '96d116bb-46a4-48bf-9459-9c455de6dc07'), (4056, '3920858a-e8fe-4c43-b8b4-45935bcec4eb'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (4583, 'a3f57615-f606-43dc-aa22-c8a64febe696'), (487, '16dd9ded-8590-4746-89e1-353d0f5bd152'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (11244, '97a1d0f6-d4f7-42ad-b163-5db44561211c'), (26099, '90ce0b25-3f8a-4fd7-8118-d75557218a1c'), (13303, 'e6783d90-3750-4204-9c37-e9973ef7bf05'), (23033, '6f279ca5-4d7c-473f-95e5-f014fba36eda')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity
models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream
tasks outperforms supervised pre-training and shows promising scaling behavior.*
In this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment:

1.  **Lower Precision**: Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.

2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.
*Vision Transformers (ViT) have shown rapid progress in computer vision tasks, achieving promising results on various benchmarks. 
However, due to the massive number of parameters and model design, e.g., attention mechanism, ViT-based models are generally 
times slower than lightweight convolutional networks. Therefore, the deployment of ViT for real-time applications is particularly 
challenging, especially on resource-constrained hardware such as mobile devices. Recent efforts try to reduce the computation 
complexity of ViT through network architecture search or hybrid design with MobileNet block, yet the inference speed is still 
unsatisfactory. This leads to an important question: can transformers run as fast as MobileNet while obtaining high performance? 
To answer this, we first revisit the network architecture and operators used in ViT-based models and identify inefficient designs.
The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.

In this guide, we will go over the effective techniques for efficient LLM deployment:

1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve computational advantages without a considerable decline in model performance.

2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.
- Most Cost-Effective Deployment: For users looking for good performance at low cost
- Best Latency Deployment: Minimizing latency for real-time services
- Best Throughput Deployment: Maximizing tokens processed per second

To keep this benchmark fair, transparent, and reproducible, we share all of the assets, code, and data we used and collected: 

- [GitHub Repository](https://github.com/philschmid/text-generation-inference-tests/tree/master/sagemaker_llm_container)
- [Raw Data](https://github.com/philschmid/text-generation-inference-tests/tree/master/results/sagemaker)
- [Spreadsheet with processed data](https://docs.google.com/spreadsheets/d/1PBjw6aG3gPaoxd53vp7ZtCdPngExi2vWPC0kPZXaKlw/edit?usp=sharing)

We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Before we get into the benchmark and data, let's look at the technologies and methods we used.
 
Here is the question: Which strategy would best enable efficient large-scale deployment of a vision model while maintaining accuracy for real-time image generation tasks?","Based on the context, the strategy that would best enable efficient large-scale deployment of a vision model while maintaining accuracy for real-time image generation tasks is using **Lower Precision** and **Flash Attention**. These techniques can accelerate training, improve accuracy, and provide computational advantages without a considerable decline in model performance, making them suitable for real-time image generation tasks."
Which strategy would best optimize the performance of a multilingual machine translation system that must scale for diverse languages and large datasets?,"[(23043, 'e61d83b5-1f81-4bac-a218-ef6fdbd96486'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (28745, 'be778586-e097-4669-897c-e0f7ae5da8bb'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (19962, '04858203-03cf-45bb-9b5a-7f506048c00c'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (8797, '0855748e-ea7f-43e6-b941-cd339499c56f'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (26732, 'c9cde91d-622f-493d-badd-ada7439748d9'), (26740, '1717231a-ec52-48aa-a32c-ff19191abfea'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (7293, 'f67a56aa-a60f-4938-b212-3d7d37f3497f'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (7296, 'b2992b22-61a5-427a-bfce-ccccf61a44ee'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (8855, 'd6b42baf-f1da-407c-9f6e-bede792b1cd2'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (9374, 'd670622a-7d07-43ea-be13-2a901b9951f9'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (8879, 'b6bcb9c3-6460-46ff-a441-44af5882b728'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (8882, 'b6f92498-3fe6-4e9b-92b8-c1aeac3e22aa'), (19643, '11f17d0a-4f66-4c28-9733-118d5b0060ae'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (11452, '7e2fd91b-ca25-40e4-9aea-47ac22782115'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (9947, '0c45b6f2-397b-4c85-8986-5fdde05232a8'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (8417, 'f2f562af-44ee-4093-ae3b-2d887cb90a71'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (11010, '08c44491-b715-4d4f-9e79-ac87d9c5c9a2'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (9722, 'd6ba2c4b-251f-4927-8327-3ec8ab6122d3'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (20776, 'd20aa45c-57fa-4650-99ff-bcee85abb47a'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (12090, 'cfd1ed06-23a3-48ac-8c24-98d2498b97ed'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (12095, 'c4744c46-a374-4937-a8a6-60a1818dbaae'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (3415, '876b5010-859a-4a05-a141-541630e6217c'), (22360, '62b0534c-a8bc-4b96-92f2-012032e87932'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (24422, 'd216e4c9-d54b-4244-a3ee-e2c1a8544b1d'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (28518, '3efafd48-920e-44b3-a932-2fa90ef7a851'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (15740, '1ff3f9f4-82bb-4766-8c01-807ece2b0afb'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (8073, '180fe363-498f-4421-9240-3ab8a0e413a0'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (8077, 'f3bb984f-ab7c-46f2-ad0b-1672833fcea5'), (1448, '58f14425-82d6-4130-83df-faf5f82799d4'), (30637, '2ef0ad62-5b35-46bc-8d70-7b5a6155cd2c'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (4534, '3ff4dd5c-528a-4039-b0d8-9c5fd34866bb'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (18886, 'a01aef5d-3c56-4f8d-8854-e68353f4d189'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Existing work in translation demonstrated the potential of massively multilingual machine translation by training a
single model able to translate between any pair of languages. However, much of this work is English-Centric by training
only on data which was translated from or to English. While this is supported by large sources of training data, it
does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation
model that can translate directly between any pair of 100 languages. We build and open source a training dataset that
covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how
to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters
to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly
*Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language 
tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, 
their training data is dominated by English, potentially limiting their cross-lingual generalization. 
In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, 
and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters 
sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size 
in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) 
and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark,
*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive
*Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
 
Here is the question: Which strategy would best optimize the performance of a multilingual machine translation system that must scale for diverse languages and large datasets?","Based on the context, the strategy that would best optimize the performance of a multilingual machine translation system that must scale for diverse languages and large datasets is a combination of dense scaling and language-specific sparse parameters, as mentioned in the first paragraph of the context. This approach allows for effective increase in model capacity, enabling the system to handle a large number of languages and datasets. Additionally, using a balanced corpus covering a diverse set of languages, as mentioned in the second paragraph, can also help improve the system"
What is the most effective strategy for training a large-scale image generation model that maintains both speed and accuracy across diverse datasets and real-time processing?,"[(22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (15880, 'e1736cb8-e5c0-4206-9c07-4736d03f05a2'), (12810, 'd8c6428d-833f-48ff-b7d6-bc7915e68e0f'), (11276, '72657258-402b-4348-8451-081f95249ada'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (7202, '8fe27cdc-8842-4f62-a856-52d8c3064ba6'), (17955, 'd88c534d-4ea0-4b89-ac58-fbabd0235f36'), (26661, 'ba608cf2-2d94-47fd-801e-7393ff76f4e5'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (11819, 'c2743ee4-5a40-4ca0-af29-a8b712a6c81c'), (1067, '5c533f5a-f138-40ef-aa01-572ed4cd6252'), (3635, 'd8ed0b3b-d807-4c01-91ca-b68789d1eab1'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (1079, '407cab29-3891-4a0b-9375-373b7427fe87'), (21049, '8462308a-79bd-461f-87fd-f98e7b1aa3d5'), (1083, '0048034e-38cf-41b5-b2d2-47d5dc051bf6'), (1087, '4ab58359-9f40-4954-9e6a-32c2c02a958e'), (13377, 'c316bc0e-0126-4c2b-9b19-bf72255b1a3f'), (66, '33c8fe4f-dd9b-482f-8ce6-d72f2dd8e94e'), (1091, '9e1ad486-0eff-40ac-9f7f-09195d38379c'), (11338, '30d2c084-1101-4560-b412-45d9eae8ef69'), (30794, '60649a7f-802b-4687-8b03-4c75a01eca78'), (590, '6523fa0b-894c-49e0-bfb1-fb3213623bf3'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (20049, 'e833dc60-daba-4293-a329-ac85b91ed922'), (30803, '04651842-da8f-4f4b-9076-9d74a4b634b6'), (23635, '8da5d714-1b83-4977-8b30-0d99b8236dac'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (12886, '18488b88-987f-476e-9eb1-02c4211c5a10'), (23636, '847215c3-b4a6-4e0a-8064-28fd730a92aa'), (9816, '5a10bbff-24a7-4d06-b2b9-87ca115873d4'), (15962, 'af48f570-c8e7-4fcb-bb75-eab627b49209'), (22624, 'f38b347a-4b69-42a8-869e-760968d2d649'), (24163, '6d254de1-aa08-4a87-9094-64313c3a0c4b'), (20068, 'd2d9fdee-7033-4cda-a724-d9c2e219656c'), (28777, 'b87ff93d-186b-4bb7-8859-a6129614b24e'), (1641, '59c67169-9a9f-407b-ae30-cf7ea4959dca'), (1645, '74486fbc-e751-4afd-9c60-0b5702718d84'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (17520, 'dff6bb94-0e41-4a27-b6cd-0407314b2936'), (1649, '96d9698f-b3b4-4603-974d-a8c398443172'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (30835, '14eaf7d7-fe70-4f18-b24a-a9fb5fbbfb2f'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (1653, 'b9e3bffb-9cd2-4d41-8983-4dee702d6904'), (2681, '0c9f9515-b990-4ed1-8294-77ca4ad60604'), (26233, 'b0309052-b6ba-4c28-a638-fdfcd3dd8388'), (1148, '0e1cdedb-ef2f-4945-9e3d-404afb046ac4'), (13955, 'b43f9f53-919a-4395-ad06-ca2a06b73c23'), (659, '77f9993f-20a6-4964-b925-7de3be25efa8'), (23191, '8c2efd68-2cbd-49b2-b5b0-9b40d0c49323'), (16536, '279b21e3-a0d1-4d27-ba89-6764a9da9659'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (25754, '72a32271-ed92-44d2-8bdb-db4aaa03c6a9'), (25757, '24c0f4d2-56ee-43a1-8911-bb06cd2c5c08'), (4253, '4d808663-a353-4ae2-9418-1ef26f0fded5'), (31393, '61e9e601-16d9-4a9d-acbe-fefc7ae49fda'), (25764, 'e6b7b172-a955-4460-a174-888528a760ca'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (31421, '872f6dc1-7198-442a-bb9c-99704ee4fec4'), (31424, '421a5427-37a0-4fc8-b5c8-333ae0302fb4'), (10435, '6f750079-d1c9-4c6a-8c55-26a9793b5546'), (31432, '86a2c216-1bce-4096-a3d1-5231869d83c6'), (14542, 'adc6532e-0c1e-4f00-a1f5-78b79796dba2'), (22739, '83649c52-3276-4a79-b854-4166d306a52a'), (22740, '42178967-045d-4482-95a3-1453ae35af86'), (29416, 'cc26e971-c084-4571-a0b9-7d98659720f5'), (16105, 'cfa816d5-09be-446f-a412-2b54cfa49648'), (10986, '43de7f8d-6eb7-420a-b184-aa48eb3517b8'), (20201, 'f9cda80f-7341-4034-9ac1-76ce0c5cc5bb'), (14065, '404c438b-31eb-4285-8506-e3180e266bf1'), (9458, '186c90ec-1442-4fa7-81ee-4f33e0ca3e17'), (9459, '50af37d9-9d34-4347-9602-65d90571a951'), (16628, '3632617e-7d09-44f4-9fff-68431b760996'), (4341, 'd6b7f7ab-fb13-44d5-a3d1-5dbcb6ad0ddf'), (7435, 'b5622f97-382f-4675-8004-53adfe83f4ac'), (6419, '42aef9bd-e9bd-469a-ac13-036843c85c48'), (10531, 'da35125a-172e-472a-90f8-dbc52c25e456'), (7460, '1a8dc7d5-208f-4c4e-8234-e5ed0e3bbf2d'), (10534, '0b1a3ae1-395d-469d-93fb-f855d838a751'), (10539, 'f28af08a-cfce-44b3-988c-a51f3824de1f'), (24875, 'b1625c6b-1283-450d-8548-58f9de5a6ff2'), (10029, 'd1a5f9fc-00c4-4974-b71e-14d74422db37'), (10542, '6cf5c516-c66b-4848-a5f4-b8b0ae2bab1c'), (24881, 'a8801494-dc55-4e6b-b468-3ab684c41019'), (24886, '6fc05d0c-f06c-41bb-b410-b41961c6fb99'), (823, '5e6cc287-531f-47c7-a00e-a0c202fd496e'), (6974, 'df32e733-6704-498b-b2b3-390af1411179'), (2879, '7d360b41-1577-4c00-97da-1afd541db6d5'), (6975, 'd66e5cdd-564c-41fe-a9d8-4ba2c2370002'), (28994, 'b2fdd130-8cb1-4172-828f-ccbf88b499bd'), (19793, '009531d3-249b-4dec-8e06-5ef961dd5152'), (17237, 'c4995dfb-4138-46e0-926b-bcf14cc30ae3'), (17240, '97601ab3-20d4-4caa-bd20-f2fdc000f5fd'), (23900, 'bdf9c5d4-29b8-480f-b334-1fd2e6ee8a76'), (24927, 'fbdbfcba-4a0c-4292-a2bc-0beb0f42a172'), (4966, '42e714b3-cfee-4118-a976-588eded30493'), (22914, '779719c4-a9bc-4ab7-bbd5-541c9e1906ce'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (22920, '500cc8f3-3dc5-41e4-a57e-78372d38d835'), (21386, '0da28613-9bbd-48d8-a129-d3c4d7cf98df'), (25486, 'eea7b0c2-d489-42d8-b858-4137b7e9c781'), (25492, 'c7fedd25-0814-419b-9629-1875cd890be1'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (25497, '6e0e229f-0982-4c89-9297-2fc1f6f059f2'), (24996, '58afc34f-c20b-4e08-8ed9-b39590112688'), (24997, 'ce0c8394-4f5b-4eea-ac39-42404b2506be'), (25000, 'c4dd2159-6af5-43ea-80dd-2affde5c9e7a'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (1450, 'b31e6fae-396f-4c6e-a1de-dd46a0d1c402'), (17337, '3df9e90b-9cd5-4aab-a4b9-af56f94cff9f'), (8644, '8e83a688-aaa4-4307-af06-1e4fff92d16a'), (26578, '05a08a96-8354-44f0-956c-62bc5f6ac12c'), (8662, '58619cbe-0e96-4b3a-a710-e101d3ab5a46'), (12247, '699d01d7-bceb-45ad-9b24-e6c2007956a7'), (477, '41321b07-71c9-4580-bb3a-a4db0b860871'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (7650, 'a4da7efa-581d-4b90-b8f5-9bdd2c079bf5'), (4583, 'a3f57615-f606-43dc-aa22-c8a64febe696'), (488, '53e9b028-d5c7-45e6-afa0-d399ca4442fd'), (6120, '7f0e3f1a-b918-45b3-9c9a-40f0e58fcba6'), (9704, '158e4e0e-b969-4bf9-bb49-62cfa96bf593'), (11244, '97a1d0f6-d4f7-42ad-b163-5db44561211c'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (23033, '6f279ca5-4d7c-473f-95e5-f014fba36eda'), (22011, 'dde393e4-b99c-47e7-82ff-4724c17236f8'), (31228, '6a3de332-01d3-429a-8f97-b4f5969219dd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity
models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8%) among methods that use only ImageNet-1K data. Transfer performance in downstream
tasks outperforms supervised pre-training and shows promising scaling behavior.*
*The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et
```

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/butterflies_final.png""/>
</div>

## Next steps

Unconditional image generation is one example of a task that can be trained. You can explore other tasks and training techniques by visiting the [🧨 Diffusers Training Examples](../training/overview) page. Here are some examples of what you can learn:

* [Textual Inversion](../training/text_inversion), an algorithm that teaches a model a specific visual concept and integrates it into the generated image.
* [DreamBooth](../training/dreambooth), a technique for generating personalized images of a subject given several input images of the subject.
* [Guide](../training/text2image) to finetuning a Stable Diffusion model on your own dataset.
* [Guide](../training/lora) to using LoRA, a memory-efficient technique for finetuning really large models faster.
method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536×1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time.*
*Diffusion models have significantly advanced the fields of image, audio, and video generation, but they depend on an iterative sampling process that causes slow generation. To overcome this limitation, we propose consistency models, a new family of models that generate high quality samples by directly mapping noise to data. They support fast one-step generation by design, while still allowing multistep sampling to trade compute for sample quality. They also support zero-shot data editing, such as image inpainting, colorization, and super-resolution, without requiring explicit training on these tasks. Consistency models can be trained either by distilling pre-trained diffusion models, or as standalone generative models altogether. Through extensive experiments, we demonstrate that they outperform existing distillation techniques for diffusion models in one- and few-step sampling, achieving the new state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for one-step
 
Here is the question: What is the most effective strategy for training a large-scale image generation model that maintains both speed and accuracy across diverse datasets and real-time processing?","Based on the context, the most effective strategy for training a large-scale image generation model that maintains both speed and accuracy across diverse datasets and real-time processing is to use a scalable approach that combines different techniques to accelerate and stabilize training at scale. This includes:

1. Using a large, curated, and diverse dataset, rather than uncurated data, to train the model.
2. Employing self-supervised methods, such as SimMIM, to reduce the need for vast labeled images.
3"
Which combination of strategies would best optimize both the accuracy and speed of a multilingual ASR system when handling different accents and noisy environments?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30239, '1f6b1371-760d-4d86-aea5-f87a82be71a1'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (30269, 'ce1b2ec6-16bc-48d4-8e48-ee802e65abe6'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (19047, '047fec8b-577a-401d-908a-4040a4a36e43'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22144, '53697ae7-3817-4e34-ac41-c9d6703921e2'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (22696, '47eb8a13-1c6a-4060-a807-7eb65a6df466'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (21191, '9cfe40a8-87c6-4a55-9f19-3458ca24d801'), (21192, '33a1caa5-4ef6-4cf7-ad0c-79c8a5bca80d'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (9950, '3570da80-429e-4059-b1b0-db0dddb1ae97'), (26338, 'c3ae9637-8b7a-42b5-af7f-76b6722ab0fc'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559'), (10551, '23b0fa1f-8a46-4da7-aac0-1805e14a220b'), (10552, '19f613db-4203-4d96-a744-9369443faac9'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (13669, 'fd5f73cd-31c4-4ed9-a912-115b0d44f3b5'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (18855, 'c347445a-0f7a-4f79-8f05-fce5efd6f4a9'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (29659, '5a15e754-3b25-4dce-88a6-79573d35cf5d'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (14317, '8c6ac79e-1f72-496b-a40f-fc1c956bf3eb'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21495, '90b03904-a64e-420c-95a4-be77d44b1afa'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use 🤗 Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
The abstract from the paper is the following:

*Recent work has demonstrated the effectiveness of cross-lingual language model pretraining for cross-lingual understanding. In this study, we present the results of two larger multilingual masked language models, with 3.5B and 10.7B parameters. Our two new models dubbed XLM-R XL and XLM-R XXL outperform XLM-R by 1.8% and 2.4% average accuracy on XNLI. Our model also outperforms the RoBERTa-Large model on several English tasks of the GLUE benchmark by 0.3% on average while handling 99 more languages. This suggests pretrained models with larger capacity may obtain both strong performance on high-resource languages while greatly improving low-resource languages. We make our code and models publicly available.*

This model was contributed by [Soonhwan-Kwon](https://github.com/Soonhwan-Kwon) and [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).
</Tip>

This guide will show you how to train a [`openai/whisper-large-v2`](https://huggingface.co/openai/whisper-large-v2) model for multilingual automatic speech recognition (ASR) using a combination of `int8` quantization and LoRA. You'll train Whisper for multilingual ASR on Marathi from the [Common Voice 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) dataset.

Before you start, make sure you have all the necessary libraries installed:

```bash
!pip install -q peft transformers datasets accelerate evaluate jiwer bitsandbytes
*Pretrained multilingual large language models have typically used heuristic temperature-based sampling to balance between different languages. However previous work has not systematically evaluated the efficacy of different pretraining language distributions across model scales. In this paper, we propose a new sampling method, UniMax, that delivers more uniform coverage of head languages while mitigating overfitting on tail languages by explicitly capping the number of repeats over each language's corpus. We perform an extensive series of ablations testing a range of sampling strategies on a suite of multilingual benchmarks, while varying model scale. We find that UniMax outperforms standard temperature-based sampling, and the benefits persist as scale increases. As part of our contribution, we release: (i) an improved and refreshed mC4 multilingual corpus consisting of 29 trillion characters across 107 languages, and (ii) a suite of pretrained umT5 model checkpoints trained with
 
Here is the question: Which combination of strategies would best optimize both the accuracy and speed of a multilingual ASR system when handling different accents and noisy environments?","Based on the context, the combination of strategies that would best optimize both the accuracy and speed of a multilingual ASR system when handling different accents and noisy environments is:

1. Using a large-scale pre-trained multilingual language model, such as XLM-R or XLM-R XL/XXL, which has been shown to perform well on cross-lingual transfer tasks and low-resource languages.
2. Fine-tuning the pre-trained model on a multilingual ASR dataset, such as Common"
"Which solution would best optimize a vision model’s performance for both image generation and real-time inference on a large-scale, diverse dataset?","[(22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (15879, '2d7ab158-bd08-47de-9651-a5b8f5c86695'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (13849, '329d6c9b-9883-4b76-9348-0fdae71d7272'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (8738, 'cc8b26e6-bfba-47c4-aceb-5d077bcadd73'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (4646, '13611a54-20bb-45b0-89e7-31b0bb22b402'), (17456, '3d0b462d-4f6f-4e29-bdf7-d84ce48e8560'), (17458, '29ac6c95-d3e2-470e-a268-83fd7c395ddd'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (21055, '85a954a1-f7e7-41d4-b632-eb770dd46fad'), (11338, '30d2c084-1101-4560-b412-45d9eae8ef69'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (30797, 'fc48d639-1e57-4d69-874a-5e06c6436fe3'), (30803, '04651842-da8f-4f4b-9076-9d74a4b634b6'), (2644, 'fa4ead9d-264a-468f-b49a-62dba8bae88e'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (22102, 'aff80ee3-6694-4bab-acd1-453fcb62fc3c'), (3158, '1eb57504-ed08-48bb-8092-c53d5c64b0c1'), (26200, 'e7b14e7f-09b7-4507-96b4-64b626ce25be'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (2654, '2c458bc9-0dd4-4019-b799-03bcf622fbfc'), (14435, 'e2ce87d2-d827-4c04-a24a-16f4612fd277'), (5733, 'd7b94280-d5e4-4cbe-8307-82acc7c174e7'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (14445, '770426d9-4a8e-467b-b7c8-9ae28fb15b50'), (14446, 'bf6ca108-543a-4397-99ef-607bc2722bac'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (14448, '137a6dbc-1a8d-494d-9bb2-9df4702ab8c9'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (18546, '6ce33cc6-972e-4a9e-9f20-f2c381f9516f'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (1148, '0e1cdedb-ef2f-4945-9e3d-404afb046ac4'), (7808, 'f7d82006-5b58-485f-b5ca-2e5043ac18d8'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (24230, 'bd10f584-b63e-4b9f-89e3-194c6bdb0f54'), (4265, 'f389f1dd-debe-4020-a0a3-4923eaae7aee'), (31411, '66249fda-ff4f-4288-9edd-39f1db558684'), (14008, 'c13cab4e-22a7-4e41-acbb-b7ad3a5fcf37'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (8902, 'e1241555-c807-459f-b8c7-bc9db99754a0'), (31432, '86a2c216-1bce-4096-a3d1-5231869d83c6'), (28874, 'c6665cf1-2cee-448d-920f-056e33354af7'), (14028, 'f8a3f61d-2e71-49d7-88a9-4d17cdd22680'), (3795, '177a5242-8be1-496a-8e8f-b2ade38bc78e'), (14041, 'bfe9d047-a549-404a-a384-445fa6966aa2'), (3815, '7fa3513f-2329-4efb-bfb6-74160ee1ef45'), (29416, 'cc26e971-c084-4571-a0b9-7d98659720f5'), (10986, '43de7f8d-6eb7-420a-b184-aa48eb3517b8'), (16111, '63930b8a-56ec-4bcf-ba31-3246ca6bab09'), (6896, 'f9ed30df-002b-44fb-b684-aaaaaaf826dc'), (4341, 'd6b7f7ab-fb13-44d5-a3d1-5dbcb6ad0ddf'), (5884, '8b1efd71-fa8f-4e3a-85c9-811e0d3446a7'), (18685, '52fdc5bf-09c1-40d7-85f6-9c586ebe5ef0'), (5886, '3c00538c-a792-4c61-b1ef-9796ea971829'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (17157, 'b9540860-ae17-4e26-b1f5-6ea2ad1ca46e'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (16660, '1dfbc513-b29b-4f26-ac16-6bccdb48a3d4'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (9507, 'e74b7275-d60d-48e5-b782-63d1d2283cdd'), (10539, 'f28af08a-cfce-44b3-988c-a51f3824de1f'), (9005, '380fd6e8-0ceb-48bc-b733-0a33abcdc371'), (16690, 'b29012bd-3ed4-4e50-a20c-aff3dde69bc9'), (8503, 'a035c2be-ea22-4ac9-8f47-6603afbcf025'), (19262, 'cb4dea97-cfcf-4d5d-827a-c683ab6dcaf2'), (4418, '9410b13d-3f78-4fa7-aeb6-8361c7619510'), (4424, '58841961-84a6-4778-91f2-533dd14faacc'), (17227, 'a2314670-5d07-4f79-8b7e-98530aeff2d8'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (23900, 'bdf9c5d4-29b8-480f-b334-1fd2e6ee8a76'), (14685, 'f6385cc5-2ea5-401f-bace-7cbe69ca1700'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (27484, 'fe3505b9-3176-4041-81f3-76181cde167a'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (4966, '42e714b3-cfee-4118-a976-588eded30493'), (4967, '43778b66-3e71-4fd3-a9bf-f2cf850f713d'), (15212, 'ca1c0ec3-0e8c-4b3f-b868-d7bf73baaf4b'), (15213, 'c7c47f42-5ef1-49ef-ac9c-bd866eaad4de'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (14706, '332b6a8d-ec91-483f-a989-0ec3c2d81a1c'), (25459, '7e9e55e8-8027-4677-b297-11f6cebd0a17'), (29045, 'a1f71cbb-556e-4015-8e63-51461e02bf4e'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (22914, '779719c4-a9bc-4ab7-bbd5-541c9e1906ce'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (24457, 'b32d2172-8d5c-4609-a629-ac9c292249dd'), (22920, '500cc8f3-3dc5-41e4-a57e-78372d38d835'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (24465, '3c2e5294-0f12-4ff4-bebb-3de6c4902446'), (23442, '35ca6462-1413-46ab-b1be-a5db65e32709'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (24476, '7416abe4-8708-4b38-a154-27508d996c92'), (17825, '25ff9fc5-3959-4e46-bc23-de61ba1996b6'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (26039, '06e385f9-19f2-4a2b-ac64-9ac03070c5eb'), (17337, '3df9e90b-9cd5-4aab-a4b9-af56f94cff9f'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (17348, '1dd5ec9a-c117-424c-a488-f266aca5e7d7'), (27083, 'fd3ceb3f-d838-4aa0-b054-2fcdd5929735'), (477, '41321b07-71c9-4580-bb3a-a4db0b860871'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (13296, '1deb15a7-660c-4670-836e-ea98398cf41b'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (11262, '5c2061dd-7364-4daa-bb21-a1404baa2b55')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Through the past five years, [OpenVINO Toolkit](https://docs.openvino.ai/) encapsulated many features for high-performance inference. Initially designed for Computer Vision models, it still dominates in this domain showing best-in-class inference performance for many contemporary models, including [Stable Diffusion](https://huggingface.co/blog/stable-diffusion-inference-intel). However, optimizing Stable Diffusion models for resource-constraint applications requires going far beyond just runtime optimizations. And this is where model optimization capabilities from OpenVINO [Neural Network Compression Framework](https://github.com/openvinotoolkit/nncf) (NNCF) come into play.
The picture above shows the results of image generation and some model characteristics. As you can see, just conversion to OpenVINO brings a significant decrease in the inference latency ( **1.9x** ). Applying 8-bit quantization boosts inference speed further leading to **3.9x** speedup compared to PyTorch. Another benefit of quantization is a significant reduction of model footprint, **0.25x** of PyTorch checkpoint, which also improves the model load time. Applying Token Merging (ToME) (with a **merging ratio of 0.4** ) on top of quantization brings **5.1x** performance speedup while keeping the footprint at the same level. We didn't provide a thorough analysis of the visual quality of the optimized models, but, as you can see, the results are quite solid.
## Setting up a Gradio Demo for EfficientNet-Lite4

EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)

Here we walk through setting up a example demo for EfficientNet-Lite4 using Gradio

First we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio interface for a user to interact with. See the full code below.
probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.*
probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.*
 
Here is the question: Which solution would best optimize a vision model’s performance for both image generation and real-time inference on a large-scale, diverse dataset?","Based on the context, the solution that would best optimize a vision model's performance for both image generation and real-time inference on a large-scale, diverse dataset is the OpenVINO Toolkit, specifically the combination of conversion to OpenVINO, 8-bit quantization, and Token Merging (ToME). This is because it provides a significant decrease in inference latency (3.9x speedup compared to PyTorch) and a significant reduction of model footprint (0.25x of Py"
What is the best method for training and deploying an ASR system that must be able to handle both noisy environments and various accents across multiple languages?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (21542, '387734b4-81ba-4aec-8bce-c7d8cf647f9f'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (30263, 'c2e24f45-db74-4055-b058-e09e7119a3a7'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (30269, 'ce1b2ec6-16bc-48d4-8e48-ee802e65abe6'), (19014, '28a449dd-bfb5-4851-90ff-3b2cd97264cc'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19032, 'f2211bdb-6d56-4c72-ba8f-afc8461cd9ef'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (19047, '047fec8b-577a-401d-908a-4040a4a36e43'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (25708, '0a161716-5f3c-45c5-aac2-4eb719f697f4'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (26255, '30372f00-e006-40e8-9ba2-91e9ccbec1b7'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (16027, '41f24992-4f5e-433c-aba4-8ed79519f16a'), (3748, '332644a1-eafc-4d25-b177-40728dd8f84b'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (21191, '9cfe40a8-87c6-4a55-9f19-3458ca24d801'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (5842, '3b64b1f4-3187-44e6-a1b6-2814799742b2'), (20696, '9326d027-ee93-4e0a-805c-6c36950bf184'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (26334, 'f86f1902-9868-454f-9ef6-f7c11ce29015'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (6907, '1be7a8dc-407e-49e2-b978-d3551d21c091'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (26946, 'd51e55e0-4a83-4946-a69c-5343acec6844'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (19290, 'bdaea1c7-fd99-40b6-873e-6221088c435a'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (18833, '675f696e-74b6-4e9a-a6c0-3ddb571a6a07'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (18839, 'ceaa5ae9-6be8-42c5-8a11-0e0d7e0c8e81'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (18844, 'c97c2cdd-44fb-4531-87d5-afef8f19b517'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (4511, 'af6fa441-8f03-460f-ad80-c0b9214b289c'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (4514, 'aba6c41a-599a-493a-bd59-de36b253a61a'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (14816, '983ebfac-d4b7-48fd-b103-53118f15e140'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: To adapt to 1000+ different vocabularies, **MMS** uses of Adapters - a training method where only a small fraction of model weights are trained.

Adapter layers act like linguistic bridges, enabling the model to leverage knowledge from one language when deciphering another.

## **Fine-tuning MMS**

**MMS** unsupervised checkpoints were pre-trained on more than **half a million** hours of audio in over **1,400** languages, ranging from 300 million to one billion parameters.

You can find the pretrained-only checkpoints on the 🤗 Hub for model sizes of 300 million parameters (300M) and one billion parameters (1B):

-   [**`mms-300m`**](https://huggingface.co/facebook/mms-300m)
-   [**`mms-1b`**](https://huggingface.co/facebook/mms-1b)

*Note*: If you want to fine-tune the base models, you can do so in the exact same way as shown in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
Tips:

- All ASR models accept a float array corresponding to the raw waveform of the speech signal. The raw waveform should be pre-processed with [`Wav2Vec2FeatureExtractor`].
- The models were trained using connectionist temporal classification (CTC) so the model output has to be decoded using
  [`Wav2Vec2CTCTokenizer`].
- You can load different language adapter weights for different languages via [`~Wav2Vec2PreTrainedModel.load_adapter`]. Language adapters only consists of roughly 2 million parameters 
  and can therefore be efficiently loaded on the fly when needed.

#### Loading
![wav2vec2\_structure](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/xls_r.png)

XLS-R shows impressive improvements over previous state-of-the-art
results on both speech recognition, speech translation and
speaker/language identification, *cf.* with Table 3-6, Table 7-10, and
Table 11-12 respectively of the official [paper](https://ai.facebook.com/blog/xls-r-self-supervised-speech-processing-for-128-languages).

Setup
--------------

In this blog, we will give an in-detail explanation of how XLS-R -
more specifically the pre-trained checkpoint
[**Wav2Vec2-XLS-R-300M**](https://huggingface.co/facebook/wav2vec2-xls-r-300m) - can be fine-tuned for ASR.

For demonstration purposes, we fine-tune the model on the low resource
ASR dataset of [Common
Voice](https://huggingface.co/datasets/common_voice) that contains only
*ca.* 4h of validated training data.
Using `gradio`, you can easily build a demo of your ASR model and share that with a testing team, or test it yourself by speaking through the microphone on your device.

This tutorial will show how to take a pretrained speech-to-text model and deploy it with a Gradio interface. We will start with a **_full-context_** model, in which the user speaks the entire audio before the prediction runs. Then we will adapt the demo to make it **_streaming_**, meaning that the audio model will convert speech as you speak. 

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started). You will also need a pretrained speech recognition model. In this tutorial, we will build demos from 2 ASR libraries:

- Transformers (for this, `pip install transformers` and `pip install torch`)
```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
 
Here is the question: What is the best method for training and deploying an ASR system that must be able to handle both noisy environments and various accents across multiple languages?","Based on the context, the best method for training and deploying an ASR system that must be able to handle both noisy environments and various accents across multiple languages is to use Adapter layers, which act like linguistic bridges, enabling the model to leverage knowledge from one language when deciphering another. This approach allows the model to adapt to 1000+ different vocabularies. Additionally, fine-tuning the pre-trained MMS model on a specific language or accent can further improve the performance."
"Which strategy would best increase the speed and accuracy of a machine translation system when handling diverse languages, particularly those with low resources?","[(23040, 'f17c1799-6922-4207-906f-59ecf4ef6c33'), (23043, 'e61d83b5-1f81-4bac-a218-ef6fdbd96486'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (25137, 'b86fb866-7ee8-4380-aaae-373189a46f55'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (21561, '68fa59f6-381e-4dee-ae59-873812588c22'), (28745, 'be778586-e097-4669-897c-e0f7ae5da8bb'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (10831, '9a5c4c34-d833-48bc-b903-b7202a7ae3c1'), (9297, 'b1ddc06d-35a1-4ac0-b68f-697de22d55e0'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (26739, '0b12c644-853c-4e0f-a818-938083349460'), (26740, '1717231a-ec52-48aa-a32c-ff19191abfea'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (7293, 'f67a56aa-a60f-4938-b212-3d7d37f3497f'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (10368, '7e1d07b3-47b3-497f-97ca-c04fed294966'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (9374, 'd670622a-7d07-43ea-be13-2a901b9951f9'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (19619, '06d68a4b-ca65-47bc-baaa-c8d78d5f9136'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (8879, 'b6bcb9c3-6460-46ff-a441-44af5882b728'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (8882, 'b6f92498-3fe6-4e9b-92b8-c1aeac3e22aa'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (23039, '4450f9a2-9c97-4909-80aa-9e8a8dd26457'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (22236, '7f5de617-4a0c-4797-b576-11a06835ca5c'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (24295, '9c24b89c-d484-4cb0-b384-1bbfa5bbd033'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (6899, '5fc72f0c-9870-4f07-bc4f-c8e16dc0b4f2'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (11010, '08c44491-b715-4d4f-9e79-ac87d9c5c9a2'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (9722, 'd6ba2c4b-251f-4927-8327-3ec8ab6122d3'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (29002, '8afe6522-4963-48c6-91de-5a7e7715c4bb'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (3415, '876b5010-859a-4a05-a141-541630e6217c'), (22360, '62b0534c-a8bc-4b96-92f2-012032e87932'), (28518, '3efafd48-920e-44b3-a932-2fa90ef7a851'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (10648, '66ffc54d-754b-40e5-b926-93bd59b981cb'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (1448, '58f14425-82d6-4130-83df-faf5f82799d4'), (7601, '3ecbf4ed-0cd3-483b-88d7-128fe443adff'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (28638, '411c979e-aeab-4b11-90b6-8352b7665aa7'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive
*Existing work in translation demonstrated the potential of massively multilingual machine translation by training a
single model able to translate between any pair of languages. However, much of this work is English-Centric by training
only on data which was translated from or to English. While this is supported by large sources of training data, it
does not reflect translation needs worldwide. In this work, we create a true Many-to-Many multilingual translation
model that can translate directly between any pair of 100 languages. We build and open source a training dataset that
covers thousands of language directions with supervised data, created through large-scale mining. Then, we explore how
to effectively increase model capacity through a combination of dense scaling and language-specific sparse parameters
to create high quality models. Our focus on non-English-Centric models brings gains of more than 10 BLEU when directly
*Large-scale autoregressive language models such as GPT-3 are few-shot learners that can perform a wide range of language 
tasks without fine-tuning. While these models are known to be able to jointly represent many different languages, 
their training data is dominated by English, potentially limiting their cross-lingual generalization. 
In this work, we train multilingual autoregressive language models on a balanced corpus covering a diverse set of languages, 
and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters 
sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size 
in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) 
and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark,
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
 
Here is the question: Which strategy would best increase the speed and accuracy of a machine translation system when handling diverse languages, particularly those with low resources?","Based on the context, the strategy that would best increase the speed and accuracy of a machine translation system when handling diverse languages, particularly those with low resources, is to use a combination of dense scaling and language-specific sparse parameters, as well as training on a balanced corpus covering a diverse set of languages. Additionally, using novel and effective data mining techniques tailored for low-resource languages, and proposing multiple architectural and training improvements to counteract overfitting while training on thousands of tasks, would also be beneficial."
Which combination of techniques would best improve the performance of a speech-to-text model that must adapt to various dialects and accents while ensuring real-time performance?,"[(18432, '7458df8c-e69b-43d8-a456-b53c44af4802'), (9728, '1cf55a61-9ed1-4a59-9f5b-74b10ffbbebe'), (9730, 'fbb81594-08b4-4f1d-a9eb-56be92d597d8'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (9729, 'adf67345-2f5a-4658-ac51-c3f2ca2c0916'), (9733, 'd8a9b090-d24d-4517-9a0b-03d326562952'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (5130, '0402365e-06b9-415e-9f92-dfe339eb3334'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (4166, 'd62d2dfe-88e9-4269-a3b8-a0c08a9ead60'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (19076, 'c3f9c5df-c349-46ea-932a-eead03777122'), (19077, 'ad635793-978a-455b-9c0f-cb60f9dae0e5'), (19085, '95ca3f5d-7594-4ebe-ae6d-1626686a69fc'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (20644, '35d54144-39c9-4e31-b7cc-fa6416b43dcb'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (15028, '86c91e1c-d5df-4053-b7f8-f475c47edcf1'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (16055, '1c40cffb-366c-48a0-b2e8-3d3a31a0feda'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (23238, '41b9ad8c-0403-4114-8159-59231afd1476'), (21191, '9cfe40a8-87c6-4a55-9f19-3458ca24d801'), (21198, '9468cc16-d5f1-4d2f-b79a-15502e9e9fee'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (15056, '3aeddfc0-2bd8-44a0-8d5d-def75c612a76'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (22235, 'c7600fbe-2a65-4774-a4a3-36b107198900'), (22236, '7f5de617-4a0c-4797-b576-11a06835ca5c'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (22239, 'f48147dc-d38a-4d1a-b0c9-76db7e13e56c'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (22241, '5b8e4b4b-9d05-4faa-bb22-3b469591c916'), (26338, 'c3ae9637-8b7a-42b5-af7f-76b6722ab0fc'), (4839, '8b5fcfe4-422d-41ad-ba2a-435d4859c7ef'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (1811, 'f4e9b143-1453-475d-a8a1-593ae0511e73'), (1812, 'abbeae44-35ad-4ce3-bc93-b8a186be4830'), (18200, 'c90b132b-e7b4-4305-99ea-9c6f13711dd2'), (18201, '2d381f7c-31b3-4fc8-a6f2-019f577e2bd0'), (16173, '021868a6-6880-43b4-aba8-34b48a2ffff3'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (13617, '5d6f1844-a996-4b2b-9cce-54cc40faf578'), (13618, 'c26d0aa6-d937-42a4-80b1-3ebe7f3941a8'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (316, '3d68da03-3df0-4ea4-8d35-55ca5e6bf70e'), (317, 'ef8b2514-cf01-4f03-896e-2692824e2715'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (13648, '5af9b7d4-1b8a-436a-bf80-acbd7647fec7'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (1890, '4d7210c7-1467-4e1b-b056-da922b7ff796'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (10615, '7bf25661-c8bb-4b65-9e16-786afc1051f8'), (18831, '73dccec5-c809-4615-b089-5d0e22f17cb5'), (18833, '675f696e-74b6-4e9a-a6c0-3ddb571a6a07'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (23453, '24bbe86e-9ba8-4256-b0fc-6707f0f6b1c0'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (9139, '2af98149-42ee-4660-988d-a8696d5547ac'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (14802, '148eaad9-71ab-478b-b6d7-1e16ddec31d1'), (14803, '989cb40f-0834-4dc5-9f2e-8f008a7490ae'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (14810, '6f2d8bf2-f5b9-44dc-9d14-30500063665c'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (9723, 'a99c19cf-b2cf-4fdf-a29a-7f4b9b2d9329'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (9726, 'ef486ead-c143-4c25-a64a-bf67a128b117'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive enables translation that preserves vocal styles and prosody. Compared to previous efforts in expressive speech research, our work addresses certain underexplored aspects of prosody, such as speech rate and pauses, while also preserving the style of one’s voice. As for SeamlessStreaming, our model leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances. As the first of its kind, SeamlessStreaming enables simultaneous speech-to-speech/text translation for multiple source and target languages. To understand the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing
audio data to learn self-supervised speech representations with w2v-BERT 2.0. Subsequently, we created a multimodal corpus of automatically aligned speech translations. Filtered and combined with human-labeled and pseudo-labeled data, we developed the first multilingual system capable of translating from and into English for both speech and text. On FLEURS, SeamlessM4T sets a new standard for translations into multiple target languages, achieving an improvement of 20% BLEU over the previous SOTA in direct speech-to-text translation. Compared to strong cascaded models, SeamlessM4T improves the quality of into-English translation by 1.3 BLEU points in speech-to-text and by 2.6 ASR-BLEU points in speech-to-speech. Tested for robustness, our system performs better against background noises and speaker variations in speech-to-text tasks compared to the current SOTA model. Critically, we evaluated SeamlessM4T on gender bias and added toxicity to assess translation safety. Finally, all
```

In our experience, obtaining satisfactory results from this model can be challenging. The quality of the speaker 
embeddings appears to be a significant factor. Since SpeechT5 was pre-trained with English x-vectors, it performs best 
when using English speaker embeddings. If the synthesized speech sounds poor, try using a different speaker embedding.

Increasing the training duration is also likely to enhance the quality of the results. Even so, the speech clearly is Dutch instead of English, and it does 
capture the voice characteristics of the speaker (compare to the original audio in the example).
Another thing to experiment with is the model's configuration. For example, try using `config.reduction_factor = 1` to 
see if this improves the results.
employs spectrograms of multiple resolutions as the input. In an evaluation on a dataset containing information on hundreds of speakers, UnivNet obtained the best objective and subjective results among competing models for both seen and unseen speakers. These results, including the best subjective score for text-to-speech, demonstrate the potential for fast adaptation to new speakers without a need for training from scratch.*
the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a
 
Here is the question: Which combination of techniques would best improve the performance of a speech-to-text model that must adapt to various dialects and accents while ensuring real-time performance?","Based on the context, the combination of techniques that would best improve the performance of a speech-to-text model that must adapt to various dialects and accents while ensuring real-time performance is:

1. Using a model like SeamlessStreaming that leverages the Efficient Monotonic Multihead Attention (EMMA) mechanism to generate low-latency target translations without waiting for complete source utterances.
2. Employing a technique like UnivNet that uses spectrograms of multiple resolutions as input and has shown the potential for"
"What strategy would most effectively enhance the performance of a machine translation system handling diverse languages, especially low-resource languages, while optimizing for real-time translation?","[(23043, 'e61d83b5-1f81-4bac-a218-ef6fdbd96486'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (25133, '61044ba2-520a-46f2-880e-b548130c4dfb'), (21561, '68fa59f6-381e-4dee-ae59-873812588c22'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (10831, '9a5c4c34-d833-48bc-b903-b7202a7ae3c1'), (9297, 'b1ddc06d-35a1-4ac0-b68f-697de22d55e0'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (30819, 'ae6db3cd-909e-4b8b-8686-9eeed34353f1'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (26740, '1717231a-ec52-48aa-a32c-ff19191abfea'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (7293, 'f67a56aa-a60f-4938-b212-3d7d37f3497f'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19604, '97b911ed-5d6d-4a6b-b2b1-60cd3abef208'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (8879, 'b6bcb9c3-6460-46ff-a441-44af5882b728'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (8886, '62356cee-1ad8-4e89-9668-09958b3cc3b2'), (19643, '11f17d0a-4f66-4c28-9733-118d5b0060ae'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (9947, '0c45b6f2-397b-4c85-8986-5fdde05232a8'), (22236, '7f5de617-4a0c-4797-b576-11a06835ca5c'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (24295, '9c24b89c-d484-4cb0-b384-1bbfa5bbd033'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (3320, '3f22941d-5a99-4599-b75a-7aac2efeee6e'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (11010, '08c44491-b715-4d4f-9e79-ac87d9c5c9a2'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (10510, '65932cde-592e-47aa-b229-26bebb986d68'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (3415, '876b5010-859a-4a05-a141-541630e6217c'), (22360, '62b0534c-a8bc-4b96-92f2-012032e87932'), (23896, '4b98400d-2e30-4f12-8806-2f035a8391a2'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (24422, 'd216e4c9-d54b-4244-a3ee-e2c1a8544b1d'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (28518, '3efafd48-920e-44b3-a932-2fa90ef7a851'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (1447, 'ea0014fb-44a1-41ef-97f6-1b472247ce5c'), (1448, '58f14425-82d6-4130-83df-faf5f82799d4'), (7601, '3ecbf4ed-0cd3-483b-88d7-128fe443adff'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (13789, '7c4a6f6b-3b93-4d84-8c87-8eb50245efba'), (28638, '411c979e-aeab-4b11-90b6-8352b7665aa7'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (9722, 'd6ba2c4b-251f-4927-8327-3ec8ab6122d3'), (9723, 'a99c19cf-b2cf-4fdf-a29a-7f4b9b2d9329'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (9725, '9281fb3c-cf9d-4dd9-83fc-0abf913c20c0'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
*Recent advancements in automatic speech translation have dramatically expanded language coverage, improved multimodal capabilities, and enabled a wide range of tasks and functionalities. That said, large-scale automatic speech translation systems today lack key features that help machine-mediated communication feel seamless when compared to human-to-human dialogue. In this work, we introduce a family of models that enable end-to-end expressive and multilingual translations in a streaming fashion. First, we contribute an improved version of the massively multilingual and multimodal SeamlessM4T model—SeamlessM4T v2. This newer model, incorporating an updated UnitY2 framework, was trained on more low-resource language data. The expanded version of SeamlessAlign adds 114,800 hours of automatically aligned data for a total of 76 languages. SeamlessM4T v2 provides the foundation on which our two newest models, SeamlessExpressive and SeamlessStreaming, are initiated. SeamlessExpressive
*Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.
However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the
200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by
first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
*Driven by the goal of eradicating language barriers on a global scale, machine translation has solidified itself as a key focus of artificial intelligence research today.
However, such efforts have coalesced around a small subset of languages, leaving behind the vast majority of mostly low-resource languages. What does it take to break the
200 language barrier while ensuring safe, high quality results, all while keeping ethical considerations in mind? In No Language Left Behind, we took on this challenge by
first contextualizing the need for low-resource language translation support through exploratory interviews with native speakers. Then, we created datasets and models aimed
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
 
Here is the question: What strategy would most effectively enhance the performance of a machine translation system handling diverse languages, especially low-resource languages, while optimizing for real-time translation?","Based on the context, the strategy that would most effectively enhance the performance of a machine translation system handling diverse languages, especially low-resource languages, while optimizing for real-time translation is to develop a conditional compute model based on Sparsely Gated Mixture of Experts, trained on data obtained with novel and effective data mining techniques tailored for low-resource languages, and incorporating multiple architectural and training improvements to counteract overfitting while training on thousands of tasks. Additionally, using a human-translated benchmark, such"
"What would be the most effective method to ensure both efficiency and adaptability when training a machine translation system for diverse languages while optimizing for low-latency, real-time inference?","[(3597, '837a4f94-d203-4386-b6fe-7eca8f6530c4'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (25133, '61044ba2-520a-46f2-880e-b548130c4dfb'), (26673, 'acb0c731-b7d9-4884-8760-09bd1882d5a6'), (25137, 'b86fb866-7ee8-4380-aaae-373189a46f55'), (18998, '989e75dd-dc43-4eb5-91cb-215c4e4adc17'), (21561, '68fa59f6-381e-4dee-ae59-873812588c22'), (25147, 'a2669d4a-4881-4334-8052-9102cf1a1249'), (19004, '112963e4-e074-4f17-aa4e-d0a1114feec3'), (19008, '3924cd7e-cd8d-4514-a715-97cc3e28b436'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (19014, '28a449dd-bfb5-4851-90ff-3b2cd97264cc'), (19016, '02c05bd4-cf3d-4ffc-a737-bb651f95a396'), (28745, 'be778586-e097-4669-897c-e0f7ae5da8bb'), (28746, '622fc13b-464c-4e19-a1b7-8fd04f0a188d'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (82, '9b1467ac-93fd-4e92-ae66-0c561a9fe744'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (5716, 'a521babf-2a86-410a-9a2c-16cd35ecd60f'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (30304, '69655f7c-2296-4da4-9562-97dbe94a3c0f'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (14444, '42f274f1-22ad-430f-a89d-972eec106f5f'), (4205, 'c62c8c1c-0bb7-4ad7-bfce-7cd1161ad7a7'), (26741, 'a54b444f-f364-4499-9236-b2faeffe4a59'), (7293, 'f67a56aa-a60f-4938-b212-3d7d37f3497f'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (10368, '7e1d07b3-47b3-497f-97ca-c04fed294966'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (26255, '30372f00-e006-40e8-9ba2-91e9ccbec1b7'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8879, 'b6bcb9c3-6460-46ff-a441-44af5882b728'), (8880, '45ae92f0-6d88-478a-8937-632e9f9db96d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (24295, '9c24b89c-d484-4cb0-b384-1bbfa5bbd033'), (5866, '4108faf4-0e48-4871-8e5b-31983d6c0129'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (11010, '08c44491-b715-4d4f-9e79-ac87d9c5c9a2'), (21251, '31eb9d16-0e2a-43a4-938b-4c96a7366457'), (23812, '98499772-c9a4-47d7-b198-bd18ff20d2d5'), (20740, '8ea0cbc1-6505-4aba-8eeb-7045546046c8'), (11012, '2c7ed7d6-155a-4473-b3f7-232806a7a924'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (23821, 'e84857ce-0800-4bd6-883c-6cc7cea44e9a'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (30534, '95e21773-3553-4d8f-8b88-fff6315116ba'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (22351, 'de4780f5-753f-4d0a-8175-6e44c6f76bb7'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (3415, '876b5010-859a-4a05-a141-541630e6217c'), (23906, '4dab7912-145b-4e91-96e6-a2c20ca73795'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (17774, 'e13cc263-c072-4a3b-9f64-7b688f1c79bf'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (8073, '180fe363-498f-4421-9240-3ab8a0e413a0'), (8074, '55f092c5-d2c2-4f8f-b841-cf0e3e9cbd05'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (10648, '66ffc54d-754b-40e5-b926-93bd59b981cb'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (7591, '833da8f3-7e22-40fc-b573-04af8220e4fd'), (7601, '3ecbf4ed-0cd3-483b-88d7-128fe443adff'), (26057, '9ed838e2-7b57-4bf0-ac30-97ec4268ad86'), (30674, 'dd0a16a7-4c70-42b0-846d-7252591ecf41'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (15826, 'acf90a4b-5709-485e-9eac-b6926bb65505'), (30677, 'a25bef54-3520-4f86-a7af-0451578e6208'), (11224, 'f888b7bb-7128-4802-8be1-00533c0c5908'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (7146, '2d034f40-d50f-4df3-a71a-df947f930dc2'), (9198, 'e47b8834-f6d6-4a1f-9198-ffa802160e1b'), (24559, 'cd5299f2-525d-4c40-939f-cfef98c3819e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (9722, 'd6ba2c4b-251f-4927-8327-3ec8ab6122d3'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (9724, 'a4c5d061-96cd-4773-8ea2-abdeb05ca680'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: the performance of these models, we combined novel and modified versions of existing automatic metrics to evaluate prosody, latency, and robustness. For human evaluations, we adapted existing protocols tailored for measuring the most relevant attributes in the preservation of meaning, naturalness, and expressivity. To ensure that our models can be used safely and responsibly, we implemented the first known red-teaming effort for multimodal machine translation, a system for the detection and mitigation of added toxicity, a systematic evaluation of gender bias, and an inaudible localized watermarking mechanism designed to dampen the impact of deepfakes. Consequently, we bring major components from SeamlessExpressive and SeamlessStreaming together to form Seamless, the first publicly available system that unlocks expressive cross-lingual communication in real-time. In sum, Seamless gives us a pivotal look at the technical foundation needed to turn the Universal Speech Translator from a
## Low-Rank Adaptation (LoRA)

<Tip>

LoRA is one of the most popular PEFT methods and a good starting point if you're just getting started with PEFT. It was originally developed for large language models but it is a tremendously popular training method for diffusion models because of its efficiency and effectiveness.

</Tip>

As mentioned briefly earlier, [LoRA](https://hf.co/papers/2106.09685) is a technique that accelerates finetuning large models while consuming less memory.

LoRA represents the weight updates ∆W with two smaller matrices (called *update matrices*) through low-rank decomposition. These new matrices can be trained to adapt to the new data while keeping the overall number of parameters low. The original weight matrix remains frozen and doesn't receive any further updates. To produce the final results, the original and extra adapted weights are combined. You could also merge the adapter weights with the base model to eliminate inference latency.
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
at narrowing the performance gap between low and high-resource languages. More specifically, we developed a conditional compute model based on Sparsely Gated Mixture of
Experts that is trained on data obtained with novel and effective data mining techniques tailored for low-resource languages. We propose multiple architectural and training
improvements to counteract overfitting while training on thousands of tasks. Critically, we evaluated the performance of over 40,000 different translation directions using
a human-translated benchmark, Flores-200, and combined human evaluation with a novel toxicity benchmark covering all languages in Flores-200 to assess translation safety.
Our model achieves an improvement of 44% BLEU relative to the previous state-of-the-art, laying important groundwork towards realizing a universal translation system.*
In our experiments, MMS' Adapter training is both more memory efficient, more robust and yields better performance for low-resource languages. For medium to high resource languages it can still be advantegous to fine-tune the whole checkpoint instead of using Adapter layers though.

![wav2vec2_structure](/blog/assets/151_mms/mms_map.png)

## **Preserving the world's language diversity**

According to https://www.ethnologue.com/ around 3000, or 40% of all ""living"" languages, are endangered due to fewer and fewer native speakers.
This trend will only continue in an increasingly globalized world. 

**MMS** is capable of transcribing many languages which are endangered, such as *Ari* or *Kaivi*. In the future, MMS can play a vital role in keeping languages alive by helping the remaining speakers to create written records and communicating in their native tongue.
 
Here is the question: What would be the most effective method to ensure both efficiency and adaptability when training a machine translation system for diverse languages while optimizing for low-latency, real-time inference?","Based on the context, the most effective method to ensure both efficiency and adaptability when training a machine translation system for diverse languages while optimizing for low-latency, real-time inference is to use Low-Rank Adaptation (LoRA) with Adapter training. This method accelerates fine-tuning large models while consuming less memory, making it efficient and effective for low-resource languages. Additionally, it allows for adaptability by training new adapter weights while keeping the original model weights frozen, which can be merged to"
"How does Hugging Face handle the process of evaluating very large language models on tasks without labeled data, and what role does zero-shot evaluation play in this process?","[(7687, '20608e74-1307-46cf-9341-09115b2bb0ac'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (21012, '4252b2f2-3c37-45fc-8476-73ec6c4c1ee7'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (16924, '38fe9939-2e02-47cc-805a-7ffc8340c582'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (13359, '6caf8e23-63aa-413c-b673-dc039aa9a2ec'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (26673, 'acb0c731-b7d9-4884-8760-09bd1882d5a6'), (24127, '474b8446-9f4d-4ec3-9b0e-78695268902d'), (22079, '16104f25-f547-44dd-9b5f-3798c428cc44'), (24128, '8475f00c-9f15-4992-a30d-b39c6d80bd0c'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8269, '63e383a9-dbfa-4c9f-a286-0668a682d313'), (3664, 'bfa637f2-8e64-45b8-9dab-ca0ca480e4c5'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (4184, '58eef19b-e0ac-4cfc-b83c-2df405fe42bf'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (10858, '33249571-1ade-46fd-ae89-e32ff806fdcd'), (23147, 'a58a5d52-847f-49bb-a8de-783fe583b3c6'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (23165, 'e03ab7b7-1296-4899-a795-d440d6735a2a'), (6270, 'c18172bc-ee0e-4969-8110-192dd5331ee7'), (24703, '24765f0c-b621-411d-824a-f9d36f2430d3'), (6271, 'db13ad85-6738-4e86-bea9-8e92682af368'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (23170, '7bd8ea6d-3f6f-4be9-914c-08cac82a76d9'), (6274, '8d7420e4-d50c-4af6-addb-8e2353cffe7e'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (23172, 'def32509-da1d-4b97-ad0b-943987ec2382'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (19085, '95ca3f5d-7594-4ebe-ae6d-1626686a69fc'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (20629, '7bef9173-19e3-43ce-a22f-8adb73202275'), (20630, '8f32ff5d-b8d4-4b24-bd11-ace0b94d3280'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (153, 'a5e94b25-4bd0-4edc-be2a-4167144129cb'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (8865, '383840dc-2a38-4164-982c-6ca6d4c15497'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (20662, 'b9d5b7a2-3272-4829-b567-ae41a047cdd8'), (5815, '7a89c479-9823-4c0f-a0de-e3ecf304e920'), (696, 'b996a5f4-7c16-44f1-aa17-bcf65b15101a'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (3793, '20be3864-830d-4112-ad66-401e5e96e6ea'), (17622, '18cebb0d-bc71-4646-91ee-08fdc8fb51a7'), (4824, '05ee0ea8-5d09-4181-beb5-75e28eca4584'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (4857, '92ddd494-af20-4518-a70a-45fc05454e67'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (20740, '8ea0cbc1-6505-4aba-8eeb-7045546046c8'), (1797, '9cf6eef7-0cf2-4e9a-9443-ab438980128c'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (1819, '842e6f2a-f115-402b-987e-3ad11f0f136b'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (17187, 'c0ff8015-235a-4a74-971c-1119a8449130'), (1829, '98ed0df5-e51e-4766-9380-1458db6d3ec6'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (18778, 'e289b3a6-ef0f-457f-a553-843fabd2902c'), (3422, '0003ea2c-f545-46e3-9729-94c1f40d9934'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (31647, '2e660078-e155-46a0-9ee9-a99bab76bb1c'), (25518, '1f847aca-c3cc-4179-894b-7deef51d2560'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (5569, '352e8632-382b-4223-8823-188eb1525f10'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (10198, '19eb7655-1d17-42bc-985e-b1a424722b72'), (30170, 'fd65dda2-bf03-4ea4-81a5-f79c9e951095'), (22493, '3d5e18a5-b466-426f-9e1f-8e5332bcb405'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (2527, 'b35cdd4c-87c8-40bf-98a7-98cf594734a7'), (2534, 'c18dd192-33c8-464e-a9d7-524374c7162d'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (14843, 'e99ac210-a8ee-46c9-ab15-45f0c9d7496a'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).

Since the process also saves the matches' history, it is possible to see precisely the results of any given model. This can, for instance, allow you to check why your model struggles with another one, most notably using another demo Space to visualize matches like [this one](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos.).

For now, **this experiment is running with the MLAgent environment SoccerTwos for the Hugging Face Deep RL Course**, however, the process and implementation, in general, are very much **environment agnostic and could be used to evaluate for free a wide range of adversarial multi-agent settings**.
--
title: ""Very Large Language Models and How to Evaluate Them"" 
thumbnail: /blog/assets/106_zero_shot_eval_on_the_hub/thumbnail.png
authors:
- user: mathemakitten
- user: Tristan
- user: abhishek
- user: lewtun
- user: douwekiela
---

# Very Large Language Models and How to Evaluate Them 


Large language models can now be evaluated on zero-shot classification tasks with [Evaluation on the Hub](https://huggingface.co/spaces/autoevaluate/model-evaluator)! 

Zero-shot evaluation is a popular way for researchers to measure the performance of large language models, as they have been [shown](https://arxiv.org/abs/2005.14165) to learn capabilities during training without explicitly being shown labeled examples. The [Inverse Scaling Prize](https://github.com/inverse-scaling/prize) is an example of a recent community effort to conduct large-scale zero-shot evaluation across model sizes and families to discover tasks on which larger models may perform worse than their smaller counterparts.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
- Our [paper](https://arxiv.org/abs/2109.02846) ""[Datasets: A Community Library for Natural Language Processing](https://arxiv.org/abs/2109.02846)"" documents the Hugging Face Datasets project that has over 300 contributors. This community project gives easy access to hundreds of datasets to researchers. It has facilitated new use cases of cross-dataset NLP, and has advanced features for tasks like indexing and streaming large datasets.
- Our collaboration with researchers from TU Darmstadt lead to another paper accepted at the conference ([""Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning""](https://arxiv.org/abs/2109.04144)). In this paper, we show that prompt-based fine-tuned language models (which achieve strong performance in few-shot setups) still suffer from learning surface heuristics (sometimes called *dataset biases*), a pitfall that zero-shot models don't exhibit.
 
Here is the question: How does Hugging Face handle the process of evaluating very large language models on tasks without labeled data, and what role does zero-shot evaluation play in this process?","Hugging Face handles the process of evaluating very large language models on tasks without labeled data through its ""Evaluation on the Hub"" tool, which supports zero-shot evaluation for language tasks. This tool allows for the evaluation of models on tasks without labeled data by concatenating completions to prompts. Additionally, Hugging Face plans to support zero-shot evaluation for language tasks that do not lend themselves to this format. The process uses several Hugging Face Datasets to provide data persistence, and it also saves the matches"
"In what way does Hugging Face’s evaluation process for large language models help detect biases in datasets, and how are these issues mitigated during model deployment?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (7687, '20608e74-1307-46cf-9341-09115b2bb0ac'), (13322, 'b0a48bd2-b0c8-4a35-a6a6-918c274e6820'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (13349, '6b2b3b26-e943-43aa-bba4-fdb96ec4ab02'), (13358, '5a6f6b09-7943-45ee-adf1-41511b760b13'), (13359, '6caf8e23-63aa-413c-b673-dc039aa9a2ec'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (11323, 'c2fb5953-52e5-48ef-bd7b-f2b14e24b166'), (11326, '24b1d1c2-4cfc-4f76-981a-e08b180448fc'), (11328, '228db1c0-a823-4e0e-b594-3594b9bd6517'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8267, '5870b739-749d-40fc-b471-e8b3aac03259'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (16461, 'e8e5c903-d205-41da-a374-3c853db11f7a'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (23172, 'def32509-da1d-4b97-ad0b-943987ec2382'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (19085, '95ca3f5d-7594-4ebe-ae6d-1626686a69fc'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (8865, '383840dc-2a38-4164-982c-6ca6d4c15497'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (29349, 'f9a62426-7040-4f1b-8771-11b8792ec32f'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5815, '7a89c479-9823-4c0f-a0de-e3ecf304e920'), (696, 'b996a5f4-7c16-44f1-aa17-bcf65b15101a'), (11452, '7e2fd91b-ca25-40e4-9aea-47ac22782115'), (10941, '3a713f0f-1a17-4b52-8cd4-d35ebf71072e'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (26821, '6fe23d6c-a3e0-4864-921e-63be56122c06'), (17622, '18cebb0d-bc71-4646-91ee-08fdc8fb51a7'), (731, 'ad4324cd-9c6c-4555-88d6-faf72cb8a9da'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (9957, '8bab9c1b-4445-47ac-86f0-34e165ef5756'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7913, '18d40909-e83c-4c62-9cee-03e8a507f58f'), (17657, '12121e46-a187-42c0-a9ad-22a96e02c8b9'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (14084, '975cef0c-44a3-4f42-bb19-d2ea4270c921'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (20771, '38817f21-13ab-43c2-a174-5b897fc1660c'), (19242, 'ac68e0a7-8ac5-4361-9b54-559389d13bb2'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (830, '4253d734-8fdc-48f0-ae58-86941ebeca66'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (28993, 'e94a1fa2-5915-4591-a20a-5ae790985a23'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (3421, '104644ea-5fc9-422a-b9e4-901aaee4414d'), (3422, '0003ea2c-f545-46e3-9729-94c1f40d9934'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (4959, '737a9e93-0ddb-4154-9fae-e4e57976cff1'), (3429, '72e30cd4-52fc-490a-8a4d-66f9dd4b7b53'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (3438, '6b08d05d-c4a6-4c31-8268-42efe7f14aa4'), (3439, '0af44d7a-c74c-4934-9280-08873b27666b'), (3441, 'a50198ff-bdce-4045-af16-a7f3e4ed8e3a'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (3447, '7a968bc4-b1e2-481b-91df-18182329a5e4'), (3459, 'bc1a44b5-f64b-4598-85e8-e3280271e021'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (12181, 'ff5a3adb-a22c-4bbb-9749-782aceee4a70'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (1514, 'd61b2641-9e09-4c4e-8e1e-72599ec17422'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (15339, '9006a23a-10b3-4cd2-a0a0-5f95871a357d'), (15340, '77986b8e-a42a-4367-9d6c-93ddeb9a16b9'), (14843, 'e99ac210-a8ee-46c9-ab15-45f0c9d7496a'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: If you want to see the work in action, check out the [Jupyter notebook](https://colab.research.google.com/drive/1-HDJUcPMKEF-E7Hapih0OmA1xTW2hdAv#scrollTo=yX8ciyVWKiuO) we created!

The workflow has two main steps:
- Prompting the language model with a predefined set of prompts (hosted on [🤗 Datasets](https://huggingface.co/datasets))
- Evaluating the generations using a metric or measurement (using [🤗 Evaluate](https://huggingface.co/docs/evaluate/index))

Let's work through bias evaluation in 3 prompt-based tasks focused on harmful language: Toxicity, Polarity, and Hurtfulness. The work we introduce here serves to demonstrate how to utilize Hugging Face libraries for bias analyses, and does not depend on the specific prompt-based dataset used. Critically, remember that recently introduced datasets for evaluating biases are initial steps that do not capture the vast range of biases that models may produce (see the Discussion section below for more details).

## Toxicity
The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).

Since the process also saves the matches' history, it is possible to see precisely the results of any given model. This can, for instance, allow you to check why your model struggles with another one, most notably using another demo Space to visualize matches like [this one](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos.).

For now, **this experiment is running with the MLAgent environment SoccerTwos for the Hugging Face Deep RL Course**, however, the process and implementation, in general, are very much **environment agnostic and could be used to evaluate for free a wide range of adversarial multi-agent settings**.
When doing online model evaluation, there is often a trade-off to be done between inference speed and accuracy or precision, whereas this is less the case for offline evaluation.

## Limitations and bias

All models and all metrics have their limitations and biases, which depend on the way in which they were trained, the data that was used, and their intended uses. It is important to measure and communicate these limitations clearly to prevent misuse and unintended impacts, for instance via [model cards](https://huggingface.co/course/chapter4/4?fw=pt) which document the training and evaluation process.

Measuring biases can be done by evaluating models on datasets such as [Wino Bias](https://huggingface.co/datasets/wino_bias) or [MD Gender Bias](https://huggingface.co/datasets/md_gender_bias), and by doing [Interactive Error Analyis](https://huggingface.co/spaces/nazneen/error-analysis) to try to identify which subsets of the evaluation dataset a model performs poorly on.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
 
Here is the question: In what way does Hugging Face’s evaluation process for large language models help detect biases in datasets, and how are these issues mitigated during model deployment?","Hugging Face's evaluation process for large language models helps detect biases in datasets through several ways:

1. **Evaluating models on datasets specifically designed to test for biases**: Such as Wino Bias and MD Gender Bias, which can identify biases in the model's performance on specific subsets of the evaluation dataset.
2. **Interactive Error Analysis**: This involves analyzing the model's performance on specific subsets of the evaluation dataset to identify where it performs poorly.
3. **Model cards and documentation**: Hugging"
"What role does the container log system play in managing large models, and how does this integrate with the model evaluation process on Hugging Face?","[(17920, '9c587d42-fc22-4562-91cb-4e195b4c2fb1'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (27672, '59487233-b752-42ea-aa5a-8cb853def35f'), (29212, 'a5e56809-c3c5-4caa-811d-eee83d06290d'), (19999, '49eb2d92-84fe-4bcf-a1c8-1bfdb01bcb11'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (29233, '74f4f91f-2db4-42a8-a508-33e42d02d65f'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (15936, 'ab08450b-3b5c-4bbc-8c0e-355d1b29997d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4179, 'b6031fcd-da9e-4a41-ac79-36bf3eaa3bd0'), (4183, 'd33bb078-e05c-4e7b-b445-322a674ac06c'), (7769, 'd240b6ea-8083-4b18-b0bd-b2cbf0cf8810'), (4192, 'fc859fbf-f289-4177-a3a6-21b11cb22ad3'), (3183, '05adffa9-2646-489c-b3ad-2357cb108a8e'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (25220, 'd9714992-eee7-4b14-bcef-6d8ebda275a0'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (30852, '140bd3bf-c87c-44f1-8231-5017fbe767e0'), (7305, 'ae446800-6311-4d46-84b9-7e3088723773'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (171, '5cd2c4b7-930f-42e0-b91f-7c8d9b1efcac'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (1208, 'f92f90e4-fea0-4a20-8453-df5327bb6e2f'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (16059, '250a678f-6a31-410f-88f6-17ac5b9cbaf8'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (5822, '95a2e3de-66e6-4f99-8e23-04e96dc3f30f'), (5826, 'cf5b8dce-2e31-49f1-a43b-78b892eec7b6'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (5829, '107bddd6-ea08-4772-9aa2-14bf3335e980'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (19683, '5d3bfb1d-f892-4db6-8596-bd364235244a'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (21755, 'e6693759-5af4-4449-9f6f-4c0e45498df9'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (16638, '6214eb65-8a1c-4818-ba45-434c36461a1f'), (20223, 'a0ff3a3b-b5b7-4fcf-9f9d-68877d1cc4ad'), (21765, '6a1e63b7-b878-4676-9087-269c0b32fba1'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (17699, 'f896ccff-a53b-4e44-ab27-f7e12cc98e4a'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (15733, 'c3b1726d-a526-47b6-ba75-aa81e9c3b724'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (403, 'b6c24f9c-2894-4834-96cd-4a13ea060bb0'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (29598, '78d2e6f1-098e-4154-9814-d3529bde1b1a'), (6050, '3cb3a71c-89ac-4817-8a94-51f7aa7240cd'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (20901, '8e4eb670-266c-471e-aef1-087bd6dfa125'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (12729, '93d429ec-9cb7-4d85-9f1e-3db488055574'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (3024, '6582a1c7-c7c9-4fcb-93db-b4c90e6e31f9'), (15312, '0f584622-999c-4d59-b800-a7909cd678cc'), (24020, '9ceff87b-6adb-4e98-ad01-dc489b9f3c39'), (15836, 'f6ad3be6-00de-45fb-8497-71c9ae81f26b'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25057, '85b645cd-7921-4981-9d5f-dcbd47710494'), (25058, 'f06d5733-7ce2-443d-85cb-88a01b88ca4f'), (5616, 'f1974887-c0b6-4d39-8068-c53a5ce076ad'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In addition to its custom ML models, Fetch uses [AWS Deep Learning Containers ](https://aws.amazon.com/machine-learning/containers/)(AWS DL Containers), which businesses can use to quickly deploy deep learning environments with optimized, prepackaged container images. This simplifies the process of using libraries from [Hugging Face Inc.](https://huggingface.co/)(Hugging Face), an artificial intelligence technology company and [AWS  Partner](https://partners.amazonaws.com/partners/0010h00001jBrjVAAS/Hugging%20Face%20Inc.). Specifically, Fetch uses the Amazon SageMaker Hugging Face Inference Toolkit, an open-source library for serving transformers models, and the Hugging Face AWS Deep Learning Container for training and inference. “Using the flexibility of the Hugging Face AWS Deep Learning Container, we could improve the quality of our models,” says Corzine. “And Hugging Face’s partnership with AWS meant that it was simple to deploy these models.”
An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.

You can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).

---

## Benchmark
Now, you can reasonably argue that ECS was not the best approach to serving ML models, but it served us up until now, and also allowed ML models to sit alongside other container based services, so it reduced cognitive load.

## What do we do now?

With Inference Endpoints, our flow looks like this:

- Train model on a GPU instance (provisioned by  [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))
- Upload to [Hugging Face Hub](https://huggingface.co/models)
- Deploy using Hugging Face Inference Endpoints.
## What were we doing?

The models that we have switched over to Inference Endpoints were previously managed internally and were running on AWS [Elastic Container Service](https://aws.amazon.com/ecs/) (ECS) backed by [AWS Fargate](https://aws.amazon.com/fargate/). This gives you a serverless cluster which can run container based tasks. Our process was as follows:

- Train model on a GPU instance (provisioned by [CML](https://cml.dev/), trained with [transformers](https://huggingface.co/docs/transformers/main/))
- Upload to [Hugging Face Hub](https://huggingface.co/models)
- Build API to serve model [(FastAPI)](https://fastapi.tiangolo.com/)
- Wrap API in container [(Docker)](https://www.docker.com/)
- Upload container to AWS [Elastic Container Repository](https://aws.amazon.com/ecr/) (ECR)
- Deploy model to ECS Cluster
So this is significantly easier. We could also use another managed service such as [SageMaker](https://aws.amazon.com/es/sagemaker/), [Seldon](https://www.seldon.io/), or [Bento ML](https://www.bentoml.com/), etc., but since we are already uploading our model to Hugging Face hub to act as a model registry, and we’re pretty invested in Hugging Face’s other tools (like transformers, and [AutoTrain](https://huggingface.co/autotrain)) using Inference Endpoints makes a lot of sense for us.


## What about Latency and Stability?

Before switching to Inference Endpoints we tested different CPU endpoints types using [ab](https://httpd.apache.org/docs/2.4/programs/ab.html).

For ECS we didn’t test so extensively, but we know that a large container had a latency of about ~200ms from an instance in the same region. The tests we did for Inference Endpoints we based on text classification model fine tuned on [RoBERTa](https://huggingface.co/roberta-base) with the following test parameters:
 
Here is the question: What role does the container log system play in managing large models, and how does this integrate with the model evaluation process on Hugging Face?","There is no mention of the container log system playing a role in managing large models or integrating with the model evaluation process on Hugging Face in the provided context. The context primarily discusses the deployment and serving of ML models using Hugging Face's tools and AWS services, but it does not mention container log systems or their role in model management or evaluation."
"What is the relationship between the Hugging Face Hub, dataset management, and the deployment of models like BigBird, especially regarding bias detection and model evaluation?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (16932, 'd165fea3-524c-4a1f-8627-923c16bdcb4f'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (6766, 'e35d1260-f910-44b2-a82d-648bc59cb455'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (24727, '710d9250-ca94-45f1-a999-cbc01b7e8ff7'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (22704, 'd853fcd0-1387-4353-a361-a5295f36b273'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24793, '6eeb4cd3-a092-447d-9528-a03119bf4a08'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (24804, '23cde3d9-30d2-4fd8-84f5-8504a15e3a42'), (7905, 'be711455-ffac-4dbb-aa38-bcb889eaadd6'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (1300, '9c9ac0e7-8716-4956-b507-5a4dc65cdf6b'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (26405, '083107e3-5c6f-4e2d-ac5e-50717bdd56d6'), (15669, '0256580b-34ce-4e4a-88aa-42403eb7a050'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8580, '14576ef8-e3f0-4ac9-bcdc-c523b7589ded'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8593, '0724f652-10b8-4c86-8dd0-53285edc20d2'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (8595, 'd6792ca7-c71d-43dd-8f05-d316ad1f0a60'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (25050, '4ea83d3e-bb4a-4ab5-b25a-c79f5c52c46a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (13814, 'd8be2885-e7f0-4d64-973b-c7c6e554c60d'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The [Hugging Face Hub](https://huggingface.co/) is a central repository where people can share and access machine learning models, datasets and demos. The Hub hosts over 190,000 machine learning models, 33,000 datasets and over 100,000 machine learning applications and demos. These models cover a wide range of tasks from pre-trained language models, text, image and audio classification models, object detection models, and a wide range of generative models. 

The models, datasets and demos hosted on the Hub span a wide range of domains and languages, with regular community efforts to expand the scope of what is available via the Hub. This blog post intends to offer people working in or with the galleries, libraries, archives and museums (GLAM) sector to understand how they can use &mdash; and contribute to &mdash; the Hugging Face Hub.

You can read the whole post or jump to the most relevant sections!
Evaluation on the Hub is meant to make your life easier. But of course, there’s a lot happening in the background. What we really like about Evaluation on the Hub: it fits so neatly into the existing Hugging Face ecosystem, we almost had to do it. Users start on dataset pages, from where they can launch evaluations or see leaderboards. The model evaluation submission interface and the leaderboards are regular Hugging Face Spaces. The evaluation backend is powered by AutoTrain, which opens up a PR on the Hub for the given model’s model card.

## DogFood - Distinguishing Dogs, Muffins and Fried Chicken

So what does it look like in practice? Let’s run through an example. Suppose you are in the business of telling apart dogs, muffins and fried chicken (a.k.a. dogfooding!).
Domain-specific datasets are vital for evaluating and training machine learning models, helping to overcome the limitations of existing models. Creating these datasets, however, is challenging, requiring significant time, resources, and domain expertise, particularly for annotating data. Maximizing the impact of this data is crucial for the benefit of both the researchers involved and their respective fields.

The Hugging Face Hub can help achieve this maximum impact. 

## What is the Hugging Face Hub?

The [Hugging Face Hub](https://huggingface.co/) has become the central hub for sharing open machine learning models, datasets and demos, hosting over 360,000 models and 70,000 datasets. The Hub enables people – including researchers – to access state-of-the-art machine learning models and datasets in a few lines of code.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
As a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.

In addition, the Hugging Face Hub offers:

-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)
-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)
-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)
-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)
-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)


### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.
 
Here is the question: What is the relationship between the Hugging Face Hub, dataset management, and the deployment of models like BigBird, especially regarding bias detection and model evaluation?","The Hugging Face Hub is a central repository that hosts a vast number of machine learning models, datasets, and demos. It provides a platform for users to share, access, and evaluate models, datasets, and demos. The Hub's dataset functionality is tightly integrated with its model evaluation capabilities, allowing users to easily evaluate and compare models on specific tasks and datasets. This integration enables users to identify biases in models and detect potential issues during the evaluation process.

In the context of model deployment, the Hugging"
"How does the Hugging Face Hub facilitate the integration of new datasets, and how does this process help evaluate large models like BigBird for biases?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (16932, 'd165fea3-524c-4a1f-8627-923c16bdcb4f'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (19504, 'eccd285a-5b4d-4f9b-8210-3849515ea05b'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (24727, '710d9250-ca94-45f1-a999-cbc01b7e8ff7'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (22704, 'd853fcd0-1387-4353-a361-a5295f36b273'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (9952, '7b5b02be-c8e3-4b66-9ea2-d73cf414e65f'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (24804, '23cde3d9-30d2-4fd8-84f5-8504a15e3a42'), (24803, '0d7264b1-bf00-4ff7-b817-27ed08d2fabc'), (24805, '5f103d70-5254-4011-b9dc-3f5b60ba28ab'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (28440, 'f186228b-4ecd-4f33-a849-39b930f0800a'), (4385, '51afe3fa-fbdc-4658-afb7-8e0f1b87741d'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (26405, '083107e3-5c6f-4e2d-ac5e-50717bdd56d6'), (4910, '4b08a4d9-9890-47a3-9b44-e19da59368c2'), (15669, '0256580b-34ce-4e4a-88aa-42403eb7a050'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (9559, 'a8284fc7-f6b2-4800-baa1-e5b290b17b91'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (3938, '96d2aac9-2c1e-4025-9c02-49d3e54a8d8b'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8580, '14576ef8-e3f0-4ac9-bcdc-c523b7589ded'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (8586, '90a0534c-f6eb-45e6-8c1b-b6c343f84cf8'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (27040, '56f391b1-2a20-4be2-af9f-8f26721aaaf2'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (26037, '7b88d72d-043f-4e7c-8cf3-6c5fc61c846b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (21962, '4ed9bb78-d26f-490e-ba3a-60bd868eff98'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (25050, '4ea83d3e-bb4a-4ab5-b25a-c79f5c52c46a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (3061, '635dc908-0e05-4094-97ee-78809069be0a'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd'), (13814, 'd8be2885-e7f0-4d64-973b-c7c6e554c60d'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Domain-specific datasets are vital for evaluating and training machine learning models, helping to overcome the limitations of existing models. Creating these datasets, however, is challenging, requiring significant time, resources, and domain expertise, particularly for annotating data. Maximizing the impact of this data is crucial for the benefit of both the researchers involved and their respective fields.

The Hugging Face Hub can help achieve this maximum impact. 

## What is the Hugging Face Hub?

The [Hugging Face Hub](https://huggingface.co/) has become the central hub for sharing open machine learning models, datasets and demos, hosting over 360,000 models and 70,000 datasets. The Hub enables people – including researchers – to access state-of-the-art machine learning models and datasets in a few lines of code.
### Support for large datasets

The Hub can host large datasets; it currently hosts datasets with multiple TBs of data.The datasets library, which users can use to download and process datasets from the Hub, supports streaming, making it possible to work with large datasets without downloading the entire dataset upfront. This can be invaluable for allowing researchers with less computational resources to work with your datasets, or to select small portions of a huge dataset for testing, development or prototyping.


<p align=""center""> 
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/researcher-dataset-sharing/filesize.png"" alt=""Screenshot of the file size information for a dataset""><br> 
<em>The Hugging Face Hub can host the large datasets often created for machine learning research.</em> 
 </p> 


## API and client library interaction with the Hub
</div>

## What's the Hugging Face Hub?

We are helping the community work together towards the goal of advancing Machine Learning 🔥.

The Hugging Face Hub is a platform with over 120k models, 20k datasets, and 50k demos in which people can easily collaborate in their ML workflows. The Hub works as a central place where anyone can share, explore, discover, and experiment with open-source Machine Learning.

No single company, including the Tech Titans, will be able to “solve AI” by themselves – the only way we'll achieve this is by sharing knowledge and resources in a community-centric approach. We are building the largest open-source collection of models, datasets, demos and metrics on the Hugging Face Hub to democratize and advance ML for everyone 🚀.
## What can you find on the Hugging Face Hub?

### Models 

The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.

### Datasets
The Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.

### Spaces

Hugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications.
--
title: ""DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub"" 
thumbnail: /blog/assets/hub_duckdb/hub_duckdb.png
authors:
- user: stevhliu
- user: lhoestq
- user: severo
---

# DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub


The Hugging Face Hub is dedicated to providing open access to datasets for everyone and giving users the tools to explore and understand them. You can find many of the datasets used to train popular large language models (LLMs) like [Falcon](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [MPT](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf), and [StarCoder](https://huggingface.co/datasets/bigcode/the-stack). There are tools for addressing fairness and bias in datasets like [Disaggregators](https://huggingface.co/spaces/society-ethics/disaggregators), and tools for previewing examples inside a dataset like the Dataset Viewer.
 
Here is the question: How does the Hugging Face Hub facilitate the integration of new datasets, and how does this process help evaluate large models like BigBird for biases?","The Hugging Face Hub facilitates the integration of new datasets by providing a platform for users to share, explore, discover, and experiment with open-source machine learning datasets. The Hub hosts over 70,000 datasets, covering a range of domains and modalities, including text, image, audio, and multi-modal datasets. The datasets library supports streaming, allowing users to work with large datasets without downloading the entire dataset upfront. This enables researchers to easily access and integrate new datasets into their workflows.

The process"
"How does Hugging Face address biases in datasets like WinoBias, and what role does zero-shot evaluation and legal frameworks play in mitigating these biases during model deployment?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13322, 'b0a48bd2-b0c8-4a35-a6a6-918c274e6820'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (13324, 'e927c83d-2a0f-4409-a00f-c0599d0ce5aa'), (13325, '16d0f9ea-2690-41bb-b5b7-ea15fa1b01e6'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (13349, '6b2b3b26-e943-43aa-bba4-fdb96ec4ab02'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (13358, '5a6f6b09-7943-45ee-adf1-41511b760b13'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (11324, 'ee709d79-fa6d-4dce-a3f0-9b943c904977'), (11326, '24b1d1c2-4cfc-4f76-981a-e08b180448fc'), (11328, '228db1c0-a823-4e0e-b594-3594b9bd6517'), (11331, 'ee6c23d0-eb20-4401-bdfb-e49c7aaaa083'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (11338, '30d2c084-1101-4560-b412-45d9eae8ef69'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (17018, '4eb5a86b-2fd2-48d3-b487-d328172d0086'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (6270, 'c18172bc-ee0e-4969-8110-192dd5331ee7'), (6271, 'db13ad85-6738-4e86-bea9-8e92682af368'), (6272, 'e07f417c-d3c6-427e-a0fc-319f3ec494c2'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (7822, '0146a8fb-e424-4024-96af-951c12eb724f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5300, 'd0d1cccb-eecd-4d2f-a930-2ebfd5ddf60c'), (5815, '7a89c479-9823-4c0f-a0de-e3ecf304e920'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (5944, 'af31af69-da28-4239-b3f5-59c624ec8d83'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (29593, '6a3d74c2-ed43-42ed-8939-0c89a33f7be8'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (30685, 'a039eae8-64c2-468f-9dfd-126ca871a519'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (4575, '5b663118-3407-4a6a-8f1f-8e73ded1dded'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (15339, '9006a23a-10b3-4cd2-a0a0-5f95871a357d'), (15340, '77986b8e-a42a-4367-9d6c-93ddeb9a16b9'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: If you want to see the work in action, check out the [Jupyter notebook](https://colab.research.google.com/drive/1-HDJUcPMKEF-E7Hapih0OmA1xTW2hdAv#scrollTo=yX8ciyVWKiuO) we created!

The workflow has two main steps:
- Prompting the language model with a predefined set of prompts (hosted on [🤗 Datasets](https://huggingface.co/datasets))
- Evaluating the generations using a metric or measurement (using [🤗 Evaluate](https://huggingface.co/docs/evaluate/index))

Let's work through bias evaluation in 3 prompt-based tasks focused on harmful language: Toxicity, Polarity, and Hurtfulness. The work we introduce here serves to demonstrate how to utilize Hugging Face libraries for bias analyses, and does not depend on the specific prompt-based dataset used. Critically, remember that recently introduced datasets for evaluating biases are initial steps that do not capture the vast range of biases that models may produce (see the Discussion section below for more details).

## Toxicity
- Our [paper](https://arxiv.org/abs/2109.02846) ""[Datasets: A Community Library for Natural Language Processing](https://arxiv.org/abs/2109.02846)"" documents the Hugging Face Datasets project that has over 300 contributors. This community project gives easy access to hundreds of datasets to researchers. It has facilitated new use cases of cross-dataset NLP, and has advanced features for tasks like indexing and streaming large datasets.
- Our collaboration with researchers from TU Darmstadt lead to another paper accepted at the conference ([""Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning""](https://arxiv.org/abs/2109.04144)). In this paper, we show that prompt-based fine-tuned language models (which achieve strong performance in few-shot setups) still suffer from learning surface heuristics (sometimes called *dataset biases*), a pitfall that zero-shot models don't exhibit.
The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).

Since the process also saves the matches' history, it is possible to see precisely the results of any given model. This can, for instance, allow you to check why your model struggles with another one, most notably using another demo Space to visualize matches like [this one](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos.).

For now, **this experiment is running with the MLAgent environment SoccerTwos for the Hugging Face Deep RL Course**, however, the process and implementation, in general, are very much **environment agnostic and could be used to evaluate for free a wide range of adversarial multi-agent settings**.
---

To use GPT-Neo or any Hugging Face model in your own application, you can [start a free trial](https://huggingface.co/pricing) of the 🤗 Accelerated Inference API.
If you need help mitigating bias in models and AI systems, or leveraging Few-Shot Learning, the 🤗 Expert Acceleration Program can [offer your team direct premium support from the Hugging Face team](https://huggingface.co/support).
## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
 
Here is the question: How does Hugging Face address biases in datasets like WinoBias, and what role does zero-shot evaluation and legal frameworks play in mitigating these biases during model deployment?","The context does not provide a direct answer to the question. It discusses the workflow for bias evaluation in prompt-based tasks, specifically toxicity, polarity, and hurtfulness, and mentions the importance of evaluating and mitigating biases in models. However, it does not specifically address how Hugging Face addresses biases in datasets like WinoBias, nor does it discuss the role of zero-shot evaluation and legal frameworks in mitigating these biases during model deployment."
"How does Hugging Face manage large models like BigBird during deployment, particularly in relation to biases, and how do logs assist in addressing dataset issues during inference?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4179, 'b6031fcd-da9e-4a41-ac79-36bf3eaa3bd0'), (1113, '5481da52-5b02-4d6f-a69d-67d49239fa4a'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (30852, '140bd3bf-c87c-44f1-8231-5017fbe767e0'), (18075, '6383cbec-9620-45d7-b7c2-2671da0730d0'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (171, '5cd2c4b7-930f-42e0-b91f-7c8d9b1efcac'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (4285, '267cef4b-a36b-4d83-89dd-8f9a385949a8'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (22207, '80cd1cdc-26bf-4c5d-8dcf-7c77954f2278'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (13508, '91853fd6-7904-4ab6-949c-cd38b42bf46a'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (8920, '15196e08-d690-45d3-b1f2-cbb03564ce1b'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (17122, '9f5d5e67-e2c9-4b8a-96c7-47699db428d3'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28912, '60b6bec2-6f8f-4af2-a24d-3c1282e096ce'), (28913, '8ebcd930-9f33-4341-9b0b-5a6fffb52797'), (30965, '6adffa38-ab02-41e9-a179-aaca1c2624e8'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (3847, 'cbdea87e-dfaa-424a-9c3e-854bfbcd052d'), (21768, '61ac4e62-e904-4c53-9623-9416d918cb0e'), (6421, 'b8aebff7-309e-4b44-a96e-026ae78e518f'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (20777, '6b031a1e-1a0f-416a-865c-e7d8c0560fbf'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (1397, '50ea51a1-c7c4-472d-b028-3435e17d1ed7'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (403, 'b6c24f9c-2894-4834-96cd-4a13ea060bb0'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (29593, '6a3d74c2-ed43-42ed-8939-0c89a33f7be8'), (6042, '5b1abefc-94b9-4cfd-99ca-23843fd92410'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (411, '96253f57-9a1a-4672-829b-a8e8241df3cf'), (29599, '143208c5-54d7-4e6b-ba1d-40ad953a9c0d'), (27040, '56f391b1-2a20-4be2-af9f-8f26721aaaf2'), (6047, '162b5640-f06a-4c27-97b1-d3c317bbb669'), (6050, '3cb3a71c-89ac-4817-8a94-51f7aa7240cd'), (7076, '422e3575-7056-4737-b4dc-2bc42a3f5bc7'), (6060, '91bbfe9c-7eca-4bf4-baa9-b087c9640f38'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (31681, 'cd70c10d-6d2b-404f-842b-5ea26dc18134'), (22477, '06267b14-5048-460b-9547-63335518d2da'), (24020, '9ceff87b-6adb-4e98-ad01-dc489b9f3c39'), (24021, '22a9faee-fca9-4a84-b0c7-5bca559b1809'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (24025, '95318624-c2ce-41c0-891f-a3b06efb2458'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31714, '77fb7510-e224-4453-b854-845455318fe6'), (31715, 'ed08f117-772f-417a-95b4-fcd4c94f4f30'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (14843, 'e99ac210-a8ee-46c9-ab15-45f0c9d7496a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ![NER](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/disaster-assets/deprem-ner.png)

The model was later used in `afetharita` to extract addresses. The parsed addresses would be sent to a geocoding API to obtain longitude and latitude, and the geolocation would then be displayed on the front-end map. For inference, we have used Inference API, which is an API that hosts model for inference and is automatically enabled when the model is pushed to Hugging Face Hub. Using Inference API for serving has saved us from pulling the model, writing an app, building a docker image, setting up CI/CD, and deploying the model to a cloud instance, where it would be extra overhead work for the DevOps and cloud teams as well. Hugging Face teams have provided us with more replicas so that there would be no downtime and the application would be robust against a lot of traffic.
--
title: ""DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub"" 
thumbnail: /blog/assets/hub_duckdb/hub_duckdb.png
authors:
- user: stevhliu
- user: lhoestq
- user: severo
---

# DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub


The Hugging Face Hub is dedicated to providing open access to datasets for everyone and giving users the tools to explore and understand them. You can find many of the datasets used to train popular large language models (LLMs) like [Falcon](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [MPT](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf), and [StarCoder](https://huggingface.co/datasets/bigcode/the-stack). There are tools for addressing fairness and bias in datasets like [Disaggregators](https://huggingface.co/spaces/society-ethics/disaggregators), and tools for previewing examples inside a dataset like the Dataset Viewer.
### Text Generation Inference

Response time and latency for concurrent users are a big challenge for serving these large models. To tackle this problem, Hugging Face has released [text-generation-inference](https://github.com/huggingface/text-generation-inference) (TGI), an open-source serving solution for large language models built on Rust, Python, and gRPc. TGI is integrated into inference solutions of Hugging Face, [Inference Endpoints](https://huggingface.co/inference-endpoints), and [Inference API](https://huggingface.co/inference-api), so you can directly create an endpoint with optimized inference with few clicks, or simply send a request to Hugging Face's Inference API to benefit from it, instead of integrating TGI to your platform. 

![Screenshot from HuggingChat](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/os_llms/huggingchat_ui.png)
After a first test on Google Colab, the Hugging Face experts guided Witty Works on deploying the model on Azure. No optimization was necessary as the model was fast enough.

```diff
Elena Nazarenko, Lead Data Scientist at Witty Works: 
“Working with Hugging Face saved us a lot of time and money. 
One can feel lost when implementing complex text classification use cases. 
As it is one of the most popular tasks, there are a lot of models on the Hub. 
The Hugging Face experts guided me through the massive amount of transformer-based 
models to choose the best possible approach. 
Plus, I felt very well supported during the model deployment”
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
 
Here is the question: How does Hugging Face manage large models like BigBird during deployment, particularly in relation to biases, and how do logs assist in addressing dataset issues during inference?","The context does not provide a direct answer to how Hugging Face manages large models like BigBird during deployment, particularly in relation to biases, and how logs assist in addressing dataset issues during inference. The provided information discusses the Hugging Face Hub, Inference API, and text generation inference, but it does not specifically address the management of large models during deployment, biases, or the role of logs in addressing dataset issues. Therefore, I cannot provide a comprehensive answer to this question based on the provided context"
"How does Hugging Face support the integration of new datasets and model evaluation for biases, particularly with large models like BigBird, and how do logs assist in this process?","[(10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (13893, '821034d7-81d2-4f43-ab41-86255b1a41a6'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4179, 'b6031fcd-da9e-4a41-ac79-36bf3eaa3bd0'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (12913, '6b795525-cd89-4895-a8f6-d7b48571a2f3'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (30852, '140bd3bf-c87c-44f1-8231-5017fbe767e0'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (4237, 'aa571318-cb21-42dc-96d5-9247c29a9ac5'), (23714, 'a2d69303-a006-44f0-8008-1accb10a3697'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (171, '5cd2c4b7-930f-42e0-b91f-7c8d9b1efcac'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7913, '18d40909-e83c-4c62-9cee-03e8a507f58f'), (22762, '50d4eede-c4bc-4f6f-9e8d-48bac04e53c9'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (15177, '3f6d83bd-818e-45ef-ae78-39a0087fc95b'), (9559, 'a8284fc7-f6b2-4800-baa1-e5b290b17b91'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (12181, 'ff5a3adb-a22c-4bbb-9749-782aceee4a70'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (27040, '56f391b1-2a20-4be2-af9f-8f26721aaaf2'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (6079, '2234d568-f21c-4b71-b407-a24d00ca3c01'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (15857, 'a948a556-2726-422c-ada1-d6f6cfeaeb9b'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).

Since the process also saves the matches' history, it is possible to see precisely the results of any given model. This can, for instance, allow you to check why your model struggles with another one, most notably using another demo Space to visualize matches like [this one](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos.).

For now, **this experiment is running with the MLAgent environment SoccerTwos for the Hugging Face Deep RL Course**, however, the process and implementation, in general, are very much **environment agnostic and could be used to evaluate for free a wide range of adversarial multi-agent settings**.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
 
Here is the question: How does Hugging Face support the integration of new datasets and model evaluation for biases, particularly with large models like BigBird, and how do logs assist in this process?","Hugging Face supports the integration of new datasets and model evaluation for biases through its infrastructure and tooling, enabling large-scale evaluation. It provides a platform for evaluating and documenting bias through model cards, datasheets, and READMEs. Additionally, Hugging Face's open-source models and datasets, including specialized models like BioBERT and SciBERT, can be used to evaluate biases in specific domains. The logs from the evaluation process, such as those from the ML-Agents environment SoccerTwos, can"
"What is the role of logs in Hugging Face’s process of model evaluation, and how does it support bias detection, particularly for large models like BigBird?","[(17920, '9c587d42-fc22-4562-91cb-4e195b4c2fb1'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (15905, 'b861418a-a3d1-4b9a-8118-c9f37a161a38'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (11330, 'd5b3264b-e32e-48b5-b502-20353b186343'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (1118, '5dd59d54-8203-423f-a71a-a3af1fffe5e4'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (30852, '140bd3bf-c87c-44f1-8231-5017fbe767e0'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (171, '5cd2c4b7-930f-42e0-b91f-7c8d9b1efcac'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (28356, 'b8b471a3-a61b-4ee5-9154-be89b6b87789'), (10966, '43bdf151-fe3f-4742-b435-208afb776cb2'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (7905, 'be711455-ffac-4dbb-aa38-bcb889eaadd6'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (30950, '8c43e960-5196-430d-a443-73c88cf050e0'), (30951, 'a57b2572-c7e0-40dd-ba85-61f537b777b7'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7913, '18d40909-e83c-4c62-9cee-03e8a507f58f'), (22762, '50d4eede-c4bc-4f6f-9e8d-48bac04e53c9'), (14056, 'e4acfaa9-c866-4fa3-b263-ebd8b7794cf3'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (30965, '6adffa38-ab02-41e9-a179-aaca1c2624e8'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (6421, 'b8aebff7-309e-4b44-a96e-026ae78e518f'), (3866, '45636a2d-f101-4a6e-83b0-d6ac592363f7'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (24899, 'd5261e5b-d3f0-43d7-aac1-57db8255c358'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (3422, '0003ea2c-f545-46e3-9729-94c1f40d9934'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (3447, '7a968bc4-b1e2-481b-91df-18182329a5e4'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (12181, 'ff5a3adb-a22c-4bbb-9749-782aceee4a70'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (12201, '5b5f1162-d4c9-4d53-8af9-3d6826c6354e'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (3024, '6582a1c7-c7c9-4fcb-93db-b4c90e6e31f9'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (24020, '9ceff87b-6adb-4e98-ad01-dc489b9f3c39'), (8662, '58619cbe-0e96-4b3a-a710-e101d3ab5a46'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (15340, '77986b8e-a42a-4367-9d6c-93ddeb9a16b9'), (23534, '2e19fd37-1c85-42ec-961a-2c1519493853'), (31728, '5ccf1245-717b-44e0-837a-e164da6b012a'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (17914, 'e3f62a9f-6c82-48c3-84c5-d5b2d2cdf56b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).

Since the process also saves the matches' history, it is possible to see precisely the results of any given model. This can, for instance, allow you to check why your model struggles with another one, most notably using another demo Space to visualize matches like [this one](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos.).

For now, **this experiment is running with the MLAgent environment SoccerTwos for the Hugging Face Deep RL Course**, however, the process and implementation, in general, are very much **environment agnostic and could be used to evaluate for free a wide range of adversarial multi-agent settings**.
After an extensive three-month evaluation period, we chose Hugging Face to host our models. During this time, we gradually increased transaction volume to their hosted models and ran numerous simulated load tests based on our worst-case scenario volumes. This process allowed us to fine-tune our system and monitor performance, ultimately giving us confidence in the inference API's ability to handle our transaction enrichment loads.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
 
Here is the question: What is the role of logs in Hugging Face’s process of model evaluation, and how does it support bias detection, particularly for large models like BigBird?","Based on the provided context, there is no direct mention of the role of logs in Hugging Face's process of model evaluation or how it supports bias detection, particularly for large models like BigBird. The context discusses the importance of model evaluation, the need for standardizing model evaluation methods, and the use of model cards and other forms of documentation to evaluate and document bias. However, it does not specifically mention logs or their role in bias detection. Therefore, I cannot provide a concise and relevant answer"
"How does Hugging Face ensure the ethical use of large models like BigBird, particularly when dealing with biases in datasets, and what role do logs play in this process?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (13320, '8272c7ae-a9bb-4f00-90df-c471493cb8aa'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (19980, '3aa3ba7e-8013-492f-b83a-91c341becb4f'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5681, '87cc4582-10fe-4332-96c6-9ddc31709023'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (5683, '3fbd0aa6-3836-4bba-965c-ec5e95b6213f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11328, '228db1c0-a823-4e0e-b594-3594b9bd6517'), (11330, 'd5b3264b-e32e-48b5-b502-20353b186343'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (1113, '5481da52-5b02-4d6f-a69d-67d49239fa4a'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (19043, '4932d585-53ec-4b0b-abaf-beb63614b0aa'), (17003, '04cc67ea-1b8c-4ddb-82ec-f36431591ae9'), (8812, '99a22463-c724-477a-8a22-0965d1ff8629'), (21623, '32bea9e3-44f8-4464-a8dd-d2dcbc3a2102'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (29349, 'f9a62426-7040-4f1b-8771-11b8792ec32f'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (4365, '0e049057-3bcb-4ecb-afc0-901cd049382c'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (15177, '3f6d83bd-818e-45ef-ae78-39a0087fc95b'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8593, '0724f652-10b8-4c86-8dd0-53285edc20d2'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (1514, 'd61b2641-9e09-4c4e-8e1e-72599ec17422'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: - monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
- robustly document our most-downloaded models with model cards that detail social impacts, biases, and intended and out-of-scope use cases,
- create audience-guiding tags, such as the “Not For All Audiences” tag that can be added to the repository’s card metadata to avoid un-requested violent and sexual content,
- promote use of [Open Responsible AI Licenses (RAIL)](https://huggingface.co/blog/open_rail) for [models](https://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license), such as with LLMs ([BLOOM](https://huggingface.co/spaces/bigscience/license), [BigCode](https://huggingface.co/spaces/bigcode/license)),
- conduct research that [analyzes](https://arxiv.org/abs/2302.04844) which models and datasets have the highest potential for, or track record of, misuse and malicious use.
As a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.

In addition, the Hugging Face Hub offers:

-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)
-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)
-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)
-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)
-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)


### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
 
Here is the question: How does Hugging Face ensure the ethical use of large models like BigBird, particularly when dealing with biases in datasets, and what role do logs play in this process?","Hugging Face ensures the ethical use of large models like BigBird by:

1. Monitoring community discussion boards to ensure users abide by the code of conduct.
2. Robustly documenting models with model cards that detail social impacts, biases, and intended and out-of-scope use cases.
3. Creating audience-guiding tags, such as the ""Not For All Audiences"" tag, to avoid un-requested violent and sexual content.
4. Promoting the use of Open Responsible AI Licenses"
"What steps does Hugging Face take to evaluate large models like BigBird for biases, and how are datasets like WinoBias managed during the evaluation process?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (6273, '7673ac47-ca59-4d63-b892-bd9c60e05d28'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (23714, 'a2d69303-a006-44f0-8008-1accb10a3697'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (23740, '0de3fbdc-faac-4328-a11d-8b3c147e4912'), (16059, '250a678f-6a31-410f-88f6-17ac5b9cbaf8'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (28356, 'b8b471a3-a61b-4ee5-9154-be89b6b87789'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (22752, '847c0672-9e44-445e-b14d-84817ad403ee'), (7905, 'be711455-ffac-4dbb-aa38-bcb889eaadd6'), (7898, 'a0553017-647f-4a6a-ba47-f2e453034fa7'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (7908, '400d298e-b436-4e5c-9dde-e839be4011f0'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7913, '18d40909-e83c-4c62-9cee-03e8a507f58f'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (5483, '7e8de001-fae0-409a-8c1a-49afe718b4d9'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (3439, '0af44d7a-c74c-4934-9280-08873b27666b'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (12179, 'dfe46110-781d-4dfe-8043-ea0fb93da98f'), (12181, 'ff5a3adb-a22c-4bbb-9749-782aceee4a70'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (5041, '812dcc27-ef00-4194-976d-6f9eb30583a9'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (22987, '62eaad04-a675-41d1-bee4-9bb620b06833'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (9694, 'd82a3bcb-3b3a-47f4-b946-77d14978e196'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25064, '099d4bbf-893a-4feb-a4ad-1dbf78a02324'), (1514, 'd61b2641-9e09-4c4e-8e1e-72599ec17422'), (15339, '9006a23a-10b3-4cd2-a0a0-5f95871a357d'), (15340, '77986b8e-a42a-4367-9d6c-93ddeb9a16b9'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (29690, 'bec5489a-802b-4c40-a81e-19c57753d246'), (24062, '17a005d1-952b-439c-bf0d-09520cded7bf')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: If you want to see the work in action, check out the [Jupyter notebook](https://colab.research.google.com/drive/1-HDJUcPMKEF-E7Hapih0OmA1xTW2hdAv#scrollTo=yX8ciyVWKiuO) we created!

The workflow has two main steps:
- Prompting the language model with a predefined set of prompts (hosted on [🤗 Datasets](https://huggingface.co/datasets))
- Evaluating the generations using a metric or measurement (using [🤗 Evaluate](https://huggingface.co/docs/evaluate/index))

Let's work through bias evaluation in 3 prompt-based tasks focused on harmful language: Toxicity, Polarity, and Hurtfulness. The work we introduce here serves to demonstrate how to utilize Hugging Face libraries for bias analyses, and does not depend on the specific prompt-based dataset used. Critically, remember that recently introduced datasets for evaluating biases are initial steps that do not capture the vast range of biases that models may produce (see the Discussion section below for more details).

## Toxicity
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
* Look at aggregated [measurements of your dataset](https://huggingface.co/spaces/huggingface/data-measurements-tool) including nPMI to surface possible stereotypical associations
* Models:
    * Make use of and contribute to [Model Cards](https://huggingface.co/docs/hub/model-cards) to share relevant insights on biases in models.
    * Use [Interactive Model Cards](https://huggingface.co/spaces/nazneen/interactive-model-cards) to visualize performance discrepancies
    * Look at [systematic model errors](https://huggingface.co/spaces/nazneen/seal) and look out for known social biases
    * Use [Evaluate](https://github.com/huggingface/evaluate) and [Evaluation on the Hub](https://huggingface.co/spaces/autoevaluate/model-evaluator) to explore [language model biases](https://huggingface.co/blog/evaluating-llm-bias) including in [large models](https://huggingface.co/blog/zero-shot-eval-on-the-hub)
<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_winobias.png"" alt=""Large model WinoBias scores computed with Evaluation on the Hub by Helen, Tristan, Abhishek, Lewis, and Douwe"" />
    <em><a href=""https://huggingface.co/spaces/sasha/BiasDetection""><a href=""https://huggingface.co/blog/zero-shot-eval-on-the-hub"">Large model WinoBias scores computed with Evaluation on the Hub</a> by <a href=""https://huggingface.co/mathemakitten"">Helen</a>, <a href=""https://huggingface.co/Tristan"">Tristan</a>, <a href=""https://huggingface.co/abhishek"">Abhishek</a>, <a href=""https://huggingface.co/lewtun"">Lewis</a>, and <a href=""https://huggingface.co/douwekiela"">Douwe</a></em>
</p>


#### Model selection/development: recommendations
 
Here is the question: What steps does Hugging Face take to evaluate large models like BigBird for biases, and how are datasets like WinoBias managed during the evaluation process?","According to the context, Hugging Face takes the following steps to evaluate large models like BigBird for biases:

1. Prompting the language model with a predefined set of prompts (hosted on 🤗 Datasets) to generate responses.
2. Evaluating the generations using a metric or measurement (using 🤗 Evaluate) to detect biases.

In the case of WinoBias, it is used as a dataset to compute bias scores for large models using Evaluation on the Hub. The evaluation"
"What role does Hugging Face’s platform play in managing large models and datasets like WinoBias, and how does zero-shot evaluation help ensure ethical deployment of models like BigBird?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (23153, '49b4c809-71ff-410a-b32d-acc1467103b4'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
leverage popular open source projects such as ray and kubeflow to deploy AI services adjacent to their private datasets, while working with Hugging Face to ensure that organizations maintain the flexibility to take advantage of the latest and greatest in open-source models. This is all without tradeoffs in total cost of ownership or performance.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
 
Here is the question: What role does Hugging Face’s platform play in managing large models and datasets like WinoBias, and how does zero-shot evaluation help ensure ethical deployment of models like BigBird?","Hugging Face's platform plays a crucial role in managing large models and datasets like WinoBias by providing a hub for thousands of models and datasets, enabling large-scale evaluation, and making tools for evaluation accessible to everyone. Zero-shot evaluation on the platform helps ensure ethical deployment of models like BigBird by allowing for consistent and reproducible evaluation methods, which can identify biases and inconsistencies in the models, thereby promoting responsible AI development and deployment."
"How does Hugging Face use logs during model training and evaluation, particularly for detecting biases in datasets like WinoBias, and how does this help in model deployment?","[(17920, '9c587d42-fc22-4562-91cb-4e195b4c2fb1'), (21506, '10e81bf1-e8b5-4532-acba-49c7f26175b9'), (30216, 'a3ec7661-7b16-423a-809a-1381d4541bd9'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23087, '78540d5c-200a-44a6-80b8-ac019e34baf9'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13874, '3aa09c5f-d782-4417-bf6d-9a16dc742ef1'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13893, '821034d7-81d2-4f43-ab41-86255b1a41a6'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4183, 'd33bb078-e05c-4e7b-b445-322a674ac06c'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (3183, '05adffa9-2646-489c-b3ad-2357cb108a8e'), (12913, '6b795525-cd89-4895-a8f6-d7b48571a2f3'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (30852, '140bd3bf-c87c-44f1-8231-5017fbe767e0'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (13974, 'ab6046ab-06b2-4059-bb4b-69d1dbceed8f'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17055, '74855ff6-c92e-434b-810c-2705d87c5aa2'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (171, '5cd2c4b7-930f-42e0-b91f-7c8d9b1efcac'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (17080, '12a29587-8112-45cb-b2c3-278597578f87'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (4285, '267cef4b-a36b-4d83-89dd-8f9a385949a8'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (13508, '91853fd6-7904-4ab6-949c-cd38b42bf46a'), (6854, '462959bd-83cb-4dab-9485-23b7022e034c'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (10966, '43bdf151-fe3f-4742-b435-208afb776cb2'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (31468, '70f96e3a-19fa-4963-8b25-b87a6a57758e'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (14585, 'edd5d46a-6a2f-488c-9d46-fb5e5ac1bdf0'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (21765, '6a1e63b7-b878-4676-9087-269c0b32fba1'), (16146, 'e100c4ec-db20-4e52-93d6-f9535f7f90b8'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (26919, '5ebb5465-6f4c-4e1c-8326-5f71516e66a4'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (13196, '5f4d0098-0f25-4cd7-9d59-4037a77c492b'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (29593, '6a3d74c2-ed43-42ed-8939-0c89a33f7be8'), (6042, '5b1abefc-94b9-4cfd-99ca-23843fd92410'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (6050, '3cb3a71c-89ac-4817-8a94-51f7aa7240cd'), (6053, 'b575f6f7-3134-49a8-8c9a-2a8136436019'), (13229, '3981f499-4b08-4449-ae2e-11d60161966f'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (31677, '693aade2-571b-4b47-946e-e1744a096cec'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (22477, '06267b14-5048-460b-9547-63335518d2da'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (31715, 'ed08f117-772f-417a-95b4-fcd4c94f4f30'), (21996, '1dbea983-ea39-49b3-a296-052b328d0fc9'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (15857, 'a948a556-2726-422c-ada1-d6f6cfeaeb9b'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).

Since the process also saves the matches' history, it is possible to see precisely the results of any given model. This can, for instance, allow you to check why your model struggles with another one, most notably using another demo Space to visualize matches like [this one](https://huggingface.co/spaces/unity/ML-Agents-SoccerTwos.).

For now, **this experiment is running with the MLAgent environment SoccerTwos for the Hugging Face Deep RL Course**, however, the process and implementation, in general, are very much **environment agnostic and could be used to evaluate for free a wide range of adversarial multi-agent settings**.
Hugging Face has always worked to make models accessible and easy to use. The `transformers` library makes it possible to load a model in a few lines of code. After a model is loaded, it can be used to make predictions on new data programmatically.  _But it’s not just programmers that are using machine learning models!_ An increasingly common scenario in machine learning is **demoing models to interdisciplinary teams** or letting **non-programmers use models** (to help discover biases, failure points, etc.). 

The **[Gradio library](https://gradio.app/)** lets machine learning developers create demos and GUIs from machine learning models very easily, and share them for free with your collaborators as easily as sharing a Google docs link. Now, we’re excited to share that the Gradio 2.0 library lets you **_load and use almost any Hugging Face model_ _with a GUI_** **_in just 1 line of code_**. Here’s an example:

![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
**Accelerate machine learning from science to production**

In addition to Hugging Face DLCs, we created a first-class Hugging Face extension for the SageMaker Python SDK to accelerate data science teams, reducing the time required to set up and run experiments from days to minutes.

You can use the Hugging Face DLCs with SageMaker's automatic model tuning to optimize your training hyperparameters and increase the accuracy of your models.

Deploy your trained models for inference with just one more line of code or select any of the 10,000+ publicly available models from the [model Hub](https://huggingface.co/models) and deploy them with SageMaker.

Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**
## What can you find on the Hugging Face Hub?

### Models 

The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.

### Datasets
The Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.

### Spaces

Hugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications.
 
Here is the question: How does Hugging Face use logs during model training and evaluation, particularly for detecting biases in datasets like WinoBias, and how does this help in model deployment?","The provided context does not explicitly mention how Hugging Face uses logs during model training and evaluation, particularly for detecting biases in datasets like WinoBias, and how this helps in model deployment. The context primarily focuses on the features and capabilities of Hugging Face, such as data persistence, model evaluation, and deployment, as well as its integration with other libraries and tools. It does not provide specific information on log usage for bias detection. Therefore, I cannot provide a comprehensive answer to this question based on"
"How does Hugging Face ensure that large models like BigBird are trained ethically, particularly in relation to dataset biases, and how does zero-shot evaluation help in this process?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (3594, '4e1600b8-d386-41a9-bc22-67a03574e9d3'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (19980, '3aa3ba7e-8013-492f-b83a-91c341becb4f'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11326, '24b1d1c2-4cfc-4f76-981a-e08b180448fc'), (11328, '228db1c0-a823-4e0e-b594-3594b9bd6517'), (22083, 'dd15b594-110b-4773-b128-0658f80e911d'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (29349, 'f9a62426-7040-4f1b-8771-11b8792ec32f'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7905, 'be711455-ffac-4dbb-aa38-bcb889eaadd6'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (3438, '6b08d05d-c4a6-4c31-8268-42efe7f14aa4'), (3439, '0af44d7a-c74c-4934-9280-08873b27666b'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (22398, '96e2d562-285e-4313-9c28-22c485856055'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (22423, '544f305f-7801-4d57-aa4b-82b92fad36a0'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (30685, 'a039eae8-64c2-468f-9dfd-126ca871a519'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25064, '099d4bbf-893a-4feb-a4ad-1dbf78a02324'), (19438, '1b351e34-f0ef-408a-b6b2-209c3c330c69'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
**Biases in training data:** Popular multimodal datasets such as [LAION-5B](https://laion.ai/blog/laion-5b/) for text-to-image, [MS-COCO](https://cocodataset.org/) for image captioning, and [VQA v2.0](https://paperswithcode.com/dataset/visual-question-answering-v2-0) for visual question answering, have been found to contain numerous biases and harmful associations ([Zhao et al 2017](https://aclanthology.org/D17-1323/), [Prabhu and Birhane, 2021](https://arxiv.org/abs/2110.01963), [Hirota et al, 2022](https://facctconference.org/static/pdfs_2022/facct22-3533184.pdf)), which can percolate into the models trained on these datasets. For example, initial results from the [Hugging Face Stable Bias project](https://huggingface.co/spaces/society-ethics/StableBias) show a lack of diversity in image generations, as well as a perpetuation of common stereotypes of cultures and identity groups. Comparing Dall-E 2 generations of CEOs (right) and managers (left), we can see that both are lacking
```

This shows us that while there is extremely valuable signal in AI evaluations, we have a lot to learn about how to compare models and calibrate these results with humans!

## Limitations and biases

Like many other language models, this alpha version of StarChat has strong to-be-addressed limitations, including a tendency to hallucinate facts and produce problematic content (especially when prompted to). 
In particular, the model hasn't been aligned to human preferences with techniques like RLHF or deployed with in-the-loop filtering of responses like ChatGPT. 
Models trained primarily on code data will also have a more skewed demographic bias commensurate with the demographics of the GitHub community, for more on this see the [StarCoder dataset](https://huggingface.co/datasets/bigcode/starcoderdata).
For more details on the model’s limitations in terms of factuality and biases, see the [model card](https://huggingface.co/HuggingFaceH4/starchat-alpha#bias-risks-and-limitations).
 
Here is the question: How does Hugging Face ensure that large models like BigBird are trained ethically, particularly in relation to dataset biases, and how does zero-shot evaluation help in this process?","Hugging Face ensures that large models like BigBird are trained ethically by promoting transparency and documentation through model cards, datasheets, and READMEs, which help identify and address biases in training data and models. They also acknowledge the limitations and biases of their models, such as demographic biases in code data and the potential to hallucinate facts and produce problematic content. Additionally, they are working on developing tools for large-scale evaluation, including zero-shot evaluation, which can help identify biases and limitations in models."
"How does Hugging Face handle the ethical evaluation of large models like BigBird, particularly in terms of biases in datasets like WinoBias, and how do logs contribute to addressing these issues?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13320, '8272c7ae-a9bb-4f00-90df-c471493cb8aa'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (1113, '5481da52-5b02-4d6f-a69d-67d49239fa4a'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (17003, '04cc67ea-1b8c-4ddb-82ec-f36431591ae9'), (8812, '99a22463-c724-477a-8a22-0965d1ff8629'), (21623, '32bea9e3-44f8-4464-a8dd-d2dcbc3a2102'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (8835, 'b0843180-2c3a-4c14-ae19-b0ee0ea96d53'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (29349, 'f9a62426-7040-4f1b-8771-11b8792ec32f'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24793, '6eeb4cd3-a092-447d-9528-a03119bf4a08'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (4365, '0e049057-3bcb-4ecb-afc0-901cd049382c'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (3439, '0af44d7a-c74c-4934-9280-08873b27666b'), (3447, '7a968bc4-b1e2-481b-91df-18182329a5e4'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (23466, '23f64c4f-e35a-45d8-adfc-44266623bd06'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (1514, 'd61b2641-9e09-4c4e-8e1e-72599ec17422'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: --
title: ""DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub"" 
thumbnail: /blog/assets/hub_duckdb/hub_duckdb.png
authors:
- user: stevhliu
- user: lhoestq
- user: severo
---

# DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub


The Hugging Face Hub is dedicated to providing open access to datasets for everyone and giving users the tools to explore and understand them. You can find many of the datasets used to train popular large language models (LLMs) like [Falcon](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [MPT](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf), and [StarCoder](https://huggingface.co/datasets/bigcode/the-stack). There are tools for addressing fairness and bias in datasets like [Disaggregators](https://huggingface.co/spaces/society-ethics/disaggregators), and tools for previewing examples inside a dataset like the Dataset Viewer.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_winobias.png"" alt=""Large model WinoBias scores computed with Evaluation on the Hub by Helen, Tristan, Abhishek, Lewis, and Douwe"" />
    <em><a href=""https://huggingface.co/spaces/sasha/BiasDetection""><a href=""https://huggingface.co/blog/zero-shot-eval-on-the-hub"">Large model WinoBias scores computed with Evaluation on the Hub</a> by <a href=""https://huggingface.co/mathemakitten"">Helen</a>, <a href=""https://huggingface.co/Tristan"">Tristan</a>, <a href=""https://huggingface.co/abhishek"">Abhishek</a>, <a href=""https://huggingface.co/lewtun"">Lewis</a>, and <a href=""https://huggingface.co/douwekiela"">Douwe</a></em>
</p>


#### Model selection/development: recommendations
* Look at aggregated [measurements of your dataset](https://huggingface.co/spaces/huggingface/data-measurements-tool) including nPMI to surface possible stereotypical associations
* Models:
    * Make use of and contribute to [Model Cards](https://huggingface.co/docs/hub/model-cards) to share relevant insights on biases in models.
    * Use [Interactive Model Cards](https://huggingface.co/spaces/nazneen/interactive-model-cards) to visualize performance discrepancies
    * Look at [systematic model errors](https://huggingface.co/spaces/nazneen/seal) and look out for known social biases
    * Use [Evaluate](https://github.com/huggingface/evaluate) and [Evaluation on the Hub](https://huggingface.co/spaces/autoevaluate/model-evaluator) to explore [language model biases](https://huggingface.co/blog/evaluating-llm-bias) including in [large models](https://huggingface.co/blog/zero-shot-eval-on-the-hub)
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
 
Here is the question: How does Hugging Face handle the ethical evaluation of large models like BigBird, particularly in terms of biases in datasets like WinoBias, and how do logs contribute to addressing these issues?","Hugging Face handles the ethical evaluation of large models like BigBird by providing tools and infrastructure for large-scale evaluation, model selection, and development. Specifically, they recommend:

1. Looking at aggregated measurements of datasets, including nPMI, to surface possible stereotypical associations.
2. Using Model Cards to share insights on biases in models and contributing to them.
3. Utilizing Interactive Model Cards to visualize performance discrepancies.
4. Examining systematic model errors and looking out for known social biases.
5"
"How does Hugging Face evaluate large models like BigBird for biases, particularly when using zero-shot tasks, and how do logs assist in identifying these biases during deployment?","[(17920, '9c587d42-fc22-4562-91cb-4e195b4c2fb1'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4179, 'b6031fcd-da9e-4a41-ac79-36bf3eaa3bd0'), (1113, '5481da52-5b02-4d6f-a69d-67d49239fa4a'), (1118, '5dd59d54-8203-423f-a71a-a3af1fffe5e4'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (22129, '6e12827f-d0c2-45d5-93c5-c456a0f17de3'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (30852, '140bd3bf-c87c-44f1-8231-5017fbe767e0'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (171, '5cd2c4b7-930f-42e0-b91f-7c8d9b1efcac'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (22194, '7ba04e95-3e33-405e-b3ed-2bb503729d69'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (4285, '267cef4b-a36b-4d83-89dd-8f9a385949a8'), (16059, '250a678f-6a31-410f-88f6-17ac5b9cbaf8'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (30933, 'c4234c13-1e37-460c-8bf4-f1507dbbdf70'), (10966, '43bdf151-fe3f-4742-b435-208afb776cb2'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (7905, 'be711455-ffac-4dbb-aa38-bcb889eaadd6'), (17122, '9f5d5e67-e2c9-4b8a-96c7-47699db428d3'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (30951, 'a57b2572-c7e0-40dd-ba85-61f537b777b7'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (14056, 'e4acfaa9-c866-4fa3-b263-ebd8b7794cf3'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28913, '8ebcd930-9f33-4341-9b0b-5a6fffb52797'), (30965, '6adffa38-ab02-41e9-a179-aaca1c2624e8'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21768, '61ac4e62-e904-4c53-9623-9416d918cb0e'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (16174, '7aeaf6a6-cf85-46b0-af0b-f0ab92bc6103'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (24899, 'd5261e5b-d3f0-43d7-aac1-57db8255c358'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (31574, '09044f40-6e31-4206-bb91-b0bbf29725e0'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (29038, 'b080a738-c248-4ce7-be8c-bf22230a2d37'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (29593, '6a3d74c2-ed43-42ed-8939-0c89a33f7be8'), (6042, '5b1abefc-94b9-4cfd-99ca-23843fd92410'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31681, 'cd70c10d-6d2b-404f-842b-5ea26dc18134'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (30685, 'a039eae8-64c2-468f-9dfd-126ca871a519'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31715, 'ed08f117-772f-417a-95b4-fcd4c94f4f30'), (25064, '099d4bbf-893a-4feb-a4ad-1dbf78a02324'), (15340, '77986b8e-a42a-4367-9d6c-93ddeb9a16b9'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (17914, 'e3f62a9f-6c82-48c3-84c5-d5b2d2cdf56b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: * Look at aggregated [measurements of your dataset](https://huggingface.co/spaces/huggingface/data-measurements-tool) including nPMI to surface possible stereotypical associations
* Models:
    * Make use of and contribute to [Model Cards](https://huggingface.co/docs/hub/model-cards) to share relevant insights on biases in models.
    * Use [Interactive Model Cards](https://huggingface.co/spaces/nazneen/interactive-model-cards) to visualize performance discrepancies
    * Look at [systematic model errors](https://huggingface.co/spaces/nazneen/seal) and look out for known social biases
    * Use [Evaluate](https://github.com/huggingface/evaluate) and [Evaluation on the Hub](https://huggingface.co/spaces/autoevaluate/model-evaluator) to explore [language model biases](https://huggingface.co/blog/evaluating-llm-bias) including in [large models](https://huggingface.co/blog/zero-shot-eval-on-the-hub)
If you want to see the work in action, check out the [Jupyter notebook](https://colab.research.google.com/drive/1-HDJUcPMKEF-E7Hapih0OmA1xTW2hdAv#scrollTo=yX8ciyVWKiuO) we created!

The workflow has two main steps:
- Prompting the language model with a predefined set of prompts (hosted on [🤗 Datasets](https://huggingface.co/datasets))
- Evaluating the generations using a metric or measurement (using [🤗 Evaluate](https://huggingface.co/docs/evaluate/index))

Let's work through bias evaluation in 3 prompt-based tasks focused on harmful language: Toxicity, Polarity, and Hurtfulness. The work we introduce here serves to demonstrate how to utilize Hugging Face libraries for bias analyses, and does not depend on the specific prompt-based dataset used. Critically, remember that recently introduced datasets for evaluating biases are initial steps that do not capture the vast range of biases that models may produce (see the Discussion section below for more details).

## Toxicity
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
--
title: ""Very Large Language Models and How to Evaluate Them"" 
thumbnail: /blog/assets/106_zero_shot_eval_on_the_hub/thumbnail.png
authors:
- user: mathemakitten
- user: Tristan
- user: abhishek
- user: lewtun
- user: douwekiela
---

# Very Large Language Models and How to Evaluate Them 


Large language models can now be evaluated on zero-shot classification tasks with [Evaluation on the Hub](https://huggingface.co/spaces/autoevaluate/model-evaluator)! 

Zero-shot evaluation is a popular way for researchers to measure the performance of large language models, as they have been [shown](https://arxiv.org/abs/2005.14165) to learn capabilities during training without explicitly being shown labeled examples. The [Inverse Scaling Prize](https://github.com/inverse-scaling/prize) is an example of a recent community effort to conduct large-scale zero-shot evaluation across model sizes and families to discover tasks on which larger models may perform worse than their smaller counterparts.
 
Here is the question: How does Hugging Face evaluate large models like BigBird for biases, particularly when using zero-shot tasks, and how do logs assist in identifying these biases during deployment?","Hugging Face evaluates large models like BigBird for biases using various tools and methods, including:

1. Aggregated measurements of the dataset, such as nPMI, to surface possible stereotypical associations.
2. Model Cards to share relevant insights on biases in models.
3. Interactive Model Cards to visualize performance discrepancies.
4. Systematic model errors and known social biases.
5. Evaluate and Evaluation on the Hub to explore language model biases, including in large models.
6. Prompting the language"
"What is the process for integrating datasets like WinoBias into the Hugging Face Hub, and how does zero-shot evaluation help detect biases in models trained on these datasets?","[(522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (9229, '3fc24bea-73cc-4368-8756-dd115fb5bff3'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (16932, 'd165fea3-524c-4a1f-8627-923c16bdcb4f'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (22079, '16104f25-f547-44dd-9b5f-3798c428cc44'), (24127, '474b8446-9f4d-4ec3-9b0e-78695268902d'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (10843, '9b0e65eb-f487-4e2f-816f-c5ac6c1f034a'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (6270, 'c18172bc-ee0e-4969-8110-192dd5331ee7'), (6271, 'db13ad85-6738-4e86-bea9-8e92682af368'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (6274, '8d7420e4-d50c-4af6-addb-8e2353cffe7e'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (4741, '210088c4-7fef-45fb-b3bd-ba5759a63329'), (24727, '710d9250-ca94-45f1-a999-cbc01b7e8ff7'), (153, 'a5e94b25-4bd0-4edc-be2a-4167144129cb'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (22704, 'd853fcd0-1387-4353-a361-a5295f36b273'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (24793, '6eeb4cd3-a092-447d-9528-a03119bf4a08'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (7905, 'be711455-ffac-4dbb-aa38-bcb889eaadd6'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (24804, '23cde3d9-30d2-4fd8-84f5-8504a15e3a42'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (8940, '6ca577b1-be21-4e9f-a749-bd03b14c2205'), (21230, 'cd52268c-23fc-439d-affa-c5c374ffb57a'), (22261, '05784f0a-ebd1-4835-a9eb-d09c01072914'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (7988, 'adcc6191-aa19-4a9a-8c82-79132357a034'), (15669, '0256580b-34ce-4e4a-88aa-42403eb7a050'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (16702, 'cf05cd6b-e33d-4bc7-a11e-5058f0d0676b'), (4937, '24ec3f32-f79f-4d91-a838-361fdd56703f'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (25440, 'f344d73e-1edb-43a3-8bab-c235c533086c'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8580, '14576ef8-e3f0-4ac9-bcdc-c523b7589ded'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (8586, '90a0534c-f6eb-45e6-8c1b-b6c343f84cf8'), (8587, '5dfac67b-30fa-4e8d-b899-f2fb32fac46c'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8595, 'd6792ca7-c71d-43dd-8f05-d316ad1f0a60'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (30685, 'a039eae8-64c2-468f-9dfd-126ca871a519'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (15857, 'a948a556-2726-422c-ada1-d6f6cfeaeb9b'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (13814, 'd8be2885-e7f0-4d64-973b-c7c6e554c60d'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Case study: Zero-shot evaluation on the WinoBias task

The [WinoBias](https://github.com/uclanlp/corefBias) dataset has been formatted as a zero-shot task where classification options are the completions. Each completion differs by the pronoun, and the target corresponds to the anti-stereotypical completion for the occupation (e.g. ""developer"" is stereotypically a male-dominated occupation, so ""she"" would be the anti-stereotypical pronoun). See [here](https://huggingface.co/datasets/mathemakitten/winobias_antistereotype_test) for an example:

![dataset](assets/106_zero_shot_eval_on_the_hub/dataset.png)

Next, we can select this newly-uploaded dataset in the Evaluation on the Hub interface using the `text_zero_shot_classification` task, select the models we’d like to evaluate, and submit our evaluation jobs! When the job has been completed, you’ll be notified by email that the autoevaluator bot has opened a new pull request with the results on the model’s Hub repository.
The Hugging Face Hub has [hundreds of object detection models](https://huggingface.co/models?pipeline_tag=object-detection) pre-trained in different datasets, able to identify and localize various object classes. 

One specific type of object detection models, called _zero-shot_, can receive additional text queries to search for target objects described in the text. These models can detect objects they haven't seen during training, instead of being constrained to the set of classes used during training.

The diversity of detectors goes beyond the range of output classes they can recognize. They vary in terms of underlying architectures, model sizes, processing speeds, and prediction accuracy.

A popular metric used to evaluate the accuracy of predictions made by an object detection model is the **Average Precision (AP)** and its variants, which will be explained later in this blog.
![dataset](assets/106_zero_shot_eval_on_the_hub/zeroshot.jpg)

## Enabling zero-shot evaluation of language models on the Hub

[Evaluation on the Hub](https://huggingface.co/blog/eval-on-the-hub) helps you evaluate any model on the Hub without writing code, and is powered by [AutoTrain](https://huggingface.co/autotrain). Now, any causal language model on the Hub can be evaluated in a zero-shot fashion. Zero-shot evaluation measures the likelihood of a trained model producing a given set of tokens and does not require any labelled training data, which allows researchers to skip expensive labelling efforts.
Evaluation on the Hub is a low-code tool which makes it simple to compare the zero-shot performance of a set of models along an axis such as FLOPS or model size, and to compare the performance of a set of models trained on a specific corpora against a different set of models. The zero-shot text classification task is extremely flexible—any dataset that can be permuted into a Winograd schema where examples to be compared only differ by a few words can be used with this task and evaluated on many models at once. Our goal is to make it simple to upload a new dataset for evaluation and enable researchers to easily benchmark many models on it.
Here is a list of things we’ll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#🤗-🤝-timm)
- [Diffusers](#🧨-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time 👁

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
 
Here is the question: What is the process for integrating datasets like WinoBias into the Hugging Face Hub, and how does zero-shot evaluation help detect biases in models trained on these datasets?","The process for integrating datasets like WinoBias into the Hugging Face Hub involves formatting the dataset as a zero-shot task where classification options are the completions, and each completion differs by the pronoun, with the target corresponding to the anti-stereotypical completion for the occupation. Once the dataset is uploaded, it can be selected in the Evaluation on the Hub interface using the `text_zero_shot_classification` task, and models can be evaluated on it without writing code. Zero-shot evaluation helps detect biases"
"How does Hugging Face ensure that large models like BigBird are ethically trained, particularly in relation to dataset biases, and how does zero-shot evaluation help in this process?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (3594, '4e1600b8-d386-41a9-bc22-67a03574e9d3'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (19980, '3aa3ba7e-8013-492f-b83a-91c341becb4f'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11326, '24b1d1c2-4cfc-4f76-981a-e08b180448fc'), (11328, '228db1c0-a823-4e0e-b594-3594b9bd6517'), (22083, 'dd15b594-110b-4773-b128-0658f80e911d'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (30883, '0939a61c-1a5d-494b-a554-2e99c7c43972'), (29349, 'f9a62426-7040-4f1b-8771-11b8792ec32f'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (30904, '77881139-e753-4c77-a942-6faa00e45dc4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7905, 'be711455-ffac-4dbb-aa38-bcb889eaadd6'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (1279, '1cbb5d0c-309e-43bd-824b-3a644a9225e1'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (3438, '6b08d05d-c4a6-4c31-8268-42efe7f14aa4'), (3439, '0af44d7a-c74c-4934-9280-08873b27666b'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (22398, '96e2d562-285e-4313-9c28-22c485856055'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (22423, '544f305f-7801-4d57-aa4b-82b92fad36a0'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (30685, 'a039eae8-64c2-468f-9dfd-126ca871a519'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25064, '099d4bbf-893a-4feb-a4ad-1dbf78a02324'), (19438, '1b351e34-f0ef-408a-b6b2-209c3c330c69'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Send us feedback!

At Hugging Face, we’re excited to continue democratizing access to state-of-the-art machine learning models, and that includes developing tools to make it easy for everyone to evaluate and probe their behavior. We’ve previously [written](https://huggingface.co/blog/eval-on-the-hub) about how important it is to standardize model evaluation methods to be consistent and reproducible, and to make tools for evaluation accessible to everyone. Future plans for Evaluation on the Hub include supporting zero-shot evaluation for language tasks which might not lend themselves to the format of concatenating completions to prompts, and adding support for even larger models.
### How you see the world of model evaluation evolving?

**Lewis:** That's a great question. So at Hugging Face, one of the things I've been working on has been trying to build the infrastructure and the tooling that enables what we call 'large-scale evaluation'. So you may know that the [Hugging Face Hub](https://huggingface.co/models) has thousands of models and datasets. But if you're trying to navigate this space you might ask yourself, 'I'm interested in question answering and want to know what the top 10 models on this particular task are'.

And at the moment, it's hard to find the answer to that, not just on the Hub, but in general in the space of machine learning this is quite hard. You often have to read papers and then you have to take those models and test them yourself manually and that's very slow and inefficient.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
- monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
- robustly document our most-downloaded models with model cards that detail social impacts, biases, and intended and out-of-scope use cases,
- create audience-guiding tags, such as the “Not For All Audiences” tag that can be added to the repository’s card metadata to avoid un-requested violent and sexual content,
- promote use of [Open Responsible AI Licenses (RAIL)](https://huggingface.co/blog/open_rail) for [models](https://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license), such as with LLMs ([BLOOM](https://huggingface.co/spaces/bigscience/license), [BigCode](https://huggingface.co/spaces/bigcode/license)),
- conduct research that [analyzes](https://arxiv.org/abs/2302.04844) which models and datasets have the highest potential for, or track record of, misuse and malicious use.
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
 
Here is the question: How does Hugging Face ensure that large models like BigBird are ethically trained, particularly in relation to dataset biases, and how does zero-shot evaluation help in this process?","Hugging Face ensures that large models like BigBird are ethically trained by promoting responsible AI practices, such as robustly documenting models with model cards that detail social impacts, biases, and intended and out-of-scope use cases. They also conduct research to analyze which models and datasets have the highest potential for, or track record of, misuse and malicious use. Additionally, they encourage the use of Open Responsible AI Licenses (RAIL) for models, which helps to mitigate potential biases and misuse.

"
"What role do legal frameworks play when handling biased datasets like WinoBias, and how do zero-shot evaluation and logs assist in mitigating these biases during model training and deployment?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13314, '2e33b999-becf-4891-af20-291e49782f33'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (13316, '3a0f93a2-ae6d-487d-a9a7-9469be424875'), (28673, '5d042da5-0429-46b6-bc0e-f68cfd69fe52'), (28674, '0ec75ceb-631a-4eba-8edc-087ef8311726'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13320, '8272c7ae-a9bb-4f00-90df-c471493cb8aa'), (13321, '10328f23-c6d0-4eca-a172-090f97ccf6a3'), (13322, 'b0a48bd2-b0c8-4a35-a6a6-918c274e6820'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (13324, 'e927c83d-2a0f-4409-a00f-c0599d0ce5aa'), (13325, '16d0f9ea-2690-41bb-b5b7-ea15fa1b01e6'), (25101, 'b5c8f3d2-e8e0-4784-a8c8-4617296394b7'), (13327, '57a734d8-5efb-4dac-8531-349471d3785b'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (13329, '80576ab8-d6ee-4387-8099-daa9aa71aa86'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (13326, 'b26c0ba4-0f18-44e6-989d-8af8b33d8a92'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (13336, '04cce2c0-470a-4c18-a15b-beb81e0299d3'), (13337, '7db5f38f-c77f-448d-9980-4eb28f1522bb'), (13339, 'f046b9a1-daad-440b-8fa6-1a8de946d278'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (13344, '8351ae83-b363-4005-b8b5-26db3adae749'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (13349, '6b2b3b26-e943-43aa-bba4-fdb96ec4ab02'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (13355, '156a7c84-39c8-410e-b1d4-8980cfee0a96'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (13358, '5a6f6b09-7943-45ee-adf1-41511b760b13'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (13365, 'bdd93c38-0a43-4037-8b92-5e85cb900937'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11324, 'ee709d79-fa6d-4dce-a3f0-9b943c904977'), (11325, '19b5b1a7-693e-4a07-a957-b3a98a0d2bea'), (11328, '228db1c0-a823-4e0e-b594-3594b9bd6517'), (11331, 'ee6c23d0-eb20-4401-bdfb-e49c7aaaa083'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (11338, '30d2c084-1101-4560-b412-45d9eae8ef69'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (11341, 'a7a24881-ef8b-4b8d-89e4-aab048edafa8'), (29265, '47595a18-5b82-4980-91ef-b2cc34ee15b8'), (29266, 'd9bdf78a-41a0-436b-869f-16dc037925ab'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (17018, '4eb5a86b-2fd2-48d3-b487-d328172d0086'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (6270, 'c18172bc-ee0e-4969-8110-192dd5331ee7'), (6271, 'db13ad85-6738-4e86-bea9-8e92682af368'), (6272, 'e07f417c-d3c6-427e-a0fc-319f3ec494c2'), (6273, '7673ac47-ca59-4d63-b892-bd9c60e05d28'), (6274, '8d7420e4-d50c-4af6-addb-8e2353cffe7e'), (9859, '3e6fc260-35b7-46ab-af85-5de4ca79b906'), (9398, 'af042cb9-2f8e-427e-9795-3d7b3b13537e'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (24785, '56e17685-7c91-4198-b869-d9bc1a18f91a'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (12022, 'ca590bdd-6993-45bf-bc5c-6216ff9e670d'), (10999, '773e67b3-120d-4606-8cb8-6270fa3f5b46'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (5915, 'ab355c6a-ef67-406f-9a8d-28ce78fee6d3'), (14128, 'cd93071b-e8e2-415e-bc24-f63e159facf7'), (14130, 'e6a2196a-65fc-43e0-8e68-f74fefcc4f58'), (11575, '28589867-4683-4d14-8fbc-3442efb0da38'), (24899, 'd5261e5b-d3f0-43d7-aac1-57db8255c358'), (21831, '42729a1e-e5c7-400b-86f3-8e2c67aaf58e'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (4444, '309a3203-b151-498e-b083-5af2cd6c30ce'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (23390, 'a0d6b001-969e-45ef-9770-61309e5be4e1'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (3424, 'f51db97e-d242-427c-ad67-c5ce0cb650ff'), (23391, 'c87286ee-04fd-4d7c-8bc2-78514a04c836'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (3438, '6b08d05d-c4a6-4c31-8268-42efe7f14aa4'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (381, '958a0249-2085-46b0-bcac-240fa9d7d1e3'), (23425, 'bae40a82-a442-41f3-a45b-e024f39cc6ca'), (2958, '9bed9944-c5a7-4000-b493-f1c9ee171d93'), (27033, '6fd8c075-fe58-4672-84f9-9ea3ca16fb7f'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (8623, '98afab6d-bc0c-4f3e-a8dd-77ecc8f705f6'), (8624, '339598bc-e183-4af9-9283-072e1969ff78'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (7109, '53a7c288-2301-40ab-9650-8970284b10e6'), (21454, '2cd24eee-92da-4286-ba0c-2c6d71d3ed27'), (28637, '9d71defc-407a-49ac-ad0f-a76eb6a9c647'), (4575, '5b663118-3407-4a6a-8f1f-8e73ded1dded'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (15339, '9006a23a-10b3-4cd2-a0a0-5f95871a357d'), (15340, '77986b8e-a42a-4367-9d6c-93ddeb9a16b9'), (13810, '428b6475-43b1-4fb0-a114-3fbad2b5dc7b'), (5622, 'ccda777f-15b6-4a2a-9ba6-f00ffacce0d2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The zero-shot text classification task takes in a dataset containing a set of prompts and possible completions. Under the hood, the completions are concatenated with the prompt and the log-probabilities for each token are summed, then normalized and compared with the correct completion to report accuracy of the task.

In this blog post, we’ll use the zero-shot text classification task to evaluate various [OPT](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) models on [WinoBias](https://uclanlp.github.io/corefBias/overview), a coreference task measuring gender bias related to occupations. WinoBias measures whether a model is more likely to pick a stereotypical pronoun to fill in a sentence mentioning an occupation, and observe that the results suggest an [inverse scaling](https://github.com/inverse-scaling/prize) trend with respect to model size.

## Case study: Zero-shot evaluation on the WinoBias task
## Case study: Zero-shot evaluation on the WinoBias task

The [WinoBias](https://github.com/uclanlp/corefBias) dataset has been formatted as a zero-shot task where classification options are the completions. Each completion differs by the pronoun, and the target corresponds to the anti-stereotypical completion for the occupation (e.g. ""developer"" is stereotypically a male-dominated occupation, so ""she"" would be the anti-stereotypical pronoun). See [here](https://huggingface.co/datasets/mathemakitten/winobias_antistereotype_test) for an example:

![dataset](assets/106_zero_shot_eval_on_the_hub/dataset.png)

Next, we can select this newly-uploaded dataset in the Evaluation on the Hub interface using the `text_zero_shot_classification` task, select the models we’d like to evaluate, and submit our evaluation jobs! When the job has been completed, you’ll be notified by email that the autoevaluator bot has opened a new pull request with the results on the model’s Hub repository.
<Question
	choices={[
		{
			text: ""It provides information about the intended use and supported tasks of the dataset so others in the community can make an informed decision about using it."",
			explain: ""Correct! Undocumented datasets may be used to train models that may not reflect the intentions of the dataset creators, or may produce models whose legal status is murky if they're trained on data that violates privacy or licensing restrictions. This isn't the only benefit, though!"",
			correct : true
		},
		{
			text: ""It helps draw attention to the biases that are present in a corpus."",
			explain: ""Correct! Almost all datasets have some form of bias, which can produce negative consequences downstream. Being aware of them helps model builders understand how to address the inherent biases. What else do dataset cards help with?"",
			correct : true
		},
		{
			text: ""It improves the chances that others in the community will use my dataset."",
#### Model selection/development: recommendations

For models just as for datasets, different tools for documentation and evaluation will provide different views of bias risks in a model which all have a part to play in helping developers choose, develop, or understand ML systems.

* Visualize
    * Generative model: visualize how the model’s outputs may reflect stereotypes
    * Classification model: visualize model errors to identify failure modes that could lead to disparate performance
* Evaluate
    * When possible, evaluate models on relevant benchmarks
* Document
    * Share your learnings from visualization and qualitative evaluation
    * Report your model’s disaggregated performance and results on applicable fairness benchmarks


## Conclusion and Overview of Bias Analysis and Documentation Tools from 🤗
If you want to see the work in action, check out the [Jupyter notebook](https://colab.research.google.com/drive/1-HDJUcPMKEF-E7Hapih0OmA1xTW2hdAv#scrollTo=yX8ciyVWKiuO) we created!

The workflow has two main steps:
- Prompting the language model with a predefined set of prompts (hosted on [🤗 Datasets](https://huggingface.co/datasets))
- Evaluating the generations using a metric or measurement (using [🤗 Evaluate](https://huggingface.co/docs/evaluate/index))

Let's work through bias evaluation in 3 prompt-based tasks focused on harmful language: Toxicity, Polarity, and Hurtfulness. The work we introduce here serves to demonstrate how to utilize Hugging Face libraries for bias analyses, and does not depend on the specific prompt-based dataset used. Critically, remember that recently introduced datasets for evaluating biases are initial steps that do not capture the vast range of biases that models may produce (see the Discussion section below for more details).

## Toxicity
 
Here is the question: What role do legal frameworks play when handling biased datasets like WinoBias, and how do zero-shot evaluation and logs assist in mitigating these biases during model training and deployment?","The question is not directly answered in the provided context. The context discusses the zero-shot text classification task, the WinoBias dataset, and the evaluation of OPT models on this dataset. It also provides information on dataset cards, model selection, and bias analysis and documentation tools. However, it does not explicitly discuss the role of legal frameworks in handling biased datasets like WinoBias, or how zero-shot evaluation and logs assist in mitigating these biases during model training and deployment. Therefore, I cannot provide"
"How does Hugging Face handle bias detection in models like BigBird, and how do legal considerations and logs ensure that datasets used for training, like WinoBias, are ethically managed?","[(13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (13319, 'ae6f3680-61f3-442f-8102-c963f8a01f1f'), (13320, '8272c7ae-a9bb-4f00-90df-c471493cb8aa'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (28683, 'aa6139c6-fe9a-4f9c-b3b6-ef9b6688cc0b'), (13324, 'e927c83d-2a0f-4409-a00f-c0599d0ce5aa'), (13325, '16d0f9ea-2690-41bb-b5b7-ea15fa1b01e6'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (13341, '7d2c3ea7-beca-4e97-99f3-01ee51aad8a8'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13345, '27c2f6b8-b760-44ba-b3d6-b8c4982637d9'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13359, '6caf8e23-63aa-413c-b673-dc039aa9a2ec'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (13368, 'a5056c43-643e-4c93-8857-0ec6d023087b'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (11322, '871c4c7b-6da4-420b-bb06-a59dbd9e18b3'), (11326, '24b1d1c2-4cfc-4f76-981a-e08b180448fc'), (11328, '228db1c0-a823-4e0e-b594-3594b9bd6517'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (11341, 'a7a24881-ef8b-4b8d-89e4-aab048edafa8'), (1113, '5481da52-5b02-4d6f-a69d-67d49239fa4a'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (19043, '4932d585-53ec-4b0b-abaf-beb63614b0aa'), (21623, '32bea9e3-44f8-4464-a8dd-d2dcbc3a2102'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (4365, '0e049057-3bcb-4ecb-afc0-901cd049382c'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (1359, '1ee49ddb-9952-4cfc-b4ed-2acfce41628f'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (5486, '2d7fe829-5136-4e91-990a-72b69d7b5a2d'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (5487, 'e1a11602-a6aa-4600-b152-18a272a1212b'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (8593, '0724f652-10b8-4c86-8dd0-53285edc20d2'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (31714, '77fb7510-e224-4453-b854-845455318fe6'), (31715, 'ed08f117-772f-417a-95b4-fcd4c94f4f30'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: **Biases in training data:** Popular multimodal datasets such as [LAION-5B](https://laion.ai/blog/laion-5b/) for text-to-image, [MS-COCO](https://cocodataset.org/) for image captioning, and [VQA v2.0](https://paperswithcode.com/dataset/visual-question-answering-v2-0) for visual question answering, have been found to contain numerous biases and harmful associations ([Zhao et al 2017](https://aclanthology.org/D17-1323/), [Prabhu and Birhane, 2021](https://arxiv.org/abs/2110.01963), [Hirota et al, 2022](https://facctconference.org/static/pdfs_2022/facct22-3533184.pdf)), which can percolate into the models trained on these datasets. For example, initial results from the [Hugging Face Stable Bias project](https://huggingface.co/spaces/society-ethics/StableBias) show a lack of diversity in image generations, as well as a perpetuation of common stereotypes of cultures and identity groups. Comparing Dall-E 2 generations of CEOs (right) and managers (left), we can see that both are lacking
In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.

In the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.

## Compliance as a Core Principle

As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.
--
title: ""DuckDB: analyze 50,000+ datasets stored on the Hugging Face Hub"" 
thumbnail: /blog/assets/hub_duckdb/hub_duckdb.png
authors:
- user: stevhliu
- user: lhoestq
- user: severo
---

# DuckDB: run SQL queries on 50,000+ datasets on the Hugging Face Hub


The Hugging Face Hub is dedicated to providing open access to datasets for everyone and giving users the tools to explore and understand them. You can find many of the datasets used to train popular large language models (LLMs) like [Falcon](https://huggingface.co/datasets/tiiuae/falcon-refinedweb), [Dolly](https://huggingface.co/datasets/databricks/databricks-dolly-15k), [MPT](https://huggingface.co/datasets/mosaicml/dolly_hhrlhf), and [StarCoder](https://huggingface.co/datasets/bigcode/the-stack). There are tools for addressing fairness and bias in datasets like [Disaggregators](https://huggingface.co/spaces/society-ethics/disaggregators), and tools for previewing examples inside a dataset like the Dataset Viewer.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
As a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.

In addition, the Hugging Face Hub offers:

-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)
-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)
-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)
-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)
-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)


### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.
 
Here is the question: How does Hugging Face handle bias detection in models like BigBird, and how do legal considerations and logs ensure that datasets used for training, like WinoBias, are ethically managed?","Hugging Face handles bias detection in models like BigBird through various methods, including:

1. **Evaluating and documenting bias**: Hugging Face recommends using model cards, datasheets, and READMEs to document bias in models. This involves sharing the results of explorations made using explorer tools and red-teaming efforts alongside model checkpoints and weights.
2. **Using tools for addressing fairness and bias**: Hugging Face provides tools like Disaggregators on the Hub to address fairness and bias in"
You are tasked to create a GUI application for a machine learning model that generates text predictions. Which combination of tools and techniques would allow you to achieve this most efficiently?,"[(7, 'bb62d85a-4224-492b-9ffe-70d571ed09ab'), (13833, '2c93d378-a8ac-4ebd-8f75-b5d44ac01949'), (19978, '713bcdb4-9de3-482f-8ec1-e46f64d20252'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (23066, 'db5b29f6-c011-4064-83aa-0677c241c9dd'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (23068, '17b19121-a9f3-446f-b249-3b1e64bae14f'), (16416, '1b6290e2-d918-4435-af5a-004a7c4401ca'), (20001, '0a107aee-98c6-468e-907d-a528753d06ff'), (18994, '4cbd542c-a475-47f8-959c-ced12502b7e0'), (17984, '27f313d0-7f48-4c78-84bd-a9b0abd0a509'), (3652, '3bc7ae81-3d64-4da8-a70e-601d9777a9c8'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (18514, '2b8c705b-aa16-4343-9147-042e810a70a2'), (18515, 'c374498f-b93d-4f34-984c-ca24c4e34045'), (26713, '0d242fd9-d15a-41f6-8731-00ea12bc9cd9'), (26717, '6640e95b-cbb6-447e-9e49-a05ca7a33070'), (7266, 'f5c4435e-d192-4727-9e6d-35acf4404166'), (18531, '519a8770-fde0-4d7f-ad9d-cbef5288d01f'), (3185, '8cacb60e-754c-4138-8848-a0779c50f5aa'), (13428, 'fde7a564-c668-4271-96cd-55c39434953d'), (21110, '3b4b59de-4da6-4c2a-89c8-597f807f0538'), (16504, '0d56f7e1-6c25-41dd-ba5f-6426af974fad'), (29818, '02cc33d8-0c79-466e-ba0c-bdadb265c65f'), (13436, 'e2b774e6-5237-4e9f-8716-229315e0fcaf'), (7827, '2b137736-ff22-4a82-a789-80f640d114a9'), (15001, '5e52bb45-1656-4310-8d72-06c79196c8f4'), (18074, '85e06b79-6141-47b5-997e-e11f4bd149d5'), (25257, 'ef9a46af-a650-4ab1-a69e-65e287089adf'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (13507, '52df2f8a-a48c-491d-9d14-1c0ca324934d'), (203, 'd52d0498-2e8d-43aa-93ae-0103097a80e2'), (10961, 'af1d1a7c-9db0-4b22-9f43-c27d9b7ed477'), (28890, 'b1ae8ca7-d69b-4326-a81e-655300398e52'), (14055, '20614178-3fe6-45b2-a00d-f1009004877c'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (24818, 'ef04f2fd-e094-4219-8c07-1c57ac45adcb'), (24823, 'fc6e31f1-cb15-461c-bf95-d4637763257d'), (2809, 'd9d57e7d-5d1f-4b55-a73f-46e33852e193'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (2820, '2c4971bb-fdfd-4d37-bfed-ab6fc9e66a1e'), (18704, '153572ea-4ba1-43c6-9820-c834edb506aa'), (31515, '0f49e666-ca33-4651-8627-9fa3f1c4d7ea'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (805, '45825ed8-8f85-40ed-a174-19eeebd97cf7'), (31016, '05908d70-238b-4150-bec9-b27c6ef05086'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (2860, 'cd0fbf96-6263-4efc-9922-f67f4cce3a51'), (2861, 'a9b4fd80-a330-4e91-934d-74d210662219'), (28973, '60eb4c59-215d-45c6-adaf-062e54bd40c4'), (5934, '67efeb59-0ad4-4fe5-b340-15a664bbc2df'), (16684, '33631500-4c49-4688-b4d3-d6e3761aaf43'), (3372, '4a7bd43f-a235-4671-b643-ff3c07f2e41f'), (4408, '09376cb7-2c34-4892-a8c1-e3d197674d5e'), (8509, 'c494626f-af41-4de1-84e1-2417d6d47083'), (8511, '6bbec25e-f97a-49b9-8451-9fc00824e1c9'), (24896, '436a35ce-6f8a-4969-b199-e5617a13da1a'), (15682, '18566622-8ac4-4b94-a48f-709a448c40d7'), (8514, 'f40eb165-8653-4867-a7a9-cb27eebdfda0'), (836, '9ee86ea3-f33f-4cf5-9e92-0ad9009e33ba'), (25419, '11c1a93f-f303-4770-a6c8-f61ecb69357c'), (27472, '4b11ae12-4926-49fc-98e1-d59d5927ff33'), (9553, '7bf59ff6-d752-4eb0-9dc8-8aacda143835'), (27473, '628787b7-dd50-49d4-bbfc-cd5771b01254'), (27475, '8fdc35b4-b9d4-4822-8a62-f9f2cca68e51'), (25428, 'd391c525-320b-49a3-83dc-e6a90c266db7'), (21356, '1292ce57-959c-4299-99a3-523f8de67d10'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (21360, '0acfab8a-7154-48e9-aff7-586b4fae9e31'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (16247, 'f9b0a4f6-f562-4898-a506-df993d6c3af8'), (31618, '58b8c823-f817-4d5b-893b-11132d6df26f'), (28556, '6de4578f-f7a2-4271-a425-65baf13f4391'), (5005, 'd1827718-52c5-46b8-a465-02f3edc3a83a'), (5013, '474395e3-99c5-4cbe-b336-48aedca62e63'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (14843, 'e99ac210-a8ee-46c9-ab15-45f0c9d7496a'), (31162, '3f7bca01-9d54-42d0-8fd1-5601733d4a2e'), (28605, '3537089c-e6d7-4929-b357-2d48cc1d3d8b'), (7618, '5d6b1d32-9297-4269-81ff-5ab5026ac5e9'), (29642, '87c63108-1e81-453f-97cd-a38d1ede6bd7'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (13269, '4164fa88-7c62-4d47-b69e-268acd3b421a'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (12757, '2a347ea7-3799-4a71-ad0e-b0851e229e64'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (13278, '69a325d1-04cf-43d4-8505-bcbf6037e685'), (13280, '2c3ad26d-8418-4d01-a881-e5dd6be54e3f'), (13282, '65364d14-5bde-4e50-87e2-a33023028d02'), (14819, 'b7397fbe-8825-4389-b08f-c72a6259b878'), (24035, 'fb6bd3d7-2158-4830-bdc5-260c21d4474b'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (13289, 'd702891b-4cde-4c13-9555-2654befa7b2a'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (13292, '2e5c6ac3-19ee-4ddd-9d08-354a10cf7a44'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (23023, 'bce01692-64e8-4d39-8f05-514945aaf065'), (14322, '4bdb55d6-f2b8-4711-b751-455c3b372162'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (7672, 'e5b9aca6-3dea-4557-af6f-509bb2a395e5'), (13307, '680c3fcc-5efa-4340-aaa3-d7528d4744d1'), (1533, '4b40d289-fed3-4f28-a896-d8b008d6cda1'), (19967, '44596c09-c3c0-43c5-9419-b9a85e560ec0')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

### Language modeling

Language modeling is a task that predicts a word in a sequence of text. It has become a very popular NLP task because a pretrained language model can be finetuned for many other downstream tasks. Lately, there has been a lot of interest in large language models (LLMs) which demonstrate zero- or few-shot learning. This means the model can solve tasks it wasn't explicitly trained to do! Language models can be used to generate fluent and convincing text, though you need to be careful since the text may not always be accurate.

There are two types of language modeling:

* causal: the model's objective is to predict the next token in a sequence, and future tokens are masked

    ```py
    >>> from transformers import pipeline

    >>> prompt = ""Hugging Face is a community-based open-source platform for machine learning.""
    >>> generator = pipeline(task=""text-generation"")
    >>> generator(prompt)  # doctest: +SKIP
```

If you want to customize your predictions using additional `kwargs` like `min_length`, check out  “Usage best practices” below. 

## Usage best practices

When using generative models, most of the time you want to configure or customize your prediction to fit your needs, for example by using beam search, configuring the max or min length of the generated sequence, or adjust the temperature to reduce repetition. The Transformers library provides different strategies and `kwargs` to do this, the Hugging Face Inference toolkit offers the same functionality using the `parameters` attribute of your request payload. Below you can find examples on how to generate text without parameters, with beam search, and using custom configurations. If you want to learn about different decoding strategies check out this [blog post](https://huggingface.co/blog/how-to-generate).

### Default request

This is an example of a default request using `greedy` search.
Here, we've created an input textbox with a label, a placeholder, and a set number of lines.
You could do the same for the output textbox, but we'll leave that for now.

We've seen that with just a few lines of code, Gradio lets you create a simple interface around any function
with any kind of inputs or outputs. In this section, we've started with a
simple textbox, but in the next sections, we'll cover other kinds of inputs and outputs. Let's now take a look at including some NLP in a Gradio application.


## 🤖 Including model predictions[[including-model-predictions]]

Let's now build a simple interface that allows you to demo a **text-generation** model like GPT-2.

We'll load our model using the `pipeline()` function from 🤗 Transformers.
If you need a quick refresher, you can go back to [that section in Chapter 1](/course/chapter1/3#text-generation).

First, we define a prediction function that takes in a text prompt and returns the text completion:
- [Conversion to Core ML](#conversion-to-core-ml). We'll use Llama 2 as a real-life example.
- [Optimization](#optimization) techniques to make your model (and app) run fast and consume as little memory as possible. This is an area that permeates across the project and there's no silver-bullet solution you can apply.
- [`swift-transformers`](#swift-transformers), our new library to help with some common tasks.
    - [Tokenizers](#tokenizers). Tokenization is the way to convert text input to the actual set of numbers that are processed by the model (and back to text from the generated predictions). This is a lot more involved than it sounds, as there are many different options and strategies.
    - [Model and Hub wrappers](#model-and-hub-wrappers). If we want to support the wide variety of models on the Hub, we can't afford to hardcode model settings. We created a simple `LanguageModel` abstraction and various utilities to download model and tokenizer configuration files from the Hub.
### preprocessing

The task of preparing raw data into a format that can be easily consumed by machine learning models. For example, text is typically preprocessed by tokenization. To gain a better idea of what preprocessing looks like for other input types, check out the [Preprocess](https://huggingface.co/docs/transformers/preprocessing) tutorial.

### pretrained model

A model that has been pretrained on some data (for instance all of Wikipedia). Pretraining methods involve a
self-supervised objective, which can be reading the text and trying to predict the next word (see [causal language
modeling](#causal-language-modeling)) or masking some words and trying to predict them (see [masked language
modeling](#masked-language-modeling-mlm)).
 
Here is the question: You are tasked to create a GUI application for a machine learning model that generates text predictions. Which combination of tools and techniques would allow you to achieve this most efficiently?","Based on the context, the most efficient combination of tools and techniques to create a GUI application for a machine learning model that generates text predictions would be:

* Using the `pipeline` function from the `transformers` library to load a pre-trained language model (e.g. GPT-2) for text generation.
* Utilizing the `gradio` library to create a simple interface around the text generation function, allowing users to input a prompt and receive a text completion.
* Implementing `"
"In a zero-shot image classification task, what feature makes CLIP different from traditional supervised vision models?","[(13826, '2a55b5cd-c496-407c-97b0-92e3ab12e0e7'), (23554, '52119c19-3173-4838-93a4-3fde72e9b37b'), (15373, 'ad77f7a7-07d1-41df-b530-8c19035ed5a6'), (15912, '0834fb95-d368-43c1-b680-9e85bf3e7cce'), (15918, 'c7ba6127-5921-4286-880d-489225907f4f'), (15919, 'b6239f7c-4806-4ec8-954f-a4696f41331f'), (15924, '0fa9c0d3-9d36-4b4c-9711-b6faf189ad89'), (31292, 'b37711fa-ba5e-4d7c-9d3d-8ff4635ba254'), (24127, '474b8446-9f4d-4ec3-9b0e-78695268902d'), (22079, '16104f25-f547-44dd-9b5f-3798c428cc44'), (11329, 'b5c39d95-7e52-4220-b14e-6cdcd2ee5964'), (22083, 'dd15b594-110b-4773-b128-0658f80e911d'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (89, '89c32447-7b9b-4dbd-8cb7-1e1f1394809d'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (10858, '33249571-1ade-46fd-ae89-e32ff806fdcd'), (10859, '7140610f-8a0a-40c5-b6af-12b5af75f3c7'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (6270, 'c18172bc-ee0e-4969-8110-192dd5331ee7'), (7806, '6d7638e5-caf5-4427-a7e6-a138c90c7e9c'), (7808, 'f7d82006-5b58-485f-b5ca-2e5043ac18d8'), (7809, '173bc460-5338-4f7c-9b69-916e9e03ecfb'), (7810, '6464d47d-5a3f-4e13-b8a1-5d521cf34e11'), (6271, 'db13ad85-6738-4e86-bea9-8e92682af368'), (7812, 'd2ee2299-ee53-476e-9f83-1c5ab9120907'), (7811, '123dcee0-2e91-4169-8a00-a5029ecbb12a'), (7814, '59fc1afc-32d4-4401-b7a7-f31ff5bf1424'), (7807, '174504e7-4feb-443c-a469-f695d0c3db35'), (15500, 'faef5ce1-f4bc-4b19-bd94-f0bd6be7eace'), (1677, 'ae024f27-e2c3-43b9-b174-47ebc2b1a20d'), (6286, '17043260-ccb0-41be-a8d7-cd50fdf8f0c8'), (6287, '89e924a6-29d0-47d3-96a0-a0d16dfc1ce2'), (145, '82bcd7b4-a187-4092-8487-e9a50c1ae2b8'), (146, '76f72a72-531b-4daa-bb90-f35cf2112614'), (659, '77f9993f-20a6-4964-b925-7de3be25efa8'), (15512, 'f4ba93f0-f790-4c60-a1d7-743f9470f787'), (153, 'a5e94b25-4bd0-4edc-be2a-4167144129cb'), (670, '577aa401-7ed7-400a-a023-60b33766e240'), (671, '11cdddb6-4b04-4b7d-be04-b2efd538cdc5'), (5793, '358177a4-5892-4a9b-9356-18b7f3fb59d8'), (678, '9a218efd-2630-4a77-a792-63de27d959d7'), (8359, '34c5841f-f7f8-42e6-9509-8e9d7143a333'), (682, 'a36d28c1-b770-43c9-b4da-cff067e67abf'), (683, '48236e33-8bc4-4797-89da-7724843cbaac'), (3247, '2bfe2a34-6078-4ce6-be6d-005071f5911e'), (5810, 'b507f388-1038-4bc8-8559-2d51ffaec850'), (3799, '1651e17d-3706-47fa-9cf4-a9f8a9ebb754'), (16090, 'de34769a-90f0-4c82-b82e-6d9a5a0e6f92'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (3826, '256f85ac-16ed-48b7-8a96-c986ccb76483'), (3831, '320e50a7-2690-4798-86c2-921ef51a4cac'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (3834, 'df9d737a-e452-4e71-9f9b-48cb3c6baaa6'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (6910, '49d3e17d-b9ae-4481-9c7a-4c59ba0a76f5'), (3334, 'be61aaba-10f0-4007-a548-48ae184f321f'), (24873, 'c1b82171-8a79-4813-bd16-bfb57517e4da'), (24875, 'b1625c6b-1283-450d-8548-58f9de5a6ff2'), (24886, '6fc05d0c-f06c-41bb-b410-b41961c6fb99'), (27446, 'a836a2b1-6614-4c59-bc09-62ded216179c'), (5944, 'af31af69-da28-4239-b3f5-59c624ec8d83'), (5945, '82b994e1-d6d9-4269-92fd-5bf9f88254b7'), (5946, '7a54286f-a79f-4ab2-b704-5487dafb7a5b'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (5948, '68fab821-06c3-4a78-96e2-4dfd616f31c0'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (14656, '812d63e0-cb77-4e29-9bbd-da3da9e4ffa5'), (5474, '1158d6f0-fad9-4bbb-babf-d6178165afc7'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (4967, '43778b66-3e71-4fd3-a9bf-f2cf850f713d'), (23415, '0d1f22cc-3c69-43b3-85fa-a13b96b6b24c'), (4983, '044a8313-5785-4ad9-a376-13fc8c2b2cfa'), (23418, '3a7e4adb-187b-4011-82af-e5fe2ed6aad5'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (14204, '23ef5336-0d61-4a22-b398-3349e2d163ca'), (14205, '2c1444e7-9718-4eec-8840-4cb117e9e104'), (14206, '5bfbab3f-215b-442a-8bd0-1810e815b192'), (31616, 'f6da62fc-c26d-4422-99b3-031dc14e92df'), (31621, '833b507c-b23d-45b8-936f-a85896b6367d'), (16264, '5d166a05-2d80-4201-ae2f-b71993797a24'), (22923, 'c7ff6a20-33ae-49f3-a056-4612d0bb44cb'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (25484, '26167595-4d5d-47d4-a36d-a245287d3eac'), (22926, 'f448c8d4-9e0f-4837-af00-f4aa2144feaa'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (22928, 'a8321404-e3d8-420e-b234-a78cf2bdc969'), (22929, '7295ab19-7283-4564-b402-8ecabb173125'), (22930, '25b5e287-3b54-40ee-9fc0-06927f1cd7cd'), (22931, 'ece59397-b752-40a0-a704-c3bd65da52a5'), (25486, 'eea7b0c2-d489-42d8-b858-4137b7e9c781'), (21400, '637da23f-83b4-4655-83e1-bd9d7d7048ee'), (25497, '6e0e229f-0982-4c89-9297-2fc1f6f059f2'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (22942, 'ace38819-03d1-441b-a456-b6507b399634'), (25000, 'c4dd2159-6af5-43ea-80dd-2affde5c9e7a'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (22967, 'e33301fd-bcf6-4639-a5b1-f164b2d685f9'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (29124, '2a4c40e0-2e01-47f4-a1b0-a84901d0c22e'), (29127, '82149023-2091-41fa-98e9-b702609ed535'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (23014, 'd29e7466-2ce3-4018-8252-e65865458a74'), (23015, 'd9177564-c31b-4316-8d08-cb422482dd8b'), (21485, '1ff92749-dc34-4f1f-94b1-ec18b41dd726')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: -->

# Zero-shot image classification

[[open-in-colab]]

Zero-shot image classification is a task that involves classifying images into different categories using a model that was
not explicitly trained on data containing labeled examples from those specific categories.

Traditionally, image classification requires training a model on a specific set of labeled images, and this model learns to
""map"" certain image features to labels. When there's a need to use such model for a classification task that introduces a
new set of labels, fine-tuning is required to ""recalibrate"" the model.

In contrast, zero-shot or open vocabulary image classification models are typically multi-modal models that have been trained on a large
dataset of images and associated descriptions. These models learn aligned vision-language representations that can be used for many downstream tasks including zero-shot image classification.
While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide joint image-text embeddings that can be used for downstream tasks such as zero-shot image classification, other models are trained on interesting downstream tasks. In addition, FLAVA is trained with both unimodal and multi-modal pre-training objectives and can be used for both unimodal vision or language tasks and multi-modal tasks.
This model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).

## Usage tips and example

CLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image
classification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text
features. Both the text and visual features are then projected to a latent space with identical dimension. The dot
product between the projected image and text features is then used as a similar score.
*The tremendous success of CLIP (Radford et al., 2021) has promoted the research and application of contrastive learning for vision-language pretraining. In this work, we construct a large-scale dataset of image-text pairs in Chinese, where most data are retrieved from publicly available datasets, and we pretrain Chinese CLIP models on the new dataset. We develop 5 Chinese CLIP models of multiple sizes, spanning from 77 to 958 million parameters. Furthermore, we propose a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized, to achieve enhanced model performance. Our comprehensive experiments demonstrate that Chinese CLIP can achieve the state-of-the-art performance on MUGE, Flickr30K-CN, and COCO-CN in the setups of zero-shot learning and finetuning, and it is able to achieve competitive performance in zero-shot image classification based on the evaluation on the ELEVATER benchmark (Li et
## Usage example

OWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](clip) as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.
 
Here is the question: In a zero-shot image classification task, what feature makes CLIP different from traditional supervised vision models?","In a zero-shot image classification task, what makes CLIP different from traditional supervised vision models is that CLIP is a multi-modal model that has been trained on a large dataset of images and associated descriptions, learning aligned vision-language representations that can be used for many downstream tasks including zero-shot image classification, whereas traditional supervised vision models require training on a specific set of labeled images and fine-tuning for new labels."
"A developer wants to create a multilingual chatbot that supports multiple tasks, including sentiment analysis and question answering. Which architecture and approach are best suited for this?","[(20481, '73e355da-709c-4976-8830-0234ff38d590'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (25632, '794f27c6-f909-4924-a98b-82dfc53e1c4e'), (31266, '29004758-00c0-483a-bd99-3c5ff3805e24'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (30784, '5abe754b-87a4-4fc2-9103-0a0a0aab8e09'), (23108, 'ff267803-88f2-46f2-83c6-eb20b65886da'), (23116, '0b0e5942-415e-4dd2-bc16-fec335b19618'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (26704, '2f49980f-b15b-490f-be22-3b23841f2787'), (26706, '527828ef-93ab-4e52-b171-0cd0d0578621'), (2133, '8dcddfca-d40c-4832-8e4b-b9274c4a6549'), (26711, '3b96a9f8-a01c-4227-bb2c-9e1018bd38dc'), (26712, 'd358d8f4-0c23-4ef6-8abb-73508c5e1c85'), (26713, '0d242fd9-d15a-41f6-8731-00ea12bc9cd9'), (26716, '5f59393b-4523-4a2a-a0c4-b94e7dfb6d2b'), (26719, '47a71a8f-4131-45e9-935c-460e3d6e1b69'), (30817, 'ce6db1b0-14b2-4588-be54-92b24764e498'), (21092, 'bbbdb5ff-7f8d-4604-af7e-4ed5ea7f9604'), (23154, '1eef1270-9a6c-4ff5-a239-fd5733b3a1a0'), (29301, '33fef011-721c-47fe-85d9-708044b9d954'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (29311, '9b91f0cd-07ae-49fd-b4bb-efe169c39b9d'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (9344, 'a7a98193-dcf7-49c5-88d1-70597c133443'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (23179, 'ac432a2f-b472-4375-834a-a539ebc96d80'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (23711, 'f40aa33e-777a-4bd4-999a-8f5447776eed'), (11427, '78f29a08-632a-4d65-ab83-d1a6d230d416'), (26792, '5b7554eb-f474-4a3b-8e42-e10d28fa87e9'), (26795, 'e0ea2900-3022-407e-be80-3e81408ea4fc'), (26804, '6f0cbac2-7e02-4afc-ab45-7ca3adc1fd1e'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (4302, 'f56b3431-7ef2-4856-a72a-725526237cb9'), (4303, '5c74c3a1-6f4b-40e1-b248-5ce25fe9347a'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (28389, '298bdf93-edee-48fb-94e3-1cf5585de1f2'), (28390, '39c5e061-d8dd-498b-a12f-8cc7f00ea6fd'), (28391, '0521891e-5937-494a-9ba7-7ec80663cd0f'), (14572, 'c9cb95f6-d4d0-427e-be4b-bb5cfac161a8'), (14573, '519760cf-bd02-40ad-9c5d-470a8fa48b0d'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (25839, '4a92026f-215f-4a87-a52b-a7baf6a8a3bd'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (28405, 'c53554e0-e701-4e27-9e1b-ec3a43294b89'), (22780, 'edd8b962-01b2-4f07-9fe7-d76733c4cb6b'), (20732, '48b508d5-f9df-44b0-8d9e-6f61719ea9c7'), (25853, '2a8b53a4-0437-4333-aaaa-503355d882ab'), (5919, 'bef2fd9d-5d6e-4d07-a8fe-1fc9829a8f50'), (5920, 'cf35c7c5-b906-4a33-b414-dc8ea789896b'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (1320, 'd5a5263a-a570-474e-8d13-92d45d7a6e95'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (26421, '2c89420e-e9cb-4967-9268-cef5f028e779'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (15681, '8372d995-5746-4c26-b286-e7d360c03126'), (29002, '8afe6522-4963-48c6-91de-5a7e7715c4bb'), (11086, '2c69e9f6-788f-44ff-8742-db7169f4ffcc'), (11089, '23dcb623-f751-470b-bdda-3a7c2e191213'), (29521, 'e9d4b581-d3f5-4257-979e-7ad041167f8b'), (29011, '43e39481-c24b-485f-a92f-42acfce7c8bb'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (28007, '3fa7ef8b-9457-4d7a-9ffd-4bee76621e2b'), (15208, 'ee05e977-f99f-4b73-a248-a9c10f227f38'), (25976, 'ed4da672-db2b-4118-83a2-18d216904824'), (24960, '8e0e4635-d47a-4795-bdab-22ffe8fea5c8'), (20865, '1756516c-23ea-4966-9cd6-4c255abb53f7'), (20864, 'f24232d4-e224-420a-bcc7-317481adc93e'), (13307, '680c3fcc-5efa-4340-aaa3-d7528d4744d1'), (2974, '03433933-b11b-4283-84ae-be6de92f23dd'), (31648, 'b9a31629-ce35-4b30-b996-79816ecb39c2'), (7591, '833da8f3-7e22-40fc-b573-04af8220e4fd'), (22445, '66ff92b2-7481-4b66-9cdf-5e8e7bced9b0'), (4533, '9a8038f2-4c4f-47ed-b0f8-049c0fb4c281'), (29626, '2f7c7b90-5297-4b10-b6e7-ee9cb0941fff'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (29627, '759e8e09-8a4b-4916-81f2-8f9b671670aa'), (21437, '88c90876-5afd-4385-a39c-2a6dad7d9e00'), (21438, 'f486d89f-3657-4546-a293-d0f7bb46e794'), (29630, '6518c847-6555-4dd6-98f0-a5a05df6c036'), (21441, '437d81e7-4ab7-4143-8a89-e25826d44a56'), (21445, '40b08046-3292-41a1-990b-69b63b4ff917'), (18886, 'a01aef5d-3c56-4f8d-8854-e68353f4d189'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (13267, 'd14a13b5-d0f7-4354-9963-a77a57948d2c'), (13269, '4164fa88-7c62-4d47-b69e-268acd3b421a'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (3548, '004bc42b-263a-417f-a8b3-89534db4cb73'), (13787, '3d112122-a48e-416f-9e0d-1d80749c9c5a'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (3547, 'c4046d4c-c32f-4748-8540-956375bf44f9'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (29162, 'f30f1bff-570c-4355-b01a-726ca1b698b9'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (16884, 'bc5ee849-d044-41ba-a614-68829cf338b4'), (20469, '64f17b2f-4f72-4e45-a181-b237c9268b84'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that
scaling neural models in the number of parameters and the size of the data they are trained on gives improved results,
we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of
skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to
their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent
persona. We show that large scale models can learn these skills when given appropriate training data and choice of
generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models
and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn
*Building open-domain chatbots is a challenging area for machine learning research. While prior work has shown that
scaling neural models in the number of parameters and the size of the data they are trained on gives improved results,
we show that other ingredients are important for a high-performing chatbot. Good conversation requires a number of
skills that an expert conversationalist blends in a seamless way: providing engaging talking points and listening to
their partners, and displaying knowledge, empathy and personality appropriately, while maintaining a consistent
persona. We show that large scale models can learn these skills when given appropriate training data and choice of
generation strategy. We build variants of these recipes with 90M, 2.7B and 9.4B parameter models, and make our models
and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn
```

Putting this together, we can create a _multimodal_ chatbot with a textbox for a user to submit text and an file upload button to submit images / audio / video files. The rest of the code looks pretty much the same as before:

$code_chatbot_multimodal
$demo_chatbot_multimodal

And you're done! That's all the code you need to build an interface for your chatbot model. Finally, we'll end our Guide with some links to Chatbots that are running on Spaces so that you can get an idea of what else is possible:

- [project-baize/Baize-7B](https://huggingface.co/spaces/project-baize/Baize-7B): A stylized chatbot that allows you to stop generation as well as regenerate responses.
- [MAGAer13/mPLUG-Owl](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl): A multimodal chatbot that allows you to upvote and downvote responses.
```

## 3. The Science Behind LLM Architectures: Strategic Selection for Long Text Inputs and Chat

So far we have looked into improving computational and memory efficiency by:

-   Casting the weights to a lower precision format
-   Replacing the self-attention algorithm with a more memory- and compute efficient version

Let's now look into how we can change the architecture of an LLM so that it is most effective and efficient for task that require long text inputs, *e.g.*:
-   Retrieval augmented Questions Answering,
-   Summarization,
-   Chat

Note that *chat* not only requires the LLM to handle long text inputs, but it also necessitates that the LLM is able to efficiently handle the back-and-forth dialogue between user and assistant (such as ChatGPT).
In this blog post, we’ll show how StarCoder can be fine-tuned for chat to create a personalised coding assistant! Dubbed StarChat, we’ll explore several technical details that arise when using large language models (LLMs) as coding assistants, including:

- How LLMs can be prompted to act like conversational agents.
- OpenAI’s [Chat Markup Language](https://github.com/openai/openai-python/blob/main/chatml.md) (or ChatML for short), which provides a structured format for conversational messages between human users and AI assistants.
- How to fine-tune a large model on a diverse corpus of dialogues with 🤗 Transformers and DeepSpeed ZeRO-3.

As a teaser of the end result, try asking StarChat a few programming questions in the demo below!

<script
	type=""module""
	src=""https://gradio.s3-us-west-2.amazonaws.com/3.28.2/gradio.js""
></script>

<gradio-app theme_mode=""light"" src=""https://huggingfaceh4-starchat-playground.hf.space""></gradio-app>
 
Here is the question: A developer wants to create a multilingual chatbot that supports multiple tasks, including sentiment analysis and question answering. Which architecture and approach are best suited for this?","Based on the context, a large language model (LLM) architecture with a multimodal approach is best suited for creating a multilingual chatbot that supports multiple tasks, including sentiment analysis and question answering. This is because LLMs can learn various skills such as providing engaging talking points, listening to partners, displaying knowledge, empathy, and personality, and maintaining a consistent persona, which are essential for a high-performing chatbot. Additionally, a multimodal approach allows the chatbot to handle different"
"When preparing data for training an image-text alignment model like CLIP, which step is crucial for effective performance?","[(22054, '1e8f1df9-fb5c-47e1-8123-1b45fb89dcb7'), (15911, 'dea3e203-7c16-410e-9667-1ee9ffb39f00'), (15912, '0834fb95-d368-43c1-b680-9e85bf3e7cce'), (22056, '7123da97-4705-4027-940b-d2d544fe439b'), (22055, '7bca344f-7c3b-48f1-834e-58bc670b2044'), (15918, 'c7ba6127-5921-4286-880d-489225907f4f'), (15919, 'b6239f7c-4806-4ec8-954f-a4696f41331f'), (22066, 'e14c1815-0914-4a4b-b7a3-9f6290337020'), (15924, '0fa9c0d3-9d36-4b4c-9711-b6faf189ad89'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (3127, '3603fd2c-d457-4a01-893d-9d26a38ff933'), (22076, '37ef4059-7542-46a2-b62d-19a38915a327'), (22078, '317f9c85-5b9d-43f5-9061-ff54e211cc06'), (22080, 'f0a7acc8-bb49-4df2-8a9a-06e9860bb69c'), (22085, 'f6050fc4-183f-46c7-9934-5cdcc7d6aa7f'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (88, '8bb2a9ef-a9ec-4471-b9ff-467c7e00b9ad'), (4204, 'eb9e94d4-02e1-461e-b271-6c8438d90b46'), (26231, 'b3e08d1a-4130-42bb-8c39-7e1831a663d6'), (7806, '6d7638e5-caf5-4427-a7e6-a138c90c7e9c'), (7807, '174504e7-4feb-443c-a469-f695d0c3db35'), (7809, '173bc460-5338-4f7c-9b69-916e9e03ecfb'), (7810, '6464d47d-5a3f-4e13-b8a1-5d521cf34e11'), (7811, '123dcee0-2e91-4169-8a00-a5029ecbb12a'), (7812, 'd2ee2299-ee53-476e-9f83-1c5ab9120907'), (7815, 'ffedd8d3-edcc-43cc-9cec-a604d063dc3e'), (6286, '17043260-ccb0-41be-a8d7-cd50fdf8f0c8'), (6287, '89e924a6-29d0-47d3-96a0-a0d16dfc1ce2'), (6291, 'b3b1e6bb-cc07-4d58-8667-8141ecd4f85f'), (667, '466a4c47-d6a2-4439-8124-9da2dea99f39'), (670, '577aa401-7ed7-400a-a023-60b33766e240'), (671, '11cdddb6-4b04-4b7d-be04-b2efd538cdc5'), (673, '657d7393-7d8d-4540-b727-e72053704128'), (5793, '358177a4-5892-4a9b-9356-18b7f3fb59d8'), (675, 'e6d47d5b-9841-4de7-a93f-86385047865b'), (678, '9a218efd-2630-4a77-a792-63de27d959d7'), (682, 'a36d28c1-b770-43c9-b4da-cff067e67abf'), (683, '48236e33-8bc4-4797-89da-7724843cbaac'), (11437, 'bcf6c108-66d8-4c82-8e0b-90a0f98d8602'), (29373, 'e19e41fa-2890-4dc5-9c37-8fe73d4a7d8e'), (17599, 'd55b5b83-8a01-465a-9e78-d5fced089f34'), (29375, '169105fe-4c29-42ce-bd87-a196ea554e7c'), (26305, 'd72a5370-1003-4496-9f34-0056f2a43174'), (29376, '8c217225-9082-4990-a340-02f60c2f084b'), (26310, '42880a29-1a9a-4f47-b818-4c8ec4ebebc7'), (28359, 'bd44fb65-c7d3-4cbb-8721-c40f8d4e871e'), (28361, 'a1632814-841b-4502-890e-a2be7eed48cd'), (28363, 'bd4d2852-2b2d-43e5-93b3-0c7a235c30d3'), (28364, '86e13021-5e46-474f-928b-57f59de16bce'), (3796, '23e48029-5d36-4631-8a93-df271cc3072e'), (3798, 'a7c1c130-12a3-42f5-9adb-9fba72949e4e'), (3799, '1651e17d-3706-47fa-9cf4-a9f8a9ebb754'), (16091, 'f4e121de-0b41-4c9e-8c14-13372d63deef'), (3804, 'b71f0121-8574-43cf-b436-d8a73827285f'), (3805, 'c1fffa18-95ea-49ae-aba6-c62e34c28f4b'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (3813, 'a41ca3f9-93fe-41df-8aaf-f96e719d9394'), (3815, '7fa3513f-2329-4efb-bfb6-74160ee1ef45'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (19192, '2767df7c-ddd7-446b-a752-0a2d87a1e317'), (3832, 'cfdcf8bc-f380-4f1b-9321-5e60c878747d'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (3837, '2b3f9754-3c83-4140-8261-a686b11fd43d'), (6910, '49d3e17d-b9ae-4481-9c7a-4c59ba0a76f5'), (5890, '196d6213-976c-4e03-ab43-414d50d862f3'), (25347, 'b38e0f87-e853-46c1-9fcb-77d766c7bbc9'), (5891, 'e903bbaa-0306-4955-8b4b-3f01e947eb7c'), (5895, '5503b4eb-706b-4c38-a5a9-fde0c005182c'), (25356, '7fc6f9b3-10fd-48c7-95bd-fd7ba0cab29a'), (15645, '803865e3-d786-42b9-bc6d-5fff2e984c9b'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (26429, '5bae9ae8-99ba-41c9-9595-ae675ca50f04'), (12102, 'b5d1013b-9f0d-4de1-a864-6ad021db7a8b'), (12103, 'a445155c-2e8e-4785-8f2e-3a6c20775a86'), (22347, 'fb96b8eb-6fef-4edd-8b78-761583967bd7'), (6993, '4243bc5b-c7a6-4a08-8545-448007bdb844'), (9555, 'c19b5f80-620b-4a82-82e1-81e843622c70'), (5466, 'e6fb1185-8250-49a9-819b-c56bf802df85'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (15204, '43dc84e7-ee77-4550-b585-25b6bfa1b2a0'), (15226, 'ef67749a-f8e0-4b7d-80df-4b93272004f3'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (15227, '783d1508-a579-481d-b6e6-850a89fbabd7'), (14204, '23ef5336-0d61-4a22-b398-3349e2d163ca'), (15230, 'c4948062-4df0-4d3e-ac44-2b700dd82ff2'), (14205, '2c1444e7-9718-4eec-8840-4cb117e9e104'), (31616, 'f6da62fc-c26d-4422-99b3-031dc14e92df'), (14208, 'd972705d-f3b5-43f6-b556-d0752ac204a0'), (31621, '833b507c-b23d-45b8-936f-a85896b6367d'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (22928, 'a8321404-e3d8-420e-b234-a78cf2bdc969'), (22929, '7295ab19-7283-4564-b402-8ecabb173125'), (22930, '25b5e287-3b54-40ee-9fc0-06927f1cd7cd'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (22967, 'e33301fd-bcf6-4639-a5b1-f164b2d685f9'), (22970, '36b2f7d0-d2f2-4773-a1a0-4253ebb02ddc'), (29122, '81bc2e07-9a53-43ff-a046-a73c2a5c8858'), (29123, '069dbe7c-7b27-40c1-9d01-30cfdd155494'), (29124, '2a4c40e0-2e01-47f4-a1b0-a84901d0c22e'), (29125, '9846454a-817c-4b33-bfb6-750ebf2a3a5d'), (2511, '9d4f9ab1-abb1-483e-84e9-4cd82481acea'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (13285, '5331eb01-d555-4c11-aff3-26cd6b4ea2a7'), (13286, '7ee1ad39-4258-4a5d-81b7-e8d7746cca3a'), (23015, 'd9177564-c31b-4316-8d08-cb422482dd8b'), (23014, 'd29e7466-2ce3-4018-8252-e65865458a74'), (20966, '61420da0-bea9-4ed4-8b90-0716f6b0a31d'), (20967, '9c9e7c2c-88b4-4ef0-9fd3-fbfea2d90e56'), (23020, '1f5e4c21-bf4a-4469-9ccb-dff7ebaaadd1')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: For CLIP, the distance is simply the cosine distance between the text and image embeddings, whereas models such as ALIGN and DeCLIP design their own distance metrics to account for noisy datasets. 

Another work, [LiT](https://arxiv.org/abs/2111.07991), introduces a simple method for fine-tuning the text encoder using the CLIP pre-training objective while keeping the image encoder frozen. The authors interpret this idea as _a way to teach the text encoder to better read image embeddings from the image encoder_. This approach has been shown to be effective and is more sample efficient than CLIP. Other works, such as [FLAVA](https://arxiv.org/abs/2112.04482), use a combination of contrastive learning and other pretraining strategies to align vision and language embeddings.

### 2) PrefixLM
The abstract from the paper is the following:

*In this work, we present a conceptually simple and effective method to train a strong bilingual multimodal representation model. 
Starting from the pretrained multimodal representation model CLIP released by OpenAI, we switched its text encoder with a pretrained 
multilingual text encoder XLM-R, and aligned both languages and image representations by a two-stage training schema consisting of 
teacher learning and contrastive learning. We validate our method through evaluations of a wide range of tasks. We set new state-of-the-art 
performances on a bunch of tasks including ImageNet-CN, Flicker30k- CN, and COCO-CN. Further, we obtain very close performances with 
CLIP on almost all tasks, suggesting that one can simply alter the text encoder in CLIP for extended capabilities such as multilingual understanding.*

This model was contributed by [jongjyh](https://huggingface.co/jongjyh).

## Usage tips and example
Contrastive learning is a commonly used pre-training objective for vision models and has proven to be a highly effective pre-training objective for vision-language models as well. Recent works such as [CLIP](https://arxiv.org/abs/2103.00020), [CLOOB](https://arxiv.org/abs/2110.11316), [ALIGN](https://arxiv.org/abs/2102.05918), and [DeCLIP](https://arxiv.org/abs/2110.05208) bridge the vision and language modalities by learning a text encoder and an image encoder jointly with a contrastive loss, using large datasets consisting of {image, caption} pairs. Contrastive learning aims to map input images and texts to the same feature space such that the distance between the embeddings of image-text pairs is minimized if they match or maximized if they don’t. 

For CLIP, the distance is simply the cosine distance between the text and image embeddings, whereas models such as ALIGN and DeCLIP design their own distance metrics to account for noisy datasets.
*Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image
Models such as [Frozen](https://arxiv.org/abs/2106.13884) and [ClipCap](https://arxiv.org/abs/2111.09734) use this Frozen PrefixLM pre-training objective. They only update the parameters of the image encoder during training to generate image embeddings that can be used as a prefix to the pre-trained, frozen language model in a similar fashion to the PrefixLM objective discussed above. Both Frozen and ClipCap are trained on aligned image-text (caption) datasets with the objective of generating the next token in the caption, given the image embeddings and the prefix text.
 
Here is the question: When preparing data for training an image-text alignment model like CLIP, which step is crucial for effective performance?","Based on the context, it can be inferred that the crucial step for effective performance when preparing data for training an image-text alignment model like CLIP is the data curation process. The text mentions that ""curated training datasets are expensive or require expert knowledge"" and that ""this costly curation process limits the size of datasets and hence hinders the scaling of trained models."" This suggests that having a high-quality, well-curated dataset is essential for effective performance."
A machine learning researcher needs to visualize and explain how a multimodal model associates text prompts with image regions. Which tools and techniques could they combine?,"[(5120, 'ddf54aa1-62ec-4203-a18e-bcc935c8a3ed'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (15891, '048e8f83-8305-4e72-ab89-b2341c265168'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (18423, 'bceac124-b7c2-4000-85e8-bc1990d7dce1'), (11289, '7f93911b-ca40-4dd9-bd7e-f9daca16ed17'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (15900, 'b14c8f27-e0e7-4ea5-aab9-e6eeaa6459a9'), (15902, '0347bfc7-e23e-4f77-98da-86bc3c7ed60a'), (23073, 'c8719aa6-c675-4f8d-a578-bcb628162ea5'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (11321, '84e11439-be14-4a58-b2de-59eaec7867a1'), (28731, '85e9fbea-6d30-4bab-84e9-166bb1cc8edc'), (3131, '7d45a093-cbb1-482d-99e1-edd22b730881'), (18500, '601132c7-a766-4738-b8f8-0a14d1084196'), (30789, '8b611d65-f397-46aa-bb7b-a9b95d2bd126'), (22085, 'f6050fc4-183f-46c7-9934-5cdcc7d6aa7f'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (11340, 'dadc6a0d-436f-4043-be3c-dc957136a39e'), (11344, '614a7745-0caa-4be3-9ac1-c8116d68a14b'), (20577, 'f3e33c55-54fa-4265-9f37-a4027f1529a8'), (20609, 'c19b3701-219d-48a8-ae4b-d6b0ad0e3b8c'), (29839, '9308a71e-710a-4606-849a-022a2df18788'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (4772, '2a92752e-7e2a-4b91-89ad-7510a946ebde'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (4804, '545adb55-d07e-4a08-b179-72243735bedb'), (29383, '6ee74234-204b-4f35-8887-34201efb61f7'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (3794, '447a3909-dff2-4d1d-8c81-5c52d46dc6da'), (1746, '49e8dce4-afe5-4268-9ab9-6f9809115e1e'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (3808, '191dcb28-30e9-4713-9ef6-65929451ac28'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (1777, '4ddc1fcc-541d-4553-b408-84494027daef'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (2807, '1f7d9bc7-0f9f-4b58-90d8-0a672234db6a'), (2809, 'd9d57e7d-5d1f-4b55-a73f-46e33852e193'), (27391, 'bf10fdf5-d841-4030-8d03-4cb6d0161538'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (18184, 'a53114c0-96cb-42ef-a44e-4a6ea9778ff1'), (1803, '3415baa0-92c7-4ef0-bc5a-bd52bc093b4e'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (18191, '1d155c0b-9666-479b-8395-ef2ea126d232'), (1297, '0110063d-229c-4fc0-a7c3-f875aa009eb2'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (13601, 'd2fcf63a-6647-41a1-8530-613f53b57a88'), (18212, '0443ccbd-6b88-4ae8-a7f6-726c9bf51897'), (13608, 'e2f6df65-9623-4dcd-b35e-179b1f087b0f'), (26409, '697d9ca9-30fb-41e8-b990-10e8ae6ab5fd'), (299, '7f2a2f1a-f503-44f9-b87a-583789044a1f'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (9005, '380fd6e8-0ceb-48bc-b733-0a33abcdc371'), (10545, '96a8e973-9ee0-4b63-98cd-8bf3f5b5693e'), (307, '6b64ed6e-acaf-4565-8edd-d7398bd867b4'), (26428, '7adb1de0-976e-4d9f-92b7-7c14a2058e41'), (13629, 'a0c0b232-e11a-4534-ad9a-c6c048de323e'), (22337, 'dcd3d938-defe-4be3-b64f-3fd23ab4f8bf'), (328, 'b732a063-90a2-48fe-a514-fdc5a199ce22'), (19795, '585fbb43-88a3-49a3-8777-60a16c1aef26'), (25429, 'e4d79f98-e0bb-400b-b218-1b9fd7be51d3'), (26454, 'e61e2e35-e392-4e1e-91e9-7974b5459759'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (26456, 'd535e51c-0a9e-42c5-b745-c0b5f992860d'), (26464, '76994069-0112-43bd-b421-de7d71617a34'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (4976, '87b17c62-4296-4ca8-92e5-ea2d927a3025'), (25974, '479bc619-7e59-47ae-ab50-93666646cebb'), (3448, 'baf2b918-5ddf-44f5-8450-cefe9dcd903f'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (31618, '58b8c823-f817-4d5b-893b-11132d6df26f'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (28563, 'fd004173-44c7-4062-a554-2f5403627150'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (12184, '7c0b3b3e-1391-4f09-b2b4-de8d215c6806'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (2975, '7ba81ebf-faef-45fd-9aa1-be279b60cabf'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (9129, '4e3c49f8-4ca3-47af-a572-5135008633d3'), (9644, '9eff65cb-9b8d-4a98-af1b-68033ff46ecc'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (9658, '7bf2ce2b-9543-4ce2-9e55-5882afea38e8'), (29123, '069dbe7c-7b27-40c1-9d01-30cfdd155494'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (10192, '1cdc71ba-92b9-4368-bac0-dc3321d9d7dc'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (29158, 'e4fbc982-e499-43ae-b82d-504a09c43430'), (29159, '27f1f229-944d-4f99-bf23-7acab0f2fec7'), (13286, '7ee1ad39-4258-4a5d-81b7-e8d7746cca3a'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (495, '48c1123e-20d8-493c-a22e-8e39989edd16'), (5111, 'e87c80c3-d7d3-4478-88c3-e8dc5a994da2'), (26623, '1ef8a113-6f68-47e9-9800-8d31b047c101')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a ""red bounding box"" or ""pointed arrow"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this
Also, as we have mentioned in a [previous newsletter](https://huggingface.co/blog/ethics-soc-2#addressing-bias-throughout-the-ml-development-cycle), there is no one single way to develop machine learning systems, and any of the steps in the development and deployment process can present opportunities to tackle bias, from who is included at the start, to defining the task, to curating the dataset, training the model, and more. This also applies to multimodal models and the ways in which they are ultimately deployed or productionized in society, since the consequences of bias in multimodal models will depend on their downstream use. For instance, if a model is used in a human-in-the-loop setting for graphic design (such as those created by [RunwayML](https://runwayml.com/ai-magic-tools/text-to-image/)), the user has numerous occasions to detect and correct bias, for instance by changing the prompt or the generation options. However, if a model is used as part of a [tool to help forensic
```

## Multimodal

Multimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. 

Although multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.

### Document question answering
*Subject-driven text-to-image generation models create novel renditions of an input subject based on text prompts. Existing models suffer from lengthy fine-tuning and difficulties preserving the subject fidelity. To overcome these limitations, we introduce BLIP-Diffusion, a new subject-driven image generation model that supports multimodal control which consumes inputs of subject images and text prompts. Unlike other subject-driven generation models, BLIP-Diffusion introduces a new multimodal encoder which is pre-trained to provide subject representation. We first pre-train the multimodal encoder following BLIP-2 to produce visual representation aligned with the text. Then we design a subject representation learning task which enables a diffusion model to leverage such visual representation and generates new subject renditions. Compared with previous methods such as DreamBooth, our model enables zero-shot subject-driven generation, and efficient fine-tuning for customized subject with
*Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose LayoutLMv3 to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt
 
Here is the question: A machine learning researcher needs to visualize and explain how a multimodal model associates text prompts with image regions. Which tools and techniques could they combine?","Based on the context, the researcher could combine the following tools and techniques:

1. Visual prompts: Use a novel multimodal model that allows users to intuitively mark images and interact with the model using natural cues like a ""red bounding box"" or ""pointed arrow"".
2. Region-specific comprehension: Utilize the model's capability of decoding arbitrary visual prompts to understand how it associates text prompts with image regions.
3. ViP-Bench: Leverage the comprehensive benchmark to assess the capability of"
A client requests a real-time application that predicts text completions for user inputs while providing feedback on the model’s confidence. What combination of tools and frameworks would meet this need?,"[(7681, '24d0d708-09ea-4a23-af11-bf703954ca3b'), (19973, '1275328e-c988-4ac4-85e8-28d97d9f1b75'), (7685, '5747e091-b999-48a3-a7dc-3f5517777c0d'), (7, 'bb62d85a-4224-492b-9ffe-70d571ed09ab'), (13833, '2c93d378-a8ac-4ebd-8f75-b5d44ac01949'), (8210, '4f0da09a-b7a9-4335-b19c-3f4c5a7996e5'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (1569, '727d5ab0-4223-4840-b7da-e1809dd82af6'), (18994, '4cbd542c-a475-47f8-959c-ced12502b7e0'), (24638, 'e51d5c4d-d26c-4446-a519-54d0ae6188d9'), (17984, '27f313d0-7f48-4c78-84bd-a9b0abd0a509'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (18514, '2b8c705b-aa16-4343-9147-042e810a70a2'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (16469, 'aba54ccf-519e-4018-816e-984adec99ff9'), (26713, '0d242fd9-d15a-41f6-8731-00ea12bc9cd9'), (26717, '6640e95b-cbb6-447e-9e49-a05ca7a33070'), (7266, 'f5c4435e-d192-4727-9e6d-35acf4404166'), (8807, '170b9534-cae9-40e8-bbe4-468c9b1e40f0'), (18545, '1fa86e23-a16e-4ca4-a19c-ec80d4845f3b'), (3185, '8cacb60e-754c-4138-8848-a0779c50f5aa'), (13427, '8e71ae41-5106-444c-b3c5-88cab0408f77'), (21110, '3b4b59de-4da6-4c2a-89c8-597f807f0538'), (13436, 'e2b774e6-5237-4e9f-8716-229315e0fcaf'), (18557, '1b858cc2-89fc-4c08-9c31-b1d1dcea8d6e'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (7827, '2b137736-ff22-4a82-a789-80f640d114a9'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (29338, '49aed807-1a59-497e-be41-abc9c83d69c3'), (26792, '5b7554eb-f474-4a3b-8e42-e10d28fa87e9'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (23217, '467c518e-4a96-4470-9a9c-2d6886c09ae2'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (8370, '3f666915-d26e-45b0-8eb4-ea77c8650309'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (26815, 'd95f0425-7a94-4507-87fa-feb86f46ed8e'), (22209, 'd7ca43f5-90c6-448e-8d33-58c7acba90c4'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (203, 'd52d0498-2e8d-43aa-93ae-0103097a80e2'), (21198, '9468cc16-d5f1-4d2f-b79a-15502e9e9fee'), (3794, '447a3909-dff2-4d1d-8c81-5c52d46dc6da'), (28890, 'b1ae8ca7-d69b-4326-a81e-655300398e52'), (13536, '1f9eb408-5c83-4947-ace1-2583c87889ad'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (14055, '20614178-3fe6-45b2-a00d-f1009004877c'), (15593, 'd41ecdb5-1abb-4bbd-a4dd-adf3baf6ee42'), (1259, 'f09c5c1c-251c-406e-becf-b8608581bf6a'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (30962, '3705225a-97e6-42d2-82ca-51232c0f85cb'), (30967, '98b738e9-7537-4605-b78b-badda0188376'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (1312, 'd4db3d51-5152-4183-8da5-f262c23a293c'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (2860, 'cd0fbf96-6263-4efc-9922-f67f4cce3a51'), (2861, 'a9b4fd80-a330-4e91-934d-74d210662219'), (28988, 'b98b42fe-1a0e-4b94-8732-c62e0af9b120'), (24896, '436a35ce-6f8a-4969-b199-e5617a13da1a'), (24898, 'ca194eff-6753-4dc6-8cf8-0ec1a9a8ba00'), (10563, '138c4559-9b2e-43b7-a3bb-746c045905f0'), (25928, '1df7a22b-c397-4191-b209-d0abc60ab401'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (10062, '888697c9-de13-4c97-81c6-b558e727f5f9'), (10063, 'e76ab115-f55d-4bdc-814d-75a5e65a5cde'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (7531, '4a77ca90-5c47-4bc7-b08d-f1cae2cf28c2'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (3441, 'a50198ff-bdce-4045-af16-a7f3e4ed8e3a'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (3449, '576aa445-2e5c-42da-b75a-3dfb7fbb1469'), (28538, 'ef0103f8-9deb-4671-9247-9681f2011c29'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (28563, 'fd004173-44c7-4062-a554-2f5403627150'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (29597, '4515c20e-ae2c-488c-849e-fe1ff4102406'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (6054, 'a4ce9cc4-5057-4732-a32b-b46b2d1d0d2c'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (13756, '9c192e5b-e67e-45d1-b202-4edddebfc1bd'), (13757, '1ca945fe-3ad2-440a-9679-60b9498487d1'), (30149, 'd5284cd6-0e89-43ab-bbe0-44c9d7f2d010'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (6100, 'ec8b2c54-36ef-48b1-a951-766bda0c3beb'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (6623, '81050a29-495d-4fe8-a414-353f8fc378fc'), (13791, 'e05b5155-d2bb-44c0-9ffd-84644f9ea728'), (1509, '845ab30e-6d6e-4d7e-891f-90ba4fad896a'), (1510, 'a7df3897-3c02-4187-b2c6-88844cd91c0a'), (25064, '099d4bbf-893a-4feb-a4ad-1dbf78a02324'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (3564, '59aff3e3-a174-4e92-8000-144134e12394'), (25069, 'a400d317-c11c-42d5-bc72-ec9c08296b20'), (24558, 'f6566e6f-098a-485a-821b-ede66c8b307c'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (9201, '0961ebd0-8bcd-4f90-a5a4-172accd5410e'), (25075, 'a8b92196-1bc9-4b1f-86a5-d990942e6a58'), (1525, '240c574d-8ddb-4583-9436-0efc20f588f7'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (7672, 'e5b9aca6-3dea-4557-af6f-509bb2a395e5'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

This line automatically downloads the MobileNet model and weights using the Keras library.

## Step 2 — Defining a `predict` function

Next, we will need to define a function that takes in the _user input_, which in this case is an image, and returns the prediction. The prediction should be returned as a dictionary whose keys are class name and values are confidence probabilities. We will load the class names from this [text file](https://git.io/JJkYN).

In the case of our pretrained model, it will look like this:

```python
import requests

# Download human-readable labels for ImageNet.
response = requests.get(""https://git.io/JJkYN"")
labels = response.text.split(""\n"")

def classify_image(inp):
  inp = inp.reshape((-1, 224, 224, 3))
  inp = tf.keras.applications.mobilenet_v2.preprocess_input(inp)
  prediction = inception_net.predict(inp).flatten()
  confidences = {labels[i]: float(prediction[i]) for i in range(1000)}
  return confidences
1. train a machine learning model to classify tweets, and
2. predict over encrypted data using this model with FHE.

The final model (Transformer representation + XGboost) has a final accuracy of 85%, which is above the transformer itself with 80% accuracy (please see this [notebook](https://github.com/zama-ai/concrete-ml/blob/release/0.4.x/use_case_examples/encrypted_sentiment_analysis/SentimentClassification.ipynb)  for the comparisons).

The FHE execution time per example is 4.4 seconds on a 16 cores cpu.

The files for deployment are used for a sentiment analysis app that allows a client to request sentiment analysis predictions from a server while keeping its data encrypted all along the chain of communication.

[Concrete-ML](https://github.com/zama-ai/concrete-ml) (Don't forget to star us on Github ⭐️💛) allows straightforward ML model building and conversion to the FHE equivalent to be able to predict over encrypted data.
Here, we've created an input textbox with a label, a placeholder, and a set number of lines.
You could do the same for the output textbox, but we'll leave that for now.

We've seen that with just a few lines of code, Gradio lets you create a simple interface around any function
with any kind of inputs or outputs. In this section, we've started with a
simple textbox, but in the next sections, we'll cover other kinds of inputs and outputs. Let's now take a look at including some NLP in a Gradio application.


## 🤖 Including model predictions[[including-model-predictions]]

Let's now build a simple interface that allows you to demo a **text-generation** model like GPT-2.

We'll load our model using the `pipeline()` function from 🤗 Transformers.
If you need a quick refresher, you can go back to [that section in Chapter 1](/course/chapter1/3#text-generation).

First, we define a prediction function that takes in a text prompt and returns the text completion:
(1) They do not time out -- most browsers raise a timeout error if they do not get a response to a POST request after a short period of time (e.g. 1 min). This can be a problem if your inference function takes longer than 1 minute to run or if many people are trying out your demo at the same time, resulting in increased latency.

(2) They allow the server to send multiple updates to the frontend. This means, for example, that the server can send a real-time ETA of how long your prediction will take to complete.

To configure the queue, simply call the `.queue()` method before launching an `Interface`, `TabbedInterface`, `ChatInterface` or any `Blocks`. Here's an example:

```py
import gradio as gr

app = gr.Interface(lambda x:x, ""image"", ""image"")
app.queue()  # <-- Sets up a queue with default parameters
app.launch()
This chapter will begin with an end-to-end example where we use a model and a tokenizer together to replicate the `pipeline()` function introduced in [Chapter 1](/course/chapter1). Next, we'll discuss the model API: we'll dive into the model and configuration classes, and show you how to load a model and how it processes numerical inputs to output predictions. 

Then we'll look at the tokenizer API, which is the other main component of the `pipeline()` function. Tokenizers take care of the first and last processing steps, handling the conversion from text to numerical inputs for the neural network, and the conversion back to text when it is needed. Finally, we'll show you how to handle sending multiple sentences through a model in a prepared batch, then wrap it all up with a closer look at the high-level `tokenizer()` function.
 
Here is the question: A client requests a real-time application that predicts text completions for user inputs while providing feedback on the model’s confidence. What combination of tools and frameworks would meet this need?","Based on the context, the combination of tools and frameworks that would meet the client's need is Gradio for building a simple interface around the model, 🤗 Transformers for loading the text-generation model (e.g. GPT-2), and Concrete-ML for enabling prediction over encrypted data using Fully Homomorphic Encryption (FHE)."
"A research group is building a multimodal assistant capable of interpreting a user's spoken command, identifying relevant objects in an image, and providing a natural language response. What system design would best meet this goal?","[(6663, '71a9b411-3ab3-47cf-af51-e0b46b769031'), (14879, 'd4697222-f29c-4e4d-9174-03a63c30c9a1'), (31266, '29004758-00c0-483a-bd99-3c5ff3805e24'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (31270, 'cfe0a8ee-b54d-4dee-9307-ca63d817a829'), (23599, '828d8abe-906a-4010-8840-d1c779d76720'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (1624, '9c6c249c-815c-471a-9fbc-0b8c0c71e013'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (29301, '33fef011-721c-47fe-85d9-708044b9d954'), (29304, '2b35efae-fcc5-4f38-b266-758e452bf98f'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (667, '466a4c47-d6a2-4439-8124-9da2dea99f39'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (15534, '9a99de56-1a64-4262-8265-64e006a774fb'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (15543, '709e20d6-7763-4624-a403-ab88f87e09bb'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (29383, '6ee74234-204b-4f35-8887-34201efb61f7'), (29384, '94c3a5bd-0c90-40a5-ad5a-e2eb7ce295d1'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (3794, '447a3909-dff2-4d1d-8c81-5c52d46dc6da'), (3803, 'f0bab2c0-7345-4a53-bc07-5c845419f9d0'), (9949, 'a276a82b-2c04-476e-a74f-60c0d59cad39'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (3808, '191dcb28-30e9-4713-9ef6-65929451ac28'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (3812, 'de79f0d8-92e4-404e-91a4-fd94a8e8b619'), (28389, '298bdf93-edee-48fb-94e3-1cf5585de1f2'), (28390, '39c5e061-d8dd-498b-a12f-8cc7f00ea6fd'), (3813, 'a41ca3f9-93fe-41df-8aaf-f96e719d9394'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (3836, '624dbdb1-8d22-46bf-baa8-4d9305029369'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (27392, '929fcc41-fb2f-4210-bb6c-0f21445e2323'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (26416, 'acbe64bd-7331-4375-a687-a5d7dad14f46'), (23865, '7bccc135-749d-4acc-a179-12c10e7b97b3'), (14650, 'd3cdd83c-ffda-4802-a1c9-58dcf9c03fa5'), (14655, 'c5adb19a-7ebd-4d40-a31d-55a6f8a1b978'), (22337, 'dcd3d938-defe-4be3-b64f-3fd23ab4f8bf'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (3923, 'e87b7120-b4af-4c3f-9448-479a9d54a9d7'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (5479, 'f00515c2-0d56-4aa1-aed4-0849d00bf783'), (5480, 'e7ff7ba7-82d7-440d-bb24-3e84cb766756'), (5482, '52c3aeba-e469-4020-af46-70be2703e308'), (25963, 'fd0bea05-61c9-4e1b-8a8b-5171642f2f0d'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (25965, '1cda8dc4-5b16-484d-b64c-99fc5b9e9d79'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (4974, '9e5f0436-fc3e-4670-806a-f47aad3fdefb'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (4979, '8d06e416-3065-4f08-81bd-cf68c44d10d0'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (4986, 'd1a0c49b-c4b7-4f64-b176-5c34266e3eaa'), (25977, '3e2a478d-229c-4b2d-959b-134ec30e88df'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (7591, '833da8f3-7e22-40fc-b573-04af8220e4fd'), (5035, '83a015d9-5914-4516-b56d-1bd17eb3d24e'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (15281, '71c204b4-8730-45e2-8557-ae7695079eb6'), (15282, 'd36e985e-02ea-49bb-852c-254ac1d2f06c'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (29626, '2f7c7b90-5297-4b10-b6e7-ee9cb0941fff'), (17342, '02b4516e-7376-4945-8b42-8837a4bb4000'), (29123, '069dbe7c-7b27-40c1-9d01-30cfdd155494'), (21445, '40b08046-3292-41a1-990b-69b63b4ff917'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (29639, 'ad7bf7d2-d8c6-47bf-9e9c-6c68383ad368'), (13267, 'd14a13b5-d0f7-4354-9963-a77a57948d2c'), (24020, '9ceff87b-6adb-4e98-ad01-dc489b9f3c39'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (29158, 'e4fbc982-e499-43ae-b82d-504a09c43430'), (29159, '27f1f229-944d-4f99-bf23-7acab0f2fec7'), (29161, '5c1a0615-2746-4f25-8cae-56a17b9d371d'), (29162, 'f30f1bff-570c-4355-b01a-726ca1b698b9'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (24041, '11dbf2a7-e5af-4779-bf28-c30fa1c3bca9'), (13298, '756a1adb-b16f-4ce2-8aa7-c3fc09ffb37a'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *While existing large vision-language multimodal models focus on whole image understanding, there is a prominent gap in achieving region-specific comprehension. Current approaches that use textual coordinates or spatial encodings often fail to provide a user-friendly interface for visual prompting. To address this challenge, we introduce a novel multimodal model capable of decoding arbitrary visual prompts. This allows users to intuitively mark images and interact with the model using natural cues like a ""red bounding box"" or ""pointed arrow"". Our simple design directly overlays visual markers onto the RGB image, eliminating the need for complex region encodings, yet achieves state-of-the-art performance on region-understanding tasks like Visual7W, PointQA, and Visual Commonsense Reasoning benchmark. Furthermore, we present ViP-Bench, a comprehensive benchmark to assess the capability of models in understanding visual prompts across multiple dimensions, enabling future research in this
## Introduction

Recent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world 
problems are inherently multimodal - they involve several distinct forms of data, such as images and text. 
Visual-language models face the challenge of combining modalities so that they can open the door to a wide range of 
applications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text 
retrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, 
identify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well 
as in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, 
and assist in various domain-specific information retrieval applications.
Models that leverage a unified multi-modal architecture to fuse visual information into a language model (LM) for image-guided tasks show impressive capabilities. However, models that solely use the PrefixLM strategy can be limited in terms of application areas as they are mainly designed for image captioning or visual question-answering downstream tasks. For example, given an image of a group of people, we can query the image to write a description of the image (e.g., “A group of people is standing together in front of a building and smiling”) or query it with questions that require visual reasoning: “How many people are wearing red t-shirts?”. On the other hand, models that learn multi-modal representations or adopt hybrid approaches can be adapted for various other downstream tasks, such as object detection and image segmentation.

#### Frozen PrefixLM
Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents
can vary as the APIs or underlying models are prone to change.

</Tip>

Transformers version v4.29.0, building on the concept of *tools* and *agents*. You can play with in
[this colab](https://colab.research.google.com/drive/1c7MHD-T1forUPGcC_jlwsIptOzpG3hSj).

In short, it provides a natural language API on top of transformers: we define a set of curated tools and design an 
agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools, 
but we'll show you how the system can be extended easily to use any tool developed by the community.

Let's start with a few examples of what can be achieved with this new API. It is particularly powerful when it comes 
to multimodal tasks, so let's take it for a spin to generate images and read text out loud.

```py
agent.run(""Caption the following image"", image=image)
this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.
 
Here is the question: A research group is building a multimodal assistant capable of interpreting a user's spoken command, identifying relevant objects in an image, and providing a natural language response. What system design would best meet this goal?","The system design that would best meet this goal is a unified multi-modal architecture that leverages a language model (LM) to fuse visual information from the image and spoken command, and provides a natural language response. This design would allow the multimodal assistant to interpret the user's spoken command, identify relevant objects in the image, and provide a response that takes into account both the visual and linguistic inputs."
"A startup wants to build an AI-powered virtual assistant that can receive input from multiple sources, such as a user’s spoken request, an uploaded image, and a typed text prompt. The assistant needs to process this multimodal input and provide a unified response in natural language. What combination of models would provide the most comprehensive solution for this requirement?","[(5130, '0402365e-06b9-415e-9f92-dfe339eb3334'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (14879, 'd4697222-f29c-4e4d-9174-03a63c30c9a1'), (31266, '29004758-00c0-483a-bd99-3c5ff3805e24'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (31270, 'cfe0a8ee-b54d-4dee-9307-ca63d817a829'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (23108, 'ff267803-88f2-46f2-83c6-eb20b65886da'), (23116, '0b0e5942-415e-4dd2-bc16-fec335b19618'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (29303, '2c0c3db8-6373-4105-bed3-b77e6ce2c1b8'), (29304, '2b35efae-fcc5-4f38-b266-758e452bf98f'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (23179, 'ac432a2f-b472-4375-834a-a539ebc96d80'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (18630, '2e13d49e-075b-4861-a334-184fd220a9ca'), (29384, '94c3a5bd-0c90-40a5-ad5a-e2eb7ce295d1'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (3794, '447a3909-dff2-4d1d-8c81-5c52d46dc6da'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (3803, 'f0bab2c0-7345-4a53-bc07-5c845419f9d0'), (28389, '298bdf93-edee-48fb-94e3-1cf5585de1f2'), (28390, '39c5e061-d8dd-498b-a12f-8cc7f00ea6fd'), (14573, '519760cf-bd02-40ad-9c5d-470a8fa48b0d'), (25839, '4a92026f-215f-4a87-a52b-a7baf6a8a3bd'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (24834, 'be08fa7e-dc7d-48b4-ac0e-07f98d18fd69'), (18180, 'b1e9f263-6f41-44a0-9088-199c4a64f3ed'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (13597, 'e3ea9024-28d8-4975-82e5-02952ad42d17'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (5936, '76780529-3d90-4c76-89f9-0e62f207fda0'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (22337, 'dcd3d938-defe-4be3-b64f-3fd23ab4f8bf'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (21342, '43c34d2b-8a1a-4a72-8117-9d57107098c8'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (4976, '87b17c62-4296-4ca8-92e5-ea2d927a3025'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (21876, '19105ac4-8965-4312-b78a-ae73f24b4835'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (7043, 'd321757e-ad5b-443b-9fe7-0d08904bce4f'), (16264, '5d166a05-2d80-4201-ae2f-b71993797a24'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (7591, '833da8f3-7e22-40fc-b573-04af8220e4fd'), (15281, '71c204b4-8730-45e2-8557-ae7695079eb6'), (9139, '2af98149-42ee-4660-988d-a8696d5547ac'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (29626, '2f7c7b90-5297-4b10-b6e7-ee9cb0941fff'), (29631, '05e259fc-f013-438c-92a0-526db3b1673f'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (21445, '40b08046-3292-41a1-990b-69b63b4ff917'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (2512, '922c050f-9165-4d33-9529-8a72a7e9ed40'), (13267, 'd14a13b5-d0f7-4354-9963-a77a57948d2c'), (24020, '9ceff87b-6adb-4e98-ad01-dc489b9f3c39'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (25056, 'e28bab38-e712-4c1f-a714-e224f1104de5'), (24549, 'e159577d-48a3-4a97-b848-68e78c570951'), (29158, 'e4fbc982-e499-43ae-b82d-504a09c43430'), (29159, '27f1f229-944d-4f99-bf23-7acab0f2fec7'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (29161, '5c1a0615-2746-4f25-8cae-56a17b9d371d'), (23018, 'c153f31e-4b06-4783-8925-ddf8e7c1cc3d'), (29162, 'f30f1bff-570c-4355-b01a-726ca1b698b9'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (24041, '11dbf2a7-e5af-4779-bf28-c30fa1c3bca9'), (13298, '756a1adb-b16f-4ce2-8aa7-c3fc09ffb37a'), (16884, 'bc5ee849-d044-41ba-a614-68829cf338b4'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Conclusion

The new Hugging Face LLM Inference DLC enables customers to easily and securely deploy open-source LLMs on Amazon SageMaker. The easy-to-use API and deployment process allows customers to build scalable AI chatbots and virtual assistants with state-of-the-art models like Open Assistant. Overall, this new DLC is going to empower developers and businesses to leverage the latest advances in natural language generation.

---

Thanks for reading! If you have any questions, feel free to contact me on [Twitter](https://twitter.com/_philschmid) or [LinkedIn](https://www.linkedin.com/in/philipp-schmid-a6a2bb196/).
this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.
```

Putting this together, we can create a _multimodal_ chatbot with a textbox for a user to submit text and an file upload button to submit images / audio / video files. The rest of the code looks pretty much the same as before:

$code_chatbot_multimodal
$demo_chatbot_multimodal

And you're done! That's all the code you need to build an interface for your chatbot model. Finally, we'll end our Guide with some links to Chatbots that are running on Spaces so that you can get an idea of what else is possible:

- [project-baize/Baize-7B](https://huggingface.co/spaces/project-baize/Baize-7B): A stylized chatbot that allows you to stop generation as well as regenerate responses.
- [MAGAer13/mPLUG-Owl](https://huggingface.co/spaces/MAGAer13/mPLUG-Owl): A multimodal chatbot that allows you to upvote and downvote responses.
-->

# Automatic speech recognition

[[open-in-colab]]

<Youtube id=""TksaY_FDgnk""/>

Automatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa use ASR models to help users everyday, and there are many other useful user-facing applications like live captioning and note-taking during meetings.

This guide will show you how to:

1. Finetune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to transcribe audio to text.
2. Use your finetuned model for inference.

<Tip>
The task illustrated in this tutorial is supported by the following model architectures:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->
```python
system_prompt = """"""Below are a series of dialogues between various people and an AI technical assistant.
The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.
The assistant is happy to help with code questions and will do their best to understand exactly what is needed.
It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.
That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.

The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).
The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.

-----
 
Here is the question: A startup wants to build an AI-powered virtual assistant that can receive input from multiple sources, such as a user’s spoken request, an uploaded image, and a typed text prompt. The assistant needs to process this multimodal input and provide a unified response in natural language. What combination of models would provide the most comprehensive solution for this requirement?","Based on the context, the most comprehensive solution for the startup's AI-powered virtual assistant would involve combining multiple models, including:

1. Automatic Speech Recognition (ASR) model, such as Wav2Vec2, to transcribe spoken requests into text.
2. Computer Vision model, such as a multimodal transformer, to process and understand uploaded images.
3. Natural Language Processing (NLP) model, such as a large language model (LLM) like Open Assistant, to process and"
A company is developing a platform to automatically annotate large datasets of image-text pairs to train a new multimodal model. They need a solution that can provide annotations without manually labeling thousands of images. Which combination of models and strategies would be most effective?,"[(18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (22053, 'e4a8d350-c84c-4e46-b354-d2620d149413'), (22054, '1e8f1df9-fb5c-47e1-8123-1b45fb89dcb7'), (31272, 'e602f819-b5fe-4ff5-abe9-d1ffb9564448'), (23599, '828d8abe-906a-4010-8840-d1c779d76720'), (24638, 'e51d5c4d-d26c-4446-a519-54d0ae6188d9'), (22085, 'f6050fc4-183f-46c7-9934-5cdcc7d6aa7f'), (30789, '8b611d65-f397-46aa-bb7b-a9b95d2bd126'), (5706, 'b4f9bb4d-87d0-469c-9df4-62e754fde9ea'), (11344, '614a7745-0caa-4be3-9ac1-c8116d68a14b'), (31320, '34a40776-7620-4c6c-9874-7d7e8586a326'), (15971, 'cbf043f4-3ae2-4d7d-9522-d396d62e55c0'), (26231, 'b3e08d1a-4130-42bb-8c39-7e1831a663d6'), (7807, '174504e7-4feb-443c-a469-f695d0c3db35'), (20609, 'c19b3701-219d-48a8-ae4b-d6b0ad0e3b8c'), (18053, '0a511240-ce1a-4ec3-8b5e-81a1f02f9227'), (6287, '89e924a6-29d0-47d3-96a0-a0d16dfc1ce2'), (14997, '63b9b6ec-6baa-4247-a808-3fa1f5673d11'), (4769, 'fa5dceb6-2532-429c-985f-1004adae4612'), (31393, '61e9e601-16d9-4a9d-acbe-fefc7ae49fda'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (11437, 'bcf6c108-66d8-4c82-8e0b-90a0f98d8602'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (4804, '545adb55-d07e-4a08-b179-72243735bedb'), (28361, 'a1632814-841b-4502-890e-a2be7eed48cd'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (4818, '41438135-696e-4fdf-a4e5-0426334541d4'), (3795, '177a5242-8be1-496a-8e8f-b2ade38bc78e'), (3796, '23e48029-5d36-4631-8a93-df271cc3072e'), (3794, '447a3909-dff2-4d1d-8c81-5c52d46dc6da'), (18130, '675ef564-4c66-4cec-8e5e-604b2a37974d'), (25822, '219e62f2-d323-4433-aeee-026f4cbc4ee2'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (3814, 'b302f98e-4c5c-4ab2-bc8c-59f02397dcc5'), (3815, '7fa3513f-2329-4efb-bfb6-74160ee1ef45'), (3816, '552516c5-f437-4446-9858-762cd89427bb'), (3818, '3c7bc0e2-d872-4115-90ab-6b6ca2e0df72'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (13548, '379ae01a-4b63-44e8-b5be-55532d2a4cef'), (3822, 'b9bc39ef-056a-47c1-9e4b-02df6b6d4287'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (1777, '4ddc1fcc-541d-4553-b408-84494027daef'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (246, 'e4536be8-5f58-4ca9-83bc-0e5471694be5'), (19192, '2767df7c-ddd7-446b-a752-0a2d87a1e317'), (3837, '2b3f9754-3c83-4140-8261-a686b11fd43d'), (6910, '49d3e17d-b9ae-4481-9c7a-4c59ba0a76f5'), (1791, '6ece2b29-05a2-4bf8-b063-f7da0cbec2ff'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (18179, 'b3900da4-5b9e-412c-be5d-1d3eddc7d2ed'), (5895, '5503b4eb-706b-4c38-a5a9-fde0c005182c'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (5898, 'a084a22d-2a09-49ca-b08f-541f0667cadf'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (13596, '081b2062-475f-400f-a3a9-90bd8470f331'), (15645, '803865e3-d786-42b9-bc6d-5fff2e984c9b'), (295, 'dc796c09-e1e4-4ec3-b523-8dd03a59d67c'), (9005, '380fd6e8-0ceb-48bc-b733-0a33abcdc371'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (2879, '7d360b41-1577-4c00-97da-1afd541db6d5'), (15190, 'ceaa1f7b-9910-435e-bea1-3d7c8cfea4f2'), (15196, '9fffeb50-1ef0-4656-9949-1b099c72b9b5'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (15204, '43dc84e7-ee77-4550-b585-25b6bfa1b2a0'), (4966, '42e714b3-cfee-4118-a976-588eded30493'), (5480, 'e7ff7ba7-82d7-440d-bb24-3e84cb766756'), (1898, 'd0cd0bd7-ad19-4c35-a562-ddc9c9499a18'), (5482, '52c3aeba-e469-4020-af46-70be2703e308'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (25965, '1cda8dc4-5b16-484d-b64c-99fc5b9e9d79'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (9066, '4923e43c-42c8-4d22-b310-aa8bf7db8e52'), (4979, '8d06e416-3065-4f08-81bd-cf68c44d10d0'), (10100, 'f941110f-659f-4316-9593-5e643df7434a'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (9115, '6e9b0c0e-e214-417d-88df-e39b7be72323'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (18360, '943eb6c5-43db-4985-ac0f-6797aa22ae25'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (5057, 'dce6dd03-0ba8-48a7-8ec3-3c1aa3f60dd5'), (29123, '069dbe7c-7b27-40c1-9d01-30cfdd155494'), (2507, 'e3274c16-e60e-45a3-85d6-b3c1800f42e7'), (2509, '53bab419-4140-4137-aa56-67f5880cf609'), (10192, '1cdc71ba-92b9-4368-bac0-dc3321d9d7dc'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (476, '26757846-2270-4ba5-9a86-a0c4ddefc9fd'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (29158, 'e4fbc982-e499-43ae-b82d-504a09c43430'), (23015, 'd9177564-c31b-4316-8d08-cb422482dd8b'), (18409, 'b5b3ae41-c0a1-430d-b91a-36f1d9c35b7c'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (5106, '72e885e0-9759-4e30-8135-d426e863dcda')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Second, be flexible in your approaches. You may need to test several different methodologies to find the best solution. A great example is OCR, in which you can use an open-source product like Tesseract, a commercial solution like Cloud Vision API, or the OCR capability inside an open-source multimodal model like [Donut](https://huggingface.co/docs/transformers/model_doc/donut).

Third, start small with annotating data and pick your tools wisely. In our experience, you can get good results with several hundred documents. So start small and carefully evaluate your performance. Once you have narrowed your overall approach, you can begin to scale up the data to maximize your predictive accuracy. When annotating, remember that some tasks like layout identification and document extraction require identifying a specific region within a document. You will want to ensure your annotation tool supports bounding boxes.

</div>
    </div>
        </div>
*Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks that require reasoning over one or multiple images to generate a text. However, the datasets used to train these models have not been released, and the collection process has not been fully specified. We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELISC, we train an 80 billion parameters vision and language model on the dataset and obtain competitive performance on various multimodal benchmarks. We release the code to reproduce the dataset along with the dataset itself.*
*The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et
-->

# Image tasks with IDEFICS

[[open-in-colab]]

While individual tasks can be tackled by fine-tuning specialized models, an alternative approach 
that has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. 
For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. 
This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can 
solve image-text tasks with a large multimodal model called IDEFICS.
*Pre-trained representations are becoming crucial for many NLP and perception tasks. While representation learning in NLP has transitioned to training on raw text without human annotations, visual and vision-language representations still rely heavily on curated training datasets that are expensive or require expert knowledge. For vision applications, representations are mostly learned using datasets with explicit class labels such as ImageNet or OpenImages. For vision-language, popular datasets like Conceptual Captions, MSCOCO, or CLIP all involve a non-trivial data collection (and cleaning) process. This costly curation process limits the size of datasets and hence hinders the scaling of trained models. In this paper, we leverage a noisy dataset of over one billion image alt-text pairs, obtained without expensive filtering or post-processing steps in the Conceptual Captions dataset. A simple dual-encoder architecture learns to align visual and language representations of the image
 
Here is the question: A company is developing a platform to automatically annotate large datasets of image-text pairs to train a new multimodal model. They need a solution that can provide annotations without manually labeling thousands of images. Which combination of models and strategies would be most effective?","Based on the context, a combination of models and strategies that would be most effective for the company's platform to automatically annotate large datasets of image-text pairs without manually labeling thousands of images is:

1. Using a large multimodal model like IDEFICS, which can handle image-text tasks without fine-tuning.
2. Leveraging OCR (Optical Character Recognition) capabilities, such as Tesseract, Cloud Vision API, or the OCR capability inside an open-source multimodal model like Donut, to"
"A non-profit organization wants to develop an AI tool that provides language translations, recognizes elements within images, and processes speech commands—all for users in low-resource settings. The solution must be lightweight and capable of running on limited hardware. Which architecture best fits these needs?","[(27140, 'e9b6a701-73ea-4220-95c6-b35ffe9aab01'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb'), (5690, '85c878e3-f8a0-4571-85d1-e2953c33312b'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (1624, '9c6c249c-815c-471a-9fbc-0b8c0c71e013'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (9948, 'b697d0c2-6839-40e8-a71c-39e7c1aad991'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (9951, 'edbc9dd6-3fb0-418b-b3c7-594f12fa8fbe'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (25839, '4a92026f-215f-4a87-a52b-a7baf6a8a3bd'), (18672, '51d56682-3787-49c9-b694-c8ccc94be302'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (20775, 'cec80fb4-fe70-4ad9-ae05-37958cfd28f6'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (30519, 'e988d4c0-44ec-4a7f-a800-615f96ae0da2'), (23865, '7bccc135-749d-4acc-a179-12c10e7b97b3'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (15681, '8372d995-5746-4c26-b286-e7d360c03126'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (23883, '1577eb91-86d7-4470-a93b-2c01504d1574'), (21340, 'e885c5b2-97f8-4983-8a96-31896d5d9470'), (16221, 'c4f242f2-51f2-4b28-9f44-e725b8705bf6'), (7006, 'ca729c38-08a8-487f-85b8-173e988c8e43'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (24477, '108827d3-9954-417d-a4ca-4f275ac82faf'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (7591, '833da8f3-7e22-40fc-b573-04af8220e4fd'), (2481, '3068e4fe-91ec-4661-a887-e85b4483b2d0'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (19383, '44c8fa9d-7167-4105-9431-707875941cb8'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (29626, '2f7c7b90-5297-4b10-b6e7-ee9cb0941fff'), (31737, '1db4c46e-0450-4584-a576-b34a60bf4b35'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (13759, 'e3f7ce62-98c1-4212-90c1-fc43513f4104'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (2513, '46aef253-eecf-48f6-8c8a-26e8d72a48f7'), (13269, '4164fa88-7c62-4d47-b69e-268acd3b421a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13273, 'cecb7e66-40c7-4f84-8223-ac32217cb7b3'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13279, '04d9d58e-0ae5-4d9f-8d8d-074d533d6b42'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (13298, '756a1adb-b16f-4ce2-8aa7-c3fc09ffb37a'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (13310, 'c6400126-c261-4fd7-aa82-0a4a91b16a06'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: However, running large language models in such an environment can be pretty resource-intensive, especially if you are not able to use hardware acceleration.

### By using an API

Today, various cloud providers propose commercial APIs to use language models. Here is the current Hugging Face offering:

The free [Inference API](https://huggingface.co/docs/api-inference/index) to allow anyone to use small to medium-sized models from the community.

The more advanced and production-ready [Inference Endpoints API](https://huggingface.co/inference-endpoints) for those who require larger models or custom inference code.

These two APIs can be used from Node using the [Hugging Face Inference API library](https://www.npmjs.com/package/@huggingface/inference) on NPM.
## Supported model architectures and frameworks

We intend to support state-of-the-art transformer architectures for natural language processing, computer vision, and speech, such as BERT, DistilBERT, ROBERTA, Vision Transformer, CLIP, and Wav2Vec2.  Of course, generative AI models will be available too (e.g., GPT2, GPT-NeoX, T5, OPT, LLaMA), including our own BLOOM and StarCoder models. Lastly, we will also support more traditional computer vision models, like ResNet and ResNext, and deep learning recommendation models, a first for us.

We'll do our best to test and validate these models for PyTorch, TensorFlow, and ONNX Runtime for the above platforms. Please remember that not all models may be available for training and inference for all frameworks or all hardware platforms.

## The road ahead
```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
Keywords: NLP, Framework, LLM

## [spaCy](https://github.com/explosion/spaCy)

[spaCy](https://github.com/explosion/spaCy) is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. It offers support for transformers models through its third party package, spacy-transformers.

Keywords: NLP, Framework

## [speechbrain](https://github.com/speechbrain/speechbrain)

SpeechBrain is an open-source and all-in-one conversational AI toolkit based on PyTorch.
The goal is to create a single, flexible, and user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies, including systems for speech recognition, speaker recognition, speech enhancement, speech separation, language identification, multi-microphone signal processing, and many others.

Keywords: Conversational, Speech

## [skorch](https://github.com/skorch-dev/skorch)
1. **Model standardization**: the [Transformer](https://arxiv.org/abs/1706.03762) architecture is now the de facto standard for Deep Learning applications like Natural Language Processing, Computer Vision, Audio, Speech, and more. It’s now easier to build tools and workflows that perform well across many use cases.
2. **Pre-trained models**: [hundreds of thousands](https://huggingface.co/models) of pre-trained models are just a click away. You can discover and test them directly on [Hugging Face](https://huggingface.co) and quickly shortlist the promising ones for your projects.
 
Here is the question: A non-profit organization wants to develop an AI tool that provides language translations, recognizes elements within images, and processes speech commands—all for users in low-resource settings. The solution must be lightweight and capable of running on limited hardware. Which architecture best fits these needs?","Based on the context, the Transformer architecture best fits the needs of the non-profit organization. This is because Transformer architectures have been successful in low-resource settings, particularly in automatic speech recognition (ASR) where they can produce high-quality results with limited labeled data. Additionally, the context mentions that the organization wants to develop an AI tool that provides language translations, recognizes elements within images, and processes speech commands, which are all areas where Transformer architectures have been shown to be effective."
A university project involves building a tool that summarizes scanned academic papers while allowing users to search for specific topics within the document without converting images into text. What model or architecture combination would be most effective?,"[(19971, '74e97bfa-94d1-4c46-87ae-d3632e72ce0c'), (12806, '4c0a9c30-8879-486c-9296-b3ca819e496a'), (13843, '0f30d8f5-9b58-4310-9057-fb71315d4812'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (546, 'e58d61ec-a59a-4e2b-ad1f-3e6b6dcf046e'), (27181, '75d8f3c7-06fb-4203-ace8-42df0acee889'), (19503, '8227f7ee-a80a-4c29-b11a-108dd1adaef9'), (27701, '347e2791-75ba-4546-b407-9249948d59a0'), (28732, '26f566d8-d9ed-4f9e-a930-8b84075bd6a7'), (18498, '899a5b0c-bdb5-4f2f-add3-c045afaedafc'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (5201, '3da20c50-b422-445f-8e73-e024352bd397'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (25171, 'da15400c-495a-4c04-a4f7-bc0e717aa6a8'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (13397, 'f3059e40-078c-413e-951e-c8538d983279'), (3158, '1eb57504-ed08-48bb-8092-c53d5c64b0c1'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (3165, '02526a17-696d-4c06-80df-fba7911d02c9'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (3167, '3da1ff3e-1da3-42ef-97e6-51557bddcbd4'), (24185, 'cda9c820-ac91-44b9-95f4-e929fec88939'), (25730, '2939ee0f-e1f7-4024-8090-a9fad3f59c7f'), (23175, '4c35f6fb-b68d-4da2-b89b-ac708bf855fd'), (23176, 'bfa2caf1-e40f-4c59-abe7-3755e3f19ce0'), (2704, 'b989d8b6-c5d2-475d-8bba-d59296683e32'), (9371, '7b487395-e96b-4d33-9523-2231254a4ce4'), (671, '11cdddb6-4b04-4b7d-be04-b2efd538cdc5'), (682, 'a36d28c1-b770-43c9-b4da-cff067e67abf'), (26283, 'a543fc7a-b3ca-4a2c-ae17-9b65597e3a06'), (22192, '821510bb-7545-4884-8a39-a9c71f46f4ed'), (22193, 'ad609dab-0c2d-4e79-959b-b3aad7d53b8c'), (29367, '8954b5d9-4dfb-4d14-868c-e726946b5055'), (5818, 'f6e34f58-93d9-4622-8388-5fea6bba5b44'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (13552, '74848e30-fb78-4cca-afe2-1d027939cfbf'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (1782, 'ab7d79dc-b5d4-4148-a520-a2456cbf8605'), (250, 'efa79a0b-ef9a-44ee-8dbc-9370a0757031'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (2820, '2c4971bb-fdfd-4d37-bfed-ab6fc9e66a1e'), (3334, 'be61aaba-10f0-4007-a548-48ae184f321f'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (18189, 'aa952c6d-906b-4c02-aa39-449b12387d9a'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (17680, '3419d462-493a-4505-8f11-5ebae03c56fb'), (1297, '0110063d-229c-4fc0-a7c3-f875aa009eb2'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (4380, 'df05b5c1-c18c-4543-959b-d611c7f7474e'), (2852, '4070192e-9515-4222-9c1e-c07081d79086'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (9004, '6c6b94e1-2417-4739-a2d9-0d862c42db59'), (14655, 'c5adb19a-7ebd-4d40-a31d-55a6f8a1b978'), (24896, '436a35ce-6f8a-4969-b199-e5617a13da1a'), (24897, 'c1d9f722-9202-46a5-b323-cc018e65afcd'), (14656, '812d63e0-cb77-4e29-9bbd-da3da9e4ffa5'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (13645, 'd6f0fb65-1955-4b71-a55f-908de666c54a'), (3921, '6311b8eb-e903-45b5-bc7c-c24e69a96ce8'), (25426, '3641d00b-400c-4fbe-b7c3-37d2774fbf9a'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (9070, 'f6396021-41a6-42e7-b12c-de89e8c6a0ea'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (8578, 'b6835785-bd61-41a6-94d0-a59988d7384f'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (17811, '525ac724-ad6c-419a-a3a2-1715aa66fbd7'), (5013, '474395e3-99c5-4cbe-b336-48aedca62e63'), (10650, 'fc308f38-142a-487d-b5a6-90dd25de25f8'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (21926, '2efaa70b-a0e2-45a7-8b28-34f8a377f986'), (30635, '1e9f8414-71ee-406e-bfe0-2d26dd64e9ac'), (2481, '3068e4fe-91ec-4661-a887-e85b4483b2d0'), (2482, '596cae45-9565-42d2-9dab-675575bfea73'), (2484, '88e72060-5aa3-4fc8-b803-de03309ff9f7'), (2486, '2d8ba467-ab34-4a64-b9e7-badb0b503981'), (26551, '4260b707-a72b-4913-ae8d-4b9759d7bae5'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (2491, 'aefe200c-ad39-44b6-a12e-575c5fcbf47f'), (18883, '63ac8671-6aae-4033-ab01-b07f79c393c5'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (5061, '5ebc4085-43ae-4488-9355-8e1b17dfd415'), (2507, 'e3274c16-e60e-45a3-85d6-b3c1800f42e7'), (2508, '03e961b4-71a0-4d72-9397-e2eb7cc4a1c8'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (2514, '1aaf99d7-9574-4a42-9441-f4b7e0b43e08'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (18905, 'a3346d1d-f6c7-4b12-8e61-f1307d4f96d1'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (990, 'b4ada367-1eb9-41b2-82a7-0e6484234d54'), (13278, '69a325d1-04cf-43d4-8505-bcbf6037e685'), (13282, '65364d14-5bde-4e50-87e2-a33023028d02'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (13286, '7ee1ad39-4258-4a5d-81b7-e8d7746cca3a'), (7143, '87cba310-97b9-4ba4-b299-e136b8779dac'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (13305, '565bfc08-7c70-4b30-bfd3-ad92dddb3ad6'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: {/if}


In this section we'll take a look at how Transformer models can be used to condense long documents into summaries, a task known as _text summarization_. This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail.

<Youtube id=""yHnr5Dk2zCI""/>
-->

# Image tasks with IDEFICS

[[open-in-colab]]

While individual tasks can be tackled by fine-tuning specialized models, an alternative approach 
that has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. 
For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. 
This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can 
solve image-text tasks with a large multimodal model called IDEFICS.
You can use summarization models to summarize research papers which would enable researchers to easily pick papers for their reading list.
For more information about the Summarization task, check out the Hugging Face course.
The abstract from the paper is the following:

*Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.*
Welcome to the Hugging Face tasks series. In this video, we will take a look at the Text Summarization task.
Summarization is a task of producing a shorter version of a document while preserving the relevant and important information in the document.
Summarization models take a document to be summarized and output the summarized text.
This task is evaluated on the ROUGE score. It’s based on the overlap between the produced sequence and the correct sequence.
You might see this as ROUGE-1, which is the overlap of single tokens and ROUGE-2, the overlap of subsequent token pairs. ROUGE-N refers to the overlap of n subsequent tokens. Here we see an example of how overlaps take place.
An example dataset used for this task is called Extreme Summarization, XSUM. This dataset contains texts and their summarized versions.
You can use summarization models to summarize research papers which would enable researchers to easily pick papers for their reading list.
 
Here is the question: A university project involves building a tool that summarizes scanned academic papers while allowing users to search for specific topics within the document without converting images into text. What model or architecture combination would be most effective?","Based on the context, the most effective model or architecture combination for the university project would be a combination of Nougat (Neural Optical Understanding for Academic Documents) and a Text Summarization model, such as a Transformer model.

Nougat is a Visual Transformer model that performs Optical Character Recognition (OCR) tasks, which can process scanned academic papers and convert them into a markup language, preserving the semantic information, including mathematical expressions.

Once the scanned papers are converted into text, a Text Sum"
"A software company needs to create an interactive educational application where students can type in questions_hard, receive answers, and upload drawings that the model can describe in text. Additionally, it should be capable of giving the students feedback on their written answers. Which system is best for integrating all these features seamlessly?","[(14339, '1f2787f7-6463-4f7e-b260-e48c517120bd'), (28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (15885, '971fc97e-36dc-4975-85ab-37248b6d6a03'), (15887, '0b5578df-2a38-411f-8319-4905d2c88f8f'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (15888, '9d894a2a-217b-4732-9a14-b74028b1380b'), (15891, '048e8f83-8305-4e72-ab89-b2341c265168'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (12831, '5764140c-b9a7-4277-9997-e0a3ddb31e81'), (23073, 'c8719aa6-c675-4f8d-a578-bcb628162ea5'), (16417, '0d19a33d-48bf-4fe6-9bc5-fb1aa7a04d3d'), (29730, '376e0c48-a592-481d-bcca-2ae37ca91cb6'), (27181, '75d8f3c7-06fb-4203-ace8-42df0acee889'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (23105, 'afb0b12c-a6c7-42be-9cc2-c1d8d5f7a64a'), (8259, 'cc89df28-12bb-4df1-b61f-a6513e59da7f'), (18508, 'ef8ba6f7-764d-4ccf-968e-0e01a8044164'), (3665, 'b726f74a-c646-4050-9e74-cdaa45984963'), (30817, 'ce6db1b0-14b2-4588-be54-92b24764e498'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (10866, '838e0ceb-935d-4c4a-aa10-8eba58dac07d'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (23169, 'd0722de5-d7fc-4a8f-ae0b-edec0a069987'), (18063, '058eb7ba-85ee-4d4d-871a-99cf94566f62'), (19088, '07600b8f-8207-4d87-9537-f4690f009206'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (26265, 'cc640035-a212-471c-b79a-5b2b8e33d260'), (9371, '7b487395-e96b-4d33-9523-2231254a4ce4'), (18596, '9a34fe86-cc32-4354-b6d6-ac6ca438a3f3'), (26791, '40f4195c-9f9f-403e-aa4d-69df440b7b16'), (26792, '5b7554eb-f474-4a3b-8e42-e10d28fa87e9'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (15534, '9a99de56-1a64-4262-8265-64e006a774fb'), (15537, 'da4d5bf4-a8cf-4d24-aace-12e949a929d3'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (15540, 'a7378dbc-d7f5-4167-a200-01aad0625463'), (15543, '709e20d6-7763-4624-a403-ab88f87e09bb'), (5818, 'f6e34f58-93d9-4622-8388-5fea6bba5b44'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (10950, '33a38108-261f-4860-9e31-e4a67f25f159'), (10954, '92fe4159-8084-4e9c-9be2-4e72a5e11928'), (10961, 'af1d1a7c-9db0-4b22-9f43-c27d9b7ed477'), (2773, '06afc023-a422-4936-9dba-57a0537261aa'), (2778, '1a34f2d1-0e82-41a5-b2da-9f4167b7e153'), (28902, '4cf0d944-f2d5-4362-a6e5-6b48fe1f6e6a'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (15593, 'd41ecdb5-1abb-4bbd-a4dd-adf3baf6ee42'), (1259, 'f09c5c1c-251c-406e-becf-b8608581bf6a'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (16636, 'd4f03fe0-0ff4-4698-9cfd-302a5ca219e2'), (28414, '0e69ac2d-e27f-4301-b639-f7070dc7a084'), (16638, '6214eb65-8a1c-4818-ba45-434c36461a1f'), (16641, '29e13c06-e204-4a93-a6ac-03bf2fc934e4'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (25370, 'f41a2ad8-af3c-4af8-8be0-220b47e4ad5f'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (1320, 'd5a5263a-a570-474e-8d13-92d45d7a6e95'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (5936, '76780529-3d90-4c76-89f9-0e62f207fda0'), (19764, '3964a67e-698f-4796-91c3-29d446e12522'), (24896, '436a35ce-6f8a-4969-b199-e5617a13da1a'), (24897, 'c1d9f722-9202-46a5-b323-cc018e65afcd'), (25411, 'a7b93bb1-bbf5-4900-acc7-4e052e84d251'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (27466, 'bfb56804-3d91-45c6-b17f-8d15d5d3a201'), (23888, '5efd6f99-8edf-425d-bf42-fbca2966a6f1'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (5479, 'f00515c2-0d56-4aa1-aed4-0849d00bf783'), (7533, '069cd7ed-2bad-4f8c-97e1-b4607c6ca77b'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (24429, '26b9c06a-a5eb-4007-b994-01f3a216ab68'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (25976, 'ed4da672-db2b-4118-83a2-18d216904824'), (25977, '3e2a478d-229c-4b2d-959b-134ec30e88df'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (2939, '9a8aae27-bee1-4bdd-a022-a9d33a6fb987'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (7043, 'd321757e-ad5b-443b-9fe7-0d08904bce4f'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (12186, 'ef188ecf-f01a-4e10-b54a-3e46c2ee0e2d'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (3507, 'e3d2edd7-6062-4b3f-9ae2-63e8a07a4b96'), (22452, '3b2f2626-17d7-4664-ab7f-fb50dda63fdc'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (13755, 'c07affda-1794-4844-a683-eb253231c213'), (29631, '05e259fc-f013-438c-92a0-526db3b1673f'), (2500, '7b02e19d-828d-4f06-b75f-01aad79db47e'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (29646, '6f481f99-fd05-4db8-b5f6-e9ab526c0191'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (15838, 'a7cc273a-40da-4c92-a3f3-afaa3e71b4c5'), (13787, '3d112122-a48e-416f-9e0d-1d80749c9c5a'), (6623, '81050a29-495d-4fe8-a414-353f8fc378fc'), (15837, '78ef6dea-b85c-4549-ad3c-0ef7afe5cd99'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (7150, '5341abcc-4f85-4a5b-a60a-8fc3b0baa3d7'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (8178, '07d39aef-2fed-48f7-a91b-835d0a2cd1b6'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (9209, 'a08065d2-5c23-4cad-b6a7-59f5556fcf93'), (10750, '820e2551-3410-4210-a73c-32820cc67dc8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: But text datasets are just the beginning. Data is represented in richer formats like 🎵 audio, 📸 images, and even a combination of audio and text or image and text. Models trained on these datasets enable awesome applications like describing what is in an image or answering questions about an image.
--
title: ""Making a web app generator with open ML models""
thumbnail: /blog/assets/153_text_to_webapp/thumbnail.jpg
authors:
- user: jbilcke-hf
---

# Making a web app generator with open ML models


As more code generation models become publicly available, it is now possible to do text-to-web and even text-to-app in ways that we couldn't imagine before.

This tutorial presents a direct approach to AI web content generation by streaming and rendering the content all in one go.

**Try the live demo here!** →  **[Webapp Factory](https://huggingface.co/spaces/jbilcke-hf/webapp-factory-wizardcoder)**

![main_demo.gif](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/153_text_to_webapp/main_demo.gif)

## Using LLM in Node apps

While we usually think of Python for everything related to AI and ML, the web development community relies heavily on JavaScript and Node.

Here are some ways you can use large language models on this platform.
<Question
	choices={[
		{
			text: ""A simple implementation of a Transformer architecture from a research article"",
			explain: ""Although it is very educational to implement your own Transformer models from scratch, this is not what we're talking about here.""
		},
		{
			text: ""A compact and self-contained block of code that can be run without any external dependencies on private files or data"",
			explain: ""Correct! Minimal reproducible examples help the library's maintainers reproduce the problem you are having, so they can find solutions faster."",
			correct: true
		},
		{
			text: ""A screenshot of the Python traceback"",
			explain: ""Try again -- although it is tempting to include a screenshot of the error you are facing when filing an issue, this makes it very difficult for others to reproduce the error.""
		},
		{
			text: ""A notebook that contains your whole analysis, including parts unrelated to the error"",
### 9. Which of those types of models would you use for summarizing texts?

<Question
	choices={[
		{
			text: ""An encoder model"",
			explain: ""An encoder model generates a representation of the whole sentence that is better suited for tasks like classification.""
		},
		{
			text: ""A decoder model"",
			explain: ""Decoder models are good for generating output text (like summaries), but they don't have the ability to exploit a context like the whole text to summarize.""
		},
		{
			text: ""A sequence-to-sequence model"",
			explain: ""Sequence-to-sequence models are perfectly suited for a summarization task."",
			correct: true
		}
	]}
/>

### 10. Which of these types of models would you use for classifying text inputs according to certain labels?
Keywords: Training, Generation

## [diffgram](https://github.com/diffgram/diffgram)

Diffgram aims to integrate human supervision into platforms. We support your team programmatically changing the UI (Schema, layout, etc.) like in Streamlit. This means that you can collect and annotate timely data from users. In other words, we are the platform behind your platform, an integrated part of your application, to ship new & better AI products faster.

Keywords: Human supervision, Platform

## [ecco](https://github.com/jalammar/ecco)

Explain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).

Keywords: Model explainability

## [s3prl](https://github.com/s3prl/s3prl)
 
Here is the question: A software company needs to create an interactive educational application where students can type in questions_hard, receive answers, and upload drawings that the model can describe in text. Additionally, it should be capable of giving the students feedback on their written answers. Which system is best for integrating all these features seamlessly?","Based on the context, the best system for integrating all these features seamlessly would be a combination of models, including a sequence-to-sequence model for describing drawings, a language model for answering questions and providing feedback on written answers, and possibly an image model for processing uploaded drawings. However, none of the mentioned systems (diffgram, ecco, s3prl) seem to provide a complete solution for this specific use case."
"A non-profit organization wants to develop an AI tool that provides language translations, recognizes elements within images, and processes speech commands—all for users in low-resource settings. The solution must be lightweight and capable of running on limited hardware. Which architecture best fits these needs?","[(27140, 'e9b6a701-73ea-4220-95c6-b35ffe9aab01'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb'), (5690, '85c878e3-f8a0-4571-85d1-e2953c33312b'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (1624, '9c6c249c-815c-471a-9fbc-0b8c0c71e013'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (9946, 'a98fe8ef-9298-4cc5-be10-2d489843c7fc'), (9948, 'b697d0c2-6839-40e8-a71c-39e7c1aad991'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (9951, 'edbc9dd6-3fb0-418b-b3c7-594f12fa8fbe'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (25839, '4a92026f-215f-4a87-a52b-a7baf6a8a3bd'), (18672, '51d56682-3787-49c9-b694-c8ccc94be302'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (20775, 'cec80fb4-fe70-4ad9-ae05-37958cfd28f6'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (30519, 'e988d4c0-44ec-4a7f-a800-615f96ae0da2'), (23865, '7bccc135-749d-4acc-a179-12c10e7b97b3'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (15681, '8372d995-5746-4c26-b286-e7d360c03126'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (23883, '1577eb91-86d7-4470-a93b-2c01504d1574'), (21340, 'e885c5b2-97f8-4983-8a96-31896d5d9470'), (16221, 'c4f242f2-51f2-4b28-9f44-e725b8705bf6'), (7006, 'ca729c38-08a8-487f-85b8-173e988c8e43'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (24477, '108827d3-9954-417d-a4ca-4f275ac82faf'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (7591, '833da8f3-7e22-40fc-b573-04af8220e4fd'), (2481, '3068e4fe-91ec-4661-a887-e85b4483b2d0'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (19383, '44c8fa9d-7167-4105-9431-707875941cb8'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (29626, '2f7c7b90-5297-4b10-b6e7-ee9cb0941fff'), (31737, '1db4c46e-0450-4584-a576-b34a60bf4b35'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (13759, 'e3f7ce62-98c1-4212-90c1-fc43513f4104'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (2513, '46aef253-eecf-48f6-8c8a-26e8d72a48f7'), (13269, '4164fa88-7c62-4d47-b69e-268acd3b421a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13273, 'cecb7e66-40c7-4f84-8223-ac32217cb7b3'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13279, '04d9d58e-0ae5-4d9f-8d8d-074d533d6b42'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (13298, '756a1adb-b16f-4ce2-8aa7-c3fc09ffb37a'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (13310, 'c6400126-c261-4fd7-aa82-0a4a91b16a06'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: However, running large language models in such an environment can be pretty resource-intensive, especially if you are not able to use hardware acceleration.

### By using an API

Today, various cloud providers propose commercial APIs to use language models. Here is the current Hugging Face offering:

The free [Inference API](https://huggingface.co/docs/api-inference/index) to allow anyone to use small to medium-sized models from the community.

The more advanced and production-ready [Inference Endpoints API](https://huggingface.co/inference-endpoints) for those who require larger models or custom inference code.

These two APIs can be used from Node using the [Hugging Face Inference API library](https://www.npmjs.com/package/@huggingface/inference) on NPM.
## Supported model architectures and frameworks

We intend to support state-of-the-art transformer architectures for natural language processing, computer vision, and speech, such as BERT, DistilBERT, ROBERTA, Vision Transformer, CLIP, and Wav2Vec2.  Of course, generative AI models will be available too (e.g., GPT2, GPT-NeoX, T5, OPT, LLaMA), including our own BLOOM and StarCoder models. Lastly, we will also support more traditional computer vision models, like ResNet and ResNext, and deep learning recommendation models, a first for us.

We'll do our best to test and validate these models for PyTorch, TensorFlow, and ONNX Runtime for the above platforms. Please remember that not all models may be available for training and inference for all frameworks or all hardware platforms.

## The road ahead
```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
Keywords: NLP, Framework, LLM

## [spaCy](https://github.com/explosion/spaCy)

[spaCy](https://github.com/explosion/spaCy) is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. It offers support for transformers models through its third party package, spacy-transformers.

Keywords: NLP, Framework

## [speechbrain](https://github.com/speechbrain/speechbrain)

SpeechBrain is an open-source and all-in-one conversational AI toolkit based on PyTorch.
The goal is to create a single, flexible, and user-friendly toolkit that can be used to easily develop state-of-the-art speech technologies, including systems for speech recognition, speaker recognition, speech enhancement, speech separation, language identification, multi-microphone signal processing, and many others.

Keywords: Conversational, Speech

## [skorch](https://github.com/skorch-dev/skorch)
1. **Model standardization**: the [Transformer](https://arxiv.org/abs/1706.03762) architecture is now the de facto standard for Deep Learning applications like Natural Language Processing, Computer Vision, Audio, Speech, and more. It’s now easier to build tools and workflows that perform well across many use cases.
2. **Pre-trained models**: [hundreds of thousands](https://huggingface.co/models) of pre-trained models are just a click away. You can discover and test them directly on [Hugging Face](https://huggingface.co) and quickly shortlist the promising ones for your projects.
 
Here is the question: A non-profit organization wants to develop an AI tool that provides language translations, recognizes elements within images, and processes speech commands—all for users in low-resource settings. The solution must be lightweight and capable of running on limited hardware. Which architecture best fits these needs?","Based on the context, the Transformer architecture best fits the needs of the non-profit organization. This is because Transformer architectures have been successful in low-resource settings, particularly in automatic speech recognition (ASR) where they can produce high-quality results with limited labeled data. Additionally, the context mentions that the organization wants to support state-of-the-art transformer architectures for natural language processing, computer vision, and speech, which aligns with their requirements."
A university project involves building a tool that summarizes scanned academic papers while allowing users to search for specific topics within the document without converting images into text. What model or architecture combination would be most effective?,"[(19971, '74e97bfa-94d1-4c46-87ae-d3632e72ce0c'), (12806, '4c0a9c30-8879-486c-9296-b3ca819e496a'), (13843, '0f30d8f5-9b58-4310-9057-fb71315d4812'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (546, 'e58d61ec-a59a-4e2b-ad1f-3e6b6dcf046e'), (27181, '75d8f3c7-06fb-4203-ace8-42df0acee889'), (19503, '8227f7ee-a80a-4c29-b11a-108dd1adaef9'), (27701, '347e2791-75ba-4546-b407-9249948d59a0'), (28732, '26f566d8-d9ed-4f9e-a930-8b84075bd6a7'), (18498, '899a5b0c-bdb5-4f2f-add3-c045afaedafc'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (5201, '3da20c50-b422-445f-8e73-e024352bd397'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (25171, 'da15400c-495a-4c04-a4f7-bc0e717aa6a8'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (13397, 'f3059e40-078c-413e-951e-c8538d983279'), (3158, '1eb57504-ed08-48bb-8092-c53d5c64b0c1'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (3165, '02526a17-696d-4c06-80df-fba7911d02c9'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (3167, '3da1ff3e-1da3-42ef-97e6-51557bddcbd4'), (24185, 'cda9c820-ac91-44b9-95f4-e929fec88939'), (25730, '2939ee0f-e1f7-4024-8090-a9fad3f59c7f'), (23175, '4c35f6fb-b68d-4da2-b89b-ac708bf855fd'), (23176, 'bfa2caf1-e40f-4c59-abe7-3755e3f19ce0'), (2704, 'b989d8b6-c5d2-475d-8bba-d59296683e32'), (9371, '7b487395-e96b-4d33-9523-2231254a4ce4'), (671, '11cdddb6-4b04-4b7d-be04-b2efd538cdc5'), (682, 'a36d28c1-b770-43c9-b4da-cff067e67abf'), (26283, 'a543fc7a-b3ca-4a2c-ae17-9b65597e3a06'), (22192, '821510bb-7545-4884-8a39-a9c71f46f4ed'), (22193, 'ad609dab-0c2d-4e79-959b-b3aad7d53b8c'), (29367, '8954b5d9-4dfb-4d14-868c-e726946b5055'), (5818, 'f6e34f58-93d9-4622-8388-5fea6bba5b44'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (13552, '74848e30-fb78-4cca-afe2-1d027939cfbf'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (1782, 'ab7d79dc-b5d4-4148-a520-a2456cbf8605'), (250, 'efa79a0b-ef9a-44ee-8dbc-9370a0757031'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (2820, '2c4971bb-fdfd-4d37-bfed-ab6fc9e66a1e'), (3334, 'be61aaba-10f0-4007-a548-48ae184f321f'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (18189, 'aa952c6d-906b-4c02-aa39-449b12387d9a'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (17680, '3419d462-493a-4505-8f11-5ebae03c56fb'), (1297, '0110063d-229c-4fc0-a7c3-f875aa009eb2'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (4380, 'df05b5c1-c18c-4543-959b-d611c7f7474e'), (2852, '4070192e-9515-4222-9c1e-c07081d79086'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (9004, '6c6b94e1-2417-4739-a2d9-0d862c42db59'), (14655, 'c5adb19a-7ebd-4d40-a31d-55a6f8a1b978'), (24896, '436a35ce-6f8a-4969-b199-e5617a13da1a'), (24897, 'c1d9f722-9202-46a5-b323-cc018e65afcd'), (14656, '812d63e0-cb77-4e29-9bbd-da3da9e4ffa5'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (13645, 'd6f0fb65-1955-4b71-a55f-908de666c54a'), (3921, '6311b8eb-e903-45b5-bc7c-c24e69a96ce8'), (25426, '3641d00b-400c-4fbe-b7c3-37d2774fbf9a'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (9070, 'f6396021-41a6-42e7-b12c-de89e8c6a0ea'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (8578, 'b6835785-bd61-41a6-94d0-a59988d7384f'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (17811, '525ac724-ad6c-419a-a3a2-1715aa66fbd7'), (5013, '474395e3-99c5-4cbe-b336-48aedca62e63'), (10650, 'fc308f38-142a-487d-b5a6-90dd25de25f8'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (21926, '2efaa70b-a0e2-45a7-8b28-34f8a377f986'), (30635, '1e9f8414-71ee-406e-bfe0-2d26dd64e9ac'), (2481, '3068e4fe-91ec-4661-a887-e85b4483b2d0'), (2482, '596cae45-9565-42d2-9dab-675575bfea73'), (2484, '88e72060-5aa3-4fc8-b803-de03309ff9f7'), (2486, '2d8ba467-ab34-4a64-b9e7-badb0b503981'), (26551, '4260b707-a72b-4913-ae8d-4b9759d7bae5'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (2491, 'aefe200c-ad39-44b6-a12e-575c5fcbf47f'), (18883, '63ac8671-6aae-4033-ab01-b07f79c393c5'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (5061, '5ebc4085-43ae-4488-9355-8e1b17dfd415'), (2507, 'e3274c16-e60e-45a3-85d6-b3c1800f42e7'), (2508, '03e961b4-71a0-4d72-9397-e2eb7cc4a1c8'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (2514, '1aaf99d7-9574-4a42-9441-f4b7e0b43e08'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (18905, 'a3346d1d-f6c7-4b12-8e61-f1307d4f96d1'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (990, 'b4ada367-1eb9-41b2-82a7-0e6484234d54'), (13278, '69a325d1-04cf-43d4-8505-bcbf6037e685'), (13282, '65364d14-5bde-4e50-87e2-a33023028d02'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (13286, '7ee1ad39-4258-4a5d-81b7-e8d7746cca3a'), (7143, '87cba310-97b9-4ba4-b299-e136b8779dac'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (13305, '565bfc08-7c70-4b30-bfd3-ad92dddb3ad6'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: {/if}


In this section we'll take a look at how Transformer models can be used to condense long documents into summaries, a task known as _text summarization_. This is one of the most challenging NLP tasks as it requires a range of abilities, such as understanding long passages and generating coherent text that captures the main topics in a document. However, when done well, text summarization is a powerful tool that can speed up various business processes by relieving the burden of domain experts to read long documents in detail.

<Youtube id=""yHnr5Dk2zCI""/>
-->

# Image tasks with IDEFICS

[[open-in-colab]]

While individual tasks can be tackled by fine-tuning specialized models, an alternative approach 
that has recently emerged and gained popularity is to use large models for a diverse set of tasks without fine-tuning. 
For instance, large language models can handle such NLP tasks as summarization, translation, classification, and more. 
This approach is no longer limited to a single modality, such as text, and in this guide, we will illustrate how you can 
solve image-text tasks with a large multimodal model called IDEFICS.
You can use summarization models to summarize research papers which would enable researchers to easily pick papers for their reading list.
For more information about the Summarization task, check out the Hugging Face course.
The abstract from the paper is the following:

*Scientific knowledge is predominantly stored in books and scientific journals, often in the form of PDFs. However, the PDF format leads to a loss of semantic information, particularly for mathematical expressions. We propose Nougat (Neural Optical Understanding for Academic Documents), a Visual Transformer model that performs an Optical Character Recognition (OCR) task for processing scientific documents into a markup language, and demonstrate the effectiveness of our model on a new dataset of scientific documents. The proposed approach offers a promising solution to enhance the accessibility of scientific knowledge in the digital age, by bridging the gap between human-readable documents and machine-readable text. We release the models and code to accelerate future work on scientific text recognition.*
Welcome to the Hugging Face tasks series. In this video, we will take a look at the Text Summarization task.
Summarization is a task of producing a shorter version of a document while preserving the relevant and important information in the document.
Summarization models take a document to be summarized and output the summarized text.
This task is evaluated on the ROUGE score. It’s based on the overlap between the produced sequence and the correct sequence.
You might see this as ROUGE-1, which is the overlap of single tokens and ROUGE-2, the overlap of subsequent token pairs. ROUGE-N refers to the overlap of n subsequent tokens. Here we see an example of how overlaps take place.
An example dataset used for this task is called Extreme Summarization, XSUM. This dataset contains texts and their summarized versions.
You can use summarization models to summarize research papers which would enable researchers to easily pick papers for their reading list.
 
Here is the question: A university project involves building a tool that summarizes scanned academic papers while allowing users to search for specific topics within the document without converting images into text. What model or architecture combination would be most effective?","Based on the context, the most effective model or architecture combination for the university project would be a combination of Nougat (Neural Optical Understanding for Academic Documents) and a Text Summarization model, such as a Transformer model.

Nougat is a Visual Transformer model that performs Optical Character Recognition (OCR) tasks, which can process scanned academic papers and convert them into a markup language, preserving the semantic information, including mathematical expressions.

Once the scanned papers are converted into text, a Text Sum"
"A software company needs to create an interactive educational application where students can type in questions_hard, receive answers, and upload drawings that the model can describe in text. Additionally, it should be capable of giving the students feedback on their written answers. Which system is best for integrating all these features seamlessly?","[(14339, '1f2787f7-6463-4f7e-b260-e48c517120bd'), (28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (15885, '971fc97e-36dc-4975-85ab-37248b6d6a03'), (15887, '0b5578df-2a38-411f-8319-4905d2c88f8f'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (15888, '9d894a2a-217b-4732-9a14-b74028b1380b'), (15891, '048e8f83-8305-4e72-ab89-b2341c265168'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (12831, '5764140c-b9a7-4277-9997-e0a3ddb31e81'), (23073, 'c8719aa6-c675-4f8d-a578-bcb628162ea5'), (16417, '0d19a33d-48bf-4fe6-9bc5-fb1aa7a04d3d'), (29730, '376e0c48-a592-481d-bcca-2ae37ca91cb6'), (27181, '75d8f3c7-06fb-4203-ace8-42df0acee889'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (23105, 'afb0b12c-a6c7-42be-9cc2-c1d8d5f7a64a'), (8259, 'cc89df28-12bb-4df1-b61f-a6513e59da7f'), (18508, 'ef8ba6f7-764d-4ccf-968e-0e01a8044164'), (3665, 'b726f74a-c646-4050-9e74-cdaa45984963'), (30817, 'ce6db1b0-14b2-4588-be54-92b24764e498'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (10866, '838e0ceb-935d-4c4a-aa10-8eba58dac07d'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (23169, 'd0722de5-d7fc-4a8f-ae0b-edec0a069987'), (18063, '058eb7ba-85ee-4d4d-871a-99cf94566f62'), (19088, '07600b8f-8207-4d87-9537-f4690f009206'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (26265, 'cc640035-a212-471c-b79a-5b2b8e33d260'), (9371, '7b487395-e96b-4d33-9523-2231254a4ce4'), (18596, '9a34fe86-cc32-4354-b6d6-ac6ca438a3f3'), (26791, '40f4195c-9f9f-403e-aa4d-69df440b7b16'), (26792, '5b7554eb-f474-4a3b-8e42-e10d28fa87e9'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (15534, '9a99de56-1a64-4262-8265-64e006a774fb'), (15537, 'da4d5bf4-a8cf-4d24-aace-12e949a929d3'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (15540, 'a7378dbc-d7f5-4167-a200-01aad0625463'), (15543, '709e20d6-7763-4624-a403-ab88f87e09bb'), (5818, 'f6e34f58-93d9-4622-8388-5fea6bba5b44'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (10950, '33a38108-261f-4860-9e31-e4a67f25f159'), (10954, '92fe4159-8084-4e9c-9be2-4e72a5e11928'), (10961, 'af1d1a7c-9db0-4b22-9f43-c27d9b7ed477'), (2773, '06afc023-a422-4936-9dba-57a0537261aa'), (2778, '1a34f2d1-0e82-41a5-b2da-9f4167b7e153'), (28902, '4cf0d944-f2d5-4362-a6e5-6b48fe1f6e6a'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (15593, 'd41ecdb5-1abb-4bbd-a4dd-adf3baf6ee42'), (1259, 'f09c5c1c-251c-406e-becf-b8608581bf6a'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (16636, 'd4f03fe0-0ff4-4698-9cfd-302a5ca219e2'), (28414, '0e69ac2d-e27f-4301-b639-f7070dc7a084'), (16638, '6214eb65-8a1c-4818-ba45-434c36461a1f'), (16641, '29e13c06-e204-4a93-a6ac-03bf2fc934e4'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (25370, 'f41a2ad8-af3c-4af8-8be0-220b47e4ad5f'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (1320, 'd5a5263a-a570-474e-8d13-92d45d7a6e95'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (5936, '76780529-3d90-4c76-89f9-0e62f207fda0'), (19764, '3964a67e-698f-4796-91c3-29d446e12522'), (24896, '436a35ce-6f8a-4969-b199-e5617a13da1a'), (24897, 'c1d9f722-9202-46a5-b323-cc018e65afcd'), (25411, 'a7b93bb1-bbf5-4900-acc7-4e052e84d251'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (27466, 'bfb56804-3d91-45c6-b17f-8d15d5d3a201'), (23888, '5efd6f99-8edf-425d-bf42-fbca2966a6f1'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (5479, 'f00515c2-0d56-4aa1-aed4-0849d00bf783'), (7533, '069cd7ed-2bad-4f8c-97e1-b4607c6ca77b'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (24429, '26b9c06a-a5eb-4007-b994-01f3a216ab68'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (25976, 'ed4da672-db2b-4118-83a2-18d216904824'), (25977, '3e2a478d-229c-4b2d-959b-134ec30e88df'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (2939, '9a8aae27-bee1-4bdd-a022-a9d33a6fb987'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (7043, 'd321757e-ad5b-443b-9fe7-0d08904bce4f'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (12186, 'ef188ecf-f01a-4e10-b54a-3e46c2ee0e2d'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (3507, 'e3d2edd7-6062-4b3f-9ae2-63e8a07a4b96'), (22452, '3b2f2626-17d7-4664-ab7f-fb50dda63fdc'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (13755, 'c07affda-1794-4844-a683-eb253231c213'), (29631, '05e259fc-f013-438c-92a0-526db3b1673f'), (2500, '7b02e19d-828d-4f06-b75f-01aad79db47e'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (29646, '6f481f99-fd05-4db8-b5f6-e9ab526c0191'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (15838, 'a7cc273a-40da-4c92-a3f3-afaa3e71b4c5'), (13787, '3d112122-a48e-416f-9e0d-1d80749c9c5a'), (6623, '81050a29-495d-4fe8-a414-353f8fc378fc'), (15837, '78ef6dea-b85c-4549-ad3c-0ef7afe5cd99'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (7150, '5341abcc-4f85-4a5b-a60a-8fc3b0baa3d7'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (8178, '07d39aef-2fed-48f7-a91b-835d0a2cd1b6'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (9209, 'a08065d2-5c23-4cad-b6a7-59f5556fcf93'), (10750, '820e2551-3410-4210-a73c-32820cc67dc8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: But text datasets are just the beginning. Data is represented in richer formats like 🎵 audio, 📸 images, and even a combination of audio and text or image and text. Models trained on these datasets enable awesome applications like describing what is in an image or answering questions about an image.
--
title: ""Making a web app generator with open ML models""
thumbnail: /blog/assets/153_text_to_webapp/thumbnail.jpg
authors:
- user: jbilcke-hf
---

# Making a web app generator with open ML models


As more code generation models become publicly available, it is now possible to do text-to-web and even text-to-app in ways that we couldn't imagine before.

This tutorial presents a direct approach to AI web content generation by streaming and rendering the content all in one go.

**Try the live demo here!** →  **[Webapp Factory](https://huggingface.co/spaces/jbilcke-hf/webapp-factory-wizardcoder)**

![main_demo.gif](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/153_text_to_webapp/main_demo.gif)

## Using LLM in Node apps

While we usually think of Python for everything related to AI and ML, the web development community relies heavily on JavaScript and Node.

Here are some ways you can use large language models on this platform.
<Question
	choices={[
		{
			text: ""A simple implementation of a Transformer architecture from a research article"",
			explain: ""Although it is very educational to implement your own Transformer models from scratch, this is not what we're talking about here.""
		},
		{
			text: ""A compact and self-contained block of code that can be run without any external dependencies on private files or data"",
			explain: ""Correct! Minimal reproducible examples help the library's maintainers reproduce the problem you are having, so they can find solutions faster."",
			correct: true
		},
		{
			text: ""A screenshot of the Python traceback"",
			explain: ""Try again -- although it is tempting to include a screenshot of the error you are facing when filing an issue, this makes it very difficult for others to reproduce the error.""
		},
		{
			text: ""A notebook that contains your whole analysis, including parts unrelated to the error"",
### 9. Which of those types of models would you use for summarizing texts?

<Question
	choices={[
		{
			text: ""An encoder model"",
			explain: ""An encoder model generates a representation of the whole sentence that is better suited for tasks like classification.""
		},
		{
			text: ""A decoder model"",
			explain: ""Decoder models are good for generating output text (like summaries), but they don't have the ability to exploit a context like the whole text to summarize.""
		},
		{
			text: ""A sequence-to-sequence model"",
			explain: ""Sequence-to-sequence models are perfectly suited for a summarization task."",
			correct: true
		}
	]}
/>

### 10. Which of these types of models would you use for classifying text inputs according to certain labels?
Keywords: Training, Generation

## [diffgram](https://github.com/diffgram/diffgram)

Diffgram aims to integrate human supervision into platforms. We support your team programmatically changing the UI (Schema, layout, etc.) like in Streamlit. This means that you can collect and annotate timely data from users. In other words, we are the platform behind your platform, an integrated part of your application, to ship new & better AI products faster.

Keywords: Human supervision, Platform

## [ecco](https://github.com/jalammar/ecco)

Explain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).

Keywords: Model explainability

## [s3prl](https://github.com/s3prl/s3prl)
 
Here is the question: A software company needs to create an interactive educational application where students can type in questions_hard, receive answers, and upload drawings that the model can describe in text. Additionally, it should be capable of giving the students feedback on their written answers. Which system is best for integrating all these features seamlessly?","Based on the context, the best system for integrating all these features seamlessly would be a combination of models, including a sequence-to-sequence model for describing drawings, a language model for answering questions and providing feedback on written answers, and possibly an image model for processing uploaded drawings. However, none of the mentioned systems (diffgram, ecco, s3prl) seem to provide a complete solution for this specific use case."
"An engineering team wants to develop a GUI-based interactive application that performs real-time image captioning. They need a solution that integrates user input, processes visual data, and generates descriptive text outputs. Which approach is optimal?","[(16417, '0d19a33d-48bf-4fe6-9bc5-fb1aa7a04d3d'), (5158, '2a7fe1df-7c54-4ebf-9da2-15523efb3fd1'), (2599, '7d4174b8-3a4d-43aa-8ce3-ba91c0a96171'), (29739, 'f92eadb5-2529-4a38-b017-f923b288142d'), (23603, 'e47ea301-58b9-4bd5-a6bd-fdb97e05c99e'), (23604, 'c054e5cf-cd13-4e07-976e-1f353b0e2eb3'), (16437, '3bab27d1-d06e-4a70-9152-ebbd0f4688cf'), (3131, '7d45a093-cbb1-482d-99e1-edd22b730881'), (30779, 'f24c023d-2f94-4b7c-8fe0-05b008a8a512'), (4162, 'c2224011-1103-4648-b879-040d33b54371'), (17475, '496f81a1-3559-4e5f-ae4f-25727935e2df'), (16450, '1373652c-6ba1-4274-9e77-d49afc9643e5'), (30788, 'e6d02e40-3fcd-462e-b943-0494c7d97dc3'), (7247, '68104547-9369-454e-82e6-bcbca59632a2'), (4177, '7910ab76-b83b-4a14-81ca-70ab557adcb2'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (11359, '97e51060-d531-47a2-b356-c6b6f5af361b'), (31327, '2b86ceba-d1b2-479e-a3b5-8a44be247b66'), (6258, 'f4508226-fc84-4774-96a7-cf99c8ab31d9'), (29299, 'd1e259d9-9419-4cb5-b669-c348ece2b6a7'), (8822, 'dcb9c3d8-55ff-48bb-a9eb-9abe2064dd19'), (26231, 'b3e08d1a-4130-42bb-8c39-7e1831a663d6'), (26251, '1484cf27-a93d-4c64-ae8b-13e293779059'), (9355, '7030bf07-6313-4ac2-aa5e-8d15392ed1ed'), (9357, '113f20e4-5162-413f-b970-a10bcdcd4f5b'), (667, '466a4c47-d6a2-4439-8124-9da2dea99f39'), (15534, '9a99de56-1a64-4262-8265-64e006a774fb'), (27311, '22f8fab9-cce6-4c5d-83a2-0260ddb6ccb7'), (2744, 'd08c34b4-b542-46ca-836f-c74cdbd80723'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (15549, '13e1bbe7-bcb5-49d3-b0c7-2c304b856cd4'), (713, '8b5ec3e1-cdd5-42db-b75a-9b349891af2e'), (29387, 'b644abd4-1495-471d-bfd9-2e6979913f36'), (2773, '06afc023-a422-4936-9dba-57a0537261aa'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (30428, '655ab130-4f4d-4e78-b978-3eee7eaa35a9'), (30432, 'd7bd7c11-79c3-483f-b5a1-0f7d04e20c25'), (3813, 'a41ca3f9-93fe-41df-8aaf-f96e719d9394'), (17639, 'f2425f3a-f95e-4f81-a0a0-57cd2a6b7a5f'), (18676, '19268cbe-5f8f-44b7-9603-fd3d64a79dc2'), (30967, '98b738e9-7537-4605-b78b-badda0188376'), (28414, '0e69ac2d-e27f-4301-b639-f7070dc7a084'), (19198, '2f14174b-90fc-4309-9a76-dd0fdb10db1b'), (27392, '929fcc41-fb2f-4210-bb6c-0f21445e2323'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (25859, '10a4fa75-8d08-4363-8a27-77df8e121d12'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (18692, 'a4f174fa-f21a-4a5e-b591-ae5e955a9ef4'), (3336, '5fc77ef0-cad0-4c5a-8739-a86c9cc9924d'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (26890, 'eb1d3989-45ae-49cd-9855-626ee4e226a6'), (29455, 'e3a1b180-f8b4-43f1-ab4e-8b05a6cbdce3'), (24340, '55c69650-20f0-4cb8-8a9f-0fc7fc7ba515'), (15645, '803865e3-d786-42b9-bc6d-5fff2e984c9b'), (28961, '281e3608-f47e-41cb-a277-7ab6b6b8459f'), (28962, '3fa2440b-60fd-4b18-abde-ff042e1ce469'), (28967, 'e191c277-966d-4bb4-84f5-e1d1f1bbdf82'), (28971, 'e64d3049-f384-4139-980b-68db3a7abd18'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (14640, '87a0cf5c-337b-45da-9bb3-4134ebc38132'), (10545, '96a8e973-9ee0-4b63-98cd-8bf3f5b5693e'), (14646, '4172ca99-eb1c-4215-b4a1-8ce67dd00737'), (14650, 'd3cdd83c-ffda-4802-a1c9-58dcf9c03fa5'), (14651, '9dd67250-bddb-40f5-b107-f0f1c82384eb'), (22337, 'dcd3d938-defe-4be3-b64f-3fd23ab4f8bf'), (22338, '8b5e0130-0498-45cc-9a3f-01a9d65aecde'), (14659, '55294e81-ae21-447b-8d0e-2daca375922f'), (3908, '91ab7337-4628-49b3-9525-25b72621e00d'), (17742, 'dd667567-f375-4a71-b695-60d6babe2ab0'), (26456, 'd535e51c-0a9e-42c5-b745-c0b5f992860d'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (1883, '2ab39a22-5b35-4643-a74f-d3f146789c79'), (26464, '76994069-0112-43bd-b421-de7d71617a34'), (5479, 'f00515c2-0d56-4aa1-aed4-0849d00bf783'), (5480, 'e7ff7ba7-82d7-440d-bb24-3e84cb766756'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (25963, 'fd0bea05-61c9-4e1b-8a8b-5171642f2f0d'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (874, 'cee76382-516d-487b-82dd-3b0539a42b14'), (4976, '87b17c62-4296-4ca8-92e5-ea2d927a3025'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (5488, 'a73038e9-e75a-48fc-94d3-32b904cd99ca'), (4979, '8d06e416-3065-4f08-81bd-cf68c44d10d0'), (25971, '3edc1ae5-33a4-45d2-beb5-d5070fe39c09'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (25974, '479bc619-7e59-47ae-ab50-93666646cebb'), (20852, 'b516f3fa-d794-46a2-a94e-d391909f98d9'), (21876, '19105ac4-8965-4312-b78a-ae73f24b4835'), (4983, '044a8313-5785-4ad9-a376-13fc8c2b2cfa'), (4986, 'd1a0c49b-c4b7-4f64-b176-5c34266e3eaa'), (4987, 'ce213b91-abda-41aa-a9e4-1bc721650b4e'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (27009, '89a947e3-8f90-4586-bc97-09506d658c3f'), (31621, '833b507c-b23d-45b8-936f-a85896b6367d'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (17811, '525ac724-ad6c-419a-a3a2-1715aa66fbd7'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (22934, '61d6db46-73e3-4b28-ae7e-592a144a4fcf'), (2458, '9bde5b5e-0ca6-4591-8cb2-3e01d5a845fd'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (10652, '67b22c05-7dd6-4e0b-a38b-0dad7c44ec83'), (10654, '047dde45-f90b-4366-b2bf-77bde03d199c'), (15782, '333223e3-2579-464c-b4a5-538a962e71a6'), (10667, '99592116-e7df-436c-aefb-86283541f025'), (11691, 'dd3ff1e4-0aa8-42d6-b141-406f1df3f143'), (30140, '8c23977e-3757-47a7-ac11-a4c7cfb19d2a'), (4045, 'd183454f-b6ee-4379-8bbb-674c8b2b0384'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (17367, '96ac68a8-96d9-460e-98ee-59b8f1dddccd'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (6107, '0262e810-470d-4c1c-9177-7e6db2b47902'), (990, 'b4ada367-1eb9-41b2-82a7-0e6484234d54'), (13285, '5331eb01-d555-4c11-aff3-26cd6b4ea2a7'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Multimodal

Multimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. 

Although multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.

### Document question answering
## Introduction

Recent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world 
problems are inherently multimodal - they involve several distinct forms of data, such as images and text. 
Visual-language models face the challenge of combining modalities so that they can open the door to a wide range of 
applications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text 
retrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, 
identify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well 
as in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, 
and assist in various domain-specific information retrieval applications.
this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.
```

Now that you have the model loaded in one of the suggested ways, let's move on to exploring tasks that you can use IDEFICS for.

## Image captioning
Image captioning is the task of predicting a caption for a given image. A common application is to aid visually impaired 
people navigate through different situations, for instance, explore image content online. 

To illustrate the task, get an image to be captioned, e.g.:

<div class=""flex justify-center"">
     <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-im-captioning.jpg"" alt=""Image of a puppy in a flower bed""/>
</div>

Photo by [Hendo Wang](https://unsplash.com/@hendoo). 

IDEFICS accepts text and image prompts. However, to caption an image, you do not have to provide a text prompt to the 
model, only the preprocessed input image. Without a text prompt, the model will start generating text from the 
BOS (beginning-of-sequence) token thus creating a caption.
Another work, FLAVA, consists of an image encoder, a text encoder, and a multi-modal encoder to fuse and align the image and text representations for multi-modal reasoning, all of which are based on transformers. In order to achieve this, FLAVA uses a variety of pre-training objectives: MLM, ITM, as well as Masked-Image Modeling (MIM), and contrastive learning.
### 5) No Training 

Finally, various optimization strategies aim to bridge image and text representations using the pre-trained image and text models or adapt pre-trained multi-modal models to new downstream tasks without additional training. 

For example, [MaGiC](https://arxiv.org/abs/2205.02655) proposes iterative optimization through a pre-trained autoregressive language model to generate a caption for the input image. To do this, MaGiC computes a CLIP-based “Magic score” using CLIP embeddings of the generated tokens and the input image.
 
Here is the question: An engineering team wants to develop a GUI-based interactive application that performs real-time image captioning. They need a solution that integrates user input, processes visual data, and generates descriptive text outputs. Which approach is optimal?","Based on the context, the optimal approach for the engineering team would be to use a multimodal model, specifically a visual-language model, that can process visual data (images) and generate descriptive text outputs. This approach would allow the team to integrate user input, process visual data, and generate captions in real-time. The model would learn relationships between image embeddings and text embeddings, enabling it to generate accurate captions for the input images."
"A team is developing an AI-based system to recognize and categorize emotions in spoken language, text, and visual facial expressions simultaneously. Which architecture would be most suitable to handle all these data types together efficiently?","[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (27140, 'e9b6a701-73ea-4220-95c6-b35ffe9aab01'), (9746, '2f3e80a0-4cd8-4751-99ee-35c942d1600a'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (6676, '0d231cb5-d381-44c6-9669-18d5df65d10d'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (16924, '38fe9939-2e02-47cc-805a-7ffc8340c582'), (24638, 'e51d5c4d-d26c-4446-a519-54d0ae6188d9'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (1624, '9c6c249c-815c-471a-9fbc-0b8c0c71e013'), (12900, '92f35220-3801-458c-9b12-66da1a19560b'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (23165, 'e03ab7b7-1296-4899-a795-d440d6735a2a'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (29311, '9b91f0cd-07ae-49fd-b4bb-efe169c39b9d'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (20634, 'b26ceca9-16f3-491c-a4e2-ce69a77a514c'), (21662, '0f5799e1-42f3-4ab0-995b-ae36cdb5b334'), (5798, '82afd79e-521d-44e3-9227-219cc6761d5e'), (26792, '5b7554eb-f474-4a3b-8e42-e10d28fa87e9'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (26799, '66f91ffb-b603-4810-93f9-a6cecffcc5d5'), (26803, '04fdeb81-8327-4c5b-9319-19e01d421fa9'), (5813, 'e726e182-34fd-432d-a671-d06bce30a20f'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (23741, '22edb548-6c1f-4518-93c2-e8068b07306f'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (4829, '2327c019-21d7-4362-844c-4ec05792b517'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (7910, '1be74156-4842-4b41-801b-f9aa4a3ac425'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (26912, '392f0ec1-ac24-4784-9546-76751e9efacb'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (4389, '8e7af4c1-93d0-4b1d-9183-c54e4fe2e23b'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (26927, '9d006505-fbb8-4671-94ad-ba94926abfa8'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (15681, '8372d995-5746-4c26-b286-e7d360c03126'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (10063, 'e76ab115-f55d-4bdc-814d-75a5e65a5cde'), (10064, 'a1bd8633-1e97-43a5-8bd6-1989114b7293'), (10067, '5cc89bb1-7995-4eb1-944f-68108dc5ed63'), (3923, 'e87b7120-b4af-4c3f-9448-479a9d54a9d7'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (18778, 'e289b3a6-ef0f-457f-a553-843fabd2902c'), (9562, 'b75f1239-64b9-4c25-9106-2b9ff2961471'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (11126, '45de41bd-bd00-49ed-9bcb-eee70fdaff81'), (16247, 'f9b0a4f6-f562-4898-a506-df993d6c3af8'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (13189, 'b68e7cd9-6e04-4639-ba99-48f4feddd940'), (13191, '24ceecc3-86af-45ca-aa46-b15bed971eb6'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (393, 'ad4a0796-fd56-4daa-b08d-12376a15836c'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (2964, '2e5d286b-3b13-48e8-895f-ec256ffed18c'), (13310, 'c6400126-c261-4fd7-aa82-0a4a91b16a06'), (23453, '24bbe86e-9ba8-4256-b0fc-6707f0f6b1c0'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (7591, '833da8f3-7e22-40fc-b573-04af8220e4fd'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (31682, '20d03ac7-1e99-48dc-b324-0cbfc547d99d'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (2513, '46aef253-eecf-48f6-8c8a-26e8d72a48f7'), (13267, 'd14a13b5-d0f7-4354-9963-a77a57948d2c'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13269, '4164fa88-7c62-4d47-b69e-268acd3b421a'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (6615, '4b4eeb94-e396-4c99-a6ba-dd6d9503e03f'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (25056, 'e28bab38-e712-4c1f-a714-e224f1104de5'), (24549, 'e159577d-48a3-4a97-b848-68e78c570951'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (1514, 'd61b2641-9e09-4c4e-8e1e-72599ec17422'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (13298, '756a1adb-b16f-4ce2-8aa7-c3fc09ffb37a'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (1525, '240c574d-8ddb-4583-9436-0efc20f588f7'), (9205, '813fcccf-2923-465b-ab73-2cfe6cc521fd'), (25082, '6bb168f2-dd49-4a22-9096-30085a0e4f4e'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
1. **Model standardization**: the [Transformer](https://arxiv.org/abs/1706.03762) architecture is now the de facto standard for Deep Learning applications like Natural Language Processing, Computer Vision, Audio, Speech, and more. It’s now easier to build tools and workflows that perform well across many use cases.
2. **Pre-trained models**: [hundreds of thousands](https://huggingface.co/models) of pre-trained models are just a click away. You can discover and test them directly on [Hugging Face](https://huggingface.co) and quickly shortlist the promising ones for your projects.
approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms
discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon
the state of the art in 9 out of the 12 tasks studied.*
## 2. How to Use Pre-trained Sentiment Analysis Models with Python

Now that we have covered what sentiment analysis is, we are ready to play with some sentiment analysis models! 🎉

On the [Hugging Face Hub](https://huggingface.co/models), we are building the largest collection of models and datasets publicly available in order to democratize machine learning 🚀. In the Hub, you can find more than 27,000 models shared by the AI community with state-of-the-art performances on tasks such as sentiment analysis, object detection, text generation, speech recognition and more. The Hub is free to use and most models have a widget that allows to test them directly on your browser!

There are more than [215 sentiment analysis models](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment) publicly available on the Hub and integrating them with Python just takes 5 lines of code:
Keywords: Training, Generation

## [diffgram](https://github.com/diffgram/diffgram)

Diffgram aims to integrate human supervision into platforms. We support your team programmatically changing the UI (Schema, layout, etc.) like in Streamlit. This means that you can collect and annotate timely data from users. In other words, we are the platform behind your platform, an integrated part of your application, to ship new & better AI products faster.

Keywords: Human supervision, Platform

## [ecco](https://github.com/jalammar/ecco)

Explain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).

Keywords: Model explainability

## [s3prl](https://github.com/s3prl/s3prl)
 
Here is the question: A team is developing an AI-based system to recognize and categorize emotions in spoken language, text, and visual facial expressions simultaneously. Which architecture would be most suitable to handle all these data types together efficiently?","Based on the context, the most suitable architecture to handle spoken language, text, and visual facial expressions simultaneously would be a multimodal Transformer architecture. This is because multimodal models can use the text and visual information together, achieving high accuracy (up to 95%) in Document AI use cases. The Transformer architecture is now the de facto standard for Deep Learning applications, making it easier to build tools and workflows that perform well across many use cases."
"A company is creating a customer service bot that can visually recognize uploaded images, understand text descriptions, and answer questions_hard in multiple languages. Which combination of models will provide the best end-to-end system?","[(13833, '2c93d378-a8ac-4ebd-8f75-b5d44ac01949'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (23066, 'db5b29f6-c011-4064-83aa-0677c241c9dd'), (27181, '75d8f3c7-06fb-4203-ace8-42df0acee889'), (24638, 'e51d5c4d-d26c-4446-a519-54d0ae6188d9'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (22085, 'f6050fc4-183f-46c7-9934-5cdcc7d6aa7f'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (79, 'ea8f3e8a-329f-4ad3-a5ea-0790ed46c4df'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (26712, 'd358d8f4-0c23-4ef6-8abb-73508c5e1c85'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (1138, '1e450391-f267-4bbe-bb5b-26cb743419c2'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (23169, 'd0722de5-d7fc-4a8f-ae0b-edec0a069987'), (3716, '7409694b-c0ab-407c-8988-a97aae91288c'), (20614, '894861fd-6231-4cd8-ab6f-0c19d9c4b753'), (23179, 'ac432a2f-b472-4375-834a-a539ebc96d80'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (14997, '63b9b6ec-6baa-4247-a808-3fa1f5673d11'), (15007, '2b1cda5f-5455-4389-bc7d-d5632dec1db7'), (11427, '78f29a08-632a-4d65-ab83-d1a6d230d416'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (1197, '12a8d1ac-5cd2-427a-a6d2-e14c3591a137'), (30897, 'c4d781f5-c154-4a61-bdd9-76776461be08'), (13489, 'c8695082-6ef1-491b-b9c5-1c0f7cdad636'), (15540, 'a7378dbc-d7f5-4167-a200-01aad0625463'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (10439, 'fc4bdfc8-688a-4a92-ac69-a91f25ed98af'), (4809, 'abf2fc26-cdb5-482e-af69-0eb9e6c18794'), (3791, '57dded52-a5aa-4446-ab27-22e93dff8a17'), (24271, '2914fa3a-25c6-4c43-8d3a-8a05c36b4f1c'), (3793, '20be3864-830d-4112-ad66-401e5e96e6ea'), (3794, '447a3909-dff2-4d1d-8c81-5c52d46dc6da'), (1746, '49e8dce4-afe5-4268-9ab9-6f9809115e1e'), (3803, 'f0bab2c0-7345-4a53-bc07-5c845419f9d0'), (3808, '191dcb28-30e9-4713-9ef6-65929451ac28'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (3815, '7fa3513f-2329-4efb-bfb6-74160ee1ef45'), (3816, '552516c5-f437-4446-9858-762cd89427bb'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (5864, 'd78a2211-a6b0-499b-99f7-f9a245d142d5'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (3822, 'b9bc39ef-056a-47c1-9e4b-02df6b6d4287'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1782, 'ab7d79dc-b5d4-4148-a520-a2456cbf8605'), (3836, '624dbdb1-8d22-46bf-baa8-4d9305029369'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (18180, 'b1e9f263-6f41-44a0-9088-199c4a64f3ed'), (13597, 'e3ea9024-28d8-4975-82e5-02952ad42d17'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (27429, 'c118cc56-4278-4340-8254-7b555d464831'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (9005, '380fd6e8-0ceb-48bc-b733-0a33abcdc371'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (14655, 'c5adb19a-7ebd-4d40-a31d-55a6f8a1b978'), (24896, '436a35ce-6f8a-4969-b199-e5617a13da1a'), (24898, 'ca194eff-6753-4dc6-8cf8-0ec1a9a8ba00'), (836, '9ee86ea3-f33f-4cf5-9e92-0ad9009e33ba'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (3923, 'e87b7120-b4af-4c3f-9448-479a9d54a9d7'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (15208, 'ee05e977-f99f-4b73-a248-a9c10f227f38'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (19820, 'afe57101-972c-4335-9e54-bc403bb9c706'), (4976, '87b17c62-4296-4ca8-92e5-ea2d927a3025'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (3440, '51f99fe7-7751-4e98-914f-d073dd0afcf5'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (7037, '858c1821-a8c8-4596-a0bb-0945fea61441'), (15741, 'e52c1082-1985-46a1-afec-26024efe0f42'), (7043, 'd321757e-ad5b-443b-9fe7-0d08904bce4f'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (5013, '474395e3-99c5-4cbe-b336-48aedca62e63'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (2482, '596cae45-9565-42d2-9dab-675575bfea73'), (29626, '2f7c7b90-5297-4b10-b6e7-ee9cb0941fff'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (2512, '922c050f-9165-4d33-9529-8a72a7e9ed40'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (13267, 'd14a13b5-d0f7-4354-9963-a77a57948d2c'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (15838, 'a7cc273a-40da-4c92-a3f3-afaa3e71b4c5'), (25056, 'e28bab38-e712-4c1f-a714-e224f1104de5'), (1509, '845ab30e-6d6e-4d7e-891f-90ba4fad896a'), (1511, 'e32d4921-2d1e-4cc3-ab12-7559f17b253e'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (8176, '2cc6d0c0-3a2d-4ebc-8266-4d2ee6bf3300'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (9205, '813fcccf-2923-465b-ab73-2cfe6cc521fd'), (16884, 'bc5ee849-d044-41ba-a614-68829cf338b4'), (13310, 'c6400126-c261-4fd7-aa82-0a4a91b16a06')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Introduction

Recent years have seen rapid advancements in computer vision and natural language processing. Still, many real-world 
problems are inherently multimodal - they involve several distinct forms of data, such as images and text. 
Visual-language models face the challenge of combining modalities so that they can open the door to a wide range of 
applications. Some of the image-to-text tasks that visual language models can tackle include image captioning, image-text 
retrieval, and visual question answering. Image captioning can aid the visually impaired, create useful product descriptions, 
identify inappropriate content beyond text, and more. Image-text retrieval can be applied in multimodal search, as well 
as in applications such as autonomous driving. Visual question-answering can aid in education, enable multimodal chatbots, 
and assist in various domain-specific information retrieval applications.
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
📝 **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.<br>
🖼️ **Computer Vision**: image classification, object detection, and segmentation.<br>
🗣️ **Audio**: automatic speech recognition and audio classification.<br>
🐙 **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model's life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.
```python
system_prompt = """"""Below are a series of dialogues between various people and an AI technical assistant.
The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.
The assistant is happy to help with code questions and will do their best to understand exactly what is needed.
It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.
That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.

The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).
The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.

-----
```python
system_prompt = """"""Below are a series of dialogues between various people and an AI technical assistant.
The assistant tries to be helpful, polite, honest, sophisticated, emotionally aware, and humble but knowledgeable.
The assistant is happy to help with code questions and will do their best to understand exactly what is needed.
It also tries to avoid giving false or misleading information, and it caveats when it isn't entirely sure about the right answer.
That said, the assistant is practical really does its best, and doesn't let caution get too much in the way of being useful.

The Starcoder models are a series of 15.5B parameter models trained on 80+ programming languages from The Stack (v1.2) (excluding opt-out requests).
The model uses Multi Query Attention, was trained using the Fill-in-the-Middle objective, and with 8,192 tokens context window for a trillion tokens of heavily deduplicated data.

-----
 
Here is the question: A company is creating a customer service bot that can visually recognize uploaded images, understand text descriptions, and answer questions_hard in multiple languages. Which combination of models will provide the best end-to-end system?","Based on the context, the best combination of models for the customer service bot would be a multimodal model that combines computer vision and natural language processing capabilities. Specifically, a visual-language model that can perform tasks such as image captioning, image-text retrieval, and visual question answering would be suitable for this application. Additionally, the model should also support multiple languages to cater to the company's requirement."
"An AI developer needs to build a tool that can transcribe audio, provide a sentiment analysis of the transcription, and describe objects in a related image. Which architecture best serves this combination of tasks?","[(27138, 'd5395d30-8040-48fd-8ac9-6dd15693aa41'), (27139, '1fb6a93e-ef9e-4008-82fe-2b3612080749'), (27140, 'e9b6a701-73ea-4220-95c6-b35ffe9aab01'), (6663, '71a9b411-3ab3-47cf-af51-e0b46b769031'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (9746, '2f3e80a0-4cd8-4751-99ee-35c942d1600a'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (9767, '3d8f899f-4403-4a4e-b5ae-f298a872a576'), (12334, '84c0ef55-03b6-40a3-892b-1499c9a28ee7'), (16432, '8ba46343-ad57-45c5-a7ba-bafdab7de798'), (563, '8888f8ab-5bb8-43ca-a74d-538e18c7da44'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (16480, '5fa6ebda-4f62-436f-bb02-06d5905be580'), (16482, 'c12fcc3c-3d46-4627-8764-67c5a05a6ffd'), (12900, '92f35220-3801-458c-9b12-66da1a19560b'), (25706, '07429df9-ec51-44b8-bbaa-43e5e886bf4d'), (25708, '0a161716-5f3c-45c5-aac2-4eb719f697f4'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (26257, 'b712e3c0-a68f-46cd-b494-797df574501c'), (26258, '38fb9c5e-f765-47b2-8837-c867349f9b13'), (11924, '58ce9090-263d-43b9-89fe-3c39b6be47b9'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (8357, '97d35a76-603c-40f5-a52e-30445c36ca76'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (3788, 'b18f7303-1b4e-49a1-8b82-39c0712dafc6'), (20185, '26583e89-c8f7-4107-a950-11734ddb8fbf'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (22238, 'fdf15257-21ba-4e9e-88ee-49da089c276f'), (9949, 'a276a82b-2c04-476e-a74f-60c0d59cad39'), (18149, '4409c89c-0a82-4595-a9db-45bae640f8aa'), (31467, 'd5b8cb99-b5ce-4df9-8ab2-5c8ebaf7db26'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (30974, '67eb48a7-3c1c-4d73-8942-e82695fd2008'), (13567, 'a86f0554-9dab-4c51-8b3e-f56d256c6804'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (265, '019434fc-3472-4519-95d1-f4195d32be71'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (26912, '392f0ec1-ac24-4784-9546-76751e9efacb'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (26927, '9d006505-fbb8-4671-94ad-ba94926abfa8'), (26416, 'acbe64bd-7331-4375-a687-a5d7dad14f46'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (10035, '93602ddb-c2a9-4bf4-b078-e2d8cfe553de'), (6455, '06bc0eb6-84e1-4545-8230-72d52923a685'), (14657, 'd5903c56-f92a-451c-b8e1-27c7ee957814'), (26946, 'd51e55e0-4a83-4946-a69c-5343acec6844'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (15182, 'c8b31071-5a0b-4acc-b9c1-b9e83c4f1c51'), (3922, '6f3ca0aa-6e55-4248-8281-52387a0ce0a2'), (3923, 'e87b7120-b4af-4c3f-9448-479a9d54a9d7'), (3928, '86d27133-b3b2-4ece-9176-9414e6b357d9'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (7006, 'ca729c38-08a8-487f-85b8-173e988c8e43'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (9085, 'b637f259-3426-45e9-95da-6b478b336443'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (24976, '81b311bd-185b-473c-92bf-e1adf46d9090'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (21926, '2efaa70b-a0e2-45a7-8b28-34f8a377f986'), (5035, '83a015d9-5914-4516-b56d-1bd17eb3d24e'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (28608, 'b59f4ec4-0f6e-4fd6-8c10-26b619178c7a'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (18379, 'd9621ef0-f6e9-4a8c-af42-3ef4f6a9dd6a'), (14798, '192f0986-4830-4336-b8ee-124dd7bb6baf'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (7123, '5550e6f5-759d-4aef-9d97-83b53144cc4b'), (5076, 'c1283b7a-1c82-4041-9cbc-196ea5a5d475'), (13269, '4164fa88-7c62-4d47-b69e-268acd3b421a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (7124, 'dc82a8b4-22d9-469e-838d-0257251354eb'), (12755, 'feb6b427-5afe-4cdc-ad5a-8a7b88151f22'), (13270, '1aaa5343-eb30-43c9-a96d-2420cabff66f'), (6615, '4b4eeb94-e396-4c99-a6ba-dd6d9503e03f'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (13273, 'cecb7e66-40c7-4f84-8223-ac32217cb7b3'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13275, '474e3cd4-bb94-4e26-9148-43bd3cd5b124'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (6620, 'fd159ffd-c7ab-4076-a045-8e8fc0b3e6be'), (13279, '04d9d58e-0ae5-4d9f-8d8d-074d533d6b42'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (24549, 'e159577d-48a3-4a97-b848-68e78c570951'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (10736, 'f59b82dd-b3d6-496c-8c64-c3a5ece2586a'), (13298, '756a1adb-b16f-4ce2-8aa7-c3fc09ffb37a'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (15864, 'd2026adf-12ba-4ff7-b278-d6d626762410'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5'), (9727, '9ab6c2c7-8d11-4e8a-ae8e-edb23e697559')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Keywords: Recommender systems, AzureML

## [lama-cleaner](https://github.com/Sanster/lama-cleaner)

Image inpainting tool powered by Stable Diffusion. Remove any unwanted object, defect, people from your pictures or erase and replace anything on your pictures.

Keywords: inpainting, SD, Stable Diffusion

## [flair](https://github.com/flairNLP/flair)

FLAIR is a powerful PyTorch NLP framework, convering several important tasks: NER, sentiment-analysis, part-of-speech tagging, text and document embeddings, among other things.

Keywords: NLP, text embedding, document embedding, biomedical, NER, PoS, sentiment-analysis

## [mindsdb](https://github.com/mindsdb/mindsdb)

MindsDB is a low-code ML platform, which automates and integrates several ML frameworks into the data stack as ""AI Tables"" to streamline the integration of AI into applications, making it accessible to developers of all skill levels.

Keywords: Database, low-code, AI table
```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
-->

# Automatic speech recognition

[[open-in-colab]]

<Youtube id=""TksaY_FDgnk""/>

Automatic speech recognition (ASR) converts a speech signal to text, mapping a sequence of audio inputs to text outputs. Virtual assistants like Siri and Alexa use ASR models to help users everyday, and there are many other useful user-facing applications like live captioning and note-taking during meetings.

This guide will show you how to:

1. Finetune [Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base) on the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset to transcribe audio to text.
2. Use your finetuned model for inference.

<Tip>
The task illustrated in this tutorial is supported by the following model architectures:

<!--This tip is automatically generated by `make fix-copies`, do not fill manually!-->
1. **Model standardization**: the [Transformer](https://arxiv.org/abs/1706.03762) architecture is now the de facto standard for Deep Learning applications like Natural Language Processing, Computer Vision, Audio, Speech, and more. It’s now easier to build tools and workflows that perform well across many use cases.
2. **Pre-trained models**: [hundreds of thousands](https://huggingface.co/models) of pre-trained models are just a click away. You can discover and test them directly on [Hugging Face](https://huggingface.co) and quickly shortlist the promising ones for your projects.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Pipelines

The pipelines are a great and easy way to use models for inference. These pipelines are objects that abstract most of
the complex code from the library, offering a simple API dedicated to several tasks, including Named Entity
Recognition, Masked Language Modeling, Sentiment Analysis, Feature Extraction and Question Answering. See the
[task summary](../task_summary) for examples of use.

There are two categories of pipeline abstractions to be aware about:

- The [`pipeline`] which is the most powerful object encapsulating all other pipelines.
- Task-specific pipelines are available for [audio](#audio), [computer vision](#computer-vision), [natural language processing](#natural-language-processing), and [multimodal](#multimodal) tasks.

## The pipeline abstraction
 
Here is the question: An AI developer needs to build a tool that can transcribe audio, provide a sentiment analysis of the transcription, and describe objects in a related image. Which architecture best serves this combination of tasks?","Based on the context, the best architecture to serve this combination of tasks is the Transformer architecture. It is mentioned as the de facto standard for Deep Learning applications like Natural Language Processing, Computer Vision, Audio, and Speech, making it suitable for tasks such as automatic speech recognition, sentiment analysis, and image description."
"A research team is working on a healthcare application that monitors a patient's facial expressions, speech, and written notes to provide emotional state analysis. Which model or combination is best suited for such a multimodal task?","[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (5127, '7c40183d-5743-428d-950d-ba6485254290'), (19976, 'ec852d21-1da0-46da-8293-267f04f04242'), (11277, 'd63b243e-e507-4422-bb8e-4e253850afa0'), (18447, 'f9b99fc9-aef1-462d-b573-b679a65b6444'), (15889, 'aad2c393-9f11-40db-b577-03aba97e12a2'), (9746, '2f3e80a0-4cd8-4751-99ee-35c942d1600a'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (5145, '558a691d-e873-4114-b59e-9c86d4765f27'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (11344, '614a7745-0caa-4be3-9ac1-c8116d68a14b'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (12900, '92f35220-3801-458c-9b12-66da1a19560b'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (23170, '7bd8ea6d-3f6f-4be9-914c-08cac82a76d9'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (23172, 'def32509-da1d-4b97-ad0b-943987ec2382'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (31385, 'd6831873-e285-4151-a7f9-9a7531e8616e'), (5798, '82afd79e-521d-44e3-9227-219cc6761d5e'), (26791, '40f4195c-9f9f-403e-aa4d-69df440b7b16'), (26792, '5b7554eb-f474-4a3b-8e42-e10d28fa87e9'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (26803, '04fdeb81-8327-4c5b-9319-19e01d421fa9'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (23741, '22edb548-6c1f-4518-93c2-e8068b07306f'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (22232, 'dd25d0b2-8692-4801-a5cd-1f418ecf7c5b'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (4837, '9597f3a5-d361-4a35-9b94-2bd9b0c999e7'), (28389, '298bdf93-edee-48fb-94e3-1cf5585de1f2'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (18184, 'a53114c0-96cb-42ef-a44e-4a6ea9778ff1'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (13601, 'd2fcf63a-6647-41a1-8530-613f53b57a88'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (299, '7f2a2f1a-f503-44f9-b87a-583789044a1f'), (4395, 'aa175699-80b5-4acb-80b2-3efe5769946f'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (15681, '8372d995-5746-4c26-b286-e7d360c03126'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (29011, '43e39481-c24b-485f-a92f-42acfce7c8bb'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (9073, '814aa859-5cdc-4add-85c5-eefdb37f0d28'), (11126, '45de41bd-bd00-49ed-9bcb-eee70fdaff81'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (23427, '44ebafae-3114-46b5-bbfa-3f2d81df8eb8'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (23450, '1dea3594-bab7-49d3-b49c-dac62d5a6b39'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (23453, '24bbe86e-9ba8-4256-b0fc-6707f0f6b1c0'), (9120, '6e6140c9-95c6-4b7e-8af0-9dfe53916a31'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (15267, 'c893dfbc-198a-4578-9809-345571353a75'), (12198, 'c617332f-ef8f-477a-803c-147aed04f986'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (23466, '23f64c4f-e35a-45d8-adfc-44266623bd06'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (9136, 'deb71de9-3c9a-4e1a-a479-be2b8b223c09'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (18367, '6a784930-de82-4a31-a463-40d44eda4423'), (29120, '1a981651-ef42-455b-967b-748d9db50a1f'), (9154, '01796b9a-d106-4ba2-9e71-8d3f585eb19d'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (5064, '442cd4f9-dd4a-42f0-bdbd-31fcb1c45bd1'), (2507, 'e3274c16-e60e-45a3-85d6-b3c1800f42e7'), (14798, '192f0986-4830-4336-b8ee-124dd7bb6baf'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (25056, 'e28bab38-e712-4c1f-a714-e224f1104de5'), (24549, 'e159577d-48a3-4a97-b848-68e78c570951'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (24554, '14ce3ca0-d60b-4342-b184-cf0578275b03'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (18414, '4bfd7618-6567-411a-9d84-140a5555b25d'), (13298, '756a1adb-b16f-4ce2-8aa7-c3fc09ffb37a'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (9715, 'd0e34992-0dc6-4492-9a30-56cbb5929ad8'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (5111, 'e87c80c3-d7d3-4478-88c3-e8dc5a994da2'), (6136, '818cf127-78a6-43ae-aad7-568810131041'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Multimodal

Multimodal tasks require a model to process multiple data modalities (text, image, audio, video) to solve a particular problem. Image captioning is an example of a multimodal task where the model takes an image as input and outputs a sequence of text describing the image or some properties of the image. 

Although multimodal models work with different data types or modalities, internally, the preprocessing steps help the model convert all the data types into embeddings (vectors or list of numbers that holds meaningful information about the data). For a task like image captioning, the model learns relationships between image embeddings and text embeddings.

### Document question answering
and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn
dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing
failure cases of our models.*
and code publicly available. Human evaluations show our best models are superior to existing approaches in multi-turn
dialogue in terms of engagingness and humanness measurements. We then discuss the limitations of this work by analyzing
failure cases of our models.*
--
title: ""A Dive into Vision-Language Models""
thumbnail: /blog//assets/128_vision_language_pretraining/thumbnail.png
authors:
- user: adirik
- user: sayakpaul
---

# A Dive into Vision-Language Models


Human learning is inherently multi-modal as jointly leveraging multiple senses helps us understand and analyze new information better. Unsurprisingly, recent advances in multi-modal learning take inspiration from the effectiveness of this process to create models that can process and link information using various modalities such as image, video, text, audio, body gestures, facial expressions, and physiological signals.
# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft’s ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face’s [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
 
Here is the question: A research team is working on a healthcare application that monitors a patient's facial expressions, speech, and written notes to provide emotional state analysis. Which model or combination is best suited for such a multimodal task?","Based on the context, a Vision-Language Model is best suited for the multimodal task of analyzing a patient's facial expressions, speech, and written notes to provide emotional state analysis. This is because Vision-Language Models are designed to process and link information from multiple modalities such as image, video, text, audio, and physiological signals, which aligns with the requirements of the healthcare application."
"When training a PEFT model with LoRA for a large language model that will be used for code generation and deployed through the Hugging Face Hub, which sequence of steps would result in the most efficient and effective deployment?","[(19973, '1275328e-c988-4ac4-85e8-28d97d9f1b75'), (21016, '3aee5e59-cf0a-42c9-b8fd-c5c8beff8ab8'), (21018, 'af28f099-cc45-41ae-a508-e4b144566557'), (21019, 'a38537ee-e27c-411c-b016-dcab7f8692c7'), (20508, '9240559d-c734-4421-8c36-e7b381e3596a'), (20509, '86fed924-5f63-4fcf-9ee3-7b8a6be59019'), (20000, '99cbdd98-6401-42b2-862f-4784d4f44e11'), (19489, '43505cd9-a764-4a3f-8290-b2ce01a4ab4b'), (19490, 'cc210326-3354-4e4b-84e0-1b6012b70b59'), (19493, 'd624476e-cbf5-43b6-b1e2-c83695c6d542'), (19495, '9bc0e727-9833-45e0-b88a-2420238c4491'), (19498, '4c5bef6d-5ea1-4100-b60b-fb08f5701bb2'), (17963, '699f67ac-0905-463c-9bf2-f788ccb327c4'), (23085, '12808357-580d-40e9-bd76-8b364e7fa8c8'), (24622, '41e20e96-6443-4e6c-a4d1-6cafc9ca0c6f'), (24621, 'ecbea3c5-8302-47b3-9b02-5b3712f8a95c'), (26673, 'acb0c731-b7d9-4884-8760-09bd1882d5a6'), (23093, '3ccab225-736a-475f-8839-556e8e060ff1'), (31286, '7f0304e3-b1bd-4e3b-beff-8dbfaad84e8e'), (31287, '0e8c4a1a-3f5e-4050-a7c0-3858a27d9d95'), (8765, '4b9d795d-1730-4540-a444-c46a34a88601'), (23102, '1c12db1f-9e07-4a0b-81f2-e7bc5d807c67'), (8779, 'db7a97ae-b320-45b3-b009-2b3d990872bd'), (21071, 'b4345c35-9aac-4c1a-b383-bbf3659388ea'), (21072, 'b89dacaa-1d44-4bfc-9020-81c56b5c8539'), (3178, '8bafdc01-c46c-4dba-8c96-efe6086ccd18'), (4205, 'c62c8c1c-0bb7-4ad7-bfce-7cd1161ad7a7'), (4718, '785354e5-eef4-4b92-ad9e-77c0b9aae06f'), (6768, '8898d7a0-b088-46fa-947b-e4d751b10926'), (26745, '509a2418-a582-44fa-9c23-427ca2329092'), (6779, 'ab08386a-9279-412f-8534-dfc9cd310a0d'), (6780, '6a86b3f3-36e4-4a0d-94c9-9d4d55c1a9cc'), (6784, '4521292e-7fd9-4907-86ec-762a7f26f9b7'), (6787, 'bf27eed4-2ffe-4237-9812-088c33404198'), (11908, '214cfd21-9476-4feb-bbfc-f9ab9d083992'), (11917, '65aab8d9-81bf-4e3e-9461-23fab2f48652'), (16526, '5fbb6e37-9683-46a8-986d-f17c0a0e9811'), (3731, '8bd6343e-272e-4480-9171-ac5ac043ab85'), (25751, '48104442-d07e-4561-93b2-4d581b29ee16'), (3737, '515efb7f-1ab1-41a2-89e5-5c66253070fa'), (3743, '6bb0bafb-4155-4cf8-8c42-aba001b61cd4'), (23209, '4492fb38-aa38-435a-b755-49ab49b50780'), (25773, '55367b69-54d4-4569-937e-a5283cbe5766'), (30385, '78bfdd40-8490-4b09-9c8f-452925f733af'), (29875, 'c2b5fd90-5fd2-4f76-99d3-867085662a98'), (30392, '46096e11-0c19-408f-be8e-bf3b9aa0c761'), (29881, 'e885b130-b98f-4764-aa32-ef8f75958911'), (30393, '55b00024-7156-47f0-9b24-0469fd804c45'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (29883, 'abf5083a-fea7-4ddf-a8a8-aa78b4c12ac6'), (29886, 'ad8e7c8a-7256-4c03-ac9c-fa7b7e149669'), (9411, 'b589c787-7098-4fa2-9fa8-8a5c3eaa5d09'), (9415, '78661659-8f11-4005-800b-e1d23d747852'), (10963, 'f47171e5-5fc8-4d52-9e0f-5d9033b1dc5f'), (7892, 'dc066a8c-b115-47a5-9575-d04237490d73'), (9429, 'f9b10760-2b6c-4577-999d-9ce25869b8f0'), (9431, '185997ab-1d50-4fe7-88c0-896da8ec8036'), (5852, '1098ab3c-f1b5-43b5-9f22-a6baa0cad8ae'), (5853, 'd202356d-e2f5-403b-af93-1cb0e3362e30'), (5861, 'd4d34dac-93b3-422f-a295-6af7b09c3588'), (30575, 'cfbeccde-beb7-4f12-b6f9-5de18b092d57'), (13045, '808eb4d2-34e6-491e-9add-de85791c82e1'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (23812, '98499772-c9a4-47d7-b198-bd18ff20d2d5'), (23813, 'e529e90a-7751-49cc-8418-262a566b79d9'), (3866, '45636a2d-f101-4a6e-83b0-d6ac592363f7'), (3870, '2edf0e5d-7599-4552-851f-8a39a8cdb76a'), (6943, 'ad8c7727-69f8-472d-b29c-cffc054a6777'), (11057, '1bb44eee-a995-4bfb-9624-5083de5e8a77'), (1332, '5eb0c671-35be-48ed-b5b3-3592a301dc84'), (1333, '9a940258-3c13-4756-9968-8b6bfb7cce89'), (8505, 'fa1d4c6f-2910-4cba-b78a-191386e0bd13'), (1338, '2a6b1b1f-465e-47b6-99b0-23743c336115'), (30523, 'e801a7e2-9c3d-4d3e-a2a9-95e64422f62c'), (30522, 'e81e4fe1-a6f7-4521-8763-f681cbd76ea1'), (30524, '23cbfd50-64f3-4645-9ec2-6780300f13b9'), (30528, '69aa4012-1b33-4373-af24-1d94ec8dd03d'), (30532, '24781ef0-dad2-4227-9968-d0f98d2d8d2e'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (30539, 'f9862907-c8f9-48be-9509-a7fb056a761b'), (20301, 'c269188c-f9f3-4358-b07a-826d2e8b56fc'), (30541, '66e657f3-787a-43f4-a2ca-ecc504697610'), (22351, 'de4780f5-753f-4d0a-8175-6e44c6f76bb7'), (30546, '76a2065f-10de-44f8-b8f2-b60455bedfe1'), (30547, '835888ee-c5dc-4746-9126-964720605e2e'), (30553, '2001ff24-24db-4f42-9a44-aa56d3630588'), (362, 'c71ab6aa-a6c8-40c7-a3a7-d837978a2e91'), (19819, '9a49e514-83b9-47b2-acdf-6987ea96d854'), (19820, 'afe57101-972c-4335-9e54-bc403bb9c706'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (26989, 'edc86974-3282-4af0-92f9-001e1a08f7ab'), (30574, 'a5e2fe61-faba-426a-b912-8dffe5299453'), (30576, '76618c07-7a1a-4ffb-bc13-6da43eb286d5'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (16754, 'b54dbe1f-d7e0-42bd-82c4-b154977da6ca'), (30579, '9ca0751a-33ff-4f8b-a7ee-49343630ad20'), (30580, '191dfe6d-6311-4aa0-a494-3f0ffb363a45'), (30581, '7b64f060-a1f9-471c-8d39-0cd77f18b180'), (16757, 'a5ccf37a-f48e-4df0-b21d-905a2f29253a'), (30578, '4dae799c-0e35-447a-8126-3f98ef267e33'), (30584, '72eded47-82e5-4d94-aff8-ea0caf3f4dc7'), (4475, 'a93fa4e5-6ae7-4d7a-befe-e29f4fed3c27'), (19838, 'e41bea64-5860-44ed-b081-b35e1b1f638c'), (13703, 'f26b10f4-11d0-4011-b019-5bd1a4f8b1dc'), (21898, 'a82df317-1d66-44d8-9ac0-6c69d868fcab'), (17808, 'f1afa6ca-1381-400c-bebd-2d6accac3c86'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (7079, '460a428b-b477-421e-a0e7-dd0744483f86'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (7602, 'b7efc55c-1172-4572-9d36-6761b72bbbb8'), (8651, 'bb7fe9c5-8a86-447b-a920-08fcbf3eb1ce'), (24018, 'd6e6cdbb-04b3-4290-ab8d-acd590e90d91'), (4062, '00961eed-09fd-4de4-9f2b-404113b1e7e2'), (25569, '450af336-adc8-4d76-ab71-18a7855ea98a'), (25570, '4a1b2435-9337-4e49-8c76-fca1ebacc6e1'), (4069, 'b1aef96e-f908-4955-9d2f-adc425d9c095'), (4070, 'fe807f3d-b428-456c-990e-8f9208dc8217'), (25580, '9163ddb3-f276-40f2-8ccf-7d55bafd5ffd'), (25583, 'e11c06ae-8388-4cb9-80dc-1cb6f5cd02f9'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

Next, import all the necessary libraries:

- 🤗 Transformers for loading the `intfloat/e5-large-v2` model and tokenizer
- 🤗 Accelerate for the training loop
- 🤗 Datasets for loading and preparing the `smangrul/amazon_esci` dataset for training and inference
- 🤗 Evaluate for evaluating the model's performance
- 🤗 PEFT for setting up the LoRA configuration and creating the PEFT model
- 🤗 huggingface_hub for uploading the trained model to HF hub
- hnswlib for creating the search index and doing fast approximate nearest neighbor search

<Tip>

It is assumed that PyTorch with CUDA support is already installed.

</Tip>

## Train

Launch the training script with `accelerate launch` and pass your hyperparameters along with the `--use_peft` argument to enable LoRA.

This guide uses the following [`LoraConfig`]:
```

So we can see that apart from the new LoRA weights that were added, only the last layer was updated. Since the LoRA weights and the last layer have comparitively few parameters, this gives us a big boost in efficiency.

## Sharing the model through Hugging Face Hub

### Pushing the model to HF Hub

With the `peft` model, it is also very easy to push a model the Hugging Face Hub. Below, we demonstrate how it works. It is assumed that you have a valid Hugging Face account and are logged in:


```python
user = ""BenjaminB""  # put your user name here
model_name = ""peft-lora-with-custom-model""
model_id = f""{user}/{model_name}""
```


```python
peft_model.push_to_hub(model_id);
```

Both methods only save the extra PEFT weights that were trained, meaning it is super efficient to store, transfer, and load. For example, this [facebook/opt-350m](https://huggingface.co/ybelkada/opt-350m-lora) model trained with LoRA only contains two files: `adapter_config.json` and `adapter_model.safetensors`. The `adapter_model.safetensors` file is just 6.3MB!

<div class=""flex flex-col justify-center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/PEFT-hub-screenshot.png""/>
  <figcaption class=""text-center"">The adapter weights for a opt-350m model stored on the Hub are only ~6MB compared to the full size of the model weights, which can be ~700MB.</figcaption>
</div>

## Inference

<Tip>

Take a look at the [AutoPeftModel](package_reference/auto_class) API reference for a complete list of available `AutoPeftModel` classes.

</Tip>
Using PEFT at Hugging Face

🤗 [Parameter-Efficient Fine-Tuning (PEFT)](https://huggingface.co/docs/peft/index) is a library for efficiently adapting pre-trained language models to various downstream applications without fine-tuning all the model’s parameters. 

## Exploring PEFT on the Hub

You can find PEFT models by filtering at the left of the [models page](https://huggingface.co/models?library=peft&sort=trending).


## Installation

To get started, you can check out the [Quick Tour in the PEFT docs](https://huggingface.co/docs/peft/quicktour). To install, follow the [PEFT installation guide](https://huggingface.co/docs/peft/install).
You can also use the following one-line install through pip:

```
$ pip install peft
```


```python
%time train(peft_model, optimizer, criterion, train_loader, valid_dataloader=valid_loader, epochs=10)
```

We get an accuracy of ~0.97, despite only training a tiny amount of parameters. That's a really nice result.

## Sharing the model through Hugging Face Hub

### Pushing the model to Hugging Face Hub

If we want to share the fine-tuned weights with the world, we can upload them to Hugging Face Hub like this:


```python
user = ""BenjaminB""  # put your user name here
model_name = ""peft-lora-with-timm-model""
model_id = f""{user}/{model_name}""
```


```python
peft_model.push_to_hub(model_id);
 
Here is the question: When training a PEFT model with LoRA for a large language model that will be used for code generation and deployed through the Hugging Face Hub, which sequence of steps would result in the most efficient and effective deployment?","Based on the context, the most efficient and effective deployment sequence for training a PEFT model with LoRA for a large language model used for code generation and deployed through the Hugging Face Hub would be:

1. Import necessary libraries: Transformers, Accelerate, Datasets, Evaluate, PEFT, huggingface_hub, and hnswlib.
2. Load the `intfloat/e5-large-v2` model and tokenizer using Transformers.
3. Set up the LoRA configuration and create the"
What is the most effective approach for creating a coding assistant that can generate data visualizations while maintaining a small deployment footprint?,"[(522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (7691, 'fa39068d-a471-4887-a405-eb0139364fec'), (19981, '558c0bd9-da7a-44c7-9580-01fc13de8ab2'), (14350, '83ad6832-6599-4e40-8639-422982b07212'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (19992, '67ddb573-ec2b-4f78-b720-ae91db3e353a'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (29222, 'f47e0b8c-3526-4297-b05b-4cb3e1e6c6f7'), (29224, 'a1b49036-83a8-4545-96ac-718a57c76b4b'), (24106, 'a5c078df-6927-4368-af69-59cd227d60e0'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (15936, 'ab08450b-3b5c-4bbc-8c0e-355d1b29997d'), (11345, '24539c36-0220-4657-a439-8a9880b8094b'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (10848, 'e24b0ae6-43f0-498e-bd66-a33c5139c50d'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (16998, '60e139ff-8758-4146-915e-e9dba50bef1e'), (27752, 'ba8de9dc-6631-4a04-ae73-15640a799b35'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (8816, '3ef2b5b5-95ca-420c-8cfb-abf706544e0b'), (29299, 'd1e259d9-9419-4cb5-b669-c348ece2b6a7'), (29300, '8e53cc0a-4b5f-47ce-89af-d9c115478c03'), (29301, '33fef011-721c-47fe-85d9-708044b9d954'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (29336, 'f04db5fb-1ac5-4f06-affc-512b050f79c1'), (29353, '161ea064-6ca3-4490-85ce-135ab0440a82'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (21175, '6624dc16-3bb1-43ed-8494-bcba02c7b96e'), (21176, '2c7c306b-d840-49b2-8ce5-f553024b9d6e'), (15540, 'a7378dbc-d7f5-4167-a200-01aad0625463'), (21178, '7b9a3d42-8115-4146-9a5b-13ff20effe02'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (5301, 'f5dc6306-4f62-46c3-9a2e-e7c26b425a5a'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (7374, 'f080a759-14b5-4d2f-a4f6-7d49cbf9c053'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (7897, 'b49e9eed-7307-41a6-ace9-39f2f03a63c0'), (13019, '4baef4d0-c3c4-4c0f-9525-be1197621a67'), (13020, '28ef4fc5-53c8-4ea8-aa7e-c936f6151740'), (7387, '49e59733-bddf-4e92-be0e-5013437fa51b'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (18676, '19268cbe-5f8f-44b7-9603-fd3d64a79dc2'), (28414, '0e69ac2d-e27f-4301-b639-f7070dc7a084'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (24834, 'be08fa7e-dc7d-48b4-ac0e-07f98d18fd69'), (16644, '48292295-18de-4870-999a-cf16ec29e3c1'), (16646, '0667ad9b-07de-4dce-94ed-e070e983843b'), (24846, '8f3aab04-758c-4a01-bb65-22c3a112c220'), (31508, 'd38f4f16-46d8-4b02-aab8-a91ca6500ba7'), (24409, 'e7d646c9-36ec-4c83-9e34-aeda45f9c767'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (11642, '767bed97-1215-4182-b266-563ab9db91f3'), (11643, '428a11f6-079f-4810-9107-b8d8deefc0de'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (24976, '81b311bd-185b-473c-92bf-e1adf46d9090'), (7057, 'a2b32ca0-9edb-46ca-9502-dd65ffb3749f'), (24975, '84ec31e0-6e0d-40d2-aeed-7c845e768cb2'), (408, 'e160735e-ba9e-4144-a8f4-5ccdb5009af7'), (410, 'a00cc96e-99eb-45c3-8625-fb3bff67e0e6'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (21923, '694681c3-541a-4354-b1a1-3cf8063ff7c1'), (24485, '4de8b02f-cf9b-40a4-9212-1d8e11cd76f5'), (7078, 'ddf6eca5-f40f-4432-82a0-55a2aede1980'), (7077, '74cd7578-3563-425a-be27-134bc2db8891'), (20395, '9d24ff73-b44c-41e5-a42e-fd54dddf1eb1'), (7083, '408ec2a3-63f7-4564-8fa9-a7a22a7a65ef'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (7093, '1da6412c-f74e-42e2-8292-cdfd3ac8c2db'), (30646, 'b6a89c9f-7d3d-4f2e-a0ae-d2f899c2e49f'), (7094, '48ee3ca2-7a36-4001-b920-9ebeb4e2eb26'), (21945, 'bb2aad32-1962-44e9-8059-67c527f5ea2b'), (21946, 'b42dae6b-b396-4eb3-a4e6-36f030f827e8'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (18881, 'a4e7d7e6-4770-459f-b76d-dbdeb2e65446'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (3035, '5c512548-7f49-487d-b32a-a32e51be3d95'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (15845, 'ad1b20f3-a36d-4007-b065-81501dd109c6'), (15846, 'ee66a65a-c35d-4ed7-acbc-15fce331450b'), (14822, '3a7ef6c9-1f57-4061-a655-76352eb3e163'), (25064, '099d4bbf-893a-4feb-a4ad-1dbf78a02324'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27109, 'e980eafb-92e5-4615-b816-fc20edbbd85f'), (15851, '0bde2ce9-28b0-4ae0-9459-a572cca7f941'), (27112, '831fcd7a-7990-4660-a592-546aa8d108bf'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (15855, '59fd244c-a8fa-4c7c-a422-3f51103f15c1'), (16372, 'e37f175f-0c5c-4ae5-ab50-be325441aaf9'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (15861, 'cdbf3fc8-ee9f-4a6a-bfd8-e1fe30982ba1'), (15863, 'e17826a6-d5b3-4729-ad97-387ae765122d'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: - Most Cost-Effective Deployment: For users looking for good performance at low cost
- Best Latency Deployment: Minimizing latency for real-time services
- Best Throughput Deployment: Maximizing tokens processed per second

To keep this benchmark fair, transparent, and reproducible, we share all of the assets, code, and data we used and collected: 

- [GitHub Repository](https://github.com/philschmid/text-generation-inference-tests/tree/master/sagemaker_llm_container)
- [Raw Data](https://github.com/philschmid/text-generation-inference-tests/tree/master/results/sagemaker)
- [Spreadsheet with processed data](https://docs.google.com/spreadsheets/d/1PBjw6aG3gPaoxd53vp7ZtCdPngExi2vWPC0kPZXaKlw/edit?usp=sharing)

We hope to enable customers to use LLMs and Llama 2 efficiently and optimally for their use case. Before we get into the benchmark and data, let's look at the technologies and methods we used.
However, while these pre-trained models can perform impressively across a range of tasks, there's an exciting possibility lying just beyond the horizon: the ability to tailor a code generation model to your specific needs. Think of personalized coding assistants which could be leveraged at an enterprise scale. 

In this blog post we show how we created HugCoder 🤗, a code LLM fine-tuned on the code contents from the public repositories of the [`huggingface` GitHub organization](https://github.com/huggingface). We will discuss our data collection workflow, our training experiments, and some interesting results. This will enable you to create your own personal copilot based on your proprietary codebase. We will leave you with a couple of further extensions of this project for experimentation. 

Let’s begin 🚀
--
title: ""Creating a Coding Assistant with StarCoder""
thumbnail: /blog/assets/starchat_alpha/thumbnail.png
authors:
- user: lewtun
- user: natolambert
- user: nazneen
- user: edbeeching
- user: teven
- user: sheonhan
- user: philschmid
- user: lvwerra
- user: srush
---

# Creating a Coding Assistant with StarCoder


If you’re a software developer, chances are that you’ve used GitHub Copilot or ChatGPT to solve programming tasks such as translating code from one language to another or generating a full implementation from a natural language query like *“Write a Python program to find the Nth Fibonacci number”*. Although impressive in their capabilities, these proprietary systems typically come with several drawbacks, including a lack of transparency on the public data used to train them and the inability to adapt them to your domain or codebase.
Voila! ⭐️

The demo at the start of this post is this 1B model running locally on my Mac laptop.

## Conclusion

In this blog plost, we saw how to finetune `starcoder` to create a personal co-pilot that knows about our code. We called it 🤗 HugCoder, as we trained it on Hugging Face code :) After looking at the data collection workflow, we compared training using QLoRA vs full fine-tuning. We also experimented by combining different LoRAs, which is still an unexplored technique in the text/code domain. For deployment, we examined remote inference using 🤗 Inference Endpoints, and also showed on-device execution of a smaller model with VS Code and MLC.

Please, let us know if you use these methods for your own codebase!


## Acknowledgements
* Folder-based builders for quickly creating an image or audio dataset
* `from_` methods for creating datasets from local files

## Folder-based builders

There are two folder-based builders, [`ImageFolder`] and [`AudioFolder`]. These are low-code methods for quickly creating an image or speech and audio dataset with several thousand examples. They are great for rapidly prototyping computer vision and speech models before scaling to a larger dataset. Folder-based builders takes your data and automatically generates the dataset's features, splits, and labels. Under the hood:
 
Here is the question: What is the most effective approach for creating a coding assistant that can generate data visualizations while maintaining a small deployment footprint?","Based on the provided context, there is no direct answer to the question about creating a coding assistant that can generate data visualizations while maintaining a small deployment footprint. The context primarily focuses on creating a coding assistant using LLMs and fine-tuning them for specific use cases, but it does not explicitly discuss data visualization or deployment footprint. Therefore, it is not possible to provide a comprehensive answer to this question based on the given context."
"When deploying a large language model for code visualization tasks across multiple users, which approach correctly balances the tradeoffs between inference speed, storage efficiency, and user request handling?","[(10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (14852, 'd7d13b4b-ceec-438e-b1e4-c311204e6387'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (6723, '01ac9d03-828c-47a2-95fc-a16de8fc474e'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (30304, '69655f7c-2296-4da4-9562-97dbe94a3c0f'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (8805, '82dbedd7-d790-4508-add3-69d6b8e877bb'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (8808, 'd2ee298b-cdc9-4e51-b273-b2029efd926b'), (6762, '03137b81-a5f9-4d57-8bf7-3108cc4865fd'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (8813, '44b608a8-324f-4a10-a217-96a4c4e84f98'), (29300, '8e53cc0a-4b5f-47ce-89af-d9c115478c03'), (29301, '33fef011-721c-47fe-85d9-708044b9d954'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (6273, '7673ac47-ca59-4d63-b892-bd9c60e05d28'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (29336, 'f04db5fb-1ac5-4f06-affc-512b050f79c1'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (29338, '49aed807-1a59-497e-be41-abc9c83d69c3'), (18074, '85e06b79-6141-47b5-997e-e11f4bd149d5'), (30376, '81f99d9e-b362-4eab-8876-3ea668b9d674'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8366, '802fb177-2a03-456c-b39c-e9d1d7f566ed'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (21176, '2c7c306b-d840-49b2-8ce5-f553024b9d6e'), (21177, '7ab5c5b7-16a5-4322-a529-c851484ee7ba'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (8392, '2f6fcdd4-f780-4959-92b2-d153fc16cdb7'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (25839, '4a92026f-215f-4a87-a52b-a7baf6a8a3bd'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (22768, '0aa32bd4-ae8d-4d98-b212-4087c9da1896'), (6903, '807eb0ed-deee-4627-8c1d-d65ce5f2f592'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (20740, '8ea0cbc1-6505-4aba-8eeb-7045546046c8'), (31005, 'dfbe26eb-d794-43e1-8e52-483e186cd71c'), (31006, '570011b0-e075-4315-97cc-b8c673d126d2'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (21320, 'fbd9fead-fcb4-4b6c-96e3-0eda7aa951da'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (3440, '51f99fe7-7751-4e98-914f-d073dd0afcf5'), (21362, '651a9fca-5d13-4c33-9dd8-27cca38b640b'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (19856, '1779d346-5968-4964-a7b4-39c87c76dd12'), (12180, '820aed82-3574-4ab3-96a2-991b0a57cbe1'), (7580, '2ad62bd9-5b54-4a36-aed6-cd1066066974'), (8608, '5f875ff2-0c64-4000-b0b6-21d5e07feeb1'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7084, '966d14eb-c2c1-4bae-96ce-6f2ced24abfc'), (7093, '1da6412c-f74e-42e2-8292-cdfd3ac8c2db'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (14821, 'c34abfed-956a-47bc-8db5-e28e8b62e1ce'), (14822, '3a7ef6c9-1f57-4061-a655-76352eb3e163'), (14823, 'c02c38d2-2a34-418e-954a-4b26b11b52e4'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5'), (16373, '7a710310-ff81-457f-9396-d32fe5aece98'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (17401, '7e11d1d5-22ca-4027-846a-3a82cc00d07b'), (17402, 'a14bfb79-d7f1-4438-a7a8-774ea9133ffd'), (14843, 'e99ac210-a8ee-46c9-ab15-45f0c9d7496a'), (14846, 'cf703fd3-cc50-49a5-bfa6-2513ea2118cb')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing
per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.*
--
title: ""Optimum-NVIDIA Unlocking blazingly fast LLM inference in just 1 line of code"" 
thumbnail: /blog/assets/optimum_nvidia/hf_nvidia_banner.png
authors:
- user: laikh-nvidia
  guest: true
- user: mfuntowicz
---


# Optimum-NVIDIA on Hugging Face enables blazingly fast LLM inference in just 1 line of code

Large Language Models (LLMs) have revolutionized natural language processing and are increasingly deployed to solve complex problems at scale. Achieving optimal performance with these models is notoriously challenging due to their unique and intense computational demands. Optimized performance of LLMs is incredibly valuable for end users looking for a snappy and responsive experience, as well as for scaled deployments where improved throughput translates to dollars saved.
[[open-in-colab]]

Large Language Models (LLMs) such as GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b), and [Llama](https://huggingface.co/meta-llama/Llama-2-70b-hf) are rapidly advancing in their ability to tackle human-centric tasks, establishing themselves as essential tools in modern knowledge-based industries.
Deploying these models in real-world tasks remains challenging, however:

-   To exhibit near-human text understanding and generation capabilities, LLMs currently require to be composed of billions of parameters (see [Kaplan et al](https://arxiv.org/abs/2001.08361), [Wei et. al](https://arxiv.org/abs/2206.07682)). This consequently amplifies the memory demands for inference.
-   In many real-world tasks, LLMs need to be given extensive contextual information. This necessitates the model's capability to manage very long input sequences during inference.
Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the way for their deployment on inference endpoints
serving multiple requests in parallel.

There is still plenty of room for improvement though:
- in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory.
Alternative options such as pipelining are currently integrated,
- the static sequence length limits the model ability to encode long contexts. It would be interesting to see if attention sinks might be a valid option to address this.
*Program synthesis strives to generate a computer program as a solution to a given problem specification. We propose a conversational program synthesis approach via large language models, which addresses the challenges of searching over a vast program space and user intent specification faced in prior approaches. Our new approach casts the process of writing a specification and program as a multi-turn conversation between a user and a system. It treats program synthesis as a sequence prediction problem, in which the specification is expressed in natural language and the desired program is conditionally sampled. We train a family of large language models, called CodeGen, on natural language and programming language data. With weak supervision in the data and the scaling up of data size and model size, conversational capacities emerge from the simple autoregressive language modeling. To study the model behavior on conversational program synthesis, we develop a multi-turn programming
 
Here is the question: When deploying a large language model for code visualization tasks across multiple users, which approach correctly balances the tradeoffs between inference speed, storage efficiency, and user request handling?","Based on the context, the correct approach to balance the tradeoffs between inference speed, storage efficiency, and user request handling when deploying a large language model for code visualization tasks across multiple users is to use Optimum-NVIDIA, which enables blazingly fast LLM inference in just 1 line of code. This approach allows for serving multiple requests in parallel, which is not too sensitive to batch size, and can improve throughput without increasing device memory. Additionally, alternative options such as pipelining are integrated"
"In implementing a multi-user code generation system that handles both text-to-image and code visualization requests, which approach properly manages memory and concurrent operations while maintaining model performance?","[(17403, '38d35ffd-8bed-4e4d-863e-d2214e1e2d06'), (11274, '77881e72-38b7-49e4-93fd-a7028e73fa0d'), (15885, '971fc97e-36dc-4975-85ab-37248b6d6a03'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (11293, 'dc3bf3a6-1caa-462a-a58e-da5e80984fcf'), (28708, '218eb18e-351e-4810-8501-392aa5b0710b'), (5158, '2a7fe1df-7c54-4ebf-9da2-15523efb3fd1'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (31288, '2a07c3d0-c7f2-467d-9412-8c1b3e9de296'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (17469, '4c181057-40b0-4750-87a7-bb3330ff02f5'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (11345, '24539c36-0220-4657-a439-8a9880b8094b'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (14434, '88961ae2-f1af-4f1b-a627-43082e2f0710'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (8805, '82dbedd7-d790-4508-add3-69d6b8e877bb'), (20068, 'd2d9fdee-7033-4cda-a724-d9c2e219656c'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (8808, 'd2ee298b-cdc9-4e51-b273-b2029efd926b'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (14447, '76a009af-60b2-497f-a33c-c412a6d21d6d'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (1185, '13e997d5-137f-444d-b40b-2f50b3de78d5'), (25764, 'e6b7b172-a955-4460-a174-888528a760ca'), (4265, 'f389f1dd-debe-4020-a0a3-4923eaae7aee'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (22195, '578c01e1-ea10-4e30-b7a2-d0c4fa57b550'), (15540, 'a7378dbc-d7f5-4167-a200-01aad0625463'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (1207, 'ba67be42-b645-475b-a449-454ae493e9ef'), (19643, '11f17d0a-4f66-4c28-9733-118d5b0060ae'), (8381, '6c73edcc-e313-4d7e-a499-32dede8f4258'), (30910, '562b2c34-aa31-45cf-a703-d544e0641ad7'), (8384, '631e3d73-c7ec-48e1-8ad8-2c3de8269b8b'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (19155, '1e812ea9-3a09-4467-be78-c43f09a00fc7'), (2773, '06afc023-a422-4936-9dba-57a0537261aa'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (19697, '2bb5d1ca-7bcf-4074-b6a2-32572672320c'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (18675, '1da20ff6-a9fc-47bd-beda-e5b849c25412'), (18676, '19268cbe-5f8f-44b7-9603-fd3d64a79dc2'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (19702, '1fcede80-72d5-462e-9e4d-bfc96074a009'), (24824, 'fdd418a8-03d9-44e2-bdb6-37f035778243'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (24837, '2dcb8c9f-c4ce-497d-8bce-11e9cff7d5b6'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (24846, '8f3aab04-758c-4a01-bb65-22c3a112c220'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (31516, 'f274d491-9d0b-4595-b65b-7a400a006c0a'), (31005, 'dfbe26eb-d794-43e1-8e52-483e186cd71c'), (31006, '570011b0-e075-4315-97cc-b8c673d126d2'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (27423, 'cdb54c20-dca4-48f4-96a7-f000fa351601'), (31016, '05908d70-238b-4150-bec9-b27c6ef05086'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (8513, '70a534c3-3feb-4d98-b433-922e968a023f'), (8528, 'b189bace-ccb7-4671-af71-98b97ec1247c'), (27475, '8fdc35b4-b9d4-4822-8a62-f9f2cca68e51'), (26454, 'e61e2e35-e392-4e1e-91e9-7974b5459759'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (17793, '8e853aa7-59db-4aaf-b04c-fe7fa8b49fcd'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (20875, 'c60f0ee3-dfe8-4e04-ae6a-dbdf52ecf36b'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (17811, '525ac724-ad6c-419a-a3a2-1715aa66fbd7'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (8608, '5f875ff2-0c64-4000-b0b6-21d5e07feeb1'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (20900, '1482385c-4de4-4a6a-abb9-eccea1d3fb5e'), (7084, '966d14eb-c2c1-4bae-96ce-6f2ced24abfc'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (31163, '5156e6c6-d4cb-43ea-a682-eed8719fcd71'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (17367, '96ac68a8-96d9-460e-98ee-59b8f1dddccd'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (31196, '1f882f74-f46b-4b40-9bf3-14c1c4a21c25'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (14821, 'c34abfed-956a-47bc-8db5-e28e8b62e1ce'), (14822, '3a7ef6c9-1f57-4061-a655-76352eb3e163'), (14823, 'c02c38d2-2a34-418e-954a-4b26b11b52e4'), (29161, '5c1a0615-2746-4f25-8cae-56a17b9d371d'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5'), (16373, '7a710310-ff81-457f-9396-d32fe5aece98'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (14843, 'e99ac210-a8ee-46c9-ab15-45f0c9d7496a'), (14846, 'cf703fd3-cc50-49a5-bfa6-2513ea2118cb')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: models to handle input types of different modalities. Implemented on large-scale paired image-text data, UniDiffuser is able to perform image, text, text-to-image, image-to-text, and image-text pair generation by setting proper timesteps without additional overhead. In particular, UniDiffuser is able to produce perceptually realistic samples in all tasks and its quantitative results (e.g., the FID and CLIP score) are not only superior to existing general-purpose models but also comparable to the bespoken models (e.g., Stable Diffusion and DALL-E 2) in representative tasks (e.g., text-to-image generation).*
- **Parallelization**. Workloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute in parallel on XLA devices. For text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance.

👉 Try it out for yourself:

[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/google/sdxl)

## Stable Diffusion XL pipeline in JAX

Upon having access to a TPU VM (TPUs higher than version 3), you should first install
a TPU-compatible version of JAX:
#### High-performance throughput for high batch sizes

Workloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute it in parallel on various XLA devices. 
For text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance. For example, running SDXL on a TPU with 8 chips will generate 8 images in the same time it takes for 1 chip to create a single image.

TPU v5e instances come in multiple shapes, including 1, 4 and 8-chip shapes, all the way up to 256 chips (a full TPU v5e pod), with ultra-fast ICI links between chips. This allows you to choose the TPU shape that best suits your use case and easily take advantage of the parallelism that JAX and TPUs provide.
*The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java,
[Stable Diffusion XL](https://huggingface.co/papers/2307.01952) (SDXL) is a powerful text-to-image generation model that iterates on the previous Stable Diffusion models in three key ways:

1. the UNet is 3x larger and SDXL combines a second text encoder (OpenCLIP ViT-bigG/14) with the original text encoder to significantly increase the number of parameters
2. introduces size and crop-conditioning to preserve training data from being discarded and gain more control over how a generated image should be cropped
3. introduces a two-stage model process; the *base* model (can also be run as a standalone model) generates an image as an input to the *refiner* model which adds additional high-quality details

This guide will show you how to use SDXL for text-to-image, image-to-image, and inpainting.

Before you begin, make sure you have the following libraries installed:
 
Here is the question: In implementing a multi-user code generation system that handles both text-to-image and code visualization requests, which approach properly manages memory and concurrent operations while maintaining model performance?","Based on the context, the approach that properly manages memory and concurrent operations while maintaining model performance is using JAX's `pmap` (parallel map) function, which allows for scaling workloads across multiple devices and executing functions in parallel on various XLA devices. This approach enables high-performance throughput for high batch sizes, making it suitable for handling multiple requests concurrently in a multi-user code generation system."
"Given a scenario where a team needs to fine-tune StarCoder for both code generation and visualization tasks while allowing multiple users to concurrently upload training data, which implementation would INCORRECTLY handle the interaction between memory management and file operations?","[(19981, '558c0bd9-da7a-44c7-9580-01fc13de8ab2'), (19985, '7b5bc375-6953-47e3-b7be-f409e038fe53'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (19992, '67ddb573-ec2b-4f78-b720-ae91db3e353a'), (11293, 'dc3bf3a6-1caa-462a-a58e-da5e80984fcf'), (20012, '84ff49d8-5a63-403c-b72e-646d7c47ac9b'), (29744, '6954830f-0ba6-4c15-a60d-3cd7e635d535'), (14386, 'b8cd4a6c-621a-48e7-a398-c9e62eee673e'), (14388, '5cf9f858-9101-44e2-8239-8859192f9e97'), (10300, '4c09aeb7-71b2-4501-8a66-fd42121a6a51'), (5695, '556c87ee-1b6b-463d-bdf7-e1a949117c70'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (29761, '04f9fa53-d75c-400d-a4ee-a93d795d3db0'), (23627, 'b789e3c2-0ef9-42fc-a372-ca94558fb295'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (27221, '3a0769e6-1ce2-4b41-9dbd-f9d0c6e8b04a'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (6754, '48421d38-5ed1-440f-b10b-84456af11c1b'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (8805, '82dbedd7-d790-4508-add3-69d6b8e877bb'), (8806, 'baeaa009-cafb-4600-93a2-3bb7cf6e70c2'), (8807, '170b9534-cae9-40e8-bbe4-468c9b1e40f0'), (8808, 'd2ee298b-cdc9-4e51-b273-b2029efd926b'), (8809, '06aa1afc-6051-461c-bab3-d3cf629fcb9e'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (8812, '99a22463-c724-477a-8a22-0965d1ff8629'), (8813, '44b608a8-324f-4a10-a217-96a4c4e84f98'), (8814, 'd2a5d8f6-917c-45c6-9d85-1945247e70c3'), (8815, 'ac730dae-13f7-48e6-882e-d777fade42b0'), (8816, '3ef2b5b5-95ca-420c-8cfb-abf706544e0b'), (29299, 'd1e259d9-9419-4cb5-b669-c348ece2b6a7'), (29300, '8e53cc0a-4b5f-47ce-89af-d9c115478c03'), (29301, '33fef011-721c-47fe-85d9-708044b9d954'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (29306, 'd84c710a-279e-4e4a-a150-150d7c50fae4'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (29312, 'f5be9fd5-8b84-4493-af7e-d241b3c4055d'), (31366, 'faca457c-cb2d-44f1-a992-b468cb6d6a45'), (20107, '292576ce-76d8-4504-8e2c-201eb04873de'), (29327, '54f6d6d5-aab2-4d36-8ed3-6fb1636d547f'), (29328, '0d50e7f8-3ef0-4ba6-96b8-c493a61b7bee'), (29330, '800fbef1-2226-4b22-b9e9-e1e37955fd53'), (29336, 'f04db5fb-1ac5-4f06-affc-512b050f79c1'), (29338, '49aed807-1a59-497e-be41-abc9c83d69c3'), (29350, '5b540cf9-e058-4625-941b-90531dd17467'), (29352, 'a3d7804f-0dac-48f5-ba8f-ffa2c4bf2cb2'), (29353, '161ea064-6ca3-4490-85ce-135ab0440a82'), (30376, '81f99d9e-b362-4eab-8876-3ea668b9d674'), (21163, '7d118280-4cd6-4c0e-91c4-dfb3d2e2f08a'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (21175, '6624dc16-3bb1-43ed-8494-bcba02c7b96e'), (21176, '2c7c306b-d840-49b2-8ce5-f553024b9d6e'), (21177, '7ab5c5b7-16a5-4322-a529-c851484ee7ba'), (21178, '7b9a3d42-8115-4146-9a5b-13ff20effe02'), (21179, 'b072bbe7-e1ae-424d-a1c7-6f7e80ecdb15'), (6337, '1c3d8092-7d15-4854-8c95-d6d89c63ebcc'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (6364, '220a3825-9318-440f-b0a5-547ed3feced8'), (22251, 'd689c5a1-3284-4355-850c-294bf162c90e'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (2831, 'cf5bdfa1-fca5-4fee-8bd3-cd94912b5bb1'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (2852, '4070192e-9515-4222-9c1e-c07081d79086'), (31013, 'c55ce3d9-ab37-45fc-bb0a-3c19c6e2452c'), (20280, '84274901-beb3-4376-9793-83448fbc5428'), (28994, 'b2fdd130-8cb1-4172-828f-ccbf88b499bd'), (6467, '8d1db7f8-7a77-43b4-9a9d-350b2887df45'), (24387, 'e78b85c6-e8ae-4879-88f0-026ddeb290f7'), (21320, 'fbd9fead-fcb4-4b6c-96e3-0eda7aa951da'), (26962, '0fda646c-a410-418e-bef2-061878be6905'), (26963, 'd37c4394-4313-4a68-a48c-e1a6e18c49dc'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (15720, 'c3517980-a237-4b69-bf3b-15b281810859'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (13705, 'c7323fb9-098f-4efc-be4c-bc153275f6cf'), (5517, '78e547ac-9ff2-4317-9554-f9c745ffeac6'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (7061, 'fb397a69-b4e2-424e-a392-b35e056c62f6'), (7062, 'c8c890e1-e7b8-4e8f-9799-111728d14f27'), (5016, '2c7de3aa-8b01-440b-8b78-5d6d18c84a7d'), (7065, '0fb31b29-d4f9-4207-9934-c683259bbb1a'), (7067, '49c345b0-3daa-45af-8b3a-880742ea3aa8'), (7069, '4d991861-7f15-4eae-8fe5-f9912d328873'), (7071, '27846f4b-aab8-4eba-91f2-1b9996b20eb3'), (7078, 'ddf6eca5-f40f-4432-82a0-55a2aede1980'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (7087, '58ace724-04a1-4d22-860e-99ed15094493'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (7093, '1da6412c-f74e-42e2-8292-cdfd3ac8c2db'), (19896, '024dae73-87a3-4714-8f2b-d1f43475c571'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (4547, '9d7a8092-0c89-45c6-990c-7ac6a0743ada'), (19411, '140ee8ae-a55f-4c8c-a331-3e7e31530f72'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (19423, '367b2543-04f2-49ce-ab8b-2dca831f8cf0'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (27108, 'e2585452-1098-4329-a2a4-fe92eab1ead6'), (27109, 'e980eafb-92e5-4615-b816-fc20edbbd85f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (16373, '7a710310-ff81-457f-9396-d32fe5aece98'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Our notebook [Dance_of_LoRAs.ipynb](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/personal_copilot/inference/Dance_of_LoRAs.ipynb) includes all the inference code and various LoRA loading combinations, like loading the chat assistant on top of `starcoder` instead of `starcodeplus`, which is the base model that we fine-tuned. 

Here, we will consider 2 abilities (`chatting/QA` and `code-completion`) on 2 data distributions (`top 10 public hf codebase` and `generic codebase`). That gives us 4 axes on which we'll carry out some qualitative evaluation analyses.

#### First, let us consider the `chatting/QA` task. 

If we disable adapters, we observe that the task fails for both datasets, as the base model (`starcoder`) is only meant for code completion and not suitable for `chatting/question-answering`. Enabling `copilot` adapter performs similar to the disabled case because this LoRA was also specifically fine-tuned for code-completion.
```

Here the `config.yaml` file specifies all the parameters associated with the dataset, model, and training - you can configure it [here](https://github.com/bigcode-project/starcoder/tree/main/chat) to adapt the training to a new dataset. Your trained model will then be available on the Hub!

## StarCoder as a coding assistant

### Generating plots

We wanted to see how our model could do with basic visualization tasks, following the [famous unicorn drawing in tikz of GPT-4](https://tex.stackexchange.com/questions/681418/draw-a-unicorn-in-tikz). To do this, we prompted our model with some coding tasks and got wonderful results! Yes, these are a little cherry picked as we only selected the completions that **wrote functioning code**, but the others were not far off!

**Example 1: bar plot**

Prompt:
## How does it work?

SafeCoder is a complete commercial solution, including service, software and support.

### Training your own SafeCoder model

StarCoder was trained in more than 80 programming languages and offers state of the art performance on [multiple benchmarks](https://huggingface.co/spaces/bigcode/multilingual-code-evals). To offer better code suggestions specifically for a SafeCoder customer, we start the engagement with an optional training phase, where the Hugging Face team works directly with the customer team to guide them through the steps to prepare and build a training code dataset, and to create their own code generation model through fine-tuning, without ever exposing their codebase to third parties or the internet.
## Future directions

We were surprised to learn that a code-generation model like StarCoder could be converted into a conversational agent with a diverse dataset like that from OpenAssistant. One possible explanation is that StarCoder has been trained on both code _and_ GitHub issues, the latter providing a rich signal of natural language content. 
We're excited to see where the community will take StarCoder - perhaps it will power the next wave of open-source assistants 🤗.

## Acknowledgements
Voila! ⭐️

The demo at the start of this post is this 1B model running locally on my Mac laptop.

## Conclusion

In this blog plost, we saw how to finetune `starcoder` to create a personal co-pilot that knows about our code. We called it 🤗 HugCoder, as we trained it on Hugging Face code :) After looking at the data collection workflow, we compared training using QLoRA vs full fine-tuning. We also experimented by combining different LoRAs, which is still an unexplored technique in the text/code domain. For deployment, we examined remote inference using 🤗 Inference Endpoints, and also showed on-device execution of a smaller model with VS Code and MLC.

Please, let us know if you use these methods for your own codebase!


## Acknowledgements
 
Here is the question: Given a scenario where a team needs to fine-tune StarCoder for both code generation and visualization tasks while allowing multiple users to concurrently upload training data, which implementation would INCORRECTLY handle the interaction between memory management and file operations?","Based on the context, the implementation that would INCORRECTLY handle the interaction between memory management and file operations is the one that uses full fine-tuning instead of QLoRA (Quantized LoRA) or adapters. This is because full fine-tuning would require loading the entire model and dataset into memory, which could lead to memory management issues, especially when multiple users are concurrently uploading training data. In contrast, QLoRA and adapters are more lightweight and can be loaded on top of the"
"When deploying a large language model for code assistance with visualization capabilities, what scenario would create a CONTRADICTORY interaction between training efficiency, file management, and prompt handling?","[(20481, '73e355da-709c-4976-8830-0234ff38d590'), (10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (16395, '8227cdab-ed0c-4745-95b8-2cd3c7df5ce4'), (16401, '83e0cf27-fc46-4cc4-974e-c1060f254d10'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (31291, '728e3f48-a642-437c-b7b8-adf9580dc871'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (7236, '2ba9c663-afec-4b25-8822-9c845fc5040a'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (8805, '82dbedd7-d790-4508-add3-69d6b8e877bb'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (6762, '03137b81-a5f9-4d57-8bf7-3108cc4865fd'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (30334, '2bb66617-068b-4b7a-a714-0e4caed78ef1'), (6271, 'db13ad85-6738-4e86-bea9-8e92682af368'), (3712, '51eb8b25-0de3-43ab-b18c-b461dad0ceee'), (9860, 'b127b619-65e1-4d79-844d-ade94f6c03a8'), (3716, '7409694b-c0ab-407c-8988-a97aae91288c'), (31366, 'faca457c-cb2d-44f1-a992-b468cb6d6a45'), (12936, 'f7f6d88b-7b8c-4334-95fc-446aa88309f7'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (29338, '49aed807-1a59-497e-be41-abc9c83d69c3'), (9371, '7b487395-e96b-4d33-9523-2231254a4ce4'), (21163, '7d118280-4cd6-4c0e-91c4-dfb3d2e2f08a'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (21176, '2c7c306b-d840-49b2-8ce5-f553024b9d6e'), (21177, '7ab5c5b7-16a5-4322-a529-c851484ee7ba'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (24265, '1016a359-ef09-4f47-9e86-9bd916e2a61e'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (3815, '7fa3513f-2329-4efb-bfb6-74160ee1ef45'), (5866, '4108faf4-0e48-4871-8e5b-31983d6c0129'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (12013, 'e538399e-20ea-45b5-9245-989cea914a13'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (7923, '3d12a194-ca8a-4ffe-9e77-8fb01bda0fd8'), (6903, '807eb0ed-deee-4627-8c1d-d65ce5f2f592'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (31005, 'dfbe26eb-d794-43e1-8e52-483e186cd71c'), (31006, '570011b0-e075-4315-97cc-b8c673d126d2'), (21281, '5d60fd29-4de1-4011-97a1-37f9e285f617'), (4902, '214fdfbb-eec1-494f-abef-703a1cbbe152'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (31016, '05908d70-238b-4150-bec9-b27c6ef05086'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (11564, 'b22defce-5c80-4f5b-8953-f9a66ddf9666'), (22337, 'dcd3d938-defe-4be3-b64f-3fd23ab4f8bf'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (21324, 'c689a247-869d-4ea7-b9f0-170aa19a493b'), (8525, '931d0e8a-fa6c-4b84-9b4a-f7329eb19392'), (26960, '0d8543ba-1ac5-42b6-8a07-6f3216b29f08'), (26454, 'e61e2e35-e392-4e1e-91e9-7974b5459759'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (16227, '052f6047-255b-488b-a997-488387ae3155'), (3440, '51f99fe7-7751-4e98-914f-d073dd0afcf5'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (21887, '2fc3be04-7823-41a9-af0e-6c649e006652'), (24968, '6e58d9ed-7fd1-469a-bd90-305d0bc248f7'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (10648, '66ffc54d-754b-40e5-b926-93bd59b981cb'), (30617, 'af7be972-7339-438b-8c50-8bc5c32d23e2'), (7580, '2ad62bd9-5b54-4a36-aed6-cd1066066974'), (8608, '5f875ff2-0c64-4000-b0b6-21d5e07feeb1'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (11684, '9cfcfbdb-6441-4284-8ef3-d14780da146f'), (7077, '74cd7578-3563-425a-be27-134bc2db8891'), (7078, 'ddf6eca5-f40f-4432-82a0-55a2aede1980'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (13759, 'e3f7ce62-98c1-4212-90c1-fc43513f4104'), (27595, '6d1182d3-71fe-43f7-9bb2-ce728541b305'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (31186, '36d5c05d-8ab6-4eb3-b089-320dda12359a'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (27109, 'e980eafb-92e5-4615-b816-fc20edbbd85f'), (1510, 'a7df3897-3c02-4187-b2c6-88844cd91c0a'), (14823, 'c02c38d2-2a34-418e-954a-4b26b11b52e4'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (14825, '9594f34e-ac6c-42f2-b10f-bd6e73059e44'), (29158, 'e4fbc982-e499-43ae-b82d-504a09c43430'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5'), (16373, '7a710310-ff81-457f-9396-d32fe5aece98'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (12282, 'b54aa9db-2082-4d68-b330-43589ff6d668'), (14846, 'cf703fd3-cc50-49a5-bfa6-2513ea2118cb'), (12287, '2495e672-afff-44de-9495-f9352da75fa6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: this objective subsumes common pretraining signals such as OCR, language modeling, image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.
*The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java,
-->


# LLM prompting guide

[[open-in-colab]]

Large Language Models such as Falcon, LLaMA, etc. are pretrained transformer models initially trained to predict the 
next token given some input text. They typically have billions of parameters and have been trained on trillions of 
tokens for an extended period of time. As a result, these models become quite powerful and versatile, and you can use 
them to solve multiple NLP tasks out of the box by instructing the models with natural language prompts.

Designing such prompts to ensure the optimal output is often called ""prompt engineering"". Prompt engineering is an 
iterative process that requires a fair amount of experimentation. Natural languages are much more flexible and expressive 
than programming languages, however, they can also introduce some ambiguity. At the same time, prompts in natural language 
are quite sensitive to changes. Even minor modifications in prompts can lead to wildly different outputs.
1) Sample inefficiency

2) Unexpected behaviors from humans’ eyes

As a first attempt, the paper [“Grounding Large Language Models with Online Reinforcement Learning”](https://arxiv.org/abs/2302.02662v1) tackled the problem of **adapting or aligning a LM to a textual environment using PPO**. They showed that the knowledge encoded in the LM lead to a fast adaptation to the environment (opening avenues for sample efficient RL agents) but also that such knowledge allowed the LM to better generalize to new tasks once aligned.

<video src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit12/papier_v4.mp4"" type=""video/mp4"" controls />
When deploying a system using RLHF, gathering the human preference data is quite expensive due to the direct integration of other human workers outside the training loop. RLHF performance is only as good as the quality of its human annotations, which takes on two varieties: human-generated text, such as fine-tuning the initial LM in InstructGPT, and labels of human preferences between model outputs.
 
Here is the question: When deploying a large language model for code assistance with visualization capabilities, what scenario would create a CONTRADICTORY interaction between training efficiency, file management, and prompt handling?","Based on the context, a scenario that would create a contradictory interaction between training efficiency, file management, and prompt handling is when selecting files from repositories with 5+ GitHub stars deteriorates performance significantly, as mentioned in the BigCode project report. This is contradictory because one would expect that selecting high-quality files from popular repositories would improve performance, but instead, it hurts it. This could lead to a contradictory interaction between training efficiency (which is negatively affected), file management (which is supposed to provide high"
"When implementing a system that handles both model training and visualization generation, which combination of architectural decisions would create an UNEXPECTED conflict between memory management, adapter handling, and file operations?","[(11274, '77881e72-38b7-49e4-93fd-a7028e73fa0d'), (25615, '01944eb1-009d-4629-9882-c62631a911ec'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (29202, 'a6666314-edd1-47cc-8628-335a50fe419d'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (17443, 'bd891b6b-f323-49c6-b503-c3e499ebcdd1'), (13352, '1a798777-bf09-408b-8677-90236be1a5f0'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (13355, '156a7c84-39c8-410e-b1d4-8980cfee0a96'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (13366, '7322936a-3fde-463f-9bc6-02789b986490'), (21561, '68fa59f6-381e-4dee-ae59-873812588c22'), (1594, '9622f28a-d122-4157-9bd9-2dd99b146101'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (29247, '39f8a2f3-0505-4085-a28e-444aee87836c'), (30788, 'e6d02e40-3fcd-462e-b943-0494c7d97dc3'), (27208, '32a4ca3b-47d8-4d27-96c9-1c3b7efd500d'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (18001, '15e975a6-c407-4186-90f6-fdb783f386cf'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (12898, 'e6cc8e4f-28e6-47d7-8121-e797b078dd9b'), (13923, '017f79ef-c6e4-436d-9120-ff659a69789e'), (14439, '86de8893-816f-4b63-b9bc-ba2c4c214252'), (15976, '091b3322-c4ef-4224-93cd-cc8201ba4ba9'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (9860, 'b127b619-65e1-4d79-844d-ade94f6c03a8'), (31366, 'faca457c-cb2d-44f1-a992-b468cb6d6a45'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30885, '564ff6e5-7fac-4e5d-9ac9-9ba6a043b086'), (21163, '7d118280-4cd6-4c0e-91c4-dfb3d2e2f08a'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (6335, '7e912b49-1ada-42b1-93aa-cb5905d48dfa'), (16577, '65b23733-c969-4f5c-815e-b4d8c66dc8a9'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (20186, 'e4800925-3dd9-4604-b5d2-755f623ef3b9'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (10986, '43de7f8d-6eb7-420a-b184-aa48eb3517b8'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (7409, 'd72827db-095c-4f27-b8e3-f2da98e57908'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (24836, 'b29f87dd-e4a7-4d1b-82e2-c2260673c83a'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (27423, 'cdb54c20-dca4-48f4-96a7-f000fa351601'), (19240, 'f56f465c-9490-4689-a1c0-baa52f049424'), (20281, '174ec7fc-1250-4b64-a2c1-2fc228947804'), (24893, 'ba24282b-69bc-4162-bc20-24e64126aaec'), (30528, '69aa4012-1b33-4373-af24-1d94ec8dd03d'), (26963, 'd37c4394-4313-4a68-a48c-e1a6e18c49dc'), (12280, '040615d3-6913-4b79-a530-25472aa800e4'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (23900, 'bdf9c5d4-29b8-480f-b334-1fd2e6ee8a76'), (25950, '9b5fe5dc-37a3-41d7-abf3-4535bc6b26a9'), (23903, '923c4027-8d23-4edc-9d3e-daf70d4b077b'), (7009, '4fe8511b-00f0-404a-934b-6f00dad4ecae'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (12282, 'b54aa9db-2082-4d68-b330-43589ff6d668'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (27501, '79d37892-6d6c-4933-9861-d5c4e4652fc7'), (15735, 'f473ff9d-2520-4b90-81e6-2bc07ffb2468'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (23930, 'b54bef38-4510-49ab-87a1-dbc20ba90085'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (22397, '31d21656-3ce7-4da6-b134-33cbd4345049'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (3459, 'bc1a44b5-f64b-4598-85e8-e3280271e021'), (5508, '18f70486-80e2-4c2d-a2f1-f9e6174711dd'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (14727, '680f190f-eca8-4942-bebc-9212cd7caa87'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (5519, '2c76f6f9-2a42-4322-941a-f447d8861e87'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (30617, 'af7be972-7339-438b-8c50-8bc5c32d23e2'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (7592, '68ddeb8e-b285-47f9-8754-09b4fd273d73'), (7595, 'e9a0cc5c-98d6-4e8d-8b64-1c28f8f15d45'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (7606, '185b4012-c5e7-4217-bf99-009e04a97e71'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (4585, '973db0d7-b3c9-4d75-8b97-961aa85fc3a8'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (11246, '41cafe12-ee25-4e13-afeb-fc0262be3c12'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (29686, '0e9bb20f-2fd2-4f43-9aa7-a299a38ed484'), (29688, 'b7fbeb38-4dfc-40ca-9dcc-95685dc043b4'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
1. Each stage of the development process, from task specification, dataset curation, and model training, to model integration and system deployment, can take steps to minimize the aspects of machine bias** that most directly depend on its choices** and technical decisions, and
2. Clear communication and **information flow between the various ML development stages** can make the difference between making choices that build on top of each other to attenuate the negative potential of bias (multipronged approach to bias mitigation, as in deployment scenario 1 above) _versus_ making choices that compound this negative potential to exacerbate the risk of harm (as in deployment scenario 3).

In the next section, we review these various stages along with some of the tools that can help us address machine bias at each of them.


## Addressing Bias throughout the ML Development Cycle

Ready for some practical advice yet? Here we go 🤗
<Tip>

While the main concepts discussed in this guide are likely applicable across frameworks, here we focus on 
PyTorch-based implementations.

</Tip>

Before diving deeper into the specifics of each technique, let's go over the rough decision process when training 
large models on a large infrastructure.

## Scalability strategy

Begin by estimating how much vRAM is required to train your model. For models hosted on the 🤗 Hub, use our 
[Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage), which gives you 
accurate calculations within a few percent margin.  

**Parallelization strategy for a single Node / multi-GPU setup**

When training a model on a single node with multiple GPUs, your choice of parallelization strategy can significantly 
impact performance. Here's a breakdown of your options:

**Case 1: Your model fits onto a single GPU**

If your model can comfortably fit onto a single GPU, you have two primary options:
Keywords: Training, Generation

## [diffgram](https://github.com/diffgram/diffgram)

Diffgram aims to integrate human supervision into platforms. We support your team programmatically changing the UI (Schema, layout, etc.) like in Streamlit. This means that you can collect and annotate timely data from users. In other words, we are the platform behind your platform, an integrated part of your application, to ship new & better AI products faster.

Keywords: Human supervision, Platform

## [ecco](https://github.com/jalammar/ecco)

Explain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).

Keywords: Model explainability

## [s3prl](https://github.com/s3prl/s3prl)
### Drawbacks
Clearly, there are also drawbacks to the single file policy two of which we quickly want to mention here.

A major goal of Transformers is to provide a unified API for both inference and training for all models so 
that a user can quickly switch between different models in her setup. However, ensuring a unified API across 
models is much more difficult if modeling files are not allowed to use abstracted logical patterns. We solve
this problem by running **a lot** of tests (*ca.* 20,000 tests are run daily at the time of writing this blog post) to ensure that models follow a consistent API. In this case, the single file policy requires us to be very rigorous when reviewing model and test additions.
 
Here is the question: When implementing a system that handles both model training and visualization generation, which combination of architectural decisions would create an UNEXPECTED conflict between memory management, adapter handling, and file operations?","Based on the context, there is no specific answer to the question. The context provides information on performance and scalability in training large transformer models, addressing bias in the ML development cycle, and introduces various tools and libraries such as diffgram, ecco, and s3prl. However, it does not provide a direct answer to the question about the combination of architectural decisions that would create an unexpected conflict between memory management, adapter handling, and file operations."
"When implementing a production code generation system that supports visualization tasks and handles file uploads from multiple users, which combination would create an INEFFICIENT interaction between adapter management, memory utilization, and background processing?","[(2576, 'b2850ab0-096f-4c74-a366-bed32cae6541'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (11293, 'dc3bf3a6-1caa-462a-a58e-da5e80984fcf'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (11295, '2bf9366b-a9e7-40a8-a7ba-93163869dccc'), (28707, '1579ae2b-a259-4e21-a4ab-6554888fc9db'), (28708, '218eb18e-351e-4810-8501-392aa5b0710b'), (5158, '2a7fe1df-7c54-4ebf-9da2-15523efb3fd1'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (30776, 'a5497ed7-9df9-460a-9ab8-8fffde7862ad'), (10300, '4c09aeb7-71b2-4501-8a66-fd42121a6a51'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (3133, '290438b8-35f5-48bf-a9b3-7395de01e68b'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (27221, '3a0769e6-1ce2-4b41-9dbd-f9d0c6e8b04a'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (7768, '224a5adb-2c8c-434b-b5b5-b6fab137f1b8'), (21081, 'e0a3e5ea-3ee7-4a1a-bb3a-5ea4032d7685'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (22119, 'b0e6ddbb-4f06-4aaa-a05e-0c06c29dcd0d'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (31366, 'faca457c-cb2d-44f1-a992-b468cb6d6a45'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (25764, 'e6b7b172-a955-4460-a174-888528a760ca'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (21163, '7d118280-4cd6-4c0e-91c4-dfb3d2e2f08a'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (24273, '9d89b90d-0743-475c-8c30-c52c8b3feee9'), (19155, '1e812ea9-3a09-4467-be78-c43f09a00fc7'), (28372, '15a0b464-97c2-440b-bdc2-bd235ffd25b0'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (19697, '2bb5d1ca-7bcf-4074-b6a2-32572672320c'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (18685, '52fdc5bf-09c1-40d7-85f6-9c586ebe5ef0'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (24835, '3eef27d6-f46c-4bfc-97e3-795dd6084cc2'), (24836, 'b29f87dd-e4a7-4d1b-82e2-c2260673c83a'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (23810, '4388669e-a841-4834-a724-ea542053f2d2'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (16142, 'c771e38b-cfa7-4d59-96c0-b3151c87a39d'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (22305, '5cfa1e99-4c0f-4093-9ed4-3209a1693753'), (31034, 'a034f9fa-fd22-41d8-a2e8-629d5c7d0e2d'), (19258, '7e2374ba-f5a1-4579-b017-e0032e14d937'), (27460, '42d93388-c034-4d39-b6e2-99a5343894ac'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (8528, 'b189bace-ccb7-4671-af71-98b97ec1247c'), (2900, '621cacc6-9b6e-4538-b4b5-bba92e480d5d'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (1881, '664f3e23-c32a-4ab5-a015-fbe79134f466'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (15720, 'c3517980-a237-4b69-bf3b-15b281810859'), (22377, '89b0baa2-d73d-414b-931d-5db81d1314b0'), (15722, 'bc56f95a-1673-473e-a458-2454bf362462'), (31596, 'dffc937e-55b3-44ec-9044-5bcda5a6f429'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (15727, 'ad759577-7206-4451-8db1-fd73d99c3223'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (23930, 'b54bef38-4510-49ab-87a1-dbc20ba90085'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (19853, '40bb732f-f865-45e3-bf29-9f8451bcb193'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (19855, '9800f1ae-0b10-4f47-8b9b-9a97f69d8a08'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (19856, '1779d346-5968-4964-a7b4-39c87c76dd12'), (7057, 'a2b32ca0-9edb-46ca-9502-dd65ffb3749f'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7083, '408ec2a3-63f7-4564-8fa9-a7a22a7a65ef'), (7084, '966d14eb-c2c1-4bae-96ce-6f2ced24abfc'), (1451, 'f095b72f-5e6e-4d36-afc8-3aaab8db07ba'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (14334, '0a0f2346-9a53-4656-8089-a80361eab1f9'), (26039, '06e385f9-19f2-4a2b-ac64-9ac03070c5eb'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (29116, '95611315-e02f-4e0d-a585-27fdb4ffaede'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (26060, 'c163368d-4735-4bb6-8bf9-5b38c03c8927'), (19411, '140ee8ae-a55f-4c8c-a331-3e7e31530f72'), (26067, '61a8f2fe-8e9a-4fd1-994b-c13c84582f4a'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (21980, '9654551a-62fd-46b7-91c6-1b2ce272aaec'), (26078, 'cfbea3c4-ec20-4c92-9d29-a46b4584cbcf'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (19428, 'de136064-8fa4-4750-89b5-91d689b7d535'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (2553, '6f02feaf-a5c0-4ce3-b09e-1d7bb121ef85'), (12284, '6d689117-c87e-4804-860a-cc690572a3f0'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (2558, '435ccb30-f9d9-453b-9f45-2bc2b472719e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: - **Parallelization**. Workloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute in parallel on XLA devices. For text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance.

👉 Try it out for yourself:

[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/google/sdxl)

## Stable Diffusion XL pipeline in JAX

Upon having access to a TPU VM (TPUs higher than version 3), you should first install
a TPU-compatible version of JAX:
#### High-performance throughput for high batch sizes

Workloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute it in parallel on various XLA devices. 
For text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance. For example, running SDXL on a TPU with 8 chips will generate 8 images in the same time it takes for 1 chip to create a single image.

TPU v5e instances come in multiple shapes, including 1, 4 and 8-chip shapes, all the way up to 256 chips (a full TPU v5e pod), with ultra-fast ICI links between chips. This allows you to choose the TPU shape that best suits your use case and easily take advantage of the parallelism that JAX and TPUs provide.
```

You may see a small performance boost in VAE decoding on multi-image batches, and there should be no performance impact on single-image batches.

## Tiled VAE

Tiled VAE processing also enables working with large images on limited VRAM (for example, generating 4k images on 8GB of VRAM) by splitting the image into overlapping tiles, decoding the tiles, and then blending the outputs together to compose the final image. You should also used tiled VAE with [`~ModelMixin.enable_xformers_memory_efficient_attention`] to reduce memory use further if you have xFormers installed.

To use tiled VAE processing, call [`~StableDiffusionPipeline.enable_vae_tiling`] on your pipeline before inference:

```python
import torch
from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler
For more details on the process, please [download our paper](https://huggingface.co/latent-consistency/lcm-lora-sdxl/resolve/main/LCM-LoRA-Technical-Report.pdf).

## Why does this matter?

Fast inference of Stable Diffusion and SDXL enables new use-cases and workflows. To name a few:

- **Accessibility**: generative tools can be used effectively by more people, even if they don’t have access to the latest hardware.
- **Faster iteration**: get more images and multiple variants in a fraction of the time! This is great for artists and researchers; whether for personal or commercial use.
- Production workloads may be possible on different accelerators, including CPUs.
- Cheaper image generation services.
### Making use of `dataloader_num_workers`

When image loading is done on CPU, a quick way to speed it up would be to allocate more subprocesses for data loading. This is very easy to do with Transformers' `TrainingArguments` (or its Optimum Habana counterpart `GaudiTrainingArguments`): you can use the `dataloader_num_workers=N` argument to set the number of subprocesses (`N`) allocated on CPU for data loading.

The default is 0, which means that data is loaded in the main process. This may not be optimal as the main process has many things to manage. We can set it to 1 to have one fully dedicated subprocess for data loading. When several subprocesses are allocated, each one of them will be responsible for preparing a batch. This means that RAM consumption will increase with the number of workers. One recommendation would be to set it to the number of CPU cores, but those cores may not be fully free so you will have to try it out to find the best configuration.
 
Here is the question: When implementing a production code generation system that supports visualization tasks and handles file uploads from multiple users, which combination would create an INEFFICIENT interaction between adapter management, memory utilization, and background processing?","Based on the context, an INEFFICIENT interaction between adapter management, memory utilization, and background processing would be created by not using `pmap` for parallelization, not using `dataloader_num_workers` to allocate multiple subprocesses for data loading, and not using tiled VAE processing for large images. This would lead to inefficient use of resources, slow processing, and potential memory issues."
"In developing a code generation system that supports visualizations and handles concurrent training requests, which scenario would create an UNEXPECTED performance bottleneck between adapter training, file management, and memory utilization?","[(25615, '01944eb1-009d-4629-9882-c62631a911ec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (22550, 'c9b2e9cd-5ce9-4394-991e-a8fcf144ff79'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (11293, 'dc3bf3a6-1caa-462a-a58e-da5e80984fcf'), (28707, '1579ae2b-a259-4e21-a4ab-6554888fc9db'), (9255, 'a1f59c27-8f7e-498f-b9f1-403f32170220'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (14388, '5cf9f858-9101-44e2-8239-8859192f9e97'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (30776, 'a5497ed7-9df9-460a-9ab8-8fffde7862ad'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (10300, '4c09aeb7-71b2-4501-8a66-fd42121a6a51'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (5695, '556c87ee-1b6b-463d-bdf7-e1a949117c70'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (17491, '02833533-a92d-42aa-a3d0-461fffbf27bc'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (27221, '3a0769e6-1ce2-4b41-9dbd-f9d0c6e8b04a'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (93, '7faf9e9d-39cb-4c7e-b26d-cdd58e13744f'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (7773, '1bee6f95-e2d3-48ee-9d13-df7015022dbf'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (9832, 'bdc6bea1-7f20-4300-aae9-3f7ea03a6783'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (9860, 'b127b619-65e1-4d79-844d-ade94f6c03a8'), (31366, 'faca457c-cb2d-44f1-a992-b468cb6d6a45'), (21149, '93f58500-464d-41bb-8670-d75c3cba97ae'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (25764, 'e6b7b172-a955-4460-a174-888528a760ca'), (21163, '7d118280-4cd6-4c0e-91c4-dfb3d2e2f08a'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (6337, '1c3d8092-7d15-4854-8c95-d6d89c63ebcc'), (19155, '1e812ea9-3a09-4467-be78-c43f09a00fc7'), (20693, '6ceda67f-d532-4fa1-99ca-1fe2e1e1fe8c'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (6364, '220a3825-9318-440f-b0a5-547ed3feced8'), (25313, '1ed5de91-8020-4aa8-b796-a3e19fe177d8'), (6370, '5d4d5a4e-c039-411b-b725-afbb79ba51aa'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (6379, '36d7ef41-e87a-403e-ba7f-e2dbd52bfc2a'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (24824, 'fdd418a8-03d9-44e2-bdb6-37f035778243'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (24835, '3eef27d6-f46c-4bfc-97e3-795dd6084cc2'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (2837, 'e2f54652-0d9d-434f-b351-5c014b71a3da'), (6940, 'fc0d9fc0-31dc-43e0-8229-840222e72b80'), (2852, '4070192e-9515-4222-9c1e-c07081d79086'), (31013, 'c55ce3d9-ab37-45fc-bb0a-3c19c6e2452c'), (31016, '05908d70-238b-4150-bec9-b27c6ef05086'), (6452, 'a0bfc105-238a-44a5-8bc3-dbd3c50aa986'), (26944, '072e725f-9ea3-4bac-8527-72d874dcd2ff'), (8528, 'b189bace-ccb7-4671-af71-98b97ec1247c'), (26962, '0fda646c-a410-418e-bef2-061878be6905'), (26963, 'd37c4394-4313-4a68-a48c-e1a6e18c49dc'), (27475, '8fdc35b4-b9d4-4822-8a62-f9f2cca68e51'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (1881, '664f3e23-c32a-4ab5-a015-fbe79134f466'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (15719, 'ad8bd069-bdd3-45ec-85fb-2f8065c7e860'), (15720, 'c3517980-a237-4b69-bf3b-15b281810859'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (3953, '51c92992-b74c-4419-9909-8e8043bbb890'), (4469, '0b4ca93b-c818-4499-9a67-b7a1d33c3d55'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (5508, '18f70486-80e2-4c2d-a2f1-f9e6174711dd'), (24969, '4f152556-8bfa-4a38-8030-becaa6e743bc'), (5517, '78e547ac-9ff2-4317-9554-f9c745ffeac6'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (30617, 'af7be972-7339-438b-8c50-8bc5c32d23e2'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (7078, 'ddf6eca5-f40f-4432-82a0-55a2aede1980'), (1450, 'b31e6fae-396f-4c6e-a1de-dd46a0d1c402'), (7595, 'e9a0cc5c-98d6-4e8d-8b64-1c28f8f15d45'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (13232, '20b2f603-8291-4ad7-ac6f-518e75571a4b'), (26039, '06e385f9-19f2-4a2b-ac64-9ac03070c5eb'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (4547, '9d7a8092-0c89-45c6-990c-7ac6a0743ada'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (31196, '1f882f74-f46b-4b40-9bf3-14c1c4a21c25'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (16373, '7a710310-ff81-457f-9396-d32fe5aece98'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (12282, 'b54aa9db-2082-4d68-b330-43589ff6d668'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
```

Combined with other approaches (gradient accumulation, gradient checkpointing, and mixed precision training), 
you can expect to get about a 3x memory improvement and even slightly higher throughput as using Adafactor. 

### multi_tensor

pytorch-nightly introduced `torch.optim._multi_tensor` which should significantly speed up the optimizers for situations 
with lots of small feature tensors. It should eventually become the default, but if you want to experiment with it sooner, take a look at this GitHub [issue](https://github.com/huggingface/transformers/issues/9965).

## Data preloading

One of the important requirements to reach great training speed is the ability to feed the GPU at the maximum speed it 
can handle. By default, everything happens in the main process, and it might not be able to read the data from disk fast 
enough, and thus create a bottleneck, leading to GPU under-utilization. Configure the following arguments to reduce the bottleneck:
## What is this about?
This is an open-source effort to train and evaluate code generation models. CodeParrot 🦜 is a GPT-2 model trained from scratch on Python code. The highlights of this project are:
- initialize and train a GPT-2 language model from scratch for code generation
- train a custom tokenizer adapted for Python code
- clean and deduplicate a large (>100GB) dataset with `datasets`
- train with `accelerate` on multiple GPUs using data parallelism and mixed precision
- continuously push checkpoints to the hub with `huggingface_hub`
- stream the dataset with `datasets` during training to avoid disk bottlenecks
- apply the `code_eval` metric in `datasets` to evaluate on [OpenAI's _HumanEval_ benchmark](https://huggingface.co/datasets/openai_humaneval)
- showcase examples for downstream tasks with code models in [examples](https://github.com/huggingface/transformers/tree/main/examples/research_projects/codeparrot/examples) folder:
    - Algorithmic complexity prediction
As a result, this implementation had some limitations:

1. **Synchronization bottleneck**: The rank 0 worker had to receive the inputs from all workers, perform the index query, and then send the results back to the other workers. This limited performance with multiple training workers.
2. **PyTorch specific**: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well.

![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Adapters

Adapter-based methods add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. The method varies depending on the adapter, it could simply be an extra added layer or it could be expressing the weight updates ∆W as a low-rank decomposition of the weight matrix. Either way, the adapters are typically small but demonstrate comparable performance to a fully finetuned model and enable training larger models with fewer resources.

This guide will give you a brief overview of the adapter methods supported by PEFT (if you're interested in learning more details about a specific method, take a look at the linked paper).

## Low-Rank Adaptation (LoRA)

<Tip>
 
Here is the question: In developing a code generation system that supports visualizations and handles concurrent training requests, which scenario would create an UNEXPECTED performance bottleneck between adapter training, file management, and memory utilization?","Based on the context, a scenario that would create an UNEXPECTED performance bottleneck between adapter training, file management, and memory utilization is when the main process is unable to read the data from disk fast enough, leading to a bottleneck and GPU under-utilization. This is mentioned in the ""Data preloading"" section, where it is stated that by default, everything happens in the main process, and it might not be able to read the data from disk fast enough, creating a bottleneck."
"When scaling a code generation and visualization system that supports both fine-tuning and inference, which architectural pattern would create an UNEXPECTED conflict between GPU memory usage, adapter management, and background task processing?","[(26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (1048, '077f9cb3-e4e5-4c39-9b4e-c82fc3e59a53'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (11293, 'dc3bf3a6-1caa-462a-a58e-da5e80984fcf'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (1057, '286fed70-f474-4493-8e5e-c4a3e544ae9a'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (27684, 'f2ce3abb-ae36-4782-abdf-2e04a9cbd843'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (14388, '5cf9f858-9101-44e2-8239-8859192f9e97'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (10304, '7939d5d1-b5ad-4196-8744-4b704b4f89f4'), (17473, '7ea7a82a-c3b8-4c05-9c1e-5acd375cd178'), (19526, '790c4385-8da2-4412-bf50-64e4a5ffadd3'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (21076, 'f7b2eb77-c2fc-4c18-926b-38ac4c905bda'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (19549, '09c34333-c237-43a8-9e29-0dda3e054d55'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (16479, 'f8325aa8-886a-43ff-9c7c-96bb8c00df22'), (5729, '8325f704-8f9e-470a-9d07-777ab1401244'), (5733, 'd7b94280-d5e4-4cbe-8307-82acc7c174e7'), (16486, '44b89854-89ae-4b45-9b98-62b3d9edc5a7'), (9832, 'bdc6bea1-7f20-4300-aae9-3f7ea03a6783'), (19568, '6b7c6d4d-3fa2-445d-89f4-171fb9e18a91'), (26229, '81993a76-b643-4458-bf4c-17b0dbd5017a'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (4217, '1a8e3437-ea12-4e7b-b9d9-6929a9a940d2'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (17069, '4496b2e2-2c6b-48b8-bf4c-cc6ff08631d2'), (19155, '1e812ea9-3a09-4467-be78-c43f09a00fc7'), (20693, '6ceda67f-d532-4fa1-99ca-1fe2e1e1fe8c'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (25318, '7f293204-2059-483b-9146-91aa485398d2'), (21226, '6057222e-cc61-4311-8050-39fc93c54367'), (10989, '6bcf6d91-b596-48cd-903e-99802046e818'), (11502, '330fe3ba-ad53-40c9-bac9-59cda30571b5'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (22768, '0aa32bd4-ae8d-4d98-b212-4087c9da1896'), (25842, '5b8c1753-2875-46c1-bc03-2f045ea3f2d1'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (22779, '1b8ccb79-44ab-4d6d-b222-a9f1fd2244d4'), (25852, 'e68a74a5-1c65-4315-9cac-e0006e78744c'), (13059, '788c6dab-5fa2-4565-bc70-01b658b4b0b1'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (16659, 'e987cb5c-a4c7-4752-a3f4-f412c4611ee6'), (16660, '1dfbc513-b29b-4f26-ac16-6bccdb48a3d4'), (16661, 'f6db86f2-9435-45f8-814c-5423a566c4e4'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (2840, 'a7a5b6a3-c3fc-42f7-8acd-d505fc631ec4'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (1328, '83e4417a-430e-4f49-bd41-8194f2f9f1de'), (11056, '4481763c-f883-4267-891f-58ed35d27b0a'), (19258, '7e2374ba-f5a1-4579-b017-e0032e14d937'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (30578, '4dae799c-0e35-447a-8126-3f98ef267e33'), (14707, '653a154c-0baf-4e34-98d6-d2362d781d53'), (23928, 'fa312533-bb24-4e27-982f-befb97bafd49'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (23931, '8f9f128a-7da6-4f3e-ac03-82ff4dad7ee6'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (19849, '6c574035-79ab-4b53-9e2c-1c285463bc09'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (7061, 'fb397a69-b4e2-424e-a392-b35e056c62f6'), (7062, 'c8c890e1-e7b8-4e8f-9799-111728d14f27'), (7064, '284ad4d1-ec71-4b4f-b163-bbf70b5378f9'), (7065, '0fb31b29-d4f9-4207-9934-c683259bbb1a'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (7592, '68ddeb8e-b285-47f9-8754-09b4fd273d73'), (10665, 'e14680b1-7738-4c0b-a87e-9a53b513187f'), (7595, 'e9a0cc5c-98d6-4e8d-8b64-1c28f8f15d45'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (26039, '06e385f9-19f2-4a2b-ac64-9ac03070c5eb'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (19391, '5daa3c15-bac8-4a3c-a34d-410179a60ddb'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (10696, 'd7e8042d-bfce-457e-a6e7-9a1041ca49c7'), (26060, 'c163368d-4735-4bb6-8bf9-5b38c03c8927'), (19407, '5f9399cd-785b-459e-bf05-4d7cc41174c0'), (19410, '7976cc34-bf07-47ba-88b4-7b4ced5300f2'), (19411, '140ee8ae-a55f-4c8c-a331-3e7e31530f72'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (26067, '61a8f2fe-8e9a-4fd1-994b-c13c84582f4a'), (24023, 'bcd43be2-5718-48a7-891e-7e52f6ce3f48'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (19417, '80d969b2-21ce-4940-9524-9ec16a17be45'), (19416, '462a3d4a-01ab-4043-92a4-6a83ce7f1226'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (19420, '77fc1221-9255-4ffb-a4cf-e269a82718ee'), (19421, '5255c4d0-d23c-4cd7-8448-2b8fe944282f'), (19422, 'deb5a362-1750-4c97-a4db-ae50380210f6'), (19426, '5402ad54-6120-439b-a190-8c348e067de1'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (28647, 'd02afa5d-00f3-427d-9bb9-077fbdf21f7f'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Considerations for Effective Autoscaling

While autoscaling offers convenient resource management, certain considerations should be kept in mind to ensure its effectiveness:

- **Model Initialization Time**: During the initialization of a new replica, the model is downloaded and loaded into memory. If your replicas have a long initialization time, autoscaling may not be as effective. This is because the average GPU utilization might fall below the threshold during that time, triggering the automatic scaling down of your endpoint.

- **Enterprise Plan Control**: If you have an [enterprise plan](https://huggingface.co/inference-endpoints/enterprise), you have full control over the autoscaling definitions. This allows you to customize the scaling thresholds, behavior and criteria based on your specific requirements.

## Scaling to 0
Each pushes forward different approaches to improve aspects of the memory allocation and management on various software.

### 2. Efficient parallelization of computations

Now that we have an efficient way to represent our data, we need a way to take the most out of the computational hardware at our disposal. 
Interestingly, when it comes to inference, CPUs have a potential advantage over GPUs in the sense they are everywhere, and they do not require specific application components and administration staff to operate them.

Modern CPUs come with many cores and complex mechanisms to increase the general performances of software. 
Yet, as we highlighted on [the first blog post](https://hf.co/blog/bert-cpu-scaling-part-1), they also have features which can be tweaked depending on the kind of workload (CPU or I/O bound) you target, to further improve performances for your application.
- Tensor Parallelism and custom cuda kernels
- Optimized transformers code for inference using [flash-attention](https://github.com/HazyResearch/flash-attention) on the most popular architectures
- Quantization with [bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
- [Continuous batching of incoming requests](https://github.com/huggingface/text-generation-inference/tree/main/router) for increased total throughput
- Accelerated weight loading (start-up time) with [safetensors](https://github.com/huggingface/safetensors)
- Logits warpers (temperature scaling, topk, repetition penalty ...)
- Watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226)
- Stop sequences, Log probabilities
- Token streaming using Server-Sent Events (SSE)

Officially supported model architectures are currently:
<Tip>

The majority of the optimizations described here also apply to multi-GPU setups!

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 is experimental and may change considerably in future versions.

</Tip>

[FlashAttention-2](https://huggingface.co/papers/2205.14135) is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:

1. additionally parallelizing the attention computation over sequence length
2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them
Autoscaling

Autoscaling allows you to dynamically adjust the number of endpoint replicas running your models based on traffic and accelerator utilization. By leveraging autoscaling, you can seamlessly handle varying workloads while optimizing costs and ensuring high availability.

## Scaling Criteria

The autoscaling process is triggered based on the accelerator's utilization metrics. The criteria for scaling differ depending on the type of accelerator being used:

- **CPU Accelerators**: A new replica is added when the average CPU utilization of all replicas reaches 80%.

- **GPU Accelerators**: A new replica is added when the average GPU utilization of all replicas over a 2-minute window reaches 80%.

It's important to note that the scaling up process takes place every minute, while the scaling down process takes 2 minutes. This frequency ensures a balance between responsiveness and stability of the autoscaling system, with a stabilization of 300 seconds once scaled up or down.
 
Here is the question: When scaling a code generation and visualization system that supports both fine-tuning and inference, which architectural pattern would create an UNEXPECTED conflict between GPU memory usage, adapter management, and background task processing?","Based on the provided context, there is no specific architectural pattern mentioned that would create an unexpected conflict between GPU memory usage, adapter management, and background task processing. The context primarily discusses autoscaling, model initialization time, and efficient parallelization of computations, but it does not provide information about a specific architectural pattern that would cause such a conflict. Therefore, it is not possible to provide a concise and relevant answer to this question based on the given context."
"When deploying a code generation system that supports multiple simultaneous users requesting both visualizations and code completions, which implementation would create an UNINTENDED interaction between memory management, request handling, and model serving?","[(10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (15885, '971fc97e-36dc-4975-85ab-37248b6d6a03'), (19983, '4062b714-20ff-4056-849e-b5a8370c101c'), (29202, 'a6666314-edd1-47cc-8628-335a50fe419d'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (17443, 'bd891b6b-f323-49c6-b503-c3e499ebcdd1'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (6723, '01ac9d03-828c-47a2-95fc-a16de8fc474e'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (15439, '6f14c25c-4ef7-4d89-92fe-807805430b3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (8805, '82dbedd7-d790-4508-add3-69d6b8e877bb'), (18534, '17e1b977-8c46-42c8-a442-b460406e3817'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (19562, '68f9ee3c-a7da-4933-9b61-4d6d35d9314e'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (27281, 'f3dd0817-48af-4ac1-90c6-b3965d561469'), (27282, '085f932c-d040-49a3-8044-6cff137fa4df'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (1185, '13e997d5-137f-444d-b40b-2f50b3de78d5'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (21163, '7d118280-4cd6-4c0e-91c4-dfb3d2e2f08a'), (2733, '93647d71-2542-44d6-8e19-fe6737bb6b5c'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (21175, '6624dc16-3bb1-43ed-8494-bcba02c7b96e'), (21176, '2c7c306b-d840-49b2-8ce5-f553024b9d6e'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (15540, 'a7378dbc-d7f5-4167-a200-01aad0625463'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (2773, '06afc023-a422-4936-9dba-57a0537261aa'), (18137, '4f4937f4-d751-41b9-8e2e-f1ed6ae2297d'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (13555, '614c828b-e60a-46cf-beba-9b0a67fd3355'), (24824, 'fdd418a8-03d9-44e2-bdb6-37f035778243'), (12026, '0c59c0a7-bb17-41b9-ac73-98736a9b573e'), (253, '552c3720-bbfa-4b2b-995c-af6815d68dfd'), (16641, '29e13c06-e204-4a93-a6ac-03bf2fc934e4'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (24836, 'b29f87dd-e4a7-4d1b-82e2-c2260673c83a'), (24837, '2dcb8c9f-c4ce-497d-8bce-11e9cff7d5b6'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (24846, '8f3aab04-758c-4a01-bb65-22c3a112c220'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (31005, 'dfbe26eb-d794-43e1-8e52-483e186cd71c'), (31006, '570011b0-e075-4315-97cc-b8c673d126d2'), (27423, 'cdb54c20-dca4-48f4-96a7-f000fa351601'), (31016, '05908d70-238b-4150-bec9-b27c6ef05086'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (11579, 'e2b354ec-02aa-4b95-9146-bb9e9a359a73'), (27460, '42d93388-c034-4d39-b6e2-99a5343894ac'), (27466, 'bfb56804-3d91-45c6-b17f-8d15d5d3a201'), (8528, 'b189bace-ccb7-4671-af71-98b97ec1247c'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (17782, '99c8f91e-122c-48d3-b93f-ec1128e97292'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (17793, '8e853aa7-59db-4aaf-b04c-fe7fa8b49fcd'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (20875, 'c60f0ee3-dfe8-4e04-ae6a-dbdf52ecf36b'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (411, '96253f57-9a1a-4672-829b-a8e8241df3cf'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (7075, 'cd0d23dc-2f6e-40b8-8969-9ad62955aa9c'), (7077, '74cd7578-3563-425a-be27-134bc2db8891'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7082, 'b972f2bc-c1a6-49e3-a883-bc6a5f268e63'), (7083, '408ec2a3-63f7-4564-8fa9-a7a22a7a65ef'), (7084, '966d14eb-c2c1-4bae-96ce-6f2ced24abfc'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (16309, '2135aa54-c407-499c-b2a2-0189de3220a5'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (31163, '5156e6c6-d4cb-43ea-a682-eed8719fcd71'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (21972, '85cbe3b6-6131-4a4a-9548-070d1c3b8c2d'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (26078, 'cfbea3c4-ec20-4c92-9d29-a46b4584cbcf'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (15841, '73c3ef6b-2bae-40e2-a3de-ab7157a99930'), (14821, 'c34abfed-956a-47bc-8db5-e28e8b62e1ce'), (14822, '3a7ef6c9-1f57-4061-a655-76352eb3e163'), (14823, 'c02c38d2-2a34-418e-954a-4b26b11b52e4'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (17403, '38d35ffd-8bed-4e4d-863e-d2214e1e2d06'), (14845, '891fe839-b160-4785-a384-31a056821744'), (14846, 'cf703fd3-cc50-49a5-bfa6-2513ea2118cb')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the way for their deployment on inference endpoints
serving multiple requests in parallel.

There is still plenty of room for improvement though:
- in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory.
Alternative options such as pipelining are currently integrated,
- the static sequence length limits the model ability to encode long contexts. It would be interesting to see if attention sinks might be a valid option to address this.
Overall, this requires less GPUs to serve all distinct models, even though we already had a way to share GPUs between deployments to maximize their compute usage. In a **2min** time frame, there are approximately **10** distinct LoRA weights that are requested. Instead of spawning 10 deployments, and keeping them warm, we simply serve all of them with 1 to 2 GPUs (or more if there is a request burst).


## Implementation

We implemented LoRA mutualization in the Inference API. When a request is performed on a model available in our platform, we first determine whether this is a LoRA or not. We then identify the base model for the LoRA and route the request to a common backend farm, with the ability to serve requests for the said model. Inference requests get served by keeping the base model warm and loading/unloading LoRAs on the fly. This way we can ultimately reuse the same compute resources to serve many distinct models at once.

### LoRA structure
We can observe that the `copilot` adapter gets it right in both cases. Therefore, it performs as expected for code-completions when working with HF specific codebase as well as generic codebases.

**Now, as a user, I want to combine the ability of `assistant` as well as `copilot`. This will enable me to use it for code completion while coding in an IDE, and also have it as a chatbot to answer my questions regarding APIs, classes, methods, documentation. It should be able to provide answers to questions like `How do I use x`, `Please write a code snippet for Y` on my codebase.**

PEFT allows you to do it via `add_weighted_adapter`. Let's create a new adapter `code_buddy` with equal weights to `assistant` and `copilot` adapters.

![combining_loras](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/personal_copilot/combine_adapters.png)
Combining Multiple Adapters

Now, let's see how `code_buddy` performs on the `chatting/question_answering` tasks.
The StarCoder models offer unique characteristics ideally suited to enterprise self-hosted solution:

- State of the art code completion results - see benchmarks in the [paper](https://huggingface.co/papers/2305.06161) and [multilingual code evaluation leaderboard](https://huggingface.co/spaces/bigcode/multilingual-code-evals)
- Designed for inference performance: a 15B parameters model with code optimizations, Multi-Query Attention for reduced memory footprint, and Flash Attention to scale to 8,192 tokens context.
- Trained on [the Stack](https://huggingface.co/datasets/bigcode/the-stack), an ethically sourced, open source code dataset containing only commercially permissible licensed code, with a developer opt-out mechanism from the get-go, refined through intensive PII removal and deduplication efforts.
Second, when you know you’ll get concurrent text generation requests, you can batch the inputs and massively increase the throughput with a small latency penalty. The model layer weights loaded into the device are now used on several input rows in parallel, which means that you’ll get more tokens out for approximately the same memory bandwidth burden. The catch with batching is that you need additional device memory (or to offload the memory somewhere) – at the end of this spectrum, you can see projects like [FlexGen](https://github.com/FMInference/FlexGen) which optimize throughput at the expense of latency.

```python
# Example showcasing the impact of batched generation. Measurement device: RTX3090
from transformers import AutoModelForCausalLM, AutoTokenizer
import time

tokenizer = AutoTokenizer.from_pretrained(""distilgpt2"")
model = AutoModelForCausalLM.from_pretrained(""distilgpt2"").to(""cuda"")
inputs = tokenizer([""Hello world""], return_tensors=""pt"").to(""cuda"")
 
Here is the question: When deploying a code generation system that supports multiple simultaneous users requesting both visualizations and code completions, which implementation would create an UNINTENDED interaction between memory management, request handling, and model serving?","Based on the context, the implementation that would create an unintended interaction between memory management, request handling, and model serving when deploying a code generation system that supports multiple simultaneous users requesting both visualizations and code completions is increasing the batch size to augment the throughput.

This is because increasing the batch size would require additional device memory, which could lead to unintended interactions between memory management, request handling, and model serving. The context mentions that the current implementation is limited by device memory, and increasing the batch size"
"When implementing a system that supports both custom visualization generation and code completion while handling model updates, which approach would create an UNEXPECTED bottleneck between training data management, inference serving, and adapter updates?","[(25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (5661, 'd8772dad-7e28-4ef1-bdae-a1b7fda9c18c'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (17968, '25abd1b0-11bc-4ed9-88b1-627350fe116f'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (21561, '68fa59f6-381e-4dee-ae59-873812588c22'), (30787, '7b96f31b-bc88-4d48-b746-78613f9db9ff'), (30794, '60649a7f-802b-4687-8b03-4c75a01eca78'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (18514, '2b8c705b-aa16-4343-9147-042e810a70a2'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (7768, '224a5adb-2c8c-434b-b5b5-b6fab137f1b8'), (22102, 'aff80ee3-6694-4bab-acd1-453fcb62fc3c'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (14444, '42f274f1-22ad-430f-a89d-972eec106f5f'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (1139, '55f9357e-08e0-41a3-b826-7b0d6c11b95d'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (24230, 'bd10f584-b63e-4b9f-89e3-194c6bdb0f54'), (4265, 'f389f1dd-debe-4020-a0a3-4923eaae7aee'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (2733, '93647d71-2542-44d6-8e19-fe6737bb6b5c'), (8366, '802fb177-2a03-456c-b39c-e9d1d7f566ed'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (22194, '7ba04e95-3e33-405e-b3ed-2bb503729d69'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22209, 'd7ca43f5-90c6-448e-8d33-58c7acba90c4'), (16577, '65b23733-c969-4f5c-815e-b4d8c66dc8a9'), (20673, '0ad1dd04-3ec3-4c8f-9d67-b44414433153'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (8391, '31b94b1d-9202-40c1-a45e-130821ef4321'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (27351, '472b1c8c-b7ef-4926-8092-42cba02c3d9c'), (24280, 'ef680020-6ecc-48bf-a65e-0761dfb2d098'), (6370, '5d4d5a4e-c039-411b-b725-afbb79ba51aa'), (25321, 'b7befe0e-86b5-4603-8086-b358c0606f7c'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (22768, '0aa32bd4-ae8d-4d98-b212-4087c9da1896'), (20730, 'b9f51b25-0a14-4b14-a1a6-92b1e0278fc3'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (31013, 'c55ce3d9-ab37-45fc-bb0a-3c19c6e2452c'), (8495, '5c788f86-a885-481c-aaef-b1ff529e9036'), (4403, 'e8bbbe12-29e1-4b33-9b38-8873d737fec4'), (8503, 'a035c2be-ea22-4ac9-8f47-6603afbcf025'), (30528, '69aa4012-1b33-4373-af24-1d94ec8dd03d'), (8514, 'f40eb165-8653-4867-a7a9-cb27eebdfda0'), (23900, 'bdf9c5d4-29b8-480f-b334-1fd2e6ee8a76'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (14685, 'f6385cc5-2ea5-401f-bace-7cbe69ca1700'), (23903, '923c4027-8d23-4edc-9d3e-daf70d4b077b'), (14684, '5ae632de-fdb4-46f2-8f90-67cc8c042104'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (14706, '332b6a8d-ec91-483f-a989-0ec3c2d81a1c'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (21362, '651a9fca-5d13-4c33-9dd8-27cca38b640b'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (23930, 'b54bef38-4510-49ab-87a1-dbc20ba90085'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (3458, '933bcf19-3ed0-4489-b20d-2fed591675c0'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (19850, '9ca3b142-5c97-4269-a829-4796ef40366c'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (19856, '1779d346-5968-4964-a7b4-39c87c76dd12'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (30617, 'af7be972-7339-438b-8c50-8bc5c32d23e2'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (7075, 'cd0d23dc-2f6e-40b8-8969-9ad62955aa9c'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (15826, 'acf90a4b-5709-485e-9eac-b6926bb65505'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (29666, 'd518d011-4a11-4759-922a-8c4af66d1d89'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (1524, '5de887ce-b7a7-4223-994c-f8f3ad8e7ec3'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Due to the improved saving procedure, training on large datasets generates small model sizes. In the example below, a BERTopic model was trained on 100,000 documents, resulting in a ~50MB model keeping all of the original’s model functionality. For inference, the model can be further reduced to only ~3MB!

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/2d1113254a370972470d42e122df150f3551cc07/blog/BERTopic/serialization.png) 
The benefits of this integration are particularly notable for production use cases. Users can now effortlessly deploy BERTopic models into their existing applications or systems, ensuring seamless integration within their data pipelines. This streamlined workflow enables faster iteration and efficient model updates and ensures consistency across different environments.

### safetensors: Ensuring Secure Model Management
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Adapters

Adapter-based methods add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. The method varies depending on the adapter, it could simply be an extra added layer or it could be expressing the weight updates ∆W as a low-rank decomposition of the weight matrix. Either way, the adapters are typically small but demonstrate comparable performance to a fully finetuned model and enable training larger models with fewer resources.

This guide will give you a brief overview of the adapter methods supported by PEFT (if you're interested in learning more details about a specific method, take a look at the linked paper).

## Low-Rank Adaptation (LoRA)

<Tip>
The end result is a model that is adapted to the code languages, standards and practices of the customer. Through this process, SafeCoder customers learn the process and build a pipeline for creating and updating their own models, ensuring no vendor lock-in, and keeping control of their AI capabilities.

### Deploying SafeCoder

During the setup phase, SafeCoder customers and Hugging Face design and provision the optimal infrastructure to support the required concurrency to offer a great developer experience. Hugging Face then builds SafeCoder inference containers that are hardware-accelerated and optimized for throughput, to be deployed by the customer on their own infrastructure.

SafeCoder inference supports various hardware to give customers a wide range of options: NVIDIA Ampere GPUs, AMD Instinct GPUs, Habana Gaudi2, AWS Inferentia 2, Intel Xeon Sapphire Rapids CPUs and more.

### Using SafeCoder
## Considerations for Effective Autoscaling

While autoscaling offers convenient resource management, certain considerations should be kept in mind to ensure its effectiveness:

- **Model Initialization Time**: During the initialization of a new replica, the model is downloaded and loaded into memory. If your replicas have a long initialization time, autoscaling may not be as effective. This is because the average GPU utilization might fall below the threshold during that time, triggering the automatic scaling down of your endpoint.

- **Enterprise Plan Control**: If you have an [enterprise plan](https://huggingface.co/inference-endpoints/enterprise), you have full control over the autoscaling definitions. This allows you to customize the scaling thresholds, behavior and criteria based on your specific requirements.

## Scaling to 0
Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the way for their deployment on inference endpoints
serving multiple requests in parallel.

There is still plenty of room for improvement though:
- in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory.
Alternative options such as pipelining are currently integrated,
- the static sequence length limits the model ability to encode long contexts. It would be interesting to see if attention sinks might be a valid option to address this.
 
Here is the question: When implementing a system that supports both custom visualization generation and code completion while handling model updates, which approach would create an UNEXPECTED bottleneck between training data management, inference serving, and adapter updates?","Based on the context, the approach that would create an UNEXPECTED bottleneck between training data management, inference serving, and adapter updates is not using adapter-based methods, such as Low-Rank Adaptation (LoRA), which add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. This is because adapter-based methods are designed to reduce memory usage and speed up training, making them well-suited for handling model updates. Without using adapter"
"In designing a production system that handles both code generation and visualization requests while supporting model adaptation, which implementation would create an UNEXPECTED interaction between model serving, data handling, and adapter management?","[(28674, '0ec75ceb-631a-4eba-8edc-087ef8311726'), (11274, '77881e72-38b7-49e4-93fd-a7028e73fa0d'), (15885, '971fc97e-36dc-4975-85ab-37248b6d6a03'), (29202, 'a6666314-edd1-47cc-8628-335a50fe419d'), (19986, 'a48956b7-e680-4f75-8fd7-9bd7d7bf8e21'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (4127, '8b77edf9-5fbe-4e5a-84df-0360075e8182'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (17443, 'bd891b6b-f323-49c6-b503-c3e499ebcdd1'), (5158, '2a7fe1df-7c54-4ebf-9da2-15523efb3fd1'), (12846, '3ef30507-d296-48aa-b87e-2282b384c5d7'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (26164, '43458a59-7a1a-4a91-a77e-375cb848caec'), (3133, '290438b8-35f5-48bf-a9b3-7395de01e68b'), (29247, '39f8a2f3-0505-4085-a28e-444aee87836c'), (18514, '2b8c705b-aa16-4343-9147-042e810a70a2'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (14439, '86de8893-816f-4b63-b9bc-ba2c4c214252'), (19562, '68f9ee3c-a7da-4933-9b61-4d6d35d9314e'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (3213, 'f579844e-8020-4f0c-bd34-e052f2bd02d3'), (27281, 'f3dd0817-48af-4ac1-90c6-b3965d561469'), (27282, '085f932c-d040-49a3-8044-6cff137fa4df'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (1185, '13e997d5-137f-444d-b40b-2f50b3de78d5'), (30885, '564ff6e5-7fac-4e5d-9ac9-9ba6a043b086'), (2730, '9a921373-9b62-4252-81a7-f1059c09fbd1'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (24758, 'd430cf8e-b321-4b0d-9ead-fa4b269907a6'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (19643, '11f17d0a-4f66-4c28-9733-118d5b0060ae'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (22225, '33434414-721b-423e-9aef-6fa82d028a76'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (18137, '4f4937f4-d751-41b9-8e2e-f1ed6ae2297d'), (20699, 'caef7991-c994-463f-b15b-cd72cfcffb49'), (15583, '0bd03221-d721-44d6-b845-89e89513d7b5'), (9441, 'a1b9ea7a-1afc-4ce3-aafc-1bcff459541a'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (31463, '173a216a-4751-442c-968d-628b2b104933'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (21230, 'cd52268c-23fc-439d-affa-c5c374ffb57a'), (26865, 'f3bdd179-05f2-4902-8280-c7891667ebb0'), (13555, '614c828b-e60a-46cf-beba-9b0a67fd3355'), (10998, '633599a3-8284-4a4a-987b-3f170a7e4efb'), (6397, 'a42d6ffe-a12b-4bd0-b1ee-ef817d09a213'), (253, '552c3720-bbfa-4b2b-995c-af6815d68dfd'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (24836, 'b29f87dd-e4a7-4d1b-82e2-c2260673c83a'), (23810, '4388669e-a841-4834-a724-ea542053f2d2'), (16135, '7571e8a8-5ff4-4358-9a88-0d57b9e70bc8'), (20749, 'b2e0610f-aa0c-4852-b1ad-2bf187ecf97e'), (23821, 'e84857ce-0800-4bd6-883c-6cc7cea44e9a'), (31516, 'f274d491-9d0b-4595-b65b-7a400a006c0a'), (31007, '2f3d4257-4818-4892-a130-983527f0545b'), (29477, 'cffeb38f-cf81-4bf4-9749-748153eb6ce6'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (29487, '2321da63-59f7-4fbb-b56d-00810837d618'), (30528, '69aa4012-1b33-4373-af24-1d94ec8dd03d'), (17732, '207efcf2-220c-4f05-a8ea-caabe161de25'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (22397, '31d21656-3ce7-4da6-b134-33cbd4345049'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (19840, 'c72c0c4e-55bb-4f0a-8a87-1858d9bad80a'), (17793, '8e853aa7-59db-4aaf-b04c-fe7fa8b49fcd'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (20875, 'c60f0ee3-dfe8-4e04-ae6a-dbdf52ecf36b'), (14731, '2f1c2099-ebc2-431b-885f-6845a75a2d23'), (20876, 'a784cd91-18de-4adf-a288-2724d2b961e1'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (7075, 'cd0d23dc-2f6e-40b8-8969-9ad62955aa9c'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (20900, '1482385c-4de4-4a6a-abb9-eccea1d3fb5e'), (19363, '0d19963c-3ae4-42af-8bc8-c4fe7a717bc8'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7083, '408ec2a3-63f7-4564-8fa9-a7a22a7a65ef'), (23469, '0f1f930e-b230-42f4-ba5c-b6d73abc59f0'), (23472, 'f143cb65-04bf-4dc6-a99b-008e386f5d9b'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (7606, '185b4012-c5e7-4217-bf99-009e04a97e71'), (17340, '4fc033a4-1c86-4be7-82c4-8401c26e36a0'), (16843, '926901bf-b2bd-4869-8ac4-10833c3d0dd1'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (21972, '85cbe3b6-6131-4a4a-9548-070d1c3b8c2d'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (15336, '807f8cec-83e0-450b-aa1a-a12265b5545d'), (24555, 'e24368a8-1b02-49dd-90c8-3fb3723439b9'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (15861, 'cdbf3fc8-ee9f-4a6a-bfd8-e1fe30982ba1'), (17399, '2b88c67c-ed4d-4dae-aee5-87f8aab6e260'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Adapt a model to a new task

Many diffusion systems share the same components, allowing you to adapt a pretrained model for one task to an entirely different task.

This guide will show you how to adapt a pretrained text-to-image model for inpainting by initializing and modifying the architecture of a pretrained [`UNet2DConditionModel`].

## Configure UNet2DConditionModel parameters

A [`UNet2DConditionModel`] by default accepts 4 channels in the [input sample](https://huggingface.co/docs/diffusers/v0.16.0/en/api/models#diffusers.UNet2DConditionModel.in_channels). For example, load a pretrained text-to-image model like [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) and take a look at the number of `in_channels`:

```py
from diffusers import StableDiffusionPipeline

pipeline = StableDiffusionPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"", use_safetensors=True)
pipeline.unet.config[""in_channels""]
4
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Adapters

Adapter-based methods add extra trainable parameters after the attention and fully-connected layers of a frozen pretrained model to reduce memory-usage and speed up training. The method varies depending on the adapter, it could simply be an extra added layer or it could be expressing the weight updates ∆W as a low-rank decomposition of the weight matrix. Either way, the adapters are typically small but demonstrate comparable performance to a fully finetuned model and enable training larger models with fewer resources.

This guide will give you a brief overview of the adapter methods supported by PEFT (if you're interested in learning more details about a specific method, take a look at the linked paper).

## Low-Rank Adaptation (LoRA)

<Tip>
## Llama-Adapter

[Llama-Adapter](https://hf.co/papers/2303.16199) is a method for adapting Llama into a instruction-following model. To help adapt the model for instruction-following, the adapter is trained with a 52K instruction-output dataset.

A set of of learnable adaption prompts are prefixed to the input instruction tokens. These are inserted into the upper layers of the model because it is better to learn with the higher-level semantics of the pretrained model. The instruction-output tokens prefixed to the input guide the adaption prompt to generate a contextual response.

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/llama-adapter.png""/>
</div>
<small><a href=""https://hf.co/papers/2303.16199"">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a></small>
Keywords: Model interpretation, Visualization

## [mlrun](https://github.com/mlrun/mlrun)

MLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications, significantly reducing engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.

Keywords: MLOps

## [FederatedScope](https://github.com/alibaba/FederatedScope)
Once the contract was signed, we began the migration of moving off our regex based system to direct an increasing amount of critical path traffic to the transformer model. Internally, we had to build some new telemetry for both model and production data monitoring. Given that this system is positioned so early in the product experience, any inaccuracies in model outcomes could significantly impact business metrics. We ran an extensive experiment where new users were split equally between the old system and the new model. We assessed model performance in conjunction with broader business metrics, such as paid user retention and engagement. The ML model clearly outperformed in terms of retention, leading us to confidently make the decision to scale the system - first to new users and then to existing users - ramping to 100% over a span of two months.
 
Here is the question: In designing a production system that handles both code generation and visualization requests while supporting model adaptation, which implementation would create an UNEXPECTED interaction between model serving, data handling, and adapter management?","Based on the context, the implementation that would create an UNEXPECTED interaction between model serving, data handling, and adapter management is [mlrun](https://github.com/mlrun/mlrun). This is because mlrun is an MLOps platform that integrates with development and CI/CD environments, automates delivery of production data, ML pipelines, and online applications, and breaks silos between data, ML, software, and DevOps/MLOps teams. This comprehensive platform may lead to unexpected interactions"
"When building a code generation system that supports both training and inference under strict memory constraints, which approach would create a MEMORY BOTTLENECK between concurrent training jobs and inference requests?","[(12289, 'fe670b2c-6435-4e7f-8e1f-e1058894e410'), (11276, '72657258-402b-4348-8451-081f95249ada'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (1053, '9f4bc2c6-a4f3-4be8-815d-c87b9a067359'), (11293, 'dc3bf3a6-1caa-462a-a58e-da5e80984fcf'), (22052, '1a82c910-0703-4313-a37c-d5ad255b37fb'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (28708, '218eb18e-351e-4810-8501-392aa5b0710b'), (7208, 'd7164bc1-bc8d-4ca1-9309-e5a8b834f98d'), (17457, '4d7ba273-f41a-4e95-a3cc-1cc63a18505d'), (17458, '29ac6c95-d3e2-470e-a268-83fd7c395ddd'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (17465, 'c4b1bd81-a86d-4c01-87e9-37c9926909fc'), (17469, '4c181057-40b0-4750-87a7-bb3330ff02f5'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19540, 'b1b16778-0b99-4188-bc83-514da8f56532'), (27221, '3a0769e6-1ce2-4b41-9dbd-f9d0c6e8b04a'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (10330, '6dfbf5a1-fc7e-4d69-9cc3-a54c7658ea1b'), (5723, '111bcbbf-0f74-4848-9a80-b24f02299fe0'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (7773, '1bee6f95-e2d3-48ee-9d13-df7015022dbf'), (5727, 'b622e8ef-3f1e-470c-a14b-8190000303dd'), (21091, '87783093-9756-4756-8b30-173539c965ca'), (5733, 'd7b94280-d5e4-4cbe-8307-82acc7c174e7'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (14444, '42f274f1-22ad-430f-a89d-972eec106f5f'), (14445, '770426d9-4a8e-467b-b7c8-9ae28fb15b50'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (1139, '55f9357e-08e0-41a3-b826-7b0d6c11b95d'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (16510, '28558a81-343e-4be7-a632-bbc1180a6dda'), (10369, 'db79384c-eaa9-489a-b9d0-97a5b2723de0'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (18073, '156a236c-723e-4010-9498-eaff060870d7'), (18074, '85e06b79-6141-47b5-997e-e11f4bd149d5'), (15523, 'c6e59814-fb02-44e0-98a0-4e0ac3142c9a'), (24230, 'bd10f584-b63e-4b9f-89e3-194c6bdb0f54'), (30378, '389e191a-617f-4c75-a35b-2b95aa203f3b'), (2733, '93647d71-2542-44d6-8e19-fe6737bb6b5c'), (22195, '578c01e1-ea10-4e30-b7a2-d0c4fa57b550'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (22209, 'd7ca43f5-90c6-448e-8d33-58c7acba90c4'), (6337, '1c3d8092-7d15-4854-8c95-d6d89c63ebcc'), (20673, '0ad1dd04-3ec3-4c8f-9d67-b44414433153'), (25313, '1ed5de91-8020-4aa8-b796-a3e19fe177d8'), (6370, '5d4d5a4e-c039-411b-b725-afbb79ba51aa'), (6371, '4d927d59-5f00-42ca-a1e6-802916ff0aeb'), (6379, '36d7ef41-e87a-403e-ba7f-e2dbd52bfc2a'), (21228, '4e0af35d-a2a3-44be-8a60-2006257963bc'), (28910, '645a05e6-4d85-49f2-92c8-8181916201d3'), (22768, '0aa32bd4-ae8d-4d98-b212-4087c9da1896'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (30456, '7f75b6b0-ffd7-420b-9078-22030660ddb2'), (22779, '1b8ccb79-44ab-4d6d-b222-a9f1fd2244d4'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (18693, 'a9a83a83-51a6-43ba-bf1b-5523b2128dee'), (15631, '6c8b54bd-d3d0-47b2-bacb-b01dfd0b0e59'), (24343, '1aa794ee-f259-4d0c-89b9-9dba2932d569'), (31013, 'c55ce3d9-ab37-45fc-bb0a-3c19c6e2452c'), (5926, '535b4e72-3c6d-40bc-a545-1815b5819cba'), (8495, '5c788f86-a885-481c-aaef-b1ff529e9036'), (21322, 'be848352-5a57-4c26-93fe-53ae088240f1'), (8018, '5c28dea5-b8ac-467e-a93e-0764afd2bebe'), (27475, '8fdc35b4-b9d4-4822-8a62-f9f2cca68e51'), (22356, '8b8030a0-df55-4369-9b05-1484d8607111'), (31573, 'f7c3e7a7-e05e-474d-8b5b-cfeafb26a2b9'), (29017, '8292e47f-e273-4659-b3e4-537049644289'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (23903, '923c4027-8d23-4edc-9d3e-daf70d4b077b'), (21344, '395c97a3-6a95-4627-8279-aad6d3f4bd32'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (3941, 'a871cd71-4386-4492-ae52-7dda1294b854'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (24457, 'b32d2172-8d5c-4609-a629-ac9c292249dd'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (5517, '78e547ac-9ff2-4317-9554-f9c745ffeac6'), (23438, '123931a8-6608-4336-86da-61b0be5ad3d2'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (30617, 'af7be972-7339-438b-8c50-8bc5c32d23e2'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (8608, '5f875ff2-0c64-4000-b0b6-21d5e07feeb1'), (17826, '25fa36a1-9fd9-4630-b761-739de4353aff'), (16809, '65c6302d-7598-4530-805c-68192eb5ec26'), (7595, 'e9a0cc5c-98d6-4e8d-8b64-1c28f8f15d45'), (26039, '06e385f9-19f2-4a2b-ac64-9ac03070c5eb'), (31163, '5156e6c6-d4cb-43ea-a682-eed8719fcd71'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (26057, '9ed838e2-7b57-4bf0-ac30-97ec4268ad86'), (7119, '8a667287-830e-4218-875f-03e7d9c5b138'), (26067, '61a8f2fe-8e9a-4fd1-994b-c13c84582f4a'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (26080, '9b91b646-fa9f-4bb6-af8a-938897f96a99'), (26085, '73ef63e9-2360-4083-beaa-53c4d4a86522'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (14822, '3a7ef6c9-1f57-4061-a655-76352eb3e163'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (8691, 'c4abf821-f5ee-419a-b75d-9cfb752e2e97'), (16373, '7a710310-ff81-457f-9396-d32fe5aece98'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (18935, 'ba4569e7-6dc8-44b3-b985-a012d849665f'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (25078, '7dc040fb-8163-4fd9-8c3f-36472a25c1a1'), (14843, 'e99ac210-a8ee-46c9-ab15-45f0c9d7496a'), (17401, '7e11d1d5-22ca-4027-846a-3a82cc00d07b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Inference

Efficient inference with large models in a production environment can be as challenging as training them. In the following 
sections we go through the steps to run inference on CPU and single/multi-GPU setups.

* [Inference on a single CPU](perf_infer_cpu)
* [Inference on a single GPU](perf_infer_gpu_one)
* [Multi-GPU inference](perf_infer_gpu_one)
* [XLA Integration for TensorFlow Models](tf_xla)


## Training and inference

Here you'll find techniques, tips and tricks that apply whether you are training a model, or running inference with it.

* [Instantiating a big model](big_models)
* [Troubleshooting performance issues](debugging)

## Contribute

This document is far from being complete and a lot more needs to be added, so if you have additions or corrections to 
make please don't hesitate to open a PR or if you aren't sure start an Issue and we can discuss the details there.
- **Training:** MoEs enable significantly more compute-efficient pretraining, but they’ve historically struggled to generalize during fine-tuning, leading to overfitting.
- **Inference:** Although a MoE might have many parameters, only some of them are used during inference. This leads to much faster inference compared to a dense model with the same number of parameters. However, all parameters need to be loaded in RAM, so memory requirements are high. For example, given a MoE like Mixtral 8x7B, we’ll need to have enough VRAM to hold a dense 47B parameter model. Why 47B parameters and not 8 x 7B = 56B? That’s because in MoE models, only the FFN layers are treated as individual experts, and the rest of the model parameters are shared. At the same time, assuming just two experts are being used per token, the inference speed (FLOPs) is like using a 12B model (as opposed to a 14B model), because it computes 2x7B matrix multiplications, but with some layers shared (more on this soon).
Habana's SDK, SynapseAI™, supports PyTorch and DeepSpeed for accelerating LLM training and inference. The [SynapseAI graph compiler](https://docs.habana.ai/en/latest/Gaudi_Overview/SynapseAI_Software_Suite.html#graph-compiler-and-runtime) will optimize the execution of the operations accumulated in the graph (e.g. operator fusion, data layout management, parallelization, pipelining and memory management, and graph-level optimizations).

Moreover, support for [HPU graphs](https://docs.habana.ai/en/latest/PyTorch/Inference_on_PyTorch/Inference_Using_HPU_Graphs.html) and [DeepSpeed-inference](https://docs.habana.ai/en/latest/PyTorch/DeepSpeed/Inference_Using_DeepSpeed.html) have just recently been introduced in SynapseAI, and these are well-suited for latency-sensitive applications as shown in our benchmark below.
These notes were written primarily for the training mode, but they should mostly apply for inference as well. For example, during inference Gradient Checkpointing is a no-op since it is only useful during training. Additionally, we found out that if you are doing a multi-GPU inference and not using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/), [Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) should provide a superior performance.


Other quick related performance notes:
- if you are training something from scratch always try to have tensors with shapes that are divisible by 16 (e.g. hidden size). For batch size try divisible by 2 at least. There are [wave and tile quanitization](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/) divisibility that is hardware-specific if you want to squeeze even higher performance from your GPUs.


### Activation Checkpointing or Gradient Checkpointing
```

The above command will also run inference as fine-tuning progresses and log the results to Weights and Biases.

**Notes**:

* SDXL's VAE is known to suffer from numerical instability issues. This is why we also expose a CLI argument namely `--pretrained_vae_model_name_or_path` that lets you specify the location of a better VAE (such as [this one](https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)).

### Finetuning the text encoder and UNet

The script also allows you to finetune the `text_encoder` along with the `unet`.

🚨 Training the text encoder requires additional memory.

Pass the `--train_text_encoder` argument to the training script to enable finetuning the `text_encoder` and `unet`:
 
Here is the question: When building a code generation system that supports both training and inference under strict memory constraints, which approach would create a MEMORY BOTTLENECK between concurrent training jobs and inference requests?","Based on the context, the approach that would create a MEMORY BOTTLENECK between concurrent training jobs and inference requests is using a Mixture of Experts (MoE) model. This is because, although a MoE model has faster inference compared to a dense model with the same number of parameters, all parameters need to be loaded in RAM, which leads to high memory requirements."
"In scaling a visualization-enhanced code assistant for multi-user deployment, which architectural decision would lead to an UNSTABLE interaction between request handling, memory allocation, and adapter usage?","[(522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (18508, 'ef8ba6f7-764d-4ccf-968e-0e01a8044164'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (22625, 'e74be5ed-cda9-49e1-b334-8bf8fac7250a'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (29307, '86ec482e-16ea-4bbd-a195-b6c599945ce9'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (31366, 'faca457c-cb2d-44f1-a992-b468cb6d6a45'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (30376, '81f99d9e-b362-4eab-8876-3ea668b9d674'), (21163, '7d118280-4cd6-4c0e-91c4-dfb3d2e2f08a'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (21176, '2c7c306b-d840-49b2-8ce5-f553024b9d6e'), (21178, '7b9a3d42-8115-4146-9a5b-13ff20effe02'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (19697, '2bb5d1ca-7bcf-4074-b6a2-32572672320c'), (11505, '1a528512-e853-454b-ba90-8c2b73dd3cf9'), (21234, '53ed4e4a-e74b-4f4b-a1a7-eeee2b356164'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (23810, '4388669e-a841-4834-a724-ea542053f2d2'), (24836, 'b29f87dd-e4a7-4d1b-82e2-c2260673c83a'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (27423, 'cdb54c20-dca4-48f4-96a7-f000fa351601'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (27460, '42d93388-c034-4d39-b6e2-99a5343894ac'), (21320, 'fbd9fead-fcb4-4b6c-96e3-0eda7aa951da'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (30584, '72eded47-82e5-4d94-aff8-ea0caf3f4dc7'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (28551, 'dbac6980-e840-4083-95f2-01acb84a46fa'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (19856, '1779d346-5968-4964-a7b4-39c87c76dd12'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (24485, '4de8b02f-cf9b-40a4-9212-1d8e11cd76f5'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7083, '408ec2a3-63f7-4564-8fa9-a7a22a7a65ef'), (7084, '966d14eb-c2c1-4bae-96ce-6f2ced24abfc'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (29116, '95611315-e02f-4e0d-a585-27fdb4ffaede'), (19392, '576d4399-a387-4d30-9a85-6ba7899f9239'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (26065, 'a9c2bea5-5ef8-44e2-899c-d4c79f2cbf82'), (19411, '140ee8ae-a55f-4c8c-a331-3e7e31530f72'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (19416, '462a3d4a-01ab-4043-92a4-6a83ce7f1226'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (26084, '09a3ae97-8160-4f7e-a533-4a7271f6b98c'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (27112, '831fcd7a-7990-4660-a592-546aa8d108bf'), (13799, '049b1794-2f12-45db-a284-7fe305094bb4'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: While local deployments are an excellent head start to building
something useful, you’d need to perform deployments that can serve many
users in real-life projects. In this post, you’ll learn how to scale the
local deployment from the previous post with Docker and Kubernetes.
Therefore, we assume some familiarity with Docker and Kubernetes.

This post builds on top of the [<u>previous post</u>](https://huggingface.co/blog/tf-serving-vision), so, we highly
recommend reading it first. You can find all the code
discussed throughout this post in [<u>this repository</u>](https://github.com/sayakpaul/deploy-hf-tf-vision-models/tree/main/hf_vision_model_onnx_gke).

# Why go with Docker and Kubernetes?

The basic workflow of scaling up a deployment like ours includes the
following steps:

- **Containerizing the application logic**: The application logic
  involves a served model that can handle requests and return
  predictions. For containerization, Docker is the industry-standard
  go-to.
Scaling to 0 replicas helps optimize cost savings by minimizing resource usage during periods of inactivity. However, it's important to be aware that scaling to 0 implies a cold start period when the endpoint receives a new request. Additionally, the HTTP server will respond with a status code `502 Bad Gateway` while the new replica is initializing. Please note that there is currently no queueing system in place for incoming requests. Therefore, we recommend developing your own request queue client-side with proper error handling to optimize throughput and latency.

The duration of the cold start period varies depending on your model's size. It is recommended to consider the potential latency impact when enabling scaling to 0 and managing user expectations.
Unfortunately, closed-source code assistant services don't share information about the underlying models, their capabilities, and their training data. 

## Transparency

In line with the [Chinchilla Scaling Law](https://arxiv.org/abs/2203.15556v1), SafeCoder is a compute-optimal model trained on 1 trillion (1,000 billion) code tokens. These tokens are extracted from [The Stack](https://huggingface.co/datasets/bigcode/the-stack), a 2.7 terabyte dataset built from permissively licensed open-source repositories. 
All efforts are made to honor opt-out requests, and we built a [tool](https://huggingface.co/spaces/bigcode/in-the-stack) that lets repository owners check if their code is part of the dataset.

In the spirit of transparency, our [research paper](https://arxiv.org/abs/2305.06161) discloses the model architecture, the training process, and detailed metrics.
- **Parallelization**. Workloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute in parallel on XLA devices. For text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance.

👉 Try it out for yourself:

[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/google/sdxl)

## Stable Diffusion XL pipeline in JAX

Upon having access to a TPU VM (TPUs higher than version 3), you should first install
a TPU-compatible version of JAX:
The StarCoder models offer unique characteristics ideally suited to enterprise self-hosted solution:

- State of the art code completion results - see benchmarks in the [paper](https://huggingface.co/papers/2305.06161) and [multilingual code evaluation leaderboard](https://huggingface.co/spaces/bigcode/multilingual-code-evals)
- Designed for inference performance: a 15B parameters model with code optimizations, Multi-Query Attention for reduced memory footprint, and Flash Attention to scale to 8,192 tokens context.
- Trained on [the Stack](https://huggingface.co/datasets/bigcode/the-stack), an ethically sourced, open source code dataset containing only commercially permissible licensed code, with a developer opt-out mechanism from the get-go, refined through intensive PII removal and deduplication efforts.
 
Here is the question: In scaling a visualization-enhanced code assistant for multi-user deployment, which architectural decision would lead to an UNSTABLE interaction between request handling, memory allocation, and adapter usage?","Based on the context, it can be inferred that not using containerization (specifically, Docker) and parallelization (using JAX's pmap) would lead to an unstable interaction between request handling, memory allocation, and adapter usage. This is because containerization helps to optimize resource usage and parallelization enables scaling across multiple devices, which are essential for handling multiple users and requests efficiently. Without these architectural decisions, the system may struggle to manage requests, allocate memory, and utilize adapters effectively, leading"
"When designing a multi-task code generation system, which approach would create an UNEXPECTED trade-off between performance, memory usage, and adapter compatibility?","[(26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (11293, 'dc3bf3a6-1caa-462a-a58e-da5e80984fcf'), (11295, '2bf9366b-a9e7-40a8-a7ba-93163869dccc'), (14388, '5cf9f858-9101-44e2-8239-8859192f9e97'), (30776, 'a5497ed7-9df9-460a-9ab8-8fffde7862ad'), (3133, '290438b8-35f5-48bf-a9b3-7395de01e68b'), (18508, 'ef8ba6f7-764d-4ccf-968e-0e01a8044164'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (9297, 'b1ddc06d-35a1-4ac0-b68f-697de22d55e0'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (6756, '66d0932f-7334-4856-b0d9-2c287e50b5f6'), (8805, '82dbedd7-d790-4508-add3-69d6b8e877bb'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (9832, 'bdc6bea1-7f20-4300-aae9-3f7ea03a6783'), (19562, '68f9ee3c-a7da-4933-9b61-4d6d35d9314e'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (19564, '2fa961ce-bef5-44e4-9ebb-5ca39d38eb3d'), (8808, 'd2ee298b-cdc9-4e51-b273-b2029efd926b'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (30373, 'adf014fd-0478-49ea-b8ec-f99c21e98539'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (24747, '7b337a69-9d6b-4734-8e30-70f5b83ccbba'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (20690, 'cbcf4eb2-979a-495d-a8ad-950b3cb6692a'), (19155, '1e812ea9-3a09-4467-be78-c43f09a00fc7'), (20693, '6ceda67f-d532-4fa1-99ca-1fe2e1e1fe8c'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (18137, '4f4937f4-d751-41b9-8e2e-f1ed6ae2297d'), (20186, 'e4800925-3dd9-4604-b5d2-755f623ef3b9'), (25318, '7f293204-2059-483b-9146-91aa485398d2'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (13555, '614c828b-e60a-46cf-beba-9b0a67fd3355'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (253, '552c3720-bbfa-4b2b-995c-af6815d68dfd'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (24835, '3eef27d6-f46c-4bfc-97e3-795dd6084cc2'), (24836, 'b29f87dd-e4a7-4d1b-82e2-c2260673c83a'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (23810, '4388669e-a841-4834-a724-ea542053f2d2'), (24842, '0d694989-93f1-48d3-b256-e21d25ab982b'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (31516, 'f274d491-9d0b-4595-b65b-7a400a006c0a'), (31005, 'dfbe26eb-d794-43e1-8e52-483e186cd71c'), (31006, '570011b0-e075-4315-97cc-b8c673d126d2'), (21317, 'e3d09f9f-54a1-46d0-8f42-deee45efbca7'), (17733, '51d3055c-1da6-4034-9b91-aa12153414bb'), (8529, '83a5303d-d0b9-4f17-99f4-d6263db35d48'), (27475, '8fdc35b4-b9d4-4822-8a62-f9f2cca68e51'), (1881, '664f3e23-c32a-4ab5-a015-fbe79134f466'), (14681, '9ca76816-f2d2-487e-966f-ca367c7f52b7'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (30578, '4dae799c-0e35-447a-8126-3f98ef267e33'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (17782, '99c8f91e-122c-48d3-b93f-ec1128e97292'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (14713, '95b5af6f-a76d-41ed-b9f6-4ac706e59c81'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (23933, '4f315f49-1d33-495b-8fd7-ec8974d8e5dc'), (14720, 'e5a863bc-b178-4fd7-bca8-5ed9e309292f'), (18309, '77eb1ea6-c7df-40f4-a112-aec1b6d6ae26'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (28585, 'd3223831-24e7-4557-a7a0-28f800f26eb0'), (7592, '68ddeb8e-b285-47f9-8754-09b4fd273d73'), (7083, '408ec2a3-63f7-4564-8fa9-a7a22a7a65ef'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (19392, '576d4399-a387-4d30-9a85-6ba7899f9239'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (26058, '35372f93-4b7f-427c-9e4b-3b73b324029d'), (26059, '056cb9da-4cf5-4de9-91bb-cd02195d02aa'), (26060, 'c163368d-4735-4bb6-8bf9-5b38c03c8927'), (19406, 'c711aaf2-7953-4aa9-97d3-0d5a01e92591'), (19411, '140ee8ae-a55f-4c8c-a331-3e7e31530f72'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (19413, '6bc14884-13de-4c7b-8165-ced54b077284'), (10707, '2645868a-3cda-4c75-a76a-16a152aa6f9d'), (19415, '23f915ac-6d6e-4824-9597-4500a0d035d0'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (19418, '7f027f88-04e9-47fb-a446-1f889375162f'), (19422, 'deb5a362-1750-4c97-a4db-ae50380210f6'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (19428, 'de136064-8fa4-4750-89b5-91d689b7d535'), (14822, '3a7ef6c9-1f57-4061-a655-76352eb3e163'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (14823, 'c02c38d2-2a34-418e-954a-4b26b11b52e4'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (13301, '40f0a375-d17e-4e42-ba0b-1a73a0ec79b5'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The global trend highlights the positive impact of the number of cores on the observed latencies. 
In most of the cases, increasing the number of cores reduces the computation time across the different workload sizes. 
Still, putting more cores to the task doesn't result in monotonic latency reductions, there is always a trade-off between the workload’s size and the number of resources you allocate to execute the job.


As you can see on the charts above, one very common pattern tends to arise from using all the cores available on systems with more than one CPU (more than one socket). 
The inter-socket communication introduces a significant latency overhead and results in very little improvement to increased latency overall.
[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:

1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.
Each pushes forward different approaches to improve aspects of the memory allocation and management on various software.

### 2. Efficient parallelization of computations

Now that we have an efficient way to represent our data, we need a way to take the most out of the computational hardware at our disposal. 
Interestingly, when it comes to inference, CPUs have a potential advantage over GPUs in the sense they are everywhere, and they do not require specific application components and administration staff to operate them.

Modern CPUs come with many cores and complex mechanisms to increase the general performances of software. 
Yet, as we highlighted on [the first blog post](https://hf.co/blog/bert-cpu-scaling-part-1), they also have features which can be tweaked depending on the kind of workload (CPU or I/O bound) you target, to further improve performances for your application.
However, as models get larger and larger, full fine-tuning becomes infeasible to train on consumer hardware. In addition, storing and deploying fine-tuned models independently for each downstream task becomes very expensive, because fine-tuned models are the same size as the original pretrained model. Parameter-Efficient Fine-tuning (PEFT) approaches are meant to address both problems!
We can observe that the `copilot` adapter gets it right in both cases. Therefore, it performs as expected for code-completions when working with HF specific codebase as well as generic codebases.

**Now, as a user, I want to combine the ability of `assistant` as well as `copilot`. This will enable me to use it for code completion while coding in an IDE, and also have it as a chatbot to answer my questions regarding APIs, classes, methods, documentation. It should be able to provide answers to questions like `How do I use x`, `Please write a code snippet for Y` on my codebase.**

PEFT allows you to do it via `add_weighted_adapter`. Let's create a new adapter `code_buddy` with equal weights to `assistant` and `copilot` adapters.

![combining_loras](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/personal_copilot/combine_adapters.png)
Combining Multiple Adapters

Now, let's see how `code_buddy` performs on the `chatting/question_answering` tasks.
 
Here is the question: When designing a multi-task code generation system, which approach would create an UNEXPECTED trade-off between performance, memory usage, and adapter compatibility?","When designing a multi-task code generation system, increasing the number of cores does not always result in monotonic latency reductions, as there is a trade-off between the workload's size and the number of resources allocated to execute the job."
"In a production environment for code visualization and training, which implementation would create CONFLICTS between adapter efficiency, model serving, and background operations?","[(19981, '558c0bd9-da7a-44c7-9580-01fc13de8ab2'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (29202, 'a6666314-edd1-47cc-8628-335a50fe419d'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (4127, '8b77edf9-5fbe-4e5a-84df-0360075e8182'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (17443, 'bd891b6b-f323-49c6-b503-c3e499ebcdd1'), (38, '7f1448df-4b4e-4cb0-b824-2176150e3e5b'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (30776, 'a5497ed7-9df9-460a-9ab8-8fffde7862ad'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (8805, '82dbedd7-d790-4508-add3-69d6b8e877bb'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (8811, 'f5698c5e-4953-429e-9054-471b3af3941a'), (4728, '16e3aca0-e5d5-4be6-946d-90258a1ebb5a'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (9860, 'b127b619-65e1-4d79-844d-ade94f6c03a8'), (6136, '818cf127-78a6-43ae-aad7-568810131041'), (1185, '13e997d5-137f-444d-b40b-2f50b3de78d5'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30885, '564ff6e5-7fac-4e5d-9ac9-9ba6a043b086'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (21176, '2c7c306b-d840-49b2-8ce5-f553024b9d6e'), (24758, 'd430cf8e-b321-4b0d-9ead-fa4b269907a6'), (21178, '7b9a3d42-8115-4146-9a5b-13ff20effe02'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (8891, '20f2b24f-7dad-4a7c-8e0e-df021996ed1d'), (21177, '7ab5c5b7-16a5-4322-a529-c851484ee7ba'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (8892, '91b7caf7-7012-4878-b2e4-e5e45737faa9'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (24279, '86400db1-18e3-43c2-84da-246efb31dfda'), (6370, '5d4d5a4e-c039-411b-b725-afbb79ba51aa'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (12013, 'e538399e-20ea-45b5-9245-989cea914a13'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (23810, '4388669e-a841-4834-a724-ea542053f2d2'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (23821, 'e84857ce-0800-4bd6-883c-6cc7cea44e9a'), (7948, '3dbb3d81-63e4-478a-a701-58fff6b304a0'), (6940, 'fc0d9fc0-31dc-43e0-8229-840222e72b80'), (31006, '570011b0-e075-4315-97cc-b8c673d126d2'), (14112, 'a2013fd2-46c9-4bbe-b338-d3d8b7870c1f'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (31016, '05908d70-238b-4150-bec9-b27c6ef05086'), (11564, 'b22defce-5c80-4f5b-8953-f9a66ddf9666'), (30528, '69aa4012-1b33-4373-af24-1d94ec8dd03d'), (16194, 'df401754-27d5-46cb-9ea1-5e7d2f80a544'), (6995, 'e46b2017-987b-4e17-89a7-e28b7239c22f'), (21343, '08be7e91-b4c4-40b2-8817-a2af8d306009'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (23930, 'b54bef38-4510-49ab-87a1-dbc20ba90085'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (5508, '18f70486-80e2-4c2d-a2f1-f9e6174711dd'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (14731, '2f1c2099-ebc2-431b-885f-6845a75a2d23'), (20875, 'c60f0ee3-dfe8-4e04-ae6a-dbdf52ecf36b'), (19853, '40bb732f-f865-45e3-bf29-9f8451bcb193'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (402, 'c6eb50da-95f0-4bba-8470-8ad12c54fba4'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (5016, '2c7de3aa-8b01-440b-8b78-5d6d18c84a7d'), (7072, '9b1f498b-8529-4819-b080-b212dcd54acb'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (19363, '0d19963c-3ae4-42af-8bc8-c4fe7a717bc8'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (7078, 'ddf6eca5-f40f-4432-82a0-55a2aede1980'), (30629, '7cf3310b-91ee-4c5f-a7ea-66073b3a9197'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (7082, 'b972f2bc-c1a6-49e3-a883-bc6a5f268e63'), (7083, '408ec2a3-63f7-4564-8fa9-a7a22a7a65ef'), (7085, '1a7faf0d-4e79-4094-9069-e75e800d3e85'), (7086, '1e8e9dbb-0ea8-4dd8-89ba-d445b3b4db20'), (7088, 'e6ce0dd7-0c32-4e56-840b-8403b18094e6'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (7606, '185b4012-c5e7-4217-bf99-009e04a97e71'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (17340, '4fc033a4-1c86-4be7-82c4-8401c26e36a0'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (6094, '4cdfb0fb-1392-476a-90e8-10ab82ff69d8'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27100, '3ac86814-cc53-4758-9569-8c92b6803467'), (27101, 'a0b5c992-ca35-43fb-8604-666943c50f3c'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (27109, 'e980eafb-92e5-4615-b816-fc20edbbd85f'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (24551, 'ab3cb15d-5aba-47cb-b554-8dde3831a847'), (15336, '807f8cec-83e0-450b-aa1a-a12265b5545d'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (13302, 'f0909c4d-cee3-4bae-96be-0a039e4f857a'), (17399, '2b88c67c-ed4d-4dae-aee5-87f8aab6e260'), (17400, '9037f4f0-0e20-48f1-84c2-4b71a6d044a7'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
These questions call attention to the method. It remains to be seen what the actual impact will be.

## The future of graphics

So what does this mean for the future of graphics? Well, let's break it up into pros/cons:

**Pros**
1. High-quality, photorealistic scenes
2. Fast, real-time rasterization
3. Relatively fast to train

**Cons**
1. High VRAM usage (4GB to view, 12GB to train)
2. Large disk size (1GB+ for a scene)
3. Incompatible with existing rendering pipelines
3. Static (for now)

So far, the original CUDA implementation has not been adapted to production rendering pipelines, like Vulkan, DirectX, WebGPU, etc, so it's yet to be seen what the impact will be.
Keywords: Model interpretation, Visualization

## [mlrun](https://github.com/mlrun/mlrun)

MLRun is an open MLOps platform for quickly building and managing continuous ML applications across their lifecycle. MLRun integrates into your development and CI/CD environment and automates the delivery of production data, ML pipelines, and online applications, significantly reducing engineering efforts, time to production, and computation resources. With MLRun, you can choose any IDE on your local machine or on the cloud. MLRun breaks the silos between data, ML, software, and DevOps/MLOps teams, enabling collaboration and fast continuous improvements.

Keywords: MLOps

## [FederatedScope](https://github.com/alibaba/FederatedScope)
## Llama-Adapter

[Llama-Adapter](https://hf.co/papers/2303.16199) is a method for adapting Llama into a instruction-following model. To help adapt the model for instruction-following, the adapter is trained with a 52K instruction-output dataset.

A set of of learnable adaption prompts are prefixed to the input instruction tokens. These are inserted into the upper layers of the model because it is better to learn with the higher-level semantics of the pretrained model. The instruction-output tokens prefixed to the input guide the adaption prompt to generate a contextual response.

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/llama-adapter.png""/>
</div>
<small><a href=""https://hf.co/papers/2303.16199"">LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention</a></small>
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples 📄
 
Here is the question: In a production environment for code visualization and training, which implementation would create CONFLICTS between adapter efficiency, model serving, and background operations?","Based on the context, the implementation that would create conflicts between adapter efficiency, model serving, and background operations in a production environment for code visualization and training is the original CUDA implementation. This is because it has not been adapted to production rendering pipelines like Vulkan, DirectX, WebGPU, etc., which could lead to conflicts and incompatibility issues."
"When deploying a visualization-enabled code generation system with support for multi-user requests, which architectural pattern would create an UNEXPECTED DELAY between request handling, visualization rendering, and inference?","[(10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (25105, '421712a8-0f69-436b-b77b-73a56f5dce9f'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (19996, '224c3d6f-e42c-48eb-87f9-64edc46d679a'), (23076, 'a2e4b167-e8ac-414f-94b9-8837ae554fbf'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (19517, '1ef0b772-2213-49fe-96d1-e7e1dc052fd0'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (16469, 'aba54ccf-519e-4018-816e-984adec99ff9'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (19543, '778e4d72-e48d-4c55-ae51-fc4cbfa4738d'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (19550, '4245b43b-0271-4e7c-989d-5fd67e28b9bb'), (8804, '6a0cddc1-e5ae-43f6-a2ed-e6885c88a3e2'), (6760, '4c999b00-d7b8-4881-b542-92cbbb0aeeef'), (19560, '2052afa9-9548-48e4-b160-94d2cd8fa764'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (19563, '505b868d-35d2-4d6c-8268-8e729abd6f63'), (19562, '68f9ee3c-a7da-4933-9b61-4d6d35d9314e'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (5743, 'c1d8b14c-c8ac-4b43-a04c-1e061196fc77'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (13952, '8896cfcd-d757-4746-86df-fadbcfee2673'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (18074, '85e06b79-6141-47b5-997e-e11f4bd149d5'), (8369, 'c2f8a3b4-6396-4f41-9ffb-d98e387a78f9'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (21173, '3602c799-96f8-4ec2-b308-2be607030f16'), (5302, '03b42784-3438-4238-ad09-da57545757dc'), (21174, 'd798dc85-4099-4a4e-bc88-a602ac15c282'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (19155, '1e812ea9-3a09-4467-be78-c43f09a00fc7'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (25838, '16204619-b73e-4058-9258-ad5e42039892'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (24817, '48d696b1-96a6-4539-9cd2-22c48bcb58af'), (21234, '53ed4e4a-e74b-4f4b-a1a7-eeee2b356164'), (18675, '1da20ff6-a9fc-47bd-beda-e5b849c25412'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (18683, '8829c723-8deb-4d91-808d-6e05be38f248'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (31006, '570011b0-e075-4315-97cc-b8c673d126d2'), (27423, 'cdb54c20-dca4-48f4-96a7-f000fa351601'), (31013, 'c55ce3d9-ab37-45fc-bb0a-3c19c6e2452c'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (27466, 'bfb56804-3d91-45c6-b17f-8d15d5d3a201'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (27481, '670fac96-da41-4462-a2f5-621a35d75778'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (19853, '40bb732f-f865-45e3-bf29-9f8451bcb193'), (24462, '5a2cbd7f-2208-4f21-bb5c-396cdaefdb98'), (7055, 'a529a5a4-301b-4348-af00-6dc922392243'), (19856, '1779d346-5968-4964-a7b4-39c87c76dd12'), (7056, 'bea77aa6-abc3-4144-b282-ce08f9d01cf4'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (17811, '525ac724-ad6c-419a-a3a2-1715aa66fbd7'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (29598, '78d2e6f1-098e-4154-9814-d3529bde1b1a'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (7075, 'cd0d23dc-2f6e-40b8-8969-9ad62955aa9c'), (7080, 'bc5b09e3-6084-4eb4-98e1-fbd27ea612f1'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (28593, 'dd489c56-f401-4d5e-b36e-53ae51cf1b28'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (6074, 'b2bb287c-83bc-498b-ba24-a5faf876961c'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (19388, '141d4b90-0e49-4309-98b8-fb6890009430'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (5576, 'b7b79481-4e09-48af-9b04-705aa6c0cc9d'), (30667, '5ab35ed2-c28e-4c60-be97-eb6e05d3cce4'), (19403, 'c0f08689-c366-48f2-a497-c82f608735e2'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27103, '326af7e1-9b24-4f9b-bc34-fa267833a2ca'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (14821, 'c34abfed-956a-47bc-8db5-e28e8b62e1ce'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (29161, '5c1a0615-2746-4f25-8cae-56a17b9d371d'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Interestingly, the deployed models latency is not too sensitive to the batch size, which opens the way for their deployment on inference endpoints
serving multiple requests in parallel.

There is still plenty of room for improvement though:
- in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory.
Alternative options such as pipelining are currently integrated,
- the static sequence length limits the model ability to encode long contexts. It would be interesting to see if attention sinks might be a valid option to address this.
## Supporting TP

Ok, we have removed most of the low-hanging fruits now we went roughly from 350ms/token
latency to 300ms/token in PP. That's a 15% reduction in latency, but it actually provided
more than that, but we were not extremely rigorous in our measuring initially so let's stick to that figure.

Then we went on to provide a TP implementation. Turned out to be much faster
than we anticipated the implementation took half a day of a single (experienced) dev.
The result is [here](https://github.com/huggingface/transformers/tree/thomas/dirty_bloom_tp/src/transformers/models/bloom). We were also able to reuse code from other projects which helped.

The latency went directly from 300ms/token to 91ms/token which is a huge improvement in user experience.
A simple 20 tokens request went from 6s to 2s which went from a ""slow"" experience to slightly delayed.
### Text Generation Inference

Response time and latency for concurrent users are a big challenge for serving these large models. To tackle this problem, Hugging Face has released [text-generation-inference](https://github.com/huggingface/text-generation-inference) (TGI), an open-source serving solution for large language models built on Rust, Python, and gRPc. TGI is integrated into inference solutions of Hugging Face, [Inference Endpoints](https://huggingface.co/inference-endpoints), and [Inference API](https://huggingface.co/inference-api), so you can directly create an endpoint with optimized inference with few clicks, or simply send a request to Hugging Face's Inference API to benefit from it, instead of integrating TGI to your platform. 

![Screenshot from HuggingChat](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/os_llms/huggingchat_ui.png)
### Text Generation Inference

[Text Generation Inference](https://github.com/huggingface/text-generation-inference) is a production ready inference 
container developed by Hugging Face to enable easy deployment of large language models. 

Its main features are:

- Continuous batching
- Token streaming using Server-Sent Events (SSE)
- Tensor Parallelism for faster inference on multiple GPUs
- Optimized transformers code using custom CUDA kernels
- Production ready logging, monitoring and tracing with Prometheus and Open Telemetry

Since v0.8.2, Text Generation Inference supports Falcon 7b and 40b models natively without relying on the Transformers
""trust remote code"" feature, allowing for airtight deployments and security audits. In addition, the Falcon 
implementation includes custom CUDA kernels to significantly decrease end-to-end latency.
Keywords: Training, Generation

## [diffgram](https://github.com/diffgram/diffgram)

Diffgram aims to integrate human supervision into platforms. We support your team programmatically changing the UI (Schema, layout, etc.) like in Streamlit. This means that you can collect and annotate timely data from users. In other words, we are the platform behind your platform, an integrated part of your application, to ship new & better AI products faster.

Keywords: Human supervision, Platform

## [ecco](https://github.com/jalammar/ecco)

Explain, analyze, and visualize NLP language models. Ecco creates interactive visualizations directly in Jupyter notebooks explaining the behavior of Transformer-based language models (like GPT2, BERT, RoBERTA, T5, and T0).

Keywords: Model explainability

## [s3prl](https://github.com/s3prl/s3prl)
 
Here is the question: When deploying a visualization-enabled code generation system with support for multi-user requests, which architectural pattern would create an UNEXPECTED DELAY between request handling, visualization rendering, and inference?","Based on the context, the architectural pattern that would create an UNEXPECTED DELAY between request handling, visualization rendering, and inference is not using pipelining. The context mentions that ""in the current implementation, the only way to augment the throughput is to increase the batch size, but it is currently limited by the device memory. Alternative options such as pipelining are currently integrated..."" This suggests that not using pipelining could lead to unexpected delays."
What is the most effective approach for implementing DreamBooth training while ensuring ethical compliance and proper documentation?,"[(11278, 'e713c8e5-c745-44dd-bc3d-29c7e29c2189'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (3601, '5b751b55-fa30-4ca3-9f29-0c2b5992aee0'), (11288, '2451d2fc-6bb5-4e2f-ad80-c64efaba643f'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (12314, 'e385ad95-96ab-4d32-aa88-01c826234363'), (26657, '4f236930-ae24-4ef1-b075-ccb4482a4900'), (26663, '0e555191-162a-45da-ad99-03e99d0199a3'), (26665, 'db193de6-8255-464f-9149-be7af6c03c58'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (26675, '7badede5-4860-404f-8bb6-ab60db480be1'), (26678, 'd17335d9-50c5-400d-bb0e-4d1d02ce9662'), (26696, '3318e3a1-8c29-4b7d-a5f7-1588fb10b135'), (26697, '80c3fc5d-b1d4-4875-a92a-61f60362bcc9'), (5710, 'f21f0562-b66c-41c2-8ed3-d515e34e88ed'), (4692, '697715c5-2ee3-4da5-9b88-90354e284c9a'), (4696, '6996fe5b-0705-49f1-9069-85ef3e39d463'), (4697, '3714f4e1-21ae-49eb-9ec0-9221657c1046'), (4702, '1b460a9c-a144-4b70-b4a3-0ba6ece900ca'), (4706, '680cae3d-7edf-4b97-9961-e420b0cc6adf'), (4719, '7325e015-6e64-402a-bdcf-d35da35b0310'), (26228, 'cd2ce019-c648-4434-8d98-916726eb00f4'), (18047, '88a9e52b-eec2-4a57-933d-f10b57098f70'), (9860, 'b127b619-65e1-4d79-844d-ade94f6c03a8'), (8847, 'ff57d993-a380-4ec6-820f-f1f51d61319f'), (26265, 'cc640035-a212-471c-b79a-5b2b8e33d260'), (24743, '7c967f07-5e4b-4076-896b-2d21640ef09e'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (22723, '73aca2a4-7e12-4a27-8803-298209ed9076'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (9428, '5b9c8a03-16ed-45d6-9ce6-2ab388d1892b'), (9429, 'f9b10760-2b6c-4577-999d-9ce25869b8f0'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (9431, '185997ab-1d50-4fe7-88c0-896da8ec8036'), (29400, '4b3191cf-c2a7-4a85-989f-9fd267cb522a'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (22746, '7c59bb0e-df03-41ef-a167-3d8a1455c057'), (12007, '9c867710-e722-43c0-b004-df6f4069ac5e'), (21225, 'e69e5399-a1d0-43f8-a7b8-e134c3416e16'), (12011, '417eb897-bb81-4c51-94d7-0906db08bbd3'), (12012, '8c88d004-cc76-4ac3-abfe-0f5ae57b7ad3'), (4361, '69a44467-ab86-4ff5-bd68-f4fcb4287814'), (4362, 'a3614fd5-f69b-44a3-90b0-01f9eb078f1a'), (6411, '787941ff-6634-421a-a05d-b3c2c1d4cda4'), (6412, 'e491859b-0c3a-451c-8d54-e5536a56bf5f'), (6413, '245f056f-3cce-4401-8130-8bf6522fd9dd'), (9486, '40c09d9f-3207-4542-b6b5-4e56a2db18db'), (6415, 'aaf6f9e4-e2b5-4d91-8c2a-89cbe4f2110b'), (6414, '591d58fa-2c3e-4261-a58a-950eee1e09d1'), (9489, '722afe74-ffcc-4ca5-9439-526a915ab9cc'), (6418, 'e13e35b8-05d3-4f14-bb06-8efe54105256'), (6419, '42aef9bd-e9bd-469a-ac13-036843c85c48'), (4363, '3a3e4a17-7092-49fb-a163-1e382dffbc6d'), (6420, 'a0d6a281-060a-42e7-b028-400eafafb07f'), (6421, 'b8aebff7-309e-4b44-a96e-026ae78e518f'), (6425, '647c3412-1976-473a-b978-2f3787e1dc3b'), (11558, 'f0e0e76d-1ed1-4de4-bed4-b83eb8297fc7'), (11562, 'a3abc2a8-536f-49c3-937d-a7167bc0f240'), (11563, 'e0a485f8-e425-4c38-8a42-7075440250af'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (30541, '66e657f3-787a-43f4-a2ca-ecc504697610'), (30544, '2a74ff39-5f3d-449f-a5ff-10bc8f7dcd3d'), (13158, 'd5de80e9-25fb-4ef5-8868-7bb0688c1611'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (361, '8f211b20-6be0-4528-a805-34a48d3c060b'), (363, 'a89eaa05-3ff1-49d2-8124-aff7fd09fdaa'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (4466, '9aa4d965-f7aa-47c9-9ef0-99efd730ae50'), (30579, '9ca0751a-33ff-4f8b-a7ee-49343630ad20'), (4467, 'e220a748-bccb-46f5-8533-dbf4767b9d3b'), (12286, '2a89892b-4a32-4801-9d33-14f95f02fce5'), (4475, 'a93fa4e5-6ae7-4d7a-befe-e29f4fed3c27'), (14716, '493155e3-f96f-48c6-be10-b3d2c158a052'), (4476, 'b80eba33-6fae-4fe9-8ba0-b91a48e87a9e'), (21374, '5f22e6c9-db26-49d7-ac1a-d830f7203d82'), (21377, '11698850-263a-4917-8e03-3d7c601995b7'), (13699, 'e6673957-64d4-4ab9-a6ab-290eebf4e65d'), (4483, 'ebf7739d-21cf-4ec6-a3db-75715a428eae'), (21383, '41387f0d-02b9-48a4-a7f4-e8318d3417a3'), (15243, '4189372b-3be4-4bca-b934-f8d9d37da251'), (21388, '7dec6a76-7037-46ee-8c41-096b85b4d1ae'), (23459, 'e479dae7-075a-43ca-aee7-6683ca38ae18'), (11684, '9cfcfbdb-6441-4284-8ef3-d14780da146f'), (11685, 'a18ef3de-ff80-4514-b1dd-0a2a98939675'), (11687, '69e7636e-6b7e-4dc2-9a53-908bca40991f'), (11689, 'f514bd1a-ea82-4bb1-a46f-23e244b0ce87'), (8618, 'b72c1623-10fb-4fdc-99e7-31e030260a85'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (433, '02562a97-e1ae-4a5a-b71c-02b035ab095a'), (3507, 'e3d2edd7-6062-4b3f-9ae2-63e8a07a4b96'), (9656, 'a77713b2-b75f-474e-8b1c-b4abc596b9bf'), (11705, 'ebf07ac5-f930-4b51-b548-ba02e7dd5d14'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (12241, '5f95fcd3-3f90-4165-ae7a-424c50eb60e9'), (27602, '7ac74ea0-7594-4d73-9158-90afe51597ea'), (27099, '3ef906f6-8dba-4063-80c0-00d2097566f5'), (27101, 'a0b5c992-ca35-43fb-8604-666943c50f3c'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (23533, '354bcc39-ac3c-4a0f-ac68-3b5a873cc33b'), (31227, 'a993a1ba-0a3e-44ca-b6ee-aae21b1be246'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (12285, 'ba282827-386a-4cba-a8fb-866207b82420'), (31230, '98a71ccc-41b5-417b-82c4-fcf53961bfde')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: DreamBooth training example for Stable Diffusion XL (SDXL)

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few (3~5) images of a subject.

The `train_dreambooth_lora_sdxl.py` script shows how to implement the training procedure and adapt it for [Stable Diffusion XL](https://huggingface.co/papers/2307.01952).

> 💡 **Note**: For now, we only allow DreamBooth fine-tuning of the SDXL UNet via LoRA. LoRA is a parameter-efficient fine-tuning technique introduced in [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685) by *Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen*. 

## Running locally with PyTorch

### Installing the dependencies

Before running the scripts, make sure to install the library's training dependencies:

**Important**
DreamBooth training example

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few(3~5) images of a subject.
The `train_dreambooth.py` script shows how to implement the training procedure and adapt it for stable diffusion.


## Running locally with PyTorch

### Installing the dependencies

Before running the scripts, make sure to install the library's training dependencies:

**Important**

To make sure you can successfully run the latest versions of the example scripts, we highly recommend **installing from source** and keeping the install up to date as we update the example scripts frequently and install some example-specific requirements. To do this, execute the following steps in a new virtual environment:
```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install -e .
```

Then cd in the example folder and run
```bash
pip install -r requirements.txt
Multi Subject DreamBooth training

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few(3~5) images of a subject.
This `train_multi_subject_dreambooth.py` script shows how to implement the training procedure for one or more subjects and adapt it for stable diffusion. Note that this code is based off of the `examples/dreambooth/train_dreambooth.py` script as of 01/06/2022.

This script was added by @kopsahlong, and is not actively maintained. However, if you come across anything that could use fixing, feel free to open an issue and tag @kopsahlong.

## Running locally with PyTorch
### Installing the dependencies

Before running the script, make sure to install the library's training dependencies:

To start, execute the following steps in a new virtual environment:
```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install -e .
* Dreambooth tends to overfit quickly. To get good-quality images, we must find a 'sweet spot' between the number of training steps and the learning rate. We recommend using a low learning rate and progressively increasing the number of steps until the results are satisfactory.
* Dreambooth needs more training steps for faces. In our experiments, 800-1200 steps worked well when using a batch size of 2 and LR of 1e-6.
* Prior preservation is important to avoid overfitting when training on faces. For other subjects, it doesn't seem to make a huge difference.
* If you see that the generated images are noisy or the quality is degraded, it likely means overfitting. First, try the steps above to avoid it. If the generated images are still noisy, use the DDIM scheduler or run more inference steps (~100 worked well in our experiments).
[DreamBooth](https://github.com/huggingface/diffusers/tree/main/examples/dreambooth) by [colossalai](https://github.com/hpcaitech/ColossalAI.git)

[DreamBooth](https://arxiv.org/abs/2208.12242) is a method to personalize text2image models like stable diffusion given just a few(3~5) images of a subject.
The `train_dreambooth_colossalai.py` script shows how to implement the training procedure and adapt it for stable diffusion.
 
Here is the question: What is the most effective approach for implementing DreamBooth training while ensuring ethical compliance and proper documentation?","Based on the provided context, there is no direct answer to the question about the most effective approach for implementing DreamBooth training while ensuring ethical compliance and proper documentation. The context primarily focuses on providing instructions for running DreamBooth training examples for Stable Diffusion XL (SDXL) and Stable Diffusion, including installing dependencies, running scripts, and tips for avoiding overfitting. It does not provide information on ethical compliance and proper documentation.

However, it is essential to note that when working with AI"
"When implementing a real-time dashboard for model monitoring, what combination of features would best support both ethical oversight and technical performance?","[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13315, '09b2a0a8-1b0f-489c-8e7c-aecab9880800'), (28677, '45f0fcb4-e1cb-4497-808a-e3001963761e'), (30727, '5a7a6803-dbbe-48fe-8ccd-60d7bb61992d'), (13325, '16d0f9ea-2690-41bb-b5b7-ea15fa1b01e6'), (15886, '1f277e8b-db79-4468-af4b-e11deb15d54a'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (30739, 'fda68c03-224d-4b2c-8385-15822bc392ef'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (12314, 'e385ad95-96ab-4d32-aa88-01c826234363'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (5683, '3fbd0aa6-3836-4bba-965c-ec5e95b6213f'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (5682, 'fb18c66c-cdd7-47e6-a4bf-022dc4f0f8af'), (13363, '6fae2d65-f5bd-43af-88f7-22383a716f40'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (11333, 'a53b0ecc-15c7-4421-9d16-6932851c6a75'), (4190, '1a5cd39c-3f05-41bf-ad18-24bde1284273'), (8806, 'baeaa009-cafb-4600-93a2-3bb7cf6e70c2'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (9859, '3e6fc260-35b7-46ab-af85-5de4ca79b906'), (9862, '33a9f610-ef4f-48c2-8df9-388fa0fd2a8f'), (19080, '94934921-1e26-4d2b-a418-99560516f2d0'), (27274, '4eb170d7-a680-4906-a7e9-6fc6734eb8e7'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (7873, '6685e912-4253-46db-a9e7-b7dfb8fc6141'), (10954, '92fe4159-8084-4e9c-9be2-4e72a5e11928'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (8913, '758cd49e-a2be-49b0-b593-78a110a09fd3'), (15057, 'dd7c818f-cdea-48c7-bced-a336200df22b'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29398, '1a9ba8a6-266c-42fc-ab9b-f0983692ce2d'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (29400, '4b3191cf-c2a7-4a85-989f-9fd267cb522a'), (29401, 'ffc0aed6-1b92-4799-baef-1e0edcb9587b'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (7901, '17026dcd-41fe-493c-8a87-b0ddc5ea48cb'), (19689, '6402466f-c27d-48ba-9c09-4a777b71c17b'), (10999, '773e67b3-120d-4606-8cb8-6270fa3f5b46'), (4361, '69a44467-ab86-4ff5-bd68-f4fcb4287814'), (4362, 'a3614fd5-f69b-44a3-90b0-01f9eb078f1a'), (4363, '3a3e4a17-7092-49fb-a163-1e382dffbc6d'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (16143, 'bcbfd73d-4617-42c4-ac93-60e0ae7f56d2'), (29503, '8d8f1ab2-d503-4300-a5cb-65e057381f71'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (23390, 'a0d6b001-969e-45ef-9770-61309e5be4e1'), (21853, '474cca01-f1f8-4899-ae29-3cbc7f3913c7'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (3437, '4c4bc7f1-62c4-4d6b-9c56-da29d6357515'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (17786, '1bbd7c71-1550-4cdd-8c79-46f50c872a73'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8624, '339598bc-e183-4af9-9283-072e1969ff78'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (8626, 'b2adf91d-3907-4c74-a7a7-bbcc199cea59'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27101, 'a0b5c992-ca35-43fb-8604-666943c50f3c'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (25058, 'f06d5733-7ce2-443d-85cb-88a01b88ca4f'), (15334, 'd746f337-15b7-4205-80bb-7b72bc0e202f'), (15335, 'fdc7d8df-5f34-4684-94ed-70f19e6041e3'), (15336, '807f8cec-83e0-450b-aa1a-a12265b5545d'), (15339, '9006a23a-10b3-4cd2-a0a0-5f95871a357d'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (5622, 'ccda777f-15b6-4a2a-9ba6-f00ffacce0d2'), (5623, '8fdc94b4-3360-405e-b141-430c2e6a5ebc'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Examples of implementations: Safety features and Mechanisms

The team works daily to make the technical and non-technical tools available to deal with the potential ethical and social risks associated with diffusion technology. Moreover, the community's input is invaluable in ensuring these features' implementation and raising awareness with us.

- [**Community tab**](https://huggingface.co/docs/hub/repositories-pull-requests-discussions): it enables the community to discuss and better collaborate on a project.

- **Bias exploration and evaluation**: the Hugging Face team provides a [space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion interactively. In this sense, we support and encourage bias explorers and evaluations.

- **Encouraging safety in deployment**
If specific ad hoc practices devoted to documentation, transparency and ethical usage of ML models are already present and improving each day (e.g., model cards, evaluation benchmarks), why shouldn't open licensing practices also be adapted to the specific capabilities and challenges stemming from ML models?
### Other key new insights

* Model cards are best filled out when done by people with different roles: Technical specifications can generally only be filled out by the developers; ethical considerations throughout are generally best informed by people who tend to work on ethical issues.
* Model users care a lot about licences -- specifically, whether a model can legally be used for a specific task.


## Appendix B: Landscape Analysis
_Overview of the state of model documentation in Machine Learning_

### MODEL CARD EXAMPLES
Examples of model cards and closely-related variants include:
In our response, we encourage the Task Force to:  

- Appoint Technical and Ethical Experts as Advisors
    - Technical experts with a track record of ethical innovation should be prioritized as advisors; they can calibrate NAIRR on not only what is technically feasible, implementable, and necessary for AI systems, but also on how to avoid exacerbating harmful biases and other malicious uses of AI systems. [Dr. Margaret Mitchell](https://www.m-mitchell.com/), one of the most prominent technical experts and ethics practitioners in the AI field and Hugging Face’s Chief Ethics Scientist, is a natural example of an external advisor.
How to minimize the risk of harm? Here are some practical recommendations.

### Best practices for responsible use

- Make sure people know which parts of their user experience depend on the outputs of the ML system  
- If possible, give users the ability to opt-out  
- Provide a mechanism for users to give feedback on the model decision, and to override it  
- Monitor feedback, especially model failures, for groups of users that may be disproportionately affected  

What needs most to be avoided is to use the model to automatically make decisions for, or about, a user, without opportunity for a human to provide input or correct the output. Several regulations, such as [GDPR](https://gdpr-info.eu/) in Europe, require that users be provided an explanation for automatic decisions made about them.

---
 
Here is the question: When implementing a real-time dashboard for model monitoring, what combination of features would best support both ethical oversight and technical performance?","Based on the context, a combination of features that would best support both ethical oversight and technical performance in a real-time dashboard for model monitoring includes:

1. **Model cards**: Providing technical specifications and ethical considerations, filled out by people with different roles (developers and ethics experts).
2. **Bias exploration and evaluation**: Integrating a space to demonstrate biases in the model interactively, supporting bias explorers and evaluations.
3. **User feedback mechanism**: Allowing users to give feedback on model decisions and"
"What is the primary benefit of employing Ray for distributed retrieval in RAG models, as opposed to PyTorch's torch.distributed?","[(4096, 'a7d24d50-4376-4631-9ddf-4d4e3e3c3eb9'), (4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (28673, '5d042da5-0429-46b6-bc0e-f68cfd69fe52'), (22529, '1ff19103-8daa-4bb0-ae94-e63b88c169a2'), (26634, 'b08afb02-fff2-4df4-b310-09795b760eda'), (10251, '31409786-ddd4-49ae-94d2-bea926b83be4'), (24603, '78522c83-d8fe-4f56-8cbe-0c5d741f50f1'), (30761, '683724e2-567a-46f0-9254-202ca2f88a96'), (21034, 'b564941b-893b-4fc9-b67c-d08edfd497e3'), (9261, '198885ca-f8cc-4c0e-b829-e1a58632ac3f'), (9262, '740244f7-7949-41ec-911d-c5ac6ce7a64f'), (9263, '2e1affcb-5fd4-4b9b-a787-6ddb6248fbfb'), (29744, '6954830f-0ba6-4c15-a60d-3cd7e635d535'), (29745, 'ed5bb6d8-e497-4707-8feb-1b30b4362980'), (17458, '29ac6c95-d3e2-470e-a268-83fd7c395ddd'), (14387, 'dd664e1e-e7c7-41fa-8f28-7ec57f939569'), (14386, 'b8cd4a6c-621a-48e7-a398-c9e62eee673e'), (26167, '571fd306-3198-44ef-a1d7-198d6a6d5854'), (29752, '866f9889-e3da-4559-b3d6-343baf84f073'), (3128, 'cd116de5-9c26-431b-aacf-ee4e4b6e9331'), (2618, '8c25f1c0-1ce2-488c-a60e-9ecdd7e2403b'), (29751, '09b22d0e-1989-4985-8183-c14df816cb62'), (29769, 'b7739621-6687-45e5-964d-cb69e022c6c8'), (17994, '34680928-da37-47ee-90c9-5404760066a2'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (28755, '8ac51c04-dd98-4293-ac4f-7a630c9784db'), (23635, '8da5d714-1b83-4977-8b30-0d99b8236dac'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (23637, 'c1c8a809-12af-48eb-9d4e-166d5a66bf0e'), (18522, 'a7bc8f48-cf4a-4fde-9b50-cf7db4d17089'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (2658, 'd27df11c-c6b8-4473-b960-2902742a35d0'), (19555, '9bb43118-305f-403d-8b60-5c20b13a050c'), (19561, 'bd6d1959-f94f-4a44-b2c4-a94c6950eb8c'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (25715, '4ffee9ef-08fd-4cad-8cc0-2649906d7721'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (11383, '7d1776e7-3738-41fc-ae14-19c3f95d3c25'), (24711, 'ff60e964-aa24-44ba-9ac0-e72b18947ea2'), (24712, 'e34b93ed-59ef-42e7-9129-b252acbf40f8'), (24715, '6a7a1cae-5191-48e3-a13d-6d456647ffac'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (31374, '9ed70363-2221-4572-96f7-8b14c9fb8411'), (17046, '660ba43d-2e1a-42b0-88a6-ca4d50a71f29'), (17049, '8685cd9b-60eb-4a2b-8cce-43d96d0c264d'), (17050, '94372e5d-1e3c-4f95-9307-d1444bf39269'), (13475, 'f1ab0094-4445-40d9-becb-aafd382c5cd1'), (26291, '49c9c734-3e96-4cdc-a308-fc5f84f4264a'), (21171, 'c2adcd5d-814f-42b8-bac7-8989008f0c48'), (26294, 'e270afea-3092-480b-ba50-52d43d31bf2b'), (29374, '221cd942-7551-4df5-9a02-d0f73e055431'), (22761, '5728fbcb-454d-473d-9a7b-47d0f24a646f'), (22772, 'd0b704d9-ea88-403d-8797-758ba929ebd4'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (28938, 'c79cdb2e-622b-41c8-bde6-21c9ca8d958a'), (25874, 'd8b23baa-4b3f-49d9-b14c-77bdf69ddb1e'), (6433, '510f65f8-5c52-495f-ad44-a069704bb9ff'), (6434, 'da7f20d6-ff0f-435c-a62d-4e0b137881a6'), (13097, 'd0ae3415-29e2-4a0a-a9e4-9f788040397f'), (31533, '19fbc1ec-55bd-4264-b56a-1b6e60093779'), (31535, 'fcd12fd9-deed-453d-bbf0-f6bad6552817'), (31536, '37cc5143-44ba-4aa9-a04f-6f0006a85b20'), (12080, '9dbbfe60-bc76-41a7-abd6-f3511f64fb33'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (9540, '8cfc2be7-ce6e-4ee3-9fa7-d4da603fc5cf'), (23365, '26869439-d39c-4f25-96bc-fa6efd3cffb6'), (12104, '91971395-1e61-4f8d-bc69-6e8b6191c233'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15723, '27f5205f-669c-45a9-912d-c8bb8f74efd5'), (15724, 'cf08e3ee-cc1f-469a-864c-615037aa2306'), (24433, 'bebc2a4b-2754-442d-8616-0f6f184afc3c'), (24434, 'bb7afb46-7718-4546-95b2-cc62410b73cb'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (3961, 'efa02508-c542-4ad1-8b61-608f4b1415de'), (21884, 'e3940e4b-cb45-4344-b157-99b3a61a4320'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (19842, '177c114f-7a6c-4116-9c04-d60f549b7fac'), (31108, 'b2a6db3a-19cd-4f85-8ebb-367f85973aa0'), (8086, '1c1af7f1-c19d-46a1-a25f-8fe6d479a26c'), (15767, 'b073d9f7-f678-42db-b6e4-9f73738cae55'), (15768, 'f0a867f0-3313-47f8-a4d9-49cb78888982'), (15772, '65169dba-99db-485c-9bff-b711a9a78463'), (15773, 'fdad774c-45be-4ff9-acc2-9e93ace35a0a'), (17822, '20439f0d-de85-4a4f-91e9-58d6c81c72a7'), (15776, '5792c81e-163d-4045-9e92-aa5c22b8bdc9'), (17826, '25fa36a1-9fd9-4630-b761-739de4353aff'), (12252, 'b6a1ac6c-8f54-462c-bcbb-00e3d41d94d7'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (4085, '8a08cc47-8750-4040-b9cf-d499f02be598'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (29177, '619af8a9-fbe3-421d-9eca-be6d9044cb79'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4092, '49e8126f-6930-48ec-abbd-632bcb85a3df'), (27638, '57f803ac-4276-4362-9712-42f92f35f465'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Instead, a framework-agnostic and a more flexible implementation for ad-hoc concurrent programming is required. [Ray](https://ray.io/) fits the bill perfectly. Ray is a simple, yet powerful Python library for general-purpose distributed and parallel programming. Using Ray for distributed document retrieval, we achieved a **2x speedup per retrieval call compared to `torch.distributed`**, and overall better fine-tuning scalability.

### Ray for Document Retrieval
![alt_text](assets/12_ray_rag/torch_distributed_document_retrieval.png ""image_tooltip"")
_Document retrieval with the torch.distributed implementation_


The main drawback of the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) implementation for document retrieval was that it latched onto the same process group used for training and only the rank 0 training worker loaded the index into memory.

As a result, this implementation had some limitations:
```

Using Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than
just the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate
processes than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.

# Evaluation
Our evaluation script enables two modes of evaluation (controlled by the `eval_mode` argument): `e2e` - end2end evaluation, returns EM (exact match) and F1 scores calculated for the downstream task and `retrieval` - which returns precision@k of the documents retrieved for provided inputs.
```
You will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.

## Document Retrieval
When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other
with [`Ray`](https://docs.ray.io/en/master/).

This option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.
By default this flag is set to `pytorch`.

For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.
![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_

To overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Ray’s stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG.
And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
 
Here is the question: What is the primary benefit of employing Ray for distributed retrieval in RAG models, as opposed to PyTorch's torch.distributed?","The primary benefit of employing Ray for distributed retrieval in RAG models, as opposed to PyTorch's torch.distributed, is that it allows multiple processes to load the index and handle retrieval queries, overcoming the limitations of torch.distributed where only the rank 0 training worker loads the index into memory. This leads to a 2x speedup per retrieval call and better fine-tuning scalability, especially in multi-GPU settings."
"In value-based reinforcement learning, how does the Bellman equation reduce computational complexity, and how might it relate to modern document retrieval mechanisms?","[(6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (29213, 'ecfbdc64-5f35-4f83-8641-32ac46207dca'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (30816, '199d05d3-a124-431e-98c0-73eb3b9d9e94'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (13926, 'c6436b34-5943-4b25-b196-20d9695a4c1d'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (11365, 'fe8da28b-5e19-460e-be89-ccd9c18b8b30'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17025, '69f0e2e7-b149-4523-a038-d069193eeec2'), (17040, '3c81255d-fba2-425b-add0-66180682f513'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (14486, '1d0b077e-bd05-4bb6-9239-cdebc73e9813'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (166, 'a7fb2601-9706-4665-a80b-8da658e759f7'), (167, 'bed696f3-f53a-41f2-8793-1fe84ebe7310'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (21186, '31cd25b9-75f3-43f6-b771-f47d73dafddd'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (20179, '43cd6317-d563-4694-aa5f-8715ac76a2cc'), (20181, 'c3d5e236-999f-4476-b4ae-60717e645952'), (20182, 'ba1d7c23-8372-4dcc-879a-a18edc0a90a6'), (20183, '0bc8e3d4-5308-48a5-a6f1-343318328e56'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23294, '1936ebac-0df8-44c6-a638-cb0f634059de'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (1335, '268a1193-d635-4530-92e3-3146245d10a0'), (1336, '329c759f-4fc3-49ae-89bd-b24d835a593b'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (20806, 'b65455ed-1e56-4734-8384-734771a4e1ef'), (14156, '2fdf5bec-b66f-4a17-8f19-95d24fdd2459'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (16743, '28949d3c-ff9f-4e2c-8682-daa6d669b12c'), (16746, '45080cea-724d-4d98-8278-7cd29fd71f9b'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23958, '82ffb2ac-f889-482f-bd2b-090b36dbcce1'), (23959, '46501f70-bf28-440b-ba81-9c88be735e91'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (23960, '85cca8b8-d29e-4219-a5ff-f837b064f343'), (23961, '13b18911-853f-4362-aad9-9563a8885e44'), (23962, '35661ef2-1497-4bbd-af50-38276811d7ce'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (925, '2b19d79e-1c48-4d05-a48d-19f5138ae7c1'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (23454, 'bb59e1c0-8193-40ce-a450-9efc2ecc6226'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13764, 'c2619e99-58b4-4943-a672-c368132c540e'), (13766, '324b6368-b377-4ac8-920b-ae7106ccadf1'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13784, 'df3f2f2b-7b61-43c5-9f8b-7c20249b29e4'), (13786, 'ce79d072-ab99-4290-b96b-8252d9463234'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (1004, '7bae84e8-1b63-41df-828f-817d61593d05'), (23535, '0de47e64-f26d-4a3b-bb54-a6517f3e5e5b'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Mid-way Quiz [[mid-way-quiz]]

The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.


### Q1: What are the two main approaches to find optimal policy?


<Question
	choices={[
		{
			text: ""Policy-based methods"",
			explain: ""With Policy-Based methods, we train the policy directly to learn which action to take given a state."",
      correct: true
		},
		{
			text: ""Random-based methods"",
			explain: """"
		},
    {
			text: ""Value-based methods"",
			explain: ""With value-based methods, we train a value function to learn which state is more valuable and use this value function to take the action that leads to it."",
      correct: true
		},
		{
			text: ""Evolution-strategies methods"",
      explain: """"
		}
	]}
/>


### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>
- The value of  \\(V(S_{t+1}) \\)  = Immediate reward  \\(R_{t+2}\\)  + Discounted value of the next state ( \\(gamma * V(S_{t+2})\\) ).
- And so on.

To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, **which is a long process.** This is equivalent **to the sum of immediate reward + the discounted value of the state that follows.**

## **Monte Carlo vs Temporal Difference Learning**

The last thing we need to talk about before diving into Q-Learning is the two ways of learning.

Remember that an RL agent **learns by interacting with its environment.** The idea is that **using the experience taken**, given the reward it gets, will **update its value or policy.**

Monte Carlo and Temporal Difference Learning are two different **strategies on how to train our value function or our policy function.** Both of them **use experience to solve the RL problem.**
## **The “Deep” in Reinforcement Learning**

⇒ What we've talked about so far is Reinforcement Learning. But where does the ""Deep"" come into play?

Deep Reinforcement Learning introduces **deep neural networks to solve Reinforcement Learning problems** — hence the name “deep”.

For instance, in the next article, we’ll work on Q-Learning (classic Reinforcement Learning) and then Deep Q-Learning both are value-based RL algorithms.

You’ll see the difference is that in the first approach, **we use a traditional algorithm** to create a Q table that helps us find what action to take for each state.

In the second approach, **we will use a Neural Network** (to approximate the q value).

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/deep.jpg"" alt=""Value based RL""/>
  <figcaption>Schema inspired by the Q learning notebook by Udacity
</figcaption>
</figure>
Summary [[summary]]

That was a lot of information! Let's summarize:

- Reinforcement Learning is a computational approach of learning from actions. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.

- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**

- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**

- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
- Reinforcement Learning is a computational approach of learning from action. We build an agent that learns from the environment **by interacting with it through trial and error** and receiving rewards (negative or positive) as feedback.
- The goal of any RL agent is to maximize its expected cumulative reward (also called expected return) because RL is based on the **reward hypothesis**, which is that **all goals can be described as the maximization of the expected cumulative reward.**
- The RL process is a loop that outputs a sequence of **state, action, reward and next state.**
- To calculate the expected cumulative reward (expected return), we discount the rewards: the rewards that come sooner (at the beginning of the game) **are more probable to happen since they are more predictable than the long term future reward.**
 
Here is the question: In value-based reinforcement learning, how does the Bellman equation reduce computational complexity, and how might it relate to modern document retrieval mechanisms?","The question is not directly answered in the context, and it seems to be asking for an external connection between the Bellman equation and modern document retrieval mechanisms, which is not discussed in the provided context. 

However, based on the context, we can try to provide a partial answer. The Bellman equation reduces computational complexity by breaking down the value function into smaller components, allowing for more efficient computation. Instead of calculating the expected return as the sum of all possible future rewards, the Bellman equation calculates"
Which feature of Gradio enhances real-time model interaction and how does it align with reinforcement learning principles for optimal decision-making?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (2053, '9354950f-a962-4289-a102-9a4de80854fa'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (5660, 'f060d5f9-0dce-4d78-8298-2524cde9ccdb'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (16416, '1b6290e2-d918-4435-af5a-004a7c4401ca'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (16469, 'aba54ccf-519e-4018-816e-984adec99ff9'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (1145, 'abe44225-c493-44fb-82d3-494085e90422'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (14465, '83b269a3-f505-41d3-82ff-1eb665eca1a9'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (14478, '583d434f-41d4-4529-bca3-6a3c23fe8382'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (18584, '815482b7-2249-4e35-a25e-18ccc16fd17b'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27349, 'ab899ec0-c793-4e15-997a-3c85ad2d12f9'), (24789, '6151fc10-9ec2-41ab-8af7-99a5aa43b076'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (24790, '0fe10157-ef28-4b0a-82b9-d74d3c464448'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (10493, '3ec8bf92-41e9-40b7-9f00-f8ed0724086b'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (27927, 'ea460800-536f-4a56-948a-42c18e8375be'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2860, 'cd0fbf96-6263-4efc-9922-f67f4cce3a51'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (5474, '1158d6f0-fad9-4bbb-babf-d6178165afc7'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (16745, 'bbcf4d12-db52-4214-bd20-fbb0a28f27e1'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (21907, '0e50272c-345a-4857-a282-c33dc7daecf5'), (5524, 'bb1d7e3e-0981-435e-af37-33e2588f6b6b'), (14231, '38cb7f87-85dc-4acf-950b-b56bbf7a2a7b'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (14249, '3bde59f7-ba94-4ec5-8768-6195057f7e50'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (14257, '04266ef8-e616-469e-b903-4a3381ca634c'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (13247, '7151a6c0-468c-45c7-9cb8-418c3953caaf'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (3541, '50f4f4c3-4378-4cff-8b04-2e6443145fad'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (29670, 'ccfff15f-115c-4de7-b620-659b6be0a991'), (5608, 'a7a2b842-db8e-4097-857c-0f4dca7d024b'), (29674, '81aa5082-4e7c-42b6-8684-491f31bb23aa'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
## Setting up a Gradio Demo for EfficientNet-Lite4

EfficientNet-Lite 4 is the largest variant and most accurate of the set of EfficientNet-Lite models. It is an integer-only quantized model that produces the highest accuracy of all of the EfficientNet models. It achieves 80.4% ImageNet top-1 accuracy, while still running in real-time (e.g. 30ms/image) on a Pixel 4 CPU. To learn more read the [model card](https://github.com/onnx/models/tree/main/vision/classification/efficientnet-lite4)

Here we walk through setting up a example demo for EfficientNet-Lite4 using Gradio

First we import our dependencies and download and load the efficientnet-lite4 model from the onnx model zoo. Then load the labels from the labels_map.txt file. We then setup our preprocessing functions, load the model for inference, and setup the inference function. Finally, the inference function is wrapped into a gradio interface for a user to interact with. See the full code below.
 
Here is the question: Which feature of Gradio enhances real-time model interaction and how does it align with reinforcement learning principles for optimal decision-making?","The feature of Gradio that enhances real-time model interaction is the ability to test the model with real data and observe the model's predictions change in real-time, which helps with debugging the model. This aligns with reinforcement learning principles for optimal decision-making by allowing for rapid experimentation and feedback, enabling developers to refine their models and make better decisions."
How do licensing practices on the Hugging Face Hub promote ethical compliance and user transparency in model deployment?,"[(15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (3616, '0709d283-044c-40ef-a53c-9892f93f74ac'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (4131, '377c0542-5954-44d4-a02c-750bc75c18ae'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5675, '278e5c07-1e92-4ba6-bfee-2f26406171c3'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (7725, '258a242e-844a-4498-bd2d-4c45f980f5f2'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (5687, '56f80e00-a026-4153-8220-a3fd825d93c4'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (12348, '57a3c79a-af62-43be-bebc-4ef70c6c1495'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (28741, '50733951-b5db-4e9b-83a4-b2911af5c989'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (8782, '4cffb526-4c01-473c-ae6a-aa99e1991fcc'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (8783, '56426659-c214-4bcb-8f80-30aa9e0e911b'), (8784, '2689d196-69e1-4f9e-8033-4ed364ef2567'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (14927, 'cd247d85-d1ee-4e3e-b3e5-9bc9d15c4563'), (25685, 'e7551698-0040-4251-b71c-50be7bc42a95'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (14944, 'cd075ce4-d02e-47be-ad35-21b38e850692'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (30821, '721f4d5f-3a80-403c-b5bd-8d94a87543e5'), (21629, '98fbe65d-63b8-42b8-b534-0cd39021b65d'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (27284, '344b1139-9e69-4a8a-a5b7-593e7b089268'), (19100, '28b56063-0856-43a8-90c0-d1124a372f5a'), (26785, 'a539b4a7-925a-4038-bb46-23b14c6dbb78'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (30380, 'b4858a62-bfab-4374-b4e5-c287a54d33be'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (7868, 'fc0025e1-0005-48de-97c7-5fe250a873d5'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (11975, 'c977182b-d101-4944-95e4-2c832d9bf6fd'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (14535, 'b9835dda-ff2d-4d55-a497-5553a72948a6'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (10461, '791d16b9-ac09-45b6-a426-0eea3d3b4063'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (10992, '004cff2f-f98d-40e2-9254-bfe3183ca2f2'), (5873, '9d5439bb-61db-43f2-ba27-d82d1ce267c5'), (14577, '03a094b5-b29c-4840-a743-d2ceb896c494'), (23284, '59c5d501-a716-4200-a252-695f07ef83dc'), (11526, '994250e0-f714-40fe-b8ff-52c2610db830'), (8970, 'd5af4c95-97c8-4196-bab1-0d74644248f4'), (4366, 'fa03c662-5522-4732-a0ba-6b673062d9af'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (12099, '97456597-42ed-4914-bc4a-1146c5fc4b6f'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (17767, 'a7af65e5-6000-4846-b469-ecc521b89dfa'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2923, '60de148c-833f-4a67-91b4-756c95903a0b'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (24436, '62eee91c-545a-404b-97af-1d99deab920c'), (897, 'f387e8a2-b1bf-48fa-a6f3-c7a015a21ae7'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (22421, 'bdae70d4-1482-4a7a-aa11-6e2aa1cd81c2'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (16791, 'e1e0fd50-9bed-4d12-950d-4c3107e00575'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (11163, '395c4c3d-8c49-42dc-9fda-2e3de70b64bc'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (3510, '4e9735e1-0a03-42b4-a237-4ea82a96730d'), (7096, '3ba57845-c986-4965-b8a2-53c99388658b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (21463, 'e97bfba4-a14f-477a-9f94-afd38c01d4c5'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (25065, '0e262c4e-5cc0-4b85-9743-25d55e76332e'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10232, '27e77ecf-9ddd-433f-bcda-08fa044e1cdc'), (22012, '5c55b2ec-3284-45a4-9f86-cf514fba02e0')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: It's the biggest update ever done to the Hub, and we can't wait to see the community members start collaborating with it 🤩.

The new ""Community"" tab also aligns with proposals in ethical ML throughout the years. Feedback and iterations have a central place in the development of ethical machine learning software. We really believe having it in the community's toolset will unlock new kinds of positive patterns in ML, collaborations, and progress.

Some example use cases for discussions and pull requests:

- Propose suggestions in model cards to improve disclosures of ethical biases.
- Let users flag concerning generations of a given Space demo.
- Provide a venue through which model and dataset authors can have a direct discussion with community members.
- Allow others to improve your repositories! For example, users might want to provide TensorFlow weights!

## Discussions

![Discussions on the Hugging Face Hub](assets/76_community_update/new-discussion.png)
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
Hugging Face’s mission is to [“democratize good machine learning”](https://huggingface.co/about). We understand the term “democratization” in this context to mean making Machine Learning systems not just easier to develop and deploy, but also easier for its many stakeholders to understand, interrogate, and critique. To that end, we have worked on fostering transparency and inclusion through our [education efforts](https://huggingface.co/learn/nlp-course/chapter1/1), [focus on documentation](https://huggingface.co/docs/hub/model-cards), [community guidelines](https://huggingface.co/blog/content-guidelines-update) and approach to [responsible openness](https://huggingface.co/blog/ethics-soc-3), as well as developing no- and low-code tools to allow people with all levels of technical background to analyze [ML datasets](https://huggingface.co/spaces/huggingface/data-measurements-tool) and [models](https://huggingface.co/spaces/society-ethics/StableBias). We believe this helps everyone
 
Here is the question: How do licensing practices on the Hugging Face Hub promote ethical compliance and user transparency in model deployment?","According to the context, licensing practices on the Hugging Face Hub promote ethical compliance and user transparency in model deployment in the following ways:

* Authors can define which license applies to the work they share, such as MIT, Apache2.0, or OpenRAIL, ensuring that users are aware of the limitations and restrictions of the models and datasets.
* The Hub relies on Model Cards, which provide handy information, including metadata, that contains valuable information about the model, including licensing information.
* Users"
"When fine-tuning RAG models, how does incorporating Ray processes affect the efficiency of contextual document retrieval?","[(3584, '50687ee5-f560-462d-8edd-ca97b26579c1'), (4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (5123, '3c49348e-9d5c-46a4-aaf6-e5f3275adbc0'), (4096, 'a7d24d50-4376-4631-9ddf-4d4e3e3c3eb9'), (12294, '5be0081b-2c76-4a1f-a869-b3fecbbd9ce4'), (26634, 'b08afb02-fff2-4df4-b310-09795b760eda'), (6679, '5355dc06-3d44-4916-a09c-fc5839bb0ca7'), (24604, '0385f67b-6e32-4932-a53b-9211bd2aa9ed'), (2613, '8ed34ac1-8e5c-4f4d-a4e5-765f43230e11'), (2618, '8c25f1c0-1ce2-488c-a60e-9ecdd7e2403b'), (2619, '024156c9-9a01-45e7-8318-ad7aea597207'), (24122, '3967b12b-479f-49b4-867a-35745704a595'), (13883, '31883003-62b6-48c7-987d-ba43573b9386'), (6726, '795b94d0-39c6-474d-8d73-0e5cb0319395'), (22093, '8f73a1ca-8b7b-42fa-886d-54a2236e825e'), (2638, 'fd31dc0c-6b80-49bb-9662-e72e93f6a307'), (21074, '808ca6dc-63be-4d6c-b1e5-63d43d71fae2'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (19028, '2b2cd572-c0b0-4841-8f1d-41c6eaf19944'), (19029, '408d71df-7e47-408f-a66e-f6c9e3711b88'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (6749, '81c8a3fc-8f17-4481-92ab-f3d126518cc2'), (30306, '22273990-7b23-4785-accd-84c550db4f53'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (1133, '5ecb1652-ef34-4ee9-aadc-fae9b6286f4d'), (2677, '2286a5b3-4921-465e-ac41-668c35e82ad0'), (26743, '7b7813e8-f2db-4dc4-aa11-195a3d0475da'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (24711, 'ff60e964-aa24-44ba-9ac0-e72b18947ea2'), (30343, '6b30a0cf-b903-492c-aba3-abc1809c663c'), (6280, '895b9441-5bad-4ad3-8118-16fa7c02947d'), (24714, '7a6500d3-a84e-491a-b1e1-a650b28bcbe9'), (24712, 'e34b93ed-59ef-42e7-9129-b252acbf40f8'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (21646, '4cde0740-3b9b-4bd8-a2df-5a3fb1d427e2'), (9872, 'f0242619-9c2e-487d-88a3-a40e416bc97c'), (24722, '9a565944-9d37-481d-a9b1-09c1be33068b'), (16534, 'd78a1548-6fd7-4191-b678-e08353d4d6ee'), (25262, '8924577a-82df-433b-b7e3-54c2823c77dd'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (29367, '8954b5d9-4dfb-4d14-868c-e726946b5055'), (9400, 'f92d27e1-ac45-4b8e-aa07-87bd0ae99c4d'), (9399, 'fed797d3-43fa-4729-91fb-241f9346e792'), (22202, '2aa42eca-37dd-4c52-ac50-71d43028fb51'), (14526, '50ba7f0b-b5bc-4765-95b8-6b353b288b4f'), (10431, '468be34b-3d1b-4491-8ea6-6717ce833d39'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (16588, '28e0b657-bebd-4eeb-a96e-fed578a3cefa'), (24273, '9d89b90d-0743-475c-8c30-c52c8b3feee9'), (24280, 'ef680020-6ecc-48bf-a65e-0761dfb2d098'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (23779, 'dd9b1fb6-c1ea-4613-82a3-8457708a0756'), (7400, '1d07b405-1719-43b8-8e99-83a98564ff4e'), (29421, 'f7479c51-0351-4be8-a415-b0d7f01e5b9e'), (16111, '63930b8a-56ec-4bcf-ba31-3246ca6bab09'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (8947, '7672a471-5549-49f3-aebe-5677229568e0'), (21251, '31eb9d16-0e2a-43a4-938b-4c96a7366457'), (21253, 'dd8c8a62-e782-4212-8d05-16b0914b678c'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (20232, '668b86d9-f6af-41ad-8730-c6c574c4a338'), (7946, 'dca97276-bfd9-4ea5-8570-0a1d528b911a'), (1806, '7e4b23b4-6f6d-4dfa-a292-cab055399716'), (18194, 'db26ab63-8606-4fce-adeb-9039344f080a'), (25874, 'd8b23baa-4b3f-49d9-b14c-77bdf69ddb1e'), (3861, 'b29474d6-97e9-4a9a-87ad-fa2fb569cd9f'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (30505, '9e92a10f-02d1-4093-aa4b-1504134b9dcc'), (21290, 'e04fb004-041f-4f28-863d-a2968988f978'), (13611, '338896a5-047e-40e7-b52a-7336ea945618'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (24883, 'afd6b8ce-4dd1-449a-b4ce-108a840c090a'), (18741, '8fdfdd79-7da9-47ef-ac09-f90ceea442f8'), (310, 'ba32687f-1ed4-4efe-8c70-d47465e2733b'), (24887, 'c51fe28a-1372-40b2-9f2f-8d5ac481a213'), (2881, '5d2c5594-1089-477a-8272-d14dc1e61b43'), (25426, '3641d00b-400c-4fbe-b7c3-37d2774fbf9a'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (23924, '2f66f36a-dc72-4c6f-807d-265a585dea5b'), (3958, '239b64f8-77de-410a-9385-deb7c1b65e54'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (3961, 'efa02508-c542-4ad1-8b61-608f4b1415de'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (7550, '7d55e630-3187-47ea-a06e-4ac35f9a8265'), (8068, 'd79b15bd-7976-444e-ae07-e20e9a89aa6e'), (3465, 'df743924-ed1e-4203-8840-9b440ee9aea1'), (11148, 'b8aba780-8123-47d8-98f8-834d14204672'), (17292, 'bf001b0b-e55a-4641-a051-5534f0c6ed11'), (19854, '6a12815b-a4c2-45b4-a373-d75c27461cbf'), (17293, '5285f352-e61b-4dfe-84a0-0ce2cf28e988'), (3472, '78eddeb6-be46-435a-b0bd-cc2263b4f9db'), (24467, '4ddbf49c-138b-42ef-8062-54f8e2303ce9'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (25498, '0eb9f721-b449-4b4f-848f-a66326761969'), (14330, '8ae393b3-c96e-4225-adde-4207e307b32a'), (12198, 'c617332f-ef8f-477a-803c-147aed04f986'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (9642, 'd662188c-32e5-4a23-bbda-7be8dbeb6a65'), (30635, '1e9f8414-71ee-406e-bfe0-2d26dd64e9ac'), (9132, 'acbb6bfd-6d43-4a24-a2a8-92f7765350e9'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (25040, 'e13749b3-38a6-4cfa-adf3-d39d677c38e3'), (17878, '64394a15-79e8-4191-adb9-2f885f52e3b0'), (2519, 'ea219184-5b7d-4421-b252-b5bd2876d98e'), (9702, '2167ddb3-8cdb-4545-8f16-929f187c6ba1'), (16367, 'd67c6d67-fcc1-467f-8631-57e9b9929de2'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (4085, '8a08cc47-8750-4040-b9cf-d499f02be598'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (18426, 'e2f413c4-5d19-4219-baa1-bc1e3f068ff1'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
```
You will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.

## Document Retrieval
When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other
with [`Ray`](https://docs.ray.io/en/master/).

This option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.
By default this flag is set to `pytorch`.

For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.
# Retrieval Augmented Generation with Huggingface Transformers and Ray


##### A guest blog post by <a href=""/amogkam"">Amog Kamsetty</a> from the Anyscale team

[Huggingface Transformers](https://huggingface.co/) recently added the [Retrieval Augmented Generation (RAG)](https://twitter.com/huggingface/status/1310597560906780680) model, a new NLP architecture that leverages external documents (like Wikipedia) to augment its knowledge and achieve state of the art results on knowledge-intensive tasks. In this blog post, we introduce the integration of [Ray](https://docs.ray.io/en/master/), a library for building scalable applications, into the RAG contextual document retrieval mechanism. This speeds up retrieval calls by 2x and improves the scalability of RAG distributed [fine-tuning](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag).



### What is Retrieval Augmented Generation (RAG)?
![alt_text](assets/12_ray_rag/rag_gif.gif ""image_tooltip"")
And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
For the Ray implementation, the index is loaded in *separate* process(es). The training workers randomly select which
retriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed_retriever` arg to `ray`.
To configure the number of retrieval workers (the number of processes that load the index), you can set the `num_retrieval_workers` flag.
Also make sure to start the Ray cluster before running fine-tuning.

```bash
# Start a single-node Ray cluster.
ray start --head

python examples/research_projects/rag/finetune_rag.py \
    --data_dir $DATA_DIR \
    --output_dir $OUTPUT_DIR \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_type rag_sequence \
    --fp16 \
    --gpus 8
    --distributed_retriever ray \
    --num_retrieval_workers 4

# Stop the ray cluster once fine-tuning has finished.
ray stop
 
Here is the question: When fine-tuning RAG models, how does incorporating Ray processes affect the efficiency of contextual document retrieval?","When fine-tuning RAG models, incorporating Ray processes leads to better retrieval performance and improved scalability for multi-GPU fine-tuning. The Ray implementation loads the document index in separate processes, allowing training workers to randomly select which retriever worker to query, which reduces the bottleneck of a single retrieval process. This results in a 2x speedup in retrieval calls and better performance with more training workers."
"What makes epsilon-greedy policies suitable for managing exploration and exploitation in value-based methods, and how might this concept inform RAG training strategies?","[(14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30292, '523f2e0b-8c7c-4d43-81b4-0bf745ad3379'), (25687, 'e0602f1c-f1fd-4fa9-a1d2-0fc616b3fb9a'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (28785, '6a4793d6-328d-4198-8f28-cb61dd76346c'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (23162, 'adfa1aae-7eea-4d74-b807-13110e10c22f'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (17547, '91c00572-19ed-4c45-a397-6e45a63c5956'), (17548, 'aefb1f79-29ee-4e4c-9c9e-fee9108f9e90'), (17549, '6cea314d-9588-4cbe-9e0c-dea7dc8ff9f0'), (17550, 'f7ea19ba-2641-4b2f-89db-c031f1abbd53'), (17552, 'e6dc2f24-99a1-4fd8-a073-c0a439c5d184'), (17553, 'a45cc6c7-9ac2-4e91-97bb-7b24a815d88f'), (17555, '96bccc70-063c-4751-acd8-ac03d5f93056'), (14486, '1d0b077e-bd05-4bb6-9239-cdebc73e9813'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (25777, '222d6fa5-c0e6-4075-bb20-83f0dfe99f5c'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25274, '0a54e74a-9c72-4f9a-b0f0-c8420921e175'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25280, '8c7846b3-bc36-403f-b031-66f0a9105705'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (20701, '04553fa1-c85e-4533-9f15-8a7240e55fc3'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23294, '1936ebac-0df8-44c6-a638-cb0f634059de'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11040, 'd3ac7329-8173-4897-b351-0738ff068eaf'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (23339, 'acc2c624-ff31-433f-be5f-f1000a8f48dc'), (20805, '751dabdb-968e-45e0-8286-ed51ace44972'), (20806, 'b65455ed-1e56-4734-8384-734771a4e1ef'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19299, '58e3889c-4edb-4806-9064-e951799e80cb'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (19313, '43028562-976a-43d3-8062-60062555356c'), (19314, '77062fd7-8f64-4bc1-9a5c-db9ac13a4e5d'), (19315, '91b8e9dc-2998-430a-9fcc-55fc69552b76'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19317, 'cab6254e-890a-47c4-a68b-1b64258cc023'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23952, 'ec7797a6-1617-49dc-9737-a162e926580a'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (23965, '50a1c8ae-9ee4-4afe-a86f-7583efa72965'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (961, 'f8df8306-bd61-414c-92fb-2f5f3e706930'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Define the epsilon-greedy policy 🤖

Epsilon-greedy is the training policy that handles the exploration/exploitation trade-off.

The idea with epsilon-greedy:

- With *probability 1 - ɛ* : **we do exploitation** (i.e. our agent selects the action with the highest state-action pair value).

- With *probability ɛ*: we do **exploration** (trying a random action).

As the training continues, we progressively **reduce the epsilon value since we will need less and less exploration and more exploitation.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg"" alt=""Q-Learning"" width=""100%""/>
The epsilon-greedy strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that, with an initial value of ɛ = 1.0:

- *With probability 1 — ɛ* : we do **exploitation** (aka our agent selects the action with the highest state-action pair value).
- With probability ɛ: **we do exploration** (trying random action).

At the beginning of the training, **the probability of doing exploration will be huge since ɛ is very high, so most of the time, we'll explore.** But as the training goes on, and consequently our **Q-table gets better and better in its estimations, we progressively reduce the epsilon value** since we will need less and less exploration and more exploitation.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-5.jpg"" alt=""Q-learning""/>


### Step 3: Perform action At, get reward Rt+1 and next state St+1 [[step3]]
Consequently, whatever method you use to solve your problem, **you will have a policy**. In the case of value-based methods, you don't train the policy: your policy **is just a simple pre-specified function** (for instance, the Greedy Policy) that uses the values given by the value-function to select its actions.

So the difference is:

- In policy-based training, **the optimal policy (denoted π\*) is found by training the policy directly.**
- In value-based training, **finding an optimal value function (denoted Q\* or V\*, we'll study the difference below) leads to having an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link between value and policy""/>

In fact, most of the time, in value-based methods, you'll use **an Epsilon-Greedy Policy** that handles the exploration/exploitation trade-off; we'll talk about this when we talk about Q-Learning in the second part of this unit.
</details>

### Q4: Can you explain what is Epsilon-Greedy Strategy?

<details>
<summary>Solution</summary>
Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that we define epsilon ɛ = 1.0:

- With *probability 1 — ɛ* : we do exploitation (aka our agent selects the action with the highest state-action pair value).
- With *probability ɛ* : we do exploration (trying random action).

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg"" alt=""Epsilon Greedy""/>


</details>

### Q5: How do we update the Q value of a state, action pair?
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-update-ex.jpg"" alt=""Q Update exercise""/>

<details>
<summary>Solution</summary>
<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/q-update-solution.jpg"" alt=""Q Update exercise""/>
**Step 1: We initialize the Q-Table**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-3.jpg"" alt=""Q-learning""/>
</figure>

We need to initialize the Q-Table for each state-action pair. **Most of the time, we initialize with values of 0.**

**Step 2: Choose action using Epsilon Greedy Strategy**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-learning-4.jpg"" alt=""Q-learning""/>
</figure>

Epsilon Greedy Strategy is a policy that handles the exploration/exploitation trade-off.

The idea is that we define epsilon ɛ = 1.0:

- *With probability 1 — ɛ* : we do **exploitation** (aka our agent selects the action with the highest state-action pair value).
- With probability ɛ: **we do exploration** (trying random action).
 
Here is the question: What makes epsilon-greedy policies suitable for managing exploration and exploitation in value-based methods, and how might this concept inform RAG training strategies?","Epsilon-greedy policies are suitable for managing exploration and exploitation in value-based methods because they strike a balance between choosing the action with the highest expected value (exploitation) and trying random actions to gather more information (exploration). The epsilon value determines the probability of exploration versus exploitation, and as the training progresses, the epsilon value is reduced to favor exploitation over exploration. This approach allows the agent to learn from its experiences and improve its policy over time. In the context of RAG training strategies"
"What role does metadata play in Hugging Face's license tagging system, and how does this enhance collaborative AI development?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (8208, 'ba672002-2d0b-4014-9efb-36c07d84a8c3'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (22547, '51e87e51-6544-440b-9057-4201251fc27d'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (2597, '6458f359-e453-452e-93f0-0a72efc03c98'), (23078, '773ea43e-498d-4d36-8809-1e19c760b774'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (28718, '20436458-7cd8-4a96-8331-99513560667b'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (1620, '5dd0aa93-2fc4-430b-b511-62f9aee29d8e'), (14424, '557cf24b-d009-4bf3-ad9a-bc944e44be56'), (22124, 'e862ae05-e350-458c-893b-9b4a3d0b9f3f'), (21623, '32bea9e3-44f8-4464-a8dd-d2dcbc3a2102'), (24184, 'a7a6758e-20d3-4752-8f8a-952376a05592'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (19582, '763ea5e2-b3ae-4de8-be1b-62fe897fd4a3'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (26754, '167102ca-f572-4d47-aa9b-041aeda233f2'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (12935, '78e0073f-75a8-4831-8522-3bbca5d23602'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (22709, '0d067896-b515-4643-b7de-b83eaba05997'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (7355, 'fadfe698-c03f-4512-b775-f152160d9589'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (25805, 'fcaebe7b-c2ec-4995-829f-39de8f874270'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (20729, 'f89bd4e9-0d48-4fd2-9f72-07ca399bc337'), (16644, '48292295-18de-4870-999a-cf16ec29e3c1'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (1297, '0110063d-229c-4fc0-a7c3-f875aa009eb2'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (11054, '397721c5-71da-4c64-ba5c-e20f0d7b0544'), (31023, '238bb199-dbdb-4206-b206-07eeba69ea3c'), (2864, 'bd4a8a28-05e5-486b-8aff-f74ae7a91a34'), (7989, '7bf24f34-1a48-44c6-b805-592b4c1b9e1d'), (31031, '306dd725-df38-43a2-a289-44917df0a6f1'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (12099, '97456597-42ed-4914-bc4a-1146c5fc4b6f'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (10576, 'c05e1ee5-97a4-4a5f-a403-219c022099f6'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (4440, '1c80d21e-9d16-4d38-8c1d-7d0e7bac3d18'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (3937, '7450b67c-b9da-4dd2-a9a5-2772aa7ae9bf'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (15738, '3bd17398-a99e-4c37-8fc7-42bb853cb70e'), (8578, 'b6835785-bd61-41a6-94d0-a59988d7384f'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (8590, 'd83fb1f9-3d9c-409e-a570-5a3a708f8463'), (3985, 'ecedc47d-0616-441d-a202-07ae59c5cb7a'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8596, 'd07e02e0-f50e-4524-90c9-9b3cae858d33'), (16791, 'e1e0fd50-9bed-4d12-950d-4c3107e00575'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (13728, '91c1a2c4-7e1d-4ddf-8e7c-768b77a28bbc'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (31670, '86835c8a-fa41-4c7a-90b5-e28107d5fcc9'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (2512, '922c050f-9165-4d33-9529-8a72a7e9ed40'), (2513, '46aef253-eecf-48f6-8c8a-26e8d72a48f7'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (3032, '222fc5a1-961d-4b64-9e4a-a414fa382d75'), (6105, '9efc65f9-7879-4b2c-8da5-721897d26d7d'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10733, '28748039-170a-4154-9b92-cb3b695eeaae'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (13817, 'f8a28592-da18-4073-9696-21473e18cb2c'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: #### Next steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
#### Next Steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
- monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
- robustly document our most-downloaded models with model cards that detail social impacts, biases, and intended and out-of-scope use cases,
- create audience-guiding tags, such as the “Not For All Audiences” tag that can be added to the repository’s card metadata to avoid un-requested violent and sexual content,
- promote use of [Open Responsible AI Licenses (RAIL)](https://huggingface.co/blog/open_rail) for [models](https://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license), such as with LLMs ([BLOOM](https://huggingface.co/spaces/bigscience/license), [BigCode](https://huggingface.co/spaces/bigcode/license)),
- conduct research that [analyzes](https://arxiv.org/abs/2302.04844) which models and datasets have the highest potential for, or track record of, misuse and malicious use.
We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#bias-risks-and-limitations), recently applied to the new Hugging Face multimodal model [IDEFICS](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct). We've worked on how to operationalize *transparency* responsibly, including [updating our Content Policy](https://huggingface.co/blog/content-guidelines-update) (spearheaded by [Giada](https://huggingface.co/giadap)). We've advanced our support of language *diversity* on the Hub by [using machine learning to improve metadata](https://huggingface.co/blog/huggy-lingo) (spearheaded by [Daniel](https://huggingface.co/davanstrien)), and our support of *rigour* in AI by [adding more descriptive statistics to datasets](https://twitter.com/polinaeterna/status/1707447966355563000) (spearheaded by [Polina](https://huggingface.co/polinaeterna)) to foster a better understanding of what AI
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
 
Here is the question: What role does metadata play in Hugging Face's license tagging system, and how does this enhance collaborative AI development?","According to the context, metadata plays a crucial role in Hugging Face's license tagging system by providing valuable information about the datasets and models hosted on the Hub. Specifically, language metadata helps identify the correct dataset for a particular use case. By enriching the Hub with metadata, Hugging Face makes it a more powerful tool for data scientists, linguists, and AI enthusiasts. This metadata is used to display license information, such as the ""Not For All Audiences"" tag, and to support responsible"
What is the computational advantage of using value-based functions in reinforcement learning compared to direct policy-based methods?,"[(24581, '22244e17-6683-4b12-bf73-b4e4b5377b96'), (1037, 'ee236590-8d9d-4d1a-b76d-0cee9e154cdf'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (12850, 'c6e54b94-1bc1-4c66-8959-7c3a3451782c'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (15941, 'b55be617-b94e-461c-9aef-0ae569761bf9'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30298, '4e59aebc-b2c4-4323-9471-a87bce987887'), (30299, '2fc3d546-824f-49c4-9d8d-066e5c7cda4e'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (11371, '7a730a29-98ba-4ed3-8803-2deedc8c54a0'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (14487, '4f24c302-152f-4c11-b198-4be00041e6a0'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25274, '0a54e74a-9c72-4f9a-b0f0-c8420921e175'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23294, '1936ebac-0df8-44c6-a638-cb0f634059de'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11047, '939d2f75-31f8-4c36-92da-3b8dadf6c83a'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19299, '58e3889c-4edb-4806-9064-e951799e80cb'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (16745, 'bbcf4d12-db52-4214-bd20-fbb0a28f27e1'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19317, 'cab6254e-890a-47c4-a68b-1b64258cc023'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23952, 'ec7797a6-1617-49dc-9737-a162e926580a'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23957, '666ef815-1fab-4378-93d7-9c5e8c83e825'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (923, '3f1f99cb-3e95-4732-9582-c75dfc21e2bb'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23964, '80a4ae6e-a7f4-4201-845f-0e0e4cfcdfed'), (924, 'a954e80d-bcae-4191-ad77-dc130d6f2b5a'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23970, '02e2356f-f06a-4242-9e9a-01da5226883c'), (23971, '02fa64da-7776-42d4-a529-913fbf0f4c8b'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13248, '60d563de-39e5-4163-89e6-86fa1da80be6'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (961, 'f8df8306-bd61-414c-92fb-2f5f3e706930'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (964, '65fb34e4-bfe0-4990-b85f-bf4e0f542e42'), (962, 'c68008e9-dc91-4db0-a2f0-89a793040fff'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
### Among the value-based methods, we can find two main strategies

- **The state-value function.** For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.
- **The action-value function.** In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.

### Epsilon-greedy strategy:

- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.
- Chooses the action with the highest expected reward with a probability of 1-epsilon.
- Chooses a random action with a probability of epsilon.
- Epsilon is typically decreased over time to shift focus towards exploitation.

### Greedy strategy:
The advantages and disadvantages of policy-gradient methods

At this point, you might ask, ""but Deep Q-Learning is excellent! Why use policy-gradient methods?"". To answer this question, let's study the **advantages and disadvantages of policy-gradient methods**.

## Advantages

There are multiple advantages over value-based methods. Let's see some of them:

### The simplicity of integration

We can estimate the policy directly without storing additional data (action values).

### Policy-gradient methods can learn a stochastic policy

Policy-gradient methods can **learn a stochastic policy while value functions can't**.

This has two consequences:

1. We **don't need to implement an exploration/exploitation trade-off by hand**. Since we output a probability distribution over actions, the agent explores **the state space without always taking the same trajectory.**
Because, in value-based, **π exists only because of the action value estimates, since policy is just a function** (for instance, greedy-policy) that will select the action with the highest value given a state.

But, with policy-based methods, we want to optimize the policy directly **without having an intermediate step of learning a value function.**

So today, **we'll study our first Policy-Based method**: Reinforce. And we'll implement it from scratch using PyTorch. Before testing its robustness using CartPole-v1, PixelCopter, and Pong.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/85_policy_gradient/envs.gif"" alt=""Environments""/>
</figure>

Let's get started,
What are the policy-based methods?

The main goal of Reinforcement learning is to **find the optimal policy \\(\pi^{*}\\) that will maximize the expected cumulative reward**.
Because Reinforcement Learning is based on the *reward hypothesis*: **all goals can be described as the maximization of the expected cumulative reward.**

For instance, in a soccer game (where you're going to train the agents in two units), the goal is to win the game. We can describe this goal in reinforcement learning as
**maximizing the number of goals scored** (when the ball crosses the goal line) into your opponent's soccer goals. And **minimizing the number of goals in your soccer goals**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg"" alt=""Soccer"" />

## Value-based, Policy-based, and Actor-critic methods

In the first unit, we saw two methods to find (or, most of the time, approximate) this optimal policy \\(\pi^{*}\\).
 
Here is the question: What is the computational advantage of using value-based functions in reinforcement learning compared to direct policy-based methods?","The computational advantage of using value-based functions in reinforcement learning compared to direct policy-based methods is that value-based methods can store additional data (action values) which can be used to estimate the policy, whereas policy-based methods do not need to store this additional data and can estimate the policy directly."
How does the modular design of Gradio contribute to scalable reinforcement learning simulations?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (27654, 'b251137d-d7d4-4fa5-be14-69555f6edf11'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (5662, '9acf2b54-137b-40ce-a6f8-eec06d51c4da'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (3119, '6a16877a-d058-484e-adf4-1e3130b324ae'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (11829, '38bd3f56-e5b3-4e10-8cb9-2c02e7602038'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (9812, 'c40eb4c6-cdfe-42c8-a63c-46be475ef695'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (9312, '14034176-8319-4870-8e63-8ce542f58d45'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29292, '4214c4f3-080e-4daf-807b-01ee54650589'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (25206, '9db87039-b249-40f0-a7a4-0f0a0ffad669'), (7799, '90107451-0822-4205-96bc-e19ca4488723'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (6791, 'caea27fd-5810-4f6f-afa1-5befc362cced'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22158, '43379d2f-56a8-485d-bdca-55c7e01941b6'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (21675, 'f912b25a-5222-43ff-86a2-960b77ef8b41'), (6826, '8aa2b833-292f-452f-a64e-c2b51b5ee38a'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (12055, '7dc896f5-36c3-4193-9361-7ef771809049'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (5524, 'bb1d7e3e-0981-435e-af37-33e2588f6b6b'), (2464, '54179c43-bf4a-4c14-b098-b5140e0a1a4b'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (12721, '26fbe79d-0795-4bcf-a444-8f56c357cc4c'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (7095, '46239874-f414-40b3-bf11-c8ec6587d46c'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ### 2. Gradio ONLY works with PyTorch models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with PyTorch models, but also works for any type of machine learning model!""
        },
        {
			text: ""False"",
			explain: ""Gradio is model agnostic, meaning you can create a demo for any type of machine learning model."",
			correct: true
        }
	]}
/>

### 3. Where can you launch a Gradio demo from?

<Question
	choices={[
        {
			text: ""Standard python IDEs"",
			explain: ""Gradio works great with your favorite IDE."",
            correct: true
        },
        {
			text: ""Google Colab notebooks"",
			explain: ""You can create and launch a demo within your Google colab notebook."",
			correct: true
        },
        {
			text: ""Jupyter notebooks"",
			explain: ""Good choice - You can create and launch a demo within your Jupyter notebook."",
			correct: true
        }
	]}
/>

### 4. Gradio is designed primarily for NLP models
```

## 5. Create Gradio Chatbot backed by Amazon SageMaker

We can also create a gradio application to chat with our model. Gradio is a python library that allows you to quickly create customizable UI components around your machine learning models. You can find more about gradio [here](https://gradio.app/).

```python
!pip install gradio  --upgrade
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
I recruited my talented housemates Ali Abdalla, Ali Abid, and Dawood Khan to release the first version of Gradio in 2019. We steadily expanded to cover more areas of machine learning including text, speech, and video. We found that it wasn't just researchers who needed to share machine learning models: interdisciplinary teams in industry, from startups to public companies, were building models and needed to debug them internally or showcase them externally. Gradio could help with both. Since we first released the library, more than 300,000 demos have been built with Gradio. We couldn't have done this without our community of contributors, our supportive investors, and the amazing Ahsen Khaliq who joined our company this year.
### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
 
Here is the question: How does the modular design of Gradio contribute to scalable reinforcement learning simulations?","The question is not answerable based on the provided context. The context does not mention anything about the modular design of Gradio or its contribution to scalable reinforcement learning simulations. It only discusses Gradio's capabilities, features, and usage in various scenarios, but not its design or relation to reinforcement learning simulations."
"Why is multi-license support essential in Hugging Face's ecosystem, and how does it facilitate ethical AI use?","[(22547, '51e87e51-6544-440b-9057-4201251fc27d'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (14358, '4732caf5-9d60-4123-a2c2-94d1b5ca78fe'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (3616, '0709d283-044c-40ef-a53c-9892f93f74ac'), (2597, '6458f359-e453-452e-93f0-0a72efc03c98'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (7725, '258a242e-844a-4498-bd2d-4c45f980f5f2'), (558, '093e0bfc-98b8-4d7b-a428-9ec96209387f'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (22588, 'a59c4df1-bdaa-4faa-b0f4-f3b5aeff16a6'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (11839, '1295e8ba-675b-4e41-9660-6a758ea89426'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (8782, '4cffb526-4c01-473c-ae6a-aa99e1991fcc'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (24657, '37943db2-8a2c-4369-80c8-e75eee95f201'), (94, 'db2da404-7a48-42c3-b246-9a0cc202fe4c'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (14944, 'cd075ce4-d02e-47be-ad35-21b38e850692'), (30821, '721f4d5f-3a80-403c-b5bd-8d94a87543e5'), (4208, '6510a508-9d37-48d7-96b5-cea2f637feae'), (21624, 'cf165b8c-348b-4ccc-acd1-944ae9c89877'), (14972, '1a9cc31a-a8ea-47b5-803e-fa461faaa9b5'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25728, 'e9a67dae-3e05-4640-afbb-816d75639376'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (12935, '78e0073f-75a8-4831-8522-3bbca5d23602'), (4745, '1a92f7c2-3ab9-4ff1-89af-f0ab573bdec0'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (8853, 'cffd4004-ae30-4ab2-aced-232215176a1d'), (3229, 'bf82b68e-7d33-457e-b6d8-fbd0466d6271'), (7837, 'fb0db92b-f560-4130-a389-df2018f97ada'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (29354, 'eb6365fe-b9fc-400e-866f-6a21ed2a810f'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (18093, '5513757a-36b3-47a3-8270-f75a1d76df5e'), (28335, '994c72af-e053-48a3-9059-b9662f76400a'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (13510, '0e4bf28c-0eed-49c8-a4a5-724ce73aa1fd'), (14535, 'b9835dda-ff2d-4d55-a497-5553a72948a6'), (11975, 'c977182b-d101-4944-95e4-2c832d9bf6fd'), (17608, '12e880e8-366a-4118-bb57-c2f40c0793cd'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (3789, '6c31804d-45f5-468e-a44c-e7060de4fdd8'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (208, 'ca329095-eb87-48a9-9393-7dafb6980978'), (21713, '18782689-5845-4f1e-92b4-3ed0000a08e1'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24286, '6bd381bd-f2c3-4435-bec4-43843d989605'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (10992, '004cff2f-f98d-40e2-9254-bfe3183ca2f2'), (14577, '03a094b5-b29c-4840-a743-d2ceb896c494'), (3329, '64a72f1a-f303-41c1-97ea-b32aeeac5fbc'), (11526, '994250e0-f714-40fe-b8ff-52c2610db830'), (8970, 'd5af4c95-97c8-4196-bab1-0d74644248f4'), (30994, '79a1b793-603c-4141-87d4-270e01bb91f7'), (8491, '3cb0ff2d-cd24-47e1-b368-9800cbb99b0f'), (31031, '306dd725-df38-43a2-a289-44917df0a6f1'), (12099, '97456597-42ed-4914-bc4a-1146c5fc4b6f'), (9030, 'f38d1e50-b06a-4f7b-b7ff-67ae2899320e'), (24905, 'f62d7196-fd88-4740-b825-0375a9e0c9f7'), (10576, 'c05e1ee5-97a4-4a5f-a403-219c022099f6'), (4441, '139619cb-68a0-41b0-9ab9-a1beba4daeb2'), (24410, '63102f98-f7b8-4a0b-8034-3e2687269cb4'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4964, '87a02f1c-9369-4f63-8050-567d148301e0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (17767, 'a7af65e5-6000-4846-b469-ecc521b89dfa'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (24436, '62eee91c-545a-404b-97af-1d99deab920c'), (897, 'f387e8a2-b1bf-48fa-a6f3-c7a015a21ae7'), (15761, '19f353ed-20f2-48e1-8cb1-4a719927586c'), (3985, 'ecedc47d-0616-441d-a202-07ae59c5cb7a'), (17301, '9e130d4e-12df-4941-91c1-6e5fd07be093'), (8599, 'b9ceb8e8-cbcd-4cd4-b341-73c69d823263'), (16791, 'e1e0fd50-9bed-4d12-950d-4c3107e00575'), (5020, '9c9df608-6264-4ae6-9f4c-394daed65dde'), (18334, 'c504a58a-f5ca-405b-9dbb-5b5f06fd1cf0'), (31136, '51caed7d-0da0-49a7-b6a2-8ee3113b8459'), (22433, 'f1052a2a-7fe3-49f0-beeb-740f4ecaa31e'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (6571, '524ec6e8-072e-4f5f-8879-d89812239e9d'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (3510, '4e9735e1-0a03-42b4-a237-4ea82a96730d'), (13239, 'f13538dd-edce-4970-a1cc-83ab6fd1a5d6'), (7096, '3ba57845-c986-4965-b8a2-53c99388658b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29121, 'a79d6e54-b6d5-4e9e-9617-8d8cf4ef5569'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (3032, '222fc5a1-961d-4b64-9e4a-a414fa382d75'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (22012, '5c55b2ec-3284-45a4-9f86-cf514fba02e0')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Hugging Face is where it is today thanks to its community of developers, so we’ve seen firsthand what open development brings to the table
to support more robust innovation for more diverse and context-specific use cases;
where developers can easily share innovative new techniques, mix and match ML components to suit their own needs,
and reliably work with full visibility into their entire stack.
We’re also acutely aware of the necessary role of transparency in supporting more accountability and inclusivity of the technology –
which we’ve worked on fostering through better documentation and accessibility of ML artifacts, education efforts,
and hosting large-scale multidisciplinary collaborations, among others.
Thus, as the EU AI Act moves toward its final phase, we believe accounting for the specific needs and strengths of open and open-source development of ML systems will be instrumental in supporting its long-term goals.
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
of Conduct](https://huggingface.co/code-of-conduct) and [content guidelines](https://huggingface.co/content-guidelines) to help guide discussions along dimensions we believe to be important for an inclusive community space. We have developed a [Private Hub](https://huggingface.co/blog/introducing-private-hub) for secure ML development, a [library for evaluation](https://huggingface.co/blog/eval-on-the-hub) to make it easier for developers to evaluate their models rigorously, [code for analyzing data for skews and biases](https://github.com/huggingface/data-measurements-tool), and [tools for tracking carbon emissions when training a model](https://huggingface.co/blog/carbon-emissions-on-the-hub). We are also developing [new open and responsible AI licensing](https://huggingface.co/blog/open_rail), a modern form of licensing that directly addresses the harms that AI systems can create. And this week, we made it possible to [“flag” model and Spaces
Hello, world!

Originating as an open-source company, Hugging Face was founded on some key ethical values in tech: _collaboration_, _responsibility_, and _transparency_. To code in an open environment means having your code – and the choices within – viewable to the world, associated with your account and available for others to critique and add to.  As the research community began using the Hugging Face Hub to host models and data, the community directly integrated _reproducibility_ as another fundamental value of the company. And as the number of datasets and models on Hugging Face grew, those working at Hugging Face implemented [documentation requirements](https://huggingface.co/docs/hub/models-cards) and [free instructive courses](https://huggingface.co/course/chapter1/1), meeting the newly emerging values defined by the research community with complementary values around _auditability_ and _understanding_ the math, code, processes and people that lead to current technology.
 
Here is the question: Why is multi-license support essential in Hugging Face's ecosystem, and how does it facilitate ethical AI use?","Multi-license support is essential in Hugging Face's ecosystem because it allows authors to define which license applies to the work they share, ensuring transparency and accountability in the use of ML models and datasets. This facilitates ethical AI use by enabling users to quickly know which license applies to which model, and to list models with a specific license, promoting responsible and inclusive development of ML systems."
"What is a key limitation of torch.distributed for document retrieval in RAG fine-tuning, and how does Ray overcome it?","[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (10251, '31409786-ddd4-49ae-94d2-bea926b83be4'), (6679, '5355dc06-3d44-4916-a09c-fc5839bb0ca7'), (24603, '78522c83-d8fe-4f56-8cbe-0c5d741f50f1'), (12317, '271019f8-cd28-4e51-814f-981ed9de6798'), (24606, '0b632df8-fda6-4a3f-9b26-aa4ce8eb417d'), (7208, 'd7164bc1-bc8d-4ca1-9309-e5a8b834f98d'), (20521, 'c8d9c234-55d3-4552-91fc-5030005cd9bf'), (14386, 'b8cd4a6c-621a-48e7-a398-c9e62eee673e'), (17458, '29ac6c95-d3e2-470e-a268-83fd7c395ddd'), (2613, '8ed34ac1-8e5c-4f4d-a4e5-765f43230e11'), (9782, '738e1e6a-9b5c-4f95-8f37-baec1c198116'), (29752, '866f9889-e3da-4559-b3d6-343baf84f073'), (2618, '8c25f1c0-1ce2-488c-a60e-9ecdd7e2403b'), (5694, 'dee73012-4ba6-4015-8099-1ff6348fc9b2'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (1100, 'b79e9cec-adcc-4a9b-981b-5a6e9fbcd005'), (6738, '001fbe14-a409-4006-b8c8-e19952cdccbc'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (19547, '8caa2fcc-7a1d-4007-9db2-8e7011fd9033'), (22110, '60c67161-62ba-4bb8-88e1-6a0592f80287'), (22111, '6cb41ff3-1249-49c2-b70e-3668ab947b7a'), (22112, '72ebbfd3-6f7e-4e49-aeee-7692ef491d72'), (22113, '14d11c06-e512-45ae-8775-9c31eed1e9d7'), (22114, '0a5ba4bb-e50a-4171-82ba-b95d1f4b573c'), (19555, '9bb43118-305f-403d-8b60-5c20b13a050c'), (4707, '0aeaff5d-3eb8-40d0-b8da-3a399195ab50'), (22116, 'e561f1a9-e1b1-4da2-9db7-3485e7719c79'), (22118, '36a5c490-635c-46e4-bbff-9f2186d8cb06'), (22119, 'b0e6ddbb-4f06-4aaa-a05e-0c06c29dcd0d'), (19561, 'bd6d1959-f94f-4a44-b2c4-a94c6950eb8c'), (22121, 'ebdea3ea-7be3-4bbe-bee4-0b68571dc003'), (19563, '505b868d-35d2-4d6c-8268-8e729abd6f63'), (11383, '7d1776e7-3738-41fc-ae14-19c3f95d3c25'), (10873, 'a4e244ba-2eea-4b67-a7e0-2be6376bcc7a'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e'), (1662, '9463d366-70d4-4377-8f4c-a43126487694'), (24711, 'ff60e964-aa24-44ba-9ac0-e72b18947ea2'), (24712, 'e34b93ed-59ef-42e7-9129-b252acbf40f8'), (5257, '01286d7b-65ff-41e0-b0b1-3ec8dcb42148'), (8330, 'b2bd4024-ac0b-42c0-9840-c85ddda39140'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (11918, '0533046d-82d3-411e-b1fa-90da52cc0fe8'), (9876, 'b5c62f13-7a9a-411b-9d22-0935ded13372'), (22177, 'fe470ead-825d-4432-ab27-1f1f4a497250'), (19106, 'f9485ca5-0c07-4a79-b3ae-363c2ee36a2b'), (7337, 'e9d756c4-fba8-4487-a964-02c57d1ae176'), (30382, 'c4079e2a-5915-47e2-b0f5-6dadaf8d6c3c'), (26294, 'e270afea-3092-480b-ba50-52d43d31bf2b'), (6338, 'f8556800-ba9c-4c54-b96e-56d695a0da64'), (709, '68b25497-c5e6-494d-aebf-9f31f7c98cc0'), (25824, 'f347de7b-7368-41a0-883a-8429d82f7692'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (17635, 'cb4c9589-721d-4162-839c-0a37be083184'), (22772, 'd0b704d9-ea88-403d-8797-758ba929ebd4'), (761, '85426fd8-7ddb-45c5-b275-4bda9bfb9212'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (16673, '19dd7de4-3435-4cd6-80bc-6a7dc5ac2cda'), (13097, 'd0ae3415-29e2-4a0a-a9e4-9f788040397f'), (17208, '2203f22a-0a57-4589-bd1b-a9d45aa91f1f'), (17209, '637b92d4-eb98-4446-b500-74fe9871a4dd'), (17210, 'bf0844da-5cf9-43fa-8032-2845dfe427c1'), (17211, '7ce7337b-fcc4-41d3-bbc0-478753c50a6a'), (17212, '3639fc6d-edf2-464b-b1f8-6a8147fa1270'), (17213, 'f849ea19-994a-4645-821c-d8c7dcea2c4d'), (17214, 'c24d07fd-0008-4815-9ccd-66957ce33488'), (17215, 'b86bbccd-3b71-4a3f-993f-a8f72ce10054'), (17216, 'e14f50a6-9791-4a08-91f2-bae04ec2151a'), (2881, '5d2c5594-1089-477a-8272-d14dc1e61b43'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (9540, '8cfc2be7-ce6e-4ee3-9fa7-d4da603fc5cf'), (11770, '2d23cee2-4878-40e3-8d34-c17acb4ff6a8'), (17770, 'e7622f83-c164-4281-816b-b028d8eab303'), (29177, '619af8a9-fbe3-421d-9eca-be6d9044cb79'), (14190, '52eac1aa-1ce8-4e04-85df-009833ff0153'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (31108, 'b2a6db3a-19cd-4f85-8ebb-367f85973aa0'), (15768, 'f0a867f0-3313-47f8-a4d9-49cb78888982'), (15772, '65169dba-99db-485c-9bff-b711a9a78463'), (17822, '20439f0d-de85-4a4f-91e9-58d6c81c72a7'), (15776, '5792c81e-163d-4045-9e92-aa5c22b8bdc9'), (17826, '25fa36a1-9fd9-4630-b761-739de4353aff'), (17315, '42e6b701-ef1a-4a5b-adbe-9e296483d4f5'), (2980, '1f273dec-8863-42c5-804e-8246f09b1434'), (2984, '59ae3b4c-4388-43a0-9dd8-80a1c32f1434'), (1470, 'b2d459cf-a7d0-4834-8979-cc59e274af59'), (1471, '0be7ac99-eca3-4e0d-86bd-8c7451edbbb6'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (31719, '9413008c-5af5-444b-ba79-f5ba91409f43'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (13300, '9d7131c8-6c02-44ee-a40d-ad03154de405'), (8179, '23697567-e880-40f6-b84e-f3b7f9525f09'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4092, '49e8126f-6930-48ec-abbd-632bcb85a3df'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Instead, a framework-agnostic and a more flexible implementation for ad-hoc concurrent programming is required. [Ray](https://ray.io/) fits the bill perfectly. Ray is a simple, yet powerful Python library for general-purpose distributed and parallel programming. Using Ray for distributed document retrieval, we achieved a **2x speedup per retrieval call compared to `torch.distributed`**, and overall better fine-tuning scalability.

### Ray for Document Retrieval
![alt_text](assets/12_ray_rag/torch_distributed_document_retrieval.png ""image_tooltip"")
_Document retrieval with the torch.distributed implementation_


The main drawback of the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) implementation for document retrieval was that it latched onto the same process group used for training and only the rank 0 training worker loaded the index into memory.

As a result, this implementation had some limitations:
![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_

To overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Ray’s stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG.
### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
```
You will then be able to pass `path/to/checkpoint` as `model_name_or_path` to the `finetune_rag.py` script.

## Document Retrieval
When running distributed fine-tuning, each training worker needs to retrieve contextual documents
for its input by querying a index loaded into memory. RAG provides two implementations for document retrieval,
one with [`torch.distributed`](https://pytorch.org/docs/stable/distributed.html) communication package and the other
with [`Ray`](https://docs.ray.io/en/master/).

This option can be configured with the `--distributed_retriever` flag which can either be set to `pytorch` or `ray`.
By default this flag is set to `pytorch`.

For the Pytorch implementation, only training worker 0 loads the index into CPU memory, and a gather/scatter pattern is used
to collect the inputs from the other training workers and send back the corresponding document embeddings.
As a result, this implementation had some limitations:

1. **Synchronization bottleneck**: The rank 0 worker had to receive the inputs from all workers, perform the index query, and then send the results back to the other workers. This limited performance with multiple training workers.
2. **PyTorch specific**: The document retrieval process group had to latch onto the existing process group used for training, meaning that PyTorch had to be used for training as well.

![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_
 
Here is the question: What is a key limitation of torch.distributed for document retrieval in RAG fine-tuning, and how does Ray overcome it?","A key limitation of torch.distributed for document retrieval in RAG fine-tuning is that it latches onto the same process group used for training and only the rank 0 training worker loads the index into memory, resulting in a synchronization bottleneck and making it PyTorch specific. Ray overcomes this limitation by using multiple processes that are separate from the training processes to load the index and handle the retrieval queries, allowing for better scalability and flexibility."
How does the action-value function in reinforcement learning influence RAG’s document retrieval strategies?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (14884, 'e0ec82a1-9dbd-4f42-b7e8-536d89bae891'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (15941, 'b55be617-b94e-461c-9aef-0ae569761bf9'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (166, 'a7fb2601-9706-4665-a80b-8da658e759f7'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25777, '222d6fa5-c0e6-4075-bb20-83f0dfe99f5c'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23294, '1936ebac-0df8-44c6-a638-cb0f634059de'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (1336, '329c759f-4fc3-49ae-89bd-b24d835a593b'), (20810, '118abb64-11e7-49fe-a65e-bdee25933d33'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (16746, '45080cea-724d-4d98-8278-7cd29fd71f9b'), (20335, '24699ce0-932d-4081-a7d8-ed55918efde0'), (19311, '6e5dbd8e-f471-4533-b4a4-74571e8d6abf'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (28534, '3e54a2ff-2d80-46f7-870f-7a3153c2036b'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23956, '9dec7d5b-756e-4a6c-ac15-1b544069a789'), (23957, '666ef815-1fab-4378-93d7-9c5e8c83e825'), (23958, '82ffb2ac-f889-482f-bd2b-090b36dbcce1'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (923, '3f1f99cb-3e95-4732-9582-c75dfc21e2bb'), (924, 'a954e80d-bcae-4191-ad77-dc130d6f2b5a'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23454, 'bb59e1c0-8193-40ce-a450-9efc2ecc6226'), (23966, '5ad52301-e36d-4241-a74f-c251f1bf2470'), (11160, 'da0b4465-7e50-41b1-9bb2-d6ce2483d770'), (925, '2b19d79e-1c48-4d05-a48d-19f5138ae7c1'), (23970, '02e2356f-f06a-4242-9e9a-01da5226883c'), (23971, '02fa64da-7776-42d4-a529-913fbf0f4c8b'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (13755, 'c07affda-1794-4844-a683-eb253231c213'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (961, 'f8df8306-bd61-414c-92fb-2f5f3e706930'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ### Among the value-based methods, we can find two main strategies

- **The state-value function.** For each state, the state-value function is the expected return if the agent starts in that state and follows the policy until the end.
- **The action-value function.** In contrast to the state-value function, the action-value calculates for each state and action pair the expected return if the agent starts in that state, takes that action, and then follows the policy forever after.

### Epsilon-greedy strategy:

- Common strategy used in reinforcement learning that involves balancing exploration and exploitation.
- Chooses the action with the highest expected reward with a probability of 1-epsilon.
- Chooses a random action with a probability of epsilon.
- Epsilon is typically decreased over time to shift focus towards exploitation.

### Greedy strategy:
```

The action space is a vector with 3 values:
- Control x, y, z movement


### Normalize observation and rewards

A good practice in reinforcement learning is to [normalize input features](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html).

For that purpose, there is a wrapper that will compute a running average and standard deviation of input features.

We also normalize rewards with this same wrapper by adding `norm_reward = True`

[You should check the documentation to fill this cell](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecnormalize)

```python
env = make_vec_env(env_id, n_envs=4)

# Adding this wrapper to normalize the observation and the reward
env = # TODO: Add the wrapper
```

#### Solution

```python
env = make_vec_env(env_id, n_envs=4)

env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_obs=10.)
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
Introduction [[introduction]]

  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png"" alt=""thumbnail""/>

In the last unit, we learned about Deep Q-Learning. In this value-based deep reinforcement learning algorithm, we **used a deep neural network to approximate the different Q-values for each possible action at a state.**

Since the beginning of the course, we have only studied value-based methods, **where we estimate a value function as an intermediate step towards finding an optimal policy.**

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg"" alt=""Link value policy"" />

In value-based methods, the policy ** \(π\) only exists because of the action value estimates since the policy is just a function** (for instance, greedy-policy) that will select the action with the highest value given a state.
### Scaling up fine-tuning
This retrieval of contextual documents is crucial for RAG's state-of-the-art results but introduces an extra layer of complexity. When scaling up the training process via a data-parallel training routine, a naive implementation of the document lookup can become a bottleneck for training. Further, the **document index** used in the retrieval component is often quite large, making it infeasible for each training worker to load its own replicated copy of the index.

The previous implementation of RAG fine-tuning leveraged the [torch.distributed](https://pytorch.org/docs/stable/distributed.html) communication package for the  document retrieval portion. However, this implementation sometimes proved to be inflexible and limited in scalability.
 
Here is the question: How does the action-value function in reinforcement learning influence RAG’s document retrieval strategies?","The action-value function in reinforcement learning does not influence RAG's document retrieval strategies. The context provided does not establish any connection between the action-value function and RAG's document retrieval strategies. The action-value function is a concept in reinforcement learning, whereas RAG's document retrieval strategies are related to natural language processing and information retrieval."
What makes Gradio’s quality checks valuable for maintaining model deployment standards?,"[(24065, 'e3e1132e-e823-411b-b7cc-d755700630aa'), (24067, '157c15d9-4820-40cb-9366-e7b656d413a7'), (28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (27654, 'b251137d-d7d4-4fa5-be14-69555f6edf11'), (24073, '6d6d9a99-657b-4749-a031-7d443a05252c'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (5651, 'fbea4144-f67b-4eaa-a37e-347dfab99d2d'), (5652, '819eb8d6-e4e9-44bd-9520-2fadf76dbe3d'), (5653, '96183704-8c3b-4f40-abcb-e94d83092caf'), (28183, 'a8cab7ae-c71e-44bd-8d55-8406dc0b8660'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (5658, 'a1d2b387-3fbe-4b05-8f7a-3b7ab5aa3769'), (28186, '2715ef7b-62a4-4ae9-95c5-944fe97fd438'), (5660, 'f060d5f9-0dce-4d78-8298-2524cde9ccdb'), (28199, '5d792e30-a6af-483e-bf0d-836c80cb5097'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (28209, '68d2780c-5f9b-479f-9212-72a4754e9969'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28232, 'edcdb710-1075-4024-937a-e1e9b421cd61'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (28248, '14667f8d-196e-48a1-9b90-85075c677427'), (12890, 'd760aaaa-a33f-407b-a4fd-7069770cdf32'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28257, 'cfc6625e-3cab-4f72-8584-37a13847b65a'), (2150, 'f095c104-a3da-40e4-8826-58c85512a8c0'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (28267, '6012369a-6c18-4403-85a8-b58f59c33f3c'), (29292, '4214c4f3-080e-4daf-807b-01ee54650589'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (25206, '9db87039-b249-40f0-a7a4-0f0a0ffad669'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (25226, '9c7b12af-d7dc-4a79-8194-5bee1f723220'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (25228, 'baea8d1e-5d4c-41d5-967b-7acff9b12ce8'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (2208, 'ba593b1c-6f5d-49c6-b377-4b55c72484e4'), (26787, '32682646-5b86-49ea-b44b-dcffacbf726f'), (2214, '025e63ca-22c5-4fa2-9b8e-44065d8d2701'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (6826, '8aa2b833-292f-452f-a64e-c2b51b5ee38a'), (2220, '700e60d0-c612-424f-b157-205616f9f254'), (21675, 'f912b25a-5222-43ff-86a2-960b77ef8b41'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (2238, '29adadfd-79f3-46a4-93d5-9cb57573ec81'), (2249, 'b6c12764-f869-4323-a0ab-efd1cbcb6ee1'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (2252, 'cf2e843d-3e19-4217-b439-3fcdb5926a08'), (2253, '528c0950-87eb-4ce0-9db7-aeaf9089c226'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (2263, 'de23a944-32ae-4aca-9342-ed441d8275b4'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (2285, '5b7edfae-d1bb-4c5d-8d8f-83b616f05d0d'), (27887, '69311dd3-e6f7-4e37-a620-9ae98462dcfd'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (27897, '00ee70f5-c8bc-45d4-9a8b-c379cafc9c97'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (2309, '49057658-3d2a-48f6-bcb5-968964f12996'), (2312, '51224360-7358-4c07-ac89-332e6ee010f8'), (26898, 'ba831bc1-46d4-4934-9a8f-689aad480ec3'), (2325, 'e39e098f-cf1e-4087-812e-2136ccec4ec7'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (12060, '38247c01-220b-423e-b551-3642278c2134'), (2335, '12335f0e-4907-4225-a481-d525a0c2e614'), (23331, '10c037c2-4dde-46da-b8d4-63c8d5d7ecf0'), (23336, '257db093-ad05-4609-861d-15343bf4fcf8'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (6954, 'dc0a4a82-5286-4d59-ac85-fe642c49151e'), (3382, 'f57e911f-3558-4cec-a7c5-dc8c73ec91ef'), (2358, '61c00040-18d9-47df-b54f-732d42c2c5f9'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (2374, '3c2fc425-71a9-4574-8e7e-9ea6de5e161a'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (2383, 'c6ad6da4-4a77-4db0-b926-6c1b77b6db81'), (2393, 'ab888fa8-fd7f-4996-8db2-bab36ec7709b'), (29028, '15a78b30-fe85-4a8b-885d-1bf6e46e88b1'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (12146, '6b4f5910-201d-414b-825a-1abf827d8df1'), (12148, 'eb4e71b6-274a-443a-850f-cda16f106a1e'), (28024, '6c568b56-28c0-4eeb-b9c5-1618913cb943'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (28082, '8c86f9c1-60c8-40ed-8084-d82bfb3fc00a'), (14259, 'a39a8b28-d140-469b-bcf6-4087261c872f'), (15795, 'aabab3d6-cbff-41a3-827f-a25abfc8958f'), (13746, '0bb29eea-90a5-4fd6-8177-0ccbbf32e132'), (27062, 'cd158b22-9fc5-49d1-afc0-8a5dfd595f1e'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (28088, '86a70b0f-cff2-4271-8ac0-c271371d3049'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (28094, 'a923dd7b-a562-4641-9f9b-f09697c8b6ee'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (3524, 'bd2974bd-58df-4915-9a42-79d2a06eaf18'), (3525, 'b0b4a2df-8e7d-43b7-8046-5668bfe2ea02'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (28112, '62dd089d-8e58-4803-b275-9ae30db8b7b5'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (3539, 'a56bb41f-11ca-453a-be72-97ceea36735d'), (11734, 'd2266dda-9dcd-4bce-b11d-eea9e47bb1e6'), (28123, '1e0f48d2-52b2-4fe6-a624-509a98f026ae'), (2013, '8247fa80-5295-40ba-80ca-5ab308278cc0'), (28126, '82468f2e-776c-4d15-8134-4912158c36b3'), (28127, 'bd655a7b-d09b-4b55-8ba0-932251f68bcf'), (29664, '55395fd7-b89c-4823-abee-a2b5747fe3af'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (2023, 'd20a455f-f235-41b7-a1c5-5b15b10c5065'), (28137, '5116f0d1-8fcf-4fed-971e-06051bd8b4b1'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283'), (28159, '82652842-43af-40f0-9e66-fbd1e5adac72')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
gradio-ui

This folder contains all of the Gradio UI and component source code.

- [set up](#setup)
- [running the application](#running-the-application)
- [local development](#local-development)
- [building for production](#building-for-production)
- [quality checks](#quality-checks)
- [ci checks](#ci-checks)

## setup

This folder is managed as 'monorepo' a multi-package repository which make dependency management very simple. In order to do this we use `pnpm` as our package manager.

Make sure [`pnpm`](https://pnpm.io/) is installed by [following the installation instructions for your system](https://pnpm.io/installation).

You will also need `node` which you probably already have

## running the application

Install all dependencies:

```bash
pnpm i
Test Strategy

Very brief, mildly aspirational test strategy document. This isn't where we are but it is where we want to get to.

This document does not detail how to setup an environment or how to run the tests locally nor does it contain any best practices that we try to follow when writing tests, that information exists in the [contributing guide](https://github.com/gradio-app/gradio/blob/main/CONTRIBUTING.md).

## Objectives

The purposes of all testing activities on Gradio fit one of the following objectives:

1. Ensure that the Gradio library functions as we expect it to.
2. Enable the maintenance team to quickly identify both the presence and source of defects.
3. Prevent regressions, i.e. if we fix something it should stay fixed.
4. Improve the quality of the codebase in order to ease maintenance efforts.
5. Reduce the amount of manual testing required.

## Scope
Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
We should not focus on code coverage but on test coverage following the below criteria:

- The documented Gradio API (that's the bit that users interact with via python) should be tested thoroughly. (1)
- Additional gradio elements that are both publicly available and used internally (such as the Python and JS client libraries) should be tested thoroughly. (1)
- Additional gradio elements that are publicly available should be tested as thoroughly as is reasonable (this could be things like demos/the gradio CLI/ other tooling). The importance of each individual component, and the appropriate investment of effort, needs to be assessed on a case-by-case basis. (1)
- Element boundaries should be tested where there is reasonable cause to do so (e.g. config generation) (1)
- Implementation details should only be tested where there is sufficient complexity to warrant it. (1)
- Bug fixes should be accompanied by tests wherever is reasonably possible. (3)

## Types of testing
 
Here is the question: What makes Gradio’s quality checks valuable for maintaining model deployment standards?","Based on the context, Gradio's quality checks are valuable for maintaining model deployment standards because they ensure that the Gradio library functions as expected, enable the maintenance team to quickly identify and source defects, prevent regressions, improve the quality of the codebase, and reduce the amount of manual testing required."
Why is the exploration-exploitation trade-off critical in reinforcement learning and how can it be mirrored in document retrieval?,"[(24580, '9e0b974c-4de1-4c4e-9ad2-c81dd70f9d85'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (26649, '9f9882a3-02b5-4685-9557-498094e25b22'), (26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (14884, 'e0ec82a1-9dbd-4f42-b7e8-536d89bae891'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14886, 'c48286c5-a1b4-43f0-ab50-f7520594909d'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (2638, 'fd31dc0c-6b80-49bb-9662-e72e93f6a307'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30293, '5d3a40e3-d4ac-44dd-978e-75cf25bfcb48'), (25688, 'be63462d-16cc-4d1e-bfbe-04d1d5b8bfdc'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (11365, 'fe8da28b-5e19-460e-be89-ccd9c18b8b30'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (11369, '2ca17ef0-26c5-4942-b384-52438a71c220'), (11370, 'c91f2f31-6675-4df6-bc5e-f8e75070f94f'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (23162, 'adfa1aae-7eea-4d74-b807-13110e10c22f'), (17022, '70223006-6f3d-4822-b7fb-f3b758beb9bd'), (17023, 'c30d55b4-06d1-4928-89ae-69e99df2681c'), (17024, 'ba063714-b3f4-4c4b-b846-bec5b7c95c81'), (17025, '69f0e2e7-b149-4523-a038-d069193eeec2'), (17547, '91c00572-19ed-4c45-a397-6e45a63c5956'), (17040, '3c81255d-fba2-425b-add0-66180682f513'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (21660, '107b725e-6c0d-4c75-84b6-b69fcdc6801f'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (163, '12b31373-c096-4126-8d61-c0cee6b23bce'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (166, 'a7fb2601-9706-4665-a80b-8da658e759f7'), (165, '41c16f94-d1b6-4437-9b63-6a249cafa0d0'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25274, '0a54e74a-9c72-4f9a-b0f0-c8420921e175'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (21183, '74d1c74c-7b3d-457c-b983-ec23b3268af3'), (21184, 'ceb0a222-b48c-4eed-bdae-b4298a4ad649'), (21185, '60c59ad2-a6f1-46ab-a50c-b3bf469e9a92'), (21186, '31cd25b9-75f3-43f6-b771-f47d73dafddd'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (20701, '04553fa1-c85e-4533-9f15-8a7240e55fc3'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (22819, 'a415afd3-d643-4e9a-bb7a-478941d50298'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (23339, 'acc2c624-ff31-433f-be5f-f1000a8f48dc'), (23340, 'a5686835-f374-4bcb-b2d6-0743f7174f78'), (23341, '0917fba5-5ba9-4803-a905-9394b17f8039'), (28984, 'e188682f-cffa-405e-921b-fc2436cfb9ad'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (1346, 'c529df38-47a0-4ceb-9ce4-5e3c1c64eb95'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19300, 'ca9fcfe5-7cb5-4873-a53a-d887c9dfa786'), (19301, '886e70c3-27fc-4272-8c14-663662ac57de'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (19304, 'b47f3bd6-724f-4cb0-adff-c4fbefb8903c'), (28527, '9dd374d0-f99b-4b6b-9b15-5f4ce6382aa4'), (19311, '6e5dbd8e-f471-4533-b4a4-74571e8d6abf'), (19313, '43028562-976a-43d3-8062-60062555356c'), (19314, '77062fd7-8f64-4bc1-9a5c-db9ac13a4e5d'), (19315, '91b8e9dc-2998-430a-9fcc-55fc69552b76'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (3465, 'df743924-ed1e-4203-8840-9b440ee9aea1'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13248, '60d563de-39e5-4163-89e6-86fa1da80be6'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13782, 'cc4cbb46-9352-4787-bd3e-b5aec0c09d4e'), (1004, '7bae84e8-1b63-41df-828f-817d61593d05'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The Exploration/Exploitation trade-off [[exp-exp-tradeoff]]

Finally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic: *the exploration/exploitation trade-off.*

- *Exploration* is exploring the environment by trying random actions in order to **find more information about the environment.**
- *Exploitation* is **exploiting known information to maximize the reward.**

Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, **we can fall into a common trap**.

Let’s take an example:

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/exp_1.jpg"" alt=""Exploration"" width=""100%"">

In this game, our mouse can have an **infinite amount of small cheese** (+1 each). But at the top of the maze, there is a gigantic sum of cheese (+1000).
### Q5: What is the exploration/exploitation tradeoff?

<details>
<summary>Solution</summary>

In Reinforcement Learning, we need to **balance how much we explore the environment and how much we exploit what we know about the environment**.

- *Exploration* is exploring the environment by **trying random actions in order to find more information about the environment**.

- *Exploitation* is **exploiting known information to maximize the reward**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/expexpltradeoff.jpg"" alt=""Exploration Exploitation Tradeoff"" width=""100%"">

</details>


### Q6: What is a policy?

<details>
<summary>Solution</summary>

- The Policy π **is the brain of our Agent**. It’s the function that tells us what action to take given the state we are in. So it defines the agent’s behavior at a given time.
For instance, an agent that does automated stock trading. For this task, there is no starting point and terminal state. **The agent keeps running until we decide to stop them.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/stock.jpg"" alt=""Stock Market""/>
</figure>

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/63_deep_rl_intro/tasks.jpg"" alt=""Tasks recap""/>
</figure>

## **Exploration/ Exploitation tradeoff**

Finally, before looking at the different methods to solve Reinforcement Learning problems, we must cover one more very important topic: *the exploration/exploitation trade-off.*

- Exploration is exploring the environment by trying random actions in order to **find more information about the environment.**

- Exploitation is **exploiting known information to maximize the reward.**


Remember, the goal of our RL agent is to maximize the expected cumulative reward. However, **we can fall into a common trap**.
### Q4: A task is an instance of a Reinforcement Learning problem. What are the two types of tasks?

<Question
	choices={[
		{
			text: ""Episodic"",
			explain: ""In Episodic task, we have a starting point and an ending point (a terminal state). This creates an episode: a list of States, Actions, Rewards, and new States. For instance, think about Super Mario Bros: an episode begin at the launch of a new Mario Level and ending when you’re killed or you reached the end of the level."",
      correct: true
		},
    {
			text: ""Recursive"",
			explain: """"
		},
    {
			text: ""Adversarial"",
			explain: """"
		},
    {
      text: ""Continuing"",
      explain: ""Continuing tasks are tasks that continue forever (no terminal state). In this case, the agent must learn how to choose the best actions and simultaneously interact with the environment."",
      correct: true
    }
	]}
/>

### Q5: What is the exploration/exploitation tradeoff?

<details>
<summary>Solution</summary>
- [What is Reinforcement Learning?](#what-is-reinforcement-learning)
  - [The big picture](#the-big-picture)
  - [A formal definition](#a-formal-definition)
- [The Reinforcement Learning Framework](#the-reinforcement-learning-framework)
  - [The RL Process](#the-rl-process)
  - [The reward hypothesis: the central idea of Reinforcement Learning](#the-reward-hypothesis-the-central-idea-of-reinforcement-learning)
  - [Markov Property](#markov-property)
  - [Observations/States Space](#observationsstates-space)
  - [Action Space](#action-space)
  - [Rewards and the discounting](#rewards-and-the-discounting)
  - [Type of tasks](#type-of-tasks)
- [Exploration/ Exploitation tradeoff](#exploration-exploitation-tradeoff)
- [The two main approaches for solving RL problems](#the-two-main-approaches-for-solving-rl-problems)
  - [The Policy π: the agent’s brain](#the-policy-π-the-agents-brain)
  - [Policy-Based Methods](#policy-based-methods)
  - [Value-based methods](#value-based-methods)
 
Here is the question: Why is the exploration-exploitation trade-off critical in reinforcement learning and how can it be mirrored in document retrieval?","The exploration-exploitation trade-off is critical in reinforcement learning because it determines how an agent balances between exploring the environment to gather more information and exploiting the current knowledge to maximize the reward. If an agent only explores, it may never exploit the knowledge to achieve the goal, and if it only exploits, it may miss out on better opportunities. This trade-off is crucial because it directly affects the agent's ability to maximize the expected cumulative reward.

In document retrieval, this trade-off can be mirrored in the"
"What are the benefits of using Hugging Face’s model card metadata for custom licensing, and how does it impact ethical AI use?","[(28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (28678, '8dfe9ddd-650b-425a-a742-d9a56022e06b'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (13323, 'bc764e36-2996-4283-877a-4101c659a974'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15889, 'aad2c393-9f11-40db-b577-03aba97e12a2'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (29212, 'a5e56809-c3c5-4caa-811d-eee83d06290d'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (28718, '20436458-7cd8-4a96-8331-99513560667b'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (29233, '74f4f91f-2db4-42a8-a508-33e42d02d65f'), (29235, '28f83b83-b564-4757-a1e4-3c0a127d21bd'), (28723, 'c23b045d-e37d-4922-ab87-54a26f4802b3'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (12348, '57a3c79a-af62-43be-bebc-4ef70c6c1495'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (12351, '3507bacd-33e0-47cf-a2a6-154bda1acd90'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (28741, '50733951-b5db-4e9b-83a4-b2911af5c989'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (14927, 'cd247d85-d1ee-4e3e-b3e5-9bc9d15c4563'), (25685, 'e7551698-0040-4251-b71c-50be7bc42a95'), (22124, 'e862ae05-e350-458c-893b-9b4a3d0b9f3f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (13447, 'b6ea664c-f662-4b91-8338-b97b37b1fb7e'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (30380, 'b4858a62-bfab-4374-b4e5-c287a54d33be'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (14509, '64200989-d1d6-4b32-8613-3c0ad1c88f4f'), (5819, 'e6d4ed34-ab03-4da4-851e-d6286e0a3718'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (5822, '95a2e3de-66e6-4f99-8e23-04e96dc3f30f'), (5823, '07fcf0fb-ec91-4401-a419-8ec49d41d591'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (5825, '02b1a8f8-63ab-44e9-a55e-de99cee8b6dc'), (5826, 'cf5b8dce-2e31-49f1-a43b-78b892eec7b6'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (5829, '107bddd6-ea08-4772-9aa2-14bf3335e980'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (10954, '92fe4159-8084-4e9c-9be2-4e72a5e11928'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (25805, 'fcaebe7b-c2ec-4995-829f-39de8f874270'), (8911, '1fed56b3-3b52-4a8a-9c2c-c33685c604d8'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (10962, '4b7ad4a9-4656-423f-be3d-0f52fec4902d'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (19683, '5d3bfb1d-f892-4db6-8596-bd364235244a'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (4451, 'b38578a8-7d2d-4289-ac44-956105d0ddd0'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (4458, '7a7a7f20-c1d0-4e6c-aff5-0291aa115cde'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (3985, 'ecedc47d-0616-441d-a202-07ae59c5cb7a'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (10655, 'b65f54ab-b114-433c-b9d8-c631d2973538'), (31136, '51caed7d-0da0-49a7-b6a2-8ee3113b8459'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (15310, '739331bc-e6f5-4e62-921e-98297df98a48'), (15312, '0f584622-999c-4d59-b800-a7909cd678cc'), (15313, 'c0939136-49c0-470b-b3cd-2559d7ce39dd'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25057, '85b645cd-7921-4981-9d5f-dcbd47710494'), (25058, 'f06d5733-7ce2-443d-85cb-88a01b88ca4f'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: - monitor our community discussion boards to ensure Hub users abide by the [code of conduct](https://huggingface.co/code-of-conduct),
- robustly document our most-downloaded models with model cards that detail social impacts, biases, and intended and out-of-scope use cases,
- create audience-guiding tags, such as the “Not For All Audiences” tag that can be added to the repository’s card metadata to avoid un-requested violent and sexual content,
- promote use of [Open Responsible AI Licenses (RAIL)](https://huggingface.co/blog/open_rail) for [models](https://www.licenses.ai/blog/2022/8/26/bigscience-open-rail-m-license), such as with LLMs ([BLOOM](https://huggingface.co/spaces/bigscience/license), [BigCode](https://huggingface.co/spaces/bigcode/license)),
- conduct research that [analyzes](https://arxiv.org/abs/2302.04844) which models and datasets have the highest potential for, or track record of, misuse and malicious use.
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
 
Here is the question: What are the benefits of using Hugging Face’s model card metadata for custom licensing, and how does it impact ethical AI use?","The benefits of using Hugging Face's model card metadata for custom licensing include:

* Enabling users to quickly know which license applies to which model
* Allowing users to list models with a specific license
* Providing valuable information for discoverability, reproducibility, and sharing of models
* Facilitating ethical AI use by making users aware of the intended and out-of-scope use cases, social impacts, and biases of the models
* Promoting responsible AI development by supporting Open Responsible AI"
How does Ray’s actor-based retrieval implementation improve multi-GPU fine-tuning for RAG models?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (4099, '3e3d1381-fe09-44ce-9c54-af79887a663e'), (1043, '4bfd4adf-5240-4e6d-bc99-abe93841574a'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (12316, 'b5c70710-fff2-4908-b500-962f6636a8b7'), (24604, '0385f67b-6e32-4932-a53b-9211bd2aa9ed'), (1055, 'ae39322b-7a82-4d70-9c6f-f6a19796ae38'), (27684, 'f2ce3abb-ae36-4782-abdf-2e04a9cbd843'), (9256, '7f549d96-c927-4084-9071-7979bb44293e'), (14386, 'b8cd4a6c-621a-48e7-a398-c9e62eee673e'), (19526, '790c4385-8da2-4412-bf50-64e4a5ffadd3'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19027, 'be488c2c-174e-4cf1-a688-da4b2d1a8ed3'), (4087, 'c7c5df1c-8f69-4329-8e63-1127804c1e96'), (19030, '888cc284-de1f-4aee-9148-8e968bcdbdbe'), (3158, '1eb57504-ed08-48bb-8092-c53d5c64b0c1'), (23642, '683e3a08-8ea3-4d72-a020-1876a44a65ed'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (16479, 'f8325aa8-886a-43ff-9c7c-96bb8c00df22'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (21090, '372a1764-bf39-49ae-833e-202715881ca0'), (13923, '017f79ef-c6e4-436d-9120-ff659a69789e'), (16486, '44b89854-89ae-4b45-9b98-62b3d9edc5a7'), (4092, '49e8126f-6930-48ec-abbd-632bcb85a3df'), (19568, '6b7c6d4d-3fa2-445d-89f4-171fb9e18a91'), (26229, '81993a76-b643-4458-bf4c-17b0dbd5017a'), (30327, 'a62abcc1-500d-4f5e-a758-64fcef6ee3e7'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (6269, '5172b166-e282-4ebc-8d62-8a41b553d543'), (651, '4ce0ecff-c937-460a-8aa7-a7b83baea662'), (24716, 'b4b38413-830c-44c2-913b-5c20d09a950e'), (24717, '5e3e3042-4084-4539-81ed-8149713aee3a'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (18062, '6bf329e4-a764-400f-8a3e-d39885c58f97'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (6356, 'fd9611a0-cae2-4941-8609-f6440c2b0285'), (25317, 'e251888b-7855-4192-8a77-60936c25167a'), (26855, 'daaa5d9b-d90e-4dcb-b7be-640944232958'), (25847, '3cb97faf-b830-4980-af57-a83fc68e1002'), (25851, '2ad08d82-5f5e-43ff-81e5-837f3ec89586'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (28936, '4362b172-8006-4ac0-b554-286c6130339b'), (28941, 'e040c6f3-5307-4e26-83e1-58cae47ef598'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (15632, 'b1d97829-ea96-4637-afb8-ccdb5a91cb2a'), (2833, 'f44aed77-24dc-4eb3-8b3d-a3c389b6e776'), (2836, 'dad2f3b4-337f-49e3-ae8a-2714f6bb79e0'), (16661, 'f6db86f2-9435-45f8-814c-5423a566c4e4'), (16660, '1dfbc513-b29b-4f26-ac16-6bccdb48a3d4'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (2838, 'bf581280-a9b8-476e-9359-bb349445e1a0'), (21274, '6dc1af30-a439-4f39-baba-fb652d70f065'), (31008, '3f401b96-9ff0-4f59-b937-0dead68da5d8'), (30505, '9e92a10f-02d1-4093-aa4b-1504134b9dcc'), (21805, 'b66d8d5b-534a-49b7-9bdc-0473323226b3'), (11056, '4481763c-f883-4267-891f-58ed35d27b0a'), (12080, '9dbbfe60-bc76-41a7-abd6-f3511f64fb33'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (21824, '023e0af2-eda1-419e-bbe4-01b652ba194d'), (31556, '4e8903fe-1f83-478a-8981-808e95896798'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (31571, '7275d35c-58b3-4440-af40-fca872c364ee'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (14706, '332b6a8d-ec91-483f-a989-0ec3c2d81a1c'), (14707, '653a154c-0baf-4e34-98d6-d2362d781d53'), (4469, '0b4ca93b-c818-4499-9a67-b7a1d33c3d55'), (1398, '509f9c15-2115-49e4-8ec4-e9e5176b6456'), (3959, '61a64b74-697b-418d-80db-6b0849c828bc'), (3960, '0dfeb014-8d4d-440c-b63e-11bf3ff8d142'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (5509, '509fc88e-4d96-444c-8ea2-74d4ce6ee3d0'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (19850, '9ca3b142-5c97-4269-a829-4796ef40366c'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (7065, '0fb31b29-d4f9-4207-9934-c683259bbb1a'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (24475, '5e552409-3f77-4e0d-a49e-ad650002587b'), (24476, '7416abe4-8708-4b38-a154-27508d996c92'), (24992, 'bfa84492-3461-4970-8f97-05e24c1e9366'), (9637, '6425d1e0-38eb-4c90-84cc-9e0ef001b34c'), (7590, '02537f3a-f9e0-46f4-ade9-6873995059b0'), (18855, 'c347445a-0f7a-4f79-8f05-fce5efd6f4a9'), (9639, '2d3203bc-7325-4666-8c43-4a7fcc89fab6'), (7601, '3ecbf4ed-0cd3-483b-88d7-128fe443adff'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (10696, 'd7e8042d-bfce-457e-a6e7-9a1041ca49c7'), (10707, '2645868a-3cda-4c75-a76a-16a152aa6f9d'), (13283, 'c14952cc-5771-4901-89f7-380171a5bc51'), (23524, 'dca5f25a-96f8-46c6-ad1d-c38fc49ba220'), (8686, '1c71d11c-78a5-45b0-83cb-0eb1e90a9142'), (11246, '41cafe12-ee25-4e13-afeb-fc0262be3c12'), (4082, '10f7efed-84cd-4b79-b547-e873c0c51980'), (4083, '02669321-da3c-44a6-9d89-341de24260e1'), (4084, '71027490-70a0-4566-bccc-6da26ddfc0d0'), (4086, '7bab29e2-3cac-418d-8b9d-f0edbb43b52e'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (14844, '4bd36fd8-e2e4-49a1-b5ca-f9683414fc32'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (4094, '49b52383-54c6-4a7e-82d6-8c5bb7a3db24'), (4095, 'e0c3cf71-afe2-41b2-b157-adf20c106a3e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: And as you can see below, using the [Ray](https://docs.ray.io/en/master/) based implementation leads to better retrieval performance for multi-GPU fine-tuning. The following results show the seconds per retrieval call and we can see that as we increase the number of GPUs that we train on, using Ray has comparatively better performance than `torch.distributed`. Also, if we increase the number of Ray processes that perform retrieval, we also get better performance with more training workers since a single retrieval process is no longer a bottleneck.
_A performance comparison of different retrieval implementations. For each document retrieval implementation, we run 500 training steps with a per-GPU batch size of 8, and measure the time it takes to retrieve the contextual documents for each batch on the rank 0 training worker. As the results show, using multiple retrieval processes improves performance, especially as we scale training to multiple GPUs._


### How do I use it?

[Huggingface](https://huggingface.co/) provides a [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning) based [fine tuning script](https://github.com/huggingface/transformers/tree/master/examples/research_projects/rag), and we extended it to add the Ray retrieval implementation as an option. 

To try it out, first install the necessary requirements


```bash
pip install ray
pip install transformers
pip install -r transformers/examples/research_projects/rag/requirements.txt
```

Using Ray can lead to retrieval speedups on multi-GPU settings since multiple processes load the index rather than
just the rank 0 training worker. Using Ray also allows you to load the index on GPU since the index is loaded on a separate
processes than the model, while with pytorch distributed retrieval, both are loaded in the same process potentially leading to GPU OOM.

# Evaluation
Our evaluation script enables two modes of evaluation (controlled by the `eval_mode` argument): `e2e` - end2end evaluation, returns EM (exact match) and F1 scores calculated for the downstream task and `retrieval` - which returns precision@k of the documents retrieved for provided inputs.
For the Ray implementation, the index is loaded in *separate* process(es). The training workers randomly select which
retriever worker to query. To use Ray for distributed retrieval, you have to set the `--distributed_retriever` arg to `ray`.
To configure the number of retrieval workers (the number of processes that load the index), you can set the `num_retrieval_workers` flag.
Also make sure to start the Ray cluster before running fine-tuning.

```bash
# Start a single-node Ray cluster.
ray start --head

python examples/research_projects/rag/finetune_rag.py \
    --data_dir $DATA_DIR \
    --output_dir $OUTPUT_DIR \
    --model_name_or_path $MODEL_NAME_OR_PATH \
    --model_type rag_sequence \
    --fp16 \
    --gpus 8
    --distributed_retriever ray \
    --num_retrieval_workers 4

# Stop the ray cluster once fine-tuning has finished.
ray stop
![alt_text](assets/12_ray_rag/ray_arch_updated.png ""image_tooltip"")
_Document retrieval with the Ray implementation_

To overcome these limitations, we introduced a novel implementation of distributed retrieval based on Ray. With [Ray’s stateful actor abstractions](https://docs.ray.io/en/master/actors.html), multiple processes that are separate from the training processes are used to load the index and handle the retrieval queries. With multiple Ray actors, retrieval is no longer a bottleneck and PyTorch is no longer a requirement for RAG.
 
Here is the question: How does Ray’s actor-based retrieval implementation improve multi-GPU fine-tuning for RAG models?","Ray's actor-based retrieval implementation improves multi-GPU fine-tuning for RAG models by allowing multiple processes to load the index and handle retrieval queries, making retrieval no longer a bottleneck. This leads to better performance and retrieval speedups on multi-GPU settings, as multiple processes load the index rather than just the rank 0 training worker. Additionally, using Ray allows the index to be loaded on a separate process than the model, which can help prevent GPU OOM issues."
"What challenge does the Bellman equation address in value-based reinforcement learning, and how might it inform efficient retrieval systems?","[(29213, 'ecfbdc64-5f35-4f83-8641-32ac46207dca'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14886, 'c48286c5-a1b4-43f0-ab50-f7520594909d'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (6214, '6ad7dd43-0e4a-4b34-b1b5-58c9470616f5'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (25688, 'be63462d-16cc-4d1e-bfbe-04d1d5b8bfdc'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21085, '649b8e4e-076b-4c60-9ec5-671461339d75'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (30812, 'f7e1e127-2236-4b1a-b0ad-74e1af99c668'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (13926, 'c6436b34-5943-4b25-b196-20d9695a4c1d'), (11367, '0901d3e0-3cfb-4006-a9bb-28b4858439df'), (11366, '35fec41b-9448-4816-baa7-d2367118aa6a'), (11365, 'fe8da28b-5e19-460e-be89-ccd9c18b8b30'), (11368, '52ff8f67-0eb6-4c08-92f9-2f889d14111b'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17025, '69f0e2e7-b149-4523-a038-d069193eeec2'), (17040, '3c81255d-fba2-425b-add0-66180682f513'), (14486, '1d0b077e-bd05-4bb6-9239-cdebc73e9813'), (21660, '107b725e-6c0d-4c75-84b6-b69fcdc6801f'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (165, '41c16f94-d1b6-4437-9b63-6a249cafa0d0'), (166, 'a7fb2601-9706-4665-a80b-8da658e759f7'), (167, 'bed696f3-f53a-41f2-8793-1fe84ebe7310'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (21183, '74d1c74c-7b3d-457c-b983-ec23b3268af3'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (20179, '43cd6317-d563-4694-aa5f-8715ac76a2cc'), (20180, 'efd0987b-da48-4366-80fb-c51276b30e44'), (20181, 'c3d5e236-999f-4476-b4ae-60717e645952'), (20182, 'ba1d7c23-8372-4dcc-879a-a18edc0a90a6'), (20183, '0bc8e3d4-5308-48a5-a6f1-343318328e56'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (22745, '178ef776-3f46-4d08-bfbe-38fccf619c70'), (20700, '6605a73c-2f2f-4887-b5de-2ee59adf388f'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23294, '1936ebac-0df8-44c6-a638-cb0f634059de'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (25372, 'fd50adcd-2129-477d-9810-19f85dbeb0bf'), (22819, 'a415afd3-d643-4e9a-bb7a-478941d50298'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (23339, 'acc2c624-ff31-433f-be5f-f1000a8f48dc'), (1335, '268a1193-d635-4530-92e3-3146245d10a0'), (1336, '329c759f-4fc3-49ae-89bd-b24d835a593b'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20806, 'b65455ed-1e56-4734-8384-734771a4e1ef'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19300, 'ca9fcfe5-7cb5-4873-a53a-d887c9dfa786'), (16743, '28949d3c-ff9f-4e2c-8682-daa6d669b12c'), (16746, '45080cea-724d-4d98-8278-7cd29fd71f9b'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (19322, 'c1e988d6-c6e3-4309-bb5f-4485f9208a8c'), (19324, 'd4d62dab-e6e1-4363-b79e-7811c007af5e'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (6021, '0ca2cfe9-2e23-47a0-bcea-198a0f6fdb49'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (23958, '82ffb2ac-f889-482f-bd2b-090b36dbcce1'), (23959, '46501f70-bf28-440b-ba81-9c88be735e91'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (23960, '85cca8b8-d29e-4219-a5ff-f837b064f343'), (23961, '13b18911-853f-4362-aad9-9563a8885e44'), (23962, '35661ef2-1497-4bbd-af50-38276811d7ce'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (925, '2b19d79e-1c48-4d05-a48d-19f5138ae7c1'), (23454, 'bb59e1c0-8193-40ce-a450-9efc2ecc6226'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (19890, 'd46d29d5-6d55-4256-9733-4c1c219f11a5'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13764, 'c2619e99-58b4-4943-a672-c368132c540e'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (13769, '3cdf785f-6262-45f4-8c6b-090d22a81c7e'), (29128, '0b14fe0a-b680-47d7-b8c8-60b485d1fd90'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (23506, '526dbd3b-8d49-4c69-b33f-3da88e48f7fa'), (13777, '679244ce-c5e4-4649-ade1-f8d91fa4825c'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13784, 'df3f2f2b-7b61-43c5-9f8b-7c20249b29e4'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (23535, '0de47e64-f26d-4a3b-bb54-a6517f3e5e5b'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Mid-way Quiz [[mid-way-quiz]]

The best way to learn and [to avoid the illusion of competence](https://www.coursera.org/lecture/learning-how-to-learn/illusions-of-competence-BuFzf) **is to test yourself.** This will help you to find **where you need to reinforce your knowledge**.


### Q1: What are the two main approaches to find optimal policy?


<Question
	choices={[
		{
			text: ""Policy-based methods"",
			explain: ""With Policy-Based methods, we train the policy directly to learn which action to take given a state."",
      correct: true
		},
		{
			text: ""Random-based methods"",
			explain: """"
		},
    {
			text: ""Value-based methods"",
			explain: ""With value-based methods, we train a value function to learn which state is more valuable and use this value function to take the action that leads to it."",
      correct: true
		},
		{
			text: ""Evolution-strategies methods"",
      explain: """"
		}
	]}
/>


### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>
- The value of  \\(V(S_{t+1}) \\)  = Immediate reward  \\(R_{t+2}\\)  + Discounted value of the next state ( \\(gamma * V(S_{t+2})\\) ).
- And so on.

To recap, the idea of the Bellman equation is that instead of calculating each value as the sum of the expected return, **which is a long process.** This is equivalent **to the sum of immediate reward + the discounted value of the state that follows.**

## **Monte Carlo vs Temporal Difference Learning**

The last thing we need to talk about before diving into Q-Learning is the two ways of learning.

Remember that an RL agent **learns by interacting with its environment.** The idea is that **using the experience taken**, given the reward it gets, will **update its value or policy.**

Monte Carlo and Temporal Difference Learning are two different **strategies on how to train our value function or our policy function.** Both of them **use experience to solve the RL problem.**
RLHF

Reinforcement learning from human feedback (RLHF) is a **methodology for integrating human data labels into a RL-based optimization process**.
It is motivated by the **challenge of modeling human preferences**.

For many questions, even if you could try and write down an equation for one ideal, humans differ on their preferences.

Updating models **based on measured data is an avenue to try and alleviate these inherently human ML problems**.

## Start Learning about RLHF

To start learning about RLHF:

1. Read this introduction: [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf).
So let's get started!

- [What is RL? A short recap](#what-is-rl-a-short-recap)
- [The two types of value-based methods](#the-two-types-of-value-based-methods)
  - [The State-Value function](#the-state-value-function)
  - [The Action-Value function](#the-action-value-function)
- [The Bellman Equation: simplify our value estimation](#the-bellman-equation-simplify-our-value-estimation)
- [Monte Carlo vs Temporal Difference Learning](#monte-carlo-vs-temporal-difference-learning)
  - [Monte Carlo: learning at the end of the episode](#monte-carlo-learning-at-the-end-of-the-episode)
  - [Temporal Difference Learning: learning at each step](#temporal-difference-learning-learning-at-each-step)


## **What is RL? A short recap**
### Q2: What is the Bellman Equation?

<details>
<summary>Solution</summary>

**The Bellman equation is a recursive equation** that works like this: instead of starting for each state from the beginning and calculating the return, we can consider the value of any state as:

Rt+1 + gamma * V(St+1)

The immediate reward + the discounted value of the state that follows

</details>

### Q3: Define each part of the Bellman Equation

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4-quiz.jpg"" alt=""Bellman equation quiz""/>


<details>
<summary>Solution</summary>

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/bellman4.jpg"" alt=""Bellman equation solution""/>

</details>

### Q4: What is the difference between Monte Carlo and Temporal Difference learning methods?
 
Here is the question: What challenge does the Bellman equation address in value-based reinforcement learning, and how might it inform efficient retrieval systems?","The Bellman equation addresses the challenge of calculating the value of a state as the sum of the expected return, which is a long process. It simplifies this process by considering the value of any state as the immediate reward plus the discounted value of the state that follows. This equation informs efficient retrieval systems by providing a recursive way to estimate the value of a state, making it more efficient to compute."
Why is multi-language support in the Hugging Face ecosystem critical for global AI adoption?,"[(10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (13317, 'a25eed4c-1cf8-4a1e-8a9a-ff9d814848c5'), (7687, '20608e74-1307-46cf-9341-09115b2bb0ac'), (19976, 'ec852d21-1da0-46da-8293-267f04f04242'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (19977, 'da27be80-5381-43b5-9542-ef033eb07df9'), (19980, '3aa3ba7e-8013-492f-b83a-91c341becb4f'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (8211, '4abed52a-3bfe-4342-8128-9fea9c7b4168'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (2595, '9eaa8955-15cc-419f-b2d9-c1f7c36014c3'), (2597, '6458f359-e453-452e-93f0-0a72efc03c98'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (86, '5967b116-bb1a-42cf-bce6-7aca5c8b0c13'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (21624, 'cf165b8c-348b-4ccc-acd1-944ae9c89877'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (26751, '12ba626a-f314-428d-aed1-17718233020a'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (25221, '70c1fbe0-61f3-4c3d-bf1d-414c436ff799'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26754, '167102ca-f572-4d47-aa9b-041aeda233f2'), (8836, 'c289aa2c-ea0e-4e05-b457-9786a0c345e3'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (8848, '4e28a35e-7050-4a59-9b09-67e43b51931f'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (11452, '7e2fd91b-ca25-40e4-9aea-47ac22782115'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (1298, '9e567bae-970e-4b21-b51e-901df35b66c9'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (20777, '6b031a1e-1a0f-416a-865c-e7d8c0560fbf'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (8494, 'ad2e3627-a124-4b42-b8e4-304f7789c8a6'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (16716, 'd9f215fb-7aa2-4b50-a40d-d2e690dc5d03'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (11611, '64b569bf-2c94-4fba-b49d-e573da5bfbf4'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (19808, '5213e06f-bf0a-45be-86be-92a7b345964b'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (31136, '51caed7d-0da0-49a7-b6a2-8ee3113b8459'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (11173, '56c58b4c-4454-4f90-88a8-4cac18860f42'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (29603, 'e6eee1c6-18dd-4dbf-b041-ce7d0504ed72'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (10198, '19eb7655-1d17-42bc-985e-b1a424722b72'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (24041, '11dbf2a7-e5af-4779-bf28-c30fa1c3bca9'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
We have also moved forward with our goals of *fairness* and *justice* with [bias and harm testing](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#bias-risks-and-limitations), recently applied to the new Hugging Face multimodal model [IDEFICS](https://huggingface.co/HuggingFaceM4/idefics-80b-instruct). We've worked on how to operationalize *transparency* responsibly, including [updating our Content Policy](https://huggingface.co/blog/content-guidelines-update) (spearheaded by [Giada](https://huggingface.co/giadap)). We've advanced our support of language *diversity* on the Hub by [using machine learning to improve metadata](https://huggingface.co/blog/huggy-lingo) (spearheaded by [Daniel](https://huggingface.co/davanstrien)), and our support of *rigour* in AI by [adding more descriptive statistics to datasets](https://twitter.com/polinaeterna/status/1707447966355563000) (spearheaded by [Polina](https://huggingface.co/polinaeterna)) to foster a better understanding of what AI
When it comes to respecting people’s privacy rights, the recent developments in ML and AI pose new questions, and engender new challenges.
We have been particularly sensitive to these challenges in our own work at Hugging Face and in our collaborations.
The [BigScience Workshop](https://huggingface.co/bigscience) that we hosted in collaboration with hundreds of researchers from many different countries and institutions
was the first Large Language Model training effort to [visibly put privacy front and center](https://linc.cnil.fr/fr/bigscience-il-faut-promouvoir-linnovation-ouverte-et-bienveillante-pour-mettre-le-respect-de-la-vie),
through a multi-pronged approach covering [data selection and governance, data processing, and model sharing](https://montrealethics.ai/category/columns/social-context-in-llm-research/).
## Optimising Transformers for Production

Transformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.

Already, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.

With a user base of more than 50,000 developers – Hugging Face has seen the fastest ever adoption of an open-source project.

Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.
_“With the new Hugging Face Hub model catalog, natively integrated within Azure Machine Learning, we are opening a new page in our partnership with Microsoft, offering a super easy way for enterprise customers to deploy Hugging Face models for real-time inference, all within their secure Azure environment.”_ said Julien Simon, Chief Evangelist at Hugging Face. 

_""The integration of Hugging Face's open-source models into Azure Machine Learning represents our commitment to empowering developers with industry-leading AI tools,""_ said John Montgomery, Corporate Vice President, Azure AI Platform at Microsoft. _""This collaboration not only simplifies the deployment process of large language models but also provides a secure and scalable environment for real-time inferencing. It's an exciting milestone in our mission to accelerate AI initiatives and bring innovative solutions to the market swiftly and securely, backed by the power of Azure infrastructure.""_
 
Here is the question: Why is multi-language support in the Hugging Face ecosystem critical for global AI adoption?","The context does not explicitly state why multi-language support in the Hugging Face ecosystem is critical for global AI adoption. However, it can be inferred that supporting language diversity on the Hub is important for fostering a better understanding of AI and making it more accessible to a broader range of users. The mention of ""language diversity"" as one of the goals of the Hugging Face ecosystem, along with the development of models like CamemBERT, which is a French-language model, suggests that supporting multiple languages is"
"What distinguishes policy-based methods from value-based methods in reinforcement learning, and how might this distinction influence RAG models?","[(24580, '9e0b974c-4de1-4c4e-9ad2-c81dd70f9d85'), (24581, '22244e17-6683-4b12-bf73-b4e4b5377b96'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (14889, 'b9d0d071-6d22-4862-96c6-ee1e72121216'), (12850, 'c6e54b94-1bc1-4c66-8959-7c3a3451782c'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (21596, '1917b3ad-29fb-45ec-89ed-8b3e7d5f7a84'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (21598, 'e113a596-30cc-441f-ac19-90b0009cc7f4'), (21600, '9af1cc4c-21f2-4e01-81b1-d37162920ffb'), (21603, '10e50aef-45eb-4a81-a628-13cb0b3d5243'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (21606, '136ec334-4b38-4957-a9a2-8b75cf91a5d1'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (14487, '4f24c302-152f-4c11-b198-4be00041e6a0'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25274, '0a54e74a-9c72-4f9a-b0f0-c8420921e175'), (25275, 'b3a67dba-27dd-4a09-ac3b-ffa416c9e289'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21210, 'a7d914db-0a82-4bef-889e-b11938bed886'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (23291, '34457bff-e882-46a1-8e35-2b91df43fb9b'), (23293, '1dfce32b-2320-4506-b5a1-e1e3e6de13a1'), (23294, '1936ebac-0df8-44c6-a638-cb0f634059de'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (29462, '1b737c13-1026-4bc9-a614-97c35ce15172'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11047, '939d2f75-31f8-4c36-92da-3b8dadf6c83a'), (28986, '7cea0709-eede-4ab3-93db-b22b18aeb6c2'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19298, '0db54d2d-7e43-465b-9abc-d60309dd5d0f'), (19299, '58e3889c-4edb-4806-9064-e951799e80cb'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19317, 'cab6254e-890a-47c4-a68b-1b64258cc023'), (19320, '35fcd7cf-be27-4756-9ebf-8b8285842ada'), (19321, '752a82eb-df17-46b7-9dc5-493c10d5edcd'), (29052, '9640f3e5-2f8a-49f2-8aa2-dae764c972b8'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (19325, 'fb7c67ab-594a-487c-ae87-f94d0dbe7c65'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23952, 'ec7797a6-1617-49dc-9737-a162e926580a'), (23953, '1db7b78a-9c7e-4ff1-a4eb-1936e0b81322'), (23954, '2cb33458-c66f-41b8-9ada-26fdc3474ece'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (31630, 'c3f4a34c-3e4b-43cf-b8e1-a0ee9657fb49'), (23956, '9dec7d5b-756e-4a6c-ac15-1b544069a789'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (921, 'b995f04e-665f-48f7-9283-e8ffa0507cd0'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (923, '3f1f99cb-3e95-4732-9582-c75dfc21e2bb'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23973, '7463664a-733b-49e6-bc75-8f035b79e78f'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (961, 'f8df8306-bd61-414c-92fb-2f5f3e706930'), (962, 'c68008e9-dc91-4db0-a2f0-89a793040fff'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (964, '65fb34e4-bfe0-4990-b85f-bf4e0f542e42'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (23505, 'ee24bc5f-8758-4ba1-85c6-eb0710c09d74'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: What are the policy-based methods?

The main goal of Reinforcement learning is to **find the optimal policy \\(\pi^{*}\\) that will maximize the expected cumulative reward**.
Because Reinforcement Learning is based on the *reward hypothesis*: **all goals can be described as the maximization of the expected cumulative reward.**

For instance, in a soccer game (where you're going to train the agents in two units), the goal is to win the game. We can describe this goal in reinforcement learning as
**maximizing the number of goals scored** (when the ball crosses the goal line) into your opponent's soccer goals. And **minimizing the number of goals in your soccer goals**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/soccer.jpg"" alt=""Soccer"" />

## Value-based, Policy-based, and Actor-critic methods

In the first unit, we saw two methods to find (or, most of the time, approximate) this optimal policy \\(\pi^{*}\\).
- [The Policy π: the agent’s brain](#the-policy-π-the-agents-brain)
  - [Policy-Based Methods](#policy-based-methods)
  - [Value-based methods](#value-based-methods)
- [The “Deep” in Reinforcement Learning](#the-deep-in-reinforcement-learning)
## What are Policy-Gradient Methods?
Policy-Gradient is a subclass of Policy-Based Methods, a category of algorithms that **aims to optimize the policy directly without using a value function using different techniques.** The difference with Policy-Based Methods is that Policy-Gradient methods are a series of algorithms that aim to optimize the policy directly **by estimating the weights of the optimal policy using Gradient Ascent.**

### An Overview of Policy Gradients
Why do we optimize the policy directly by estimating the weights of an optimal policy using Gradient Ascent in Policy Gradients Methods?

Remember that reinforcement learning aims **to find an optimal behavior strategy (policy) to maximize its expected cumulative reward.**

We also need to remember that a policy is a function that **given a state, outputs, a distribution over actions** (in our case using a stochastic policy).
There are **two** ways to find your optimal policy:

- By **training your policy directly**: policy-based methods.
- By **training a value function** that tells us the expected return the agent will get at each state and use this function to define our policy: value-based methods.

- Finally, we spoke about Deep RL because **we introduce deep neural networks to estimate the action to take (policy-based) or to estimate the value of a state (value-based) hence the name ""deep.""**

# Let's train our first Deep Reinforcement Learning agent and upload it to the Hub 🚀

## Get a certificate 🎓

To validate this hands-on for the [certification process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process), you need to push your trained model to the Hub and **get a result of >= 200**.
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
 
Here is the question: What distinguishes policy-based methods from value-based methods in reinforcement learning, and how might this distinction influence RAG models?","According to the context, policy-based methods directly learn to approximate the optimal policy without learning a value function, whereas value-based methods estimate a value function as an intermediate step towards finding an optimal policy. This distinction means that policy-based methods output a probability distribution over actions, whereas value-based methods use the learned value function to define the policy. This difference in approach might influence RAG (Reinforcement Learning Agent) models by affecting how they learn to make decisions and interact with their environment."
How do Gradio's setup and local development processes support scalable AI workflows?,"[(8203, '6c0cf928-5dfb-4006-ac6d-7ab27cd5875a'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (3102, '6c7f3c2e-5b16-4ca0-8363-ead695f3bae8'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (28246, '4a3bf591-3b47-4caa-bd34-c5f1bc064e2f'), (28248, '14667f8d-196e-48a1-9b90-85075c677427'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (11357, 'a2d06311-cad3-4127-8cc1-1b7e15255d0f'), (11358, 'd8407ad8-7a13-4c9a-901c-6a4d4502993d'), (28255, 'bcb424b2-729f-4991-87ac-45ef5ce77853'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (11360, '5df29451-6693-4cac-9c11-3d3838496bfc'), (28257, 'cfc6625e-3cab-4f72-8584-37a13847b65a'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (27756, '8f9de8cd-281f-441a-b692-3d1b132a6728'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (6259, 'e1dad236-a338-4128-821c-38c369e6a47e'), (6261, 'd0ed0a3b-3fb6-4a46-9e26-d04afabaf82f'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (6791, 'caea27fd-5810-4f6f-afa1-5befc362cced'), (4238, '2e5da79f-70d6-43e2-93ea-032c4d4eae1e'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (17068, 'e8d4188d-34a3-4d81-96d0-85488e9eb25a'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (9403, 'f02196ba-b6c3-4f63-8626-a0d8278fce15'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (14042, '6d5f441d-7e53-469a-af95-3fdc35f8fa6b'), (14043, '969903ce-08d6-4aa6-ae88-ddf707c7c323'), (6876, '83d18f4a-a9ff-488a-a396-55120e57e9f4'), (14044, 'd92bd84e-b4e9-4201-9419-e9da2be5cde5'), (17119, '3362d998-e1dd-4fe9-838e-5136a991278a'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (27897, '00ee70f5-c8bc-45d4-9a8b-c379cafc9c97'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (19214, '2d9ea80a-98e3-4bca-91ce-fe9e72bdb0ca'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (18718, '22d2a501-6bbd-4f0f-ae74-291e43ad8509'), (4894, 'd603d064-88fb-46b9-a244-2a6cd2c44fd8'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (6953, '5b5e8c9e-10d8-4ea2-930a-08ac3425c937'), (6954, 'dc0a4a82-5286-4d59-ac85-fe642c49151e'), (30507, '71f21d73-1d7a-46ab-8ace-0c85a938a6c6'), (3382, 'f57e911f-3558-4cec-a7c5-dc8c73ec91ef'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (2372, '03aacefb-6869-419b-80ba-b72212d966d5'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (2374, '3c2fc425-71a9-4574-8e7e-9ea6de5e161a'), (2381, '25022ddc-db5e-48bf-a2bf-918a00646b76'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (2383, 'c6ad6da4-4a77-4db0-b926-6c1b77b6db81'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (15700, '126d851b-8f50-401a-affc-d7dfc8c561ac'), (15705, 'fc5a9db9-4e60-4317-8a58-015ff53348be'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (16762, '0a98d923-bf03-4726-a7c3-4f2c56457e83'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (5524, 'bb1d7e3e-0981-435e-af37-33e2588f6b6b'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14247, '99b38e8f-68a6-4442-a93a-76803370e58f'), (14249, '3bde59f7-ba94-4ec5-8768-6195057f7e50'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (13746, '0bb29eea-90a5-4fd6-8177-0ccbbf32e132'), (11188, '4db7e165-b873-4551-a375-e397ff14275b'), (11192, 'a6746ae6-12c0-4e20-a9c7-16c2d49dc376'), (23483, '48efe26f-2d36-4283-91e0-1e19ee13177c'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (23997, 'f5eb3def-edf7-4dfa-b630-80e186c92aed'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (24000, 'acaf5458-ee13-44f6-8d46-53aa40c214ad'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (9177, 'f2603eb7-ed07-4331-8252-28f988a21715'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (29668, '5c89bc22-3e14-4df7-a592-2cbebc2f87e9'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (2023, 'd20a455f-f235-41b7-a1c5-5b15b10c5065'), (29674, '81aa5082-4e7c-42b6-8684-491f31bb23aa'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: gradio-ui

This folder contains all of the Gradio UI and component source code.

- [set up](#setup)
- [running the application](#running-the-application)
- [local development](#local-development)
- [building for production](#building-for-production)
- [quality checks](#quality-checks)
- [ci checks](#ci-checks)

## setup

This folder is managed as 'monorepo' a multi-package repository which make dependency management very simple. In order to do this we use `pnpm` as our package manager.

Make sure [`pnpm`](https://pnpm.io/) is installed by [following the installation instructions for your system](https://pnpm.io/installation).

You will also need `node` which you probably already have

## running the application

Install all dependencies:

```bash
pnpm i
## The Workflow

The Custom Components workflow consists of 4 steps: create, dev, build, and publish.

1. create: creates a template for you to start developing a custom component.
2. dev: launches a development server with a sample app & hot reloading allowing you to easily develop your custom component
3. build: builds a python package containing to your custom component's Python and JavaScript code -- this makes things official!
4. publish: uploads your package to [PyPi](https://pypi.org/) and/or a sample app to [HuggingFace Spaces](https://hf.co/spaces).

Each of these steps is done via the Custom Component CLI. You can invoke it with `gradio cc` or `gradio component`

Tip: Run `gradio cc --help` to get a help menu of all available commands. You can also append `--help` to any command name to bring up a help page for that command, e.g. `gradio cc create --help`.

## 1. create

Bootstrap a new template by running the following in any working directory:
Custom Components in 5 minutes

Gradio 4.0 introduces Custom Components -- the ability for developers to create their own custom components and use them in Gradio apps.
You can publish your components as Python packages so that other users can use them as well.
Users will be able to use all of Gradio's existing functions, such as `gr.Blocks`, `gr.Interface`, API usage, themes, etc. with Custom Components.
This guide will cover how to get started making custom components.

## Installation

You will need to have:

* Python 3.8+ (<a href=""https://www.python.org/downloads/"" target=""_blank"">install here</a>)
* Node.js v16.14+ (<a href=""https://nodejs.dev/en/download/package-manager/"" target=""_blank"">install here</a>)
* npm 9+ (<a href=""https://docs.npmjs.com/downloading-and-installing-node-js-and-npm/"" target=""_blank"">install here</a>)
* Gradio 4.0+ (`pip install --upgrade gradio`)

## The Workflow

The Custom Components workflow consists of 4 steps: create, dev, build, and publish.
### 4. Gradio is designed primarily for NLP models

<Question
	choices={[
        {
			text: ""True"",
			explain: ""Gradio works with pretty much any data type, not just NLP.""
        },
        {
			text: ""False"",
			explain: ""Gradio supplies developers with a library of pre-built components for pretty much all data types."",
            correct: true
        }
	]}
/>

### 5. Which of the following features are supported by Gradio?
Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

> [!TIP]
 > When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=""https://www.gradio.app/guides/developing-faster-with-reload-mode"">Hot Reloading Guide</a>.


**Understanding the `Interface` Class**

You'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs. 

The `Interface` class has three core arguments:
 
Here is the question: How do Gradio's setup and local development processes support scalable AI workflows?","Gradio's setup and local development processes support scalable AI workflows through its monorepo structure, which simplifies dependency management using `pnpm`. The Custom Components workflow, consisting of create, dev, build, and publish steps, enables developers to create, develop, and deploy custom components efficiently. The `gradio cc` CLI provides a streamlined experience for creating, developing, and publishing custom components. Additionally, Gradio's hot reload mode allows for rapid development and testing of AI applications. Overall,"
What is the first step in debugging errors in a fine-tuned Transformer model when working with external datasets?,"[(24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (4629, '65a68f3d-7555-49cf-a137-b0000b75f9d0'), (1560, '9c881267-9303-4686-81be-1ac7f7fd886c'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (13892, '18f9d2dd-b24d-40df-bf4b-70633772beaa'), (30278, '9a49a14a-c17e-40eb-b72a-5621d602f5b0'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (26202, 'a3e63e78-75c5-4e73-a5b3-110285882551'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (20071, '163a9ba3-cb9e-4a4f-bdd6-07ceaa145535'), (23142, '8fd73c82-896f-4e49-9763-6a97ee39a709'), (23145, '093af302-133e-498d-a722-88a8b6199cee'), (12912, '426a098c-d467-4adb-a3a9-bb78cb34af6b'), (18549, 'd84bf0de-2759-4c1e-9a94-cc27a2136a9b'), (1150, 'b91271f2-c6c3-489b-b0cd-5c23b6b7df5e'), (3715, 'c4518fec-8276-4e52-964a-6148955da040'), (3724, '1c0e1b75-0bd9-45a1-af24-e9cf468d539c'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (20117, '344e2a72-849d-4598-a5f5-e5a7a605e227'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (23723, '2de4e811-42ea-47bb-b611-e241662b0083'), (1195, '3e7433ba-fae2-4e13-8802-9ff9da0b8900'), (6317, '56ffe729-9ef7-4575-b631-fdfdf483ee5f'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (6316, '979e27ae-ecb6-421a-9259-ab1fcb97b027'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (23729, '5d899127-2a50-48fa-9d69-557ec6ed9ded'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (702, '43197162-b6f7-4689-ab00-064a4b19af86'), (1215, '5739b2fb-8f4c-41a1-b697-0e0fd5f0a76f'), (1216, '485aadcf-fe5c-4160-a137-0b1dc3b07685'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (30918, '5636518a-1eb1-4077-baf0-ae21e3344396'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (19668, '55b764e4-abdd-42d3-aae5-2fbaedf7cc68'), (19669, 'aee04d72-b4ef-45e8-8857-bc8147c6672b'), (1240, 'ae168361-4065-4c53-9046-4d69f34c3e19'), (1241, '906de20e-0472-4937-8882-c423749564ec'), (17628, '216e7cec-f307-4f22-873e-a4c5479c0a41'), (1246, 'f1c4bb4f-9bb9-4e34-a9a3-2f65bf45e38a'), (24287, '0f309be3-9d92-421d-bae1-8bfa12a4840d'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (30945, '1f492b07-5014-4107-a135-6821e6248736'), (30944, '650f5a5a-ee1b-47bc-83ff-f6cf7a368a11'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (1257, 'e17a5fc9-f0ae-4120-af12-4c50cfd1be95'), (31467, 'd5b8cb99-b5ce-4df9-8ab2-5c8ebaf7db26'), (11500, 'defee3f2-8e67-4389-bb9e-ddd62fbd2657'), (7920, 'b0ec8a3b-aacb-4851-a377-64d834457caf'), (30961, '3948329f-86ce-4dd0-9d40-bade197ac0eb'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (17154, '2fb79b0d-c61b-4500-9151-440dcfada262'), (28938, 'c79cdb2e-622b-41c8-bde6-21c9ca8d958a'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (28952, '79800c43-9871-4516-b042-0c6ed54ed114'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (20252, 'b9e133f5-90ab-47a2-9af2-55d084a98c15'), (15142, '10c62a15-e400-4c35-93a8-017a7ab8b3d6'), (5416, 'ae0beed4-07b2-4cd5-9638-323b957185ef'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (19273, '633f3c37-14a3-4e59-a9dc-66a1ca867541'), (20298, 'cf07b1e2-6cae-4114-989b-151be05e8ba7'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (7534, '4354b918-223f-4fd0-92ea-e59bd0b20f6b'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (8576, '774d75b2-2b47-4c0a-b365-891414ebb5df'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (16274, 'acff4162-cf17-4871-b231-dd1b3baf8903'), (20887, 'e4c88d19-b6b7-484a-b21a-646c0f2984c3'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (20905, '7f194331-ef4b-4aff-9ae8-6a824fa631f5'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (20908, 'ee5a5b82-d78b-40de-b0b2-ed026ae576d3'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (20933, '4c0a1879-65bd-4145-81d4-aa0b9c5d6d0d'), (20934, 'fca222e3-d4bf-4031-863b-50a1512af5b9'), (11209, '3b5ff375-2e1c-4a92-8077-43f3d30f62ae'), (22476, '6a8f51fd-9677-457f-8034-63e0566b51c3'), (20948, '7445ee23-4b44-4cf9-93da-c4aa8ae24afa'), (4065, '40ca2d84-7561-4754-bbb3-490ed78468fc'), (4067, 'db389377-70fa-4887-84a7-4ea544f7efea'), (3047, '613dfbaa-b77d-455c-a74c-88e0a04c24de'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (24057, 'd292faa1-de56-402c-878a-e2d243e5db03'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(""glue"", ""mnli"")

model_checkpoint = ""distilbert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples[""premise""], examples[""hypothesis""], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)
n this video, we will see how to debug an error you encounter when running trainer.train(). As an example, we will use this script that finetunes a bert model on the GLUE MNLI dataset. Checkout the videos linked below to see how we came to such a script, here we want to learn how to debug the problems in it. Running the script gives us an error pretty fast. It happens at the line where we feed the inputs to the model, according to the traceback. That tells us there is a problem there, but the problem could come from many different causes. To debug an error in a training, you need to make sure each step of the training pipeline works as intended. This means checking that the inputs of your dataset are correct, you can batch them together, feed them through the model to get a loss, then compute the gradients of that loss before performing an optimizer step. So let's start by looking at the training dataset this Trainer is using. There is definitely a problem there as we see texts and not
What to do when you get an error[[what-to-do-when-you-get-an-error]]

<CourseFloatingBanner chapter={8}
  classNames=""absolute z-10 right-0 top-0""
  notebooks={[
    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb""},
    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter8/section2.ipynb""},
]} />

In this section we'll look at some common errors that can occur when you're trying to generate predictions from your freshly tuned Transformer model. This will prepare you for [section 4](/course/chapter8/section4), where we'll explore how to debug the training phase itself.

<Youtube id=""DQ-CpJn6Rc4""/>
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""N9kO52itd0Q""/>

The problem when you encounter an error in `model.fit()` is that it could come from multiple sources, as training usually brings together a lot of things that you've been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `model.fit()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""L-WSwUWde1U""/>

The problem when you encounter an error in `trainer.train()` is that it could come from multiple sources, as the `Trainer` usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.
 
Here is the question: What is the first step in debugging errors in a fine-tuned Transformer model when working with external datasets?","The first step in debugging errors in a fine-tuned Transformer model when working with external datasets is to manually go through the entire training pipeline to identify where the error occurs. This involves checking each step of the pipeline, including the dataset, batching, model, loss function, optimizer, and evaluation metric, to determine where things went wrong."
How does the PatchTST model improve computational efficiency for long-term time series forecasting?,"[(7682, '69400042-ec99-4c8a-bf23-7e37c59fe21f'), (517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (520, 'b101edb2-f02a-4bd8-b69e-0e3ba6a11f4d'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (12298, '75d1fbe7-d1d3-4868-b940-d25e740f5bdd'), (24592, '1bc68948-b978-48cf-aee4-932675f8db96'), (6209, 'dc759306-d7ce-4e78-b89d-4681bc9dd25d'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (6212, '9d2539d2-4c9d-4ea5-8dad-8105ad0d656e'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29771, '54c867d1-dd3a-4a9e-97f5-dac70011ef4b'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29786, '945fd8e4-77cb-409e-af9d-4d7171e7ea93'), (20570, '5dd4252e-a29c-4f30-ae23-7b7de46bb5a0'), (29818, '02cc33d8-0c79-466e-ba0c-bdadb265c65f'), (29820, '49d6a04e-36ec-4a38-adbf-50f525d423b1'), (29821, '84d9d326-f3f5-4a05-b9fc-f14b56bf8280'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (17031, '3f233fb1-a6a0-4c1c-8465-4333aaaee216'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (22657, '9268b17f-2fc5-4ae4-8106-8039de92a69b'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (9877, '02d00698-e099-4595-9b5b-915a102c8c31'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (749, 'aadabaa8-059f-4e53-bb4a-defc70cc04cc'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (765, '65b17d57-f43a-42e2-a6d4-b96ae347324c'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5379, '287c1ed6-130b-4784-8132-3f5287c3471c'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (20229, 'cecdc71b-2ef4-4100-9920-876a3adacde0'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (779, 'c84b16d0-6361-44a8-b2c9-cc03a7a33748'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5396, '18d8ed78-6549-4720-9c12-5ae48fb89fc8'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (5923, 'd508a7d3-3d71-4f96-9bcd-951254088aab'), (805, '45825ed8-8f85-40ed-a174-19eeebd97cf7'), (807, 'ce4fda9f-c07a-4ca5-8538-9f8ededb01e2'), (808, '2232e5cb-cb93-4ea7-8c40-d51528355f6c'), (5419, 'ca377909-1f4d-48de-af71-01684b43fc59'), (812, 'ecabe7c7-05c4-4cfa-9e6e-c24df8d129b4'), (5420, '19d2ca4f-3e1d-4789-848e-5363e52ea2e9'), (5422, 'c12a80a5-4e72-4a35-b1ea-7113b01b5c00'), (815, '00347210-f522-4fa2-a819-98958034885b'), (814, '3c86feb6-9865-4995-82a9-c9e1bafa680f'), (5426, '5052dd1e-66fa-4818-9c3d-a9efed8343c7'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15696, 'f5c82a25-56d1-4160-a926-0d697826a6bb'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (27532, '2ee290a6-5920-46c1-894f-e17c3af8e39f'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (29586, '67cb4809-8361-4ee5-9dd4-cb31eef9b1bf'), (23442, '35ca6462-1413-46ab-b1be-a5db65e32709'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (23993, '1ab46630-fc89-4db9-8991-e991994175e0'), (20415, '0ff6dba8-cfe6-4157-a337-93d21a8a4a9e'), (17857, '640d96b2-19b8-40ad-a0f5-1a0a7b13b9d1'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (15338, 'bb8b9cde-8808-4da8-8151-75ed6150d4b8'), (16364, 'd30aeb8c-78e2-4444-b089-e1fd3fe37afe'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
1. **Quadratic computation of canonical self-attention:** The vanilla Transformer has a computational complexity of \\(O(T^2 D)\\) where \\(T\\) is the time series length and \\(D\\) is the dimension of the hidden states. For long sequence time-series forecasting (also known as the _LSTF problem_), this might be really computationally expensive. To solve this problem, Informer employs a new self-attention mechanism called _ProbSparse_ attention, which has \\(O(T \log T)\\) time and space complexity.
1. **Memory bottleneck when stacking layers:** When stacking \\(N\\) encoder/decoder layers, the vanilla Transformer has a memory usage of \\(O(N T^2)\\), which limits the model's capacity for long sequences. Informer uses a _Distilling_ operation, for reducing the input size between layers into its half slice. By doing so, it reduces the whole memory usage to be \\(O(N\cdot T \log T)\\).
*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
 
Here is the question: How does the PatchTST model improve computational efficiency for long-term time series forecasting?","According to the context, the PatchTST model improves computational efficiency for long-term time series forecasting by segmenting time series into subseries-level patches, which are served as input tokens to the Transformer. This patching design has a three-fold benefit: 

1. Local semantic information is retained in the embedding.
2. Computation and memory usage of the attention maps are quadratically reduced given the same look-back window.
3. The model can attend longer history.

This design enables the model to reduce"
"What is a common pitfall when loading external datasets for NLP pipelines, and how can it be mitigated?","[(13825, 'f1446f89-52a4-4aaf-ab1a-9b92f5010ce7'), (9, 'ee938445-5e5d-4e33-b4a3-e17998c36b16'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (24590, 'f3c34a84-1359-476f-9296-e21dd27ebdb9'), (26641, '972af779-bf9e-4865-bebd-97def3d5dd6d'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (26651, 'f1b12a30-cfeb-4c01-a3a4-7f8d4640303f'), (3611, '5acb0ab4-672d-47da-9ea1-6cdc6cb1d8db'), (8223, '2a8810cd-a76d-40d2-acfb-a7c4dede20d4'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (28719, '348f9b74-e568-4574-9dab-2e6621e37239'), (24640, '815d70f7-2c5b-4af2-8a4e-fa377374ffad'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (28739, '50948ec6-1039-4799-9d5c-f657b72f8d39'), (8266, 'c6c13dde-7bcb-40a7-b9d7-2832d1279dc1'), (5707, 'f1652f24-131f-48b0-a83d-0e7049f1189e'), (25175, '1e8094f9-39e6-4247-b6d2-5b55744f798d'), (25176, '754d1c50-6a81-4ccd-99cf-0c9628db876b'), (25179, '998f3841-d018-4936-9340-732b8282f2e1'), (28767, '553d5e61-e486-48b6-b9ad-9a9ef6b3e3af'), (19040, 'fd7fbc60-e179-4a50-ad6d-74b9dbce7e55'), (19041, 'e0aa2a3f-538a-4293-a270-06da8e763a95'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (7272, '90527d40-f42d-47ed-a5d3-255fa1425973'), (24682, '6246272f-fdbc-42c1-80f5-adc29d46040f'), (13422, '7fcc282e-51e4-4714-887d-b6075a5d4b55'), (24691, 'e2088a54-3bfc-40df-b1bd-8d4fb59815b9'), (4212, '12f87a03-1269-4e9f-938f-0f142e08b79a'), (23159, '8df7603d-0d12-428d-8ffd-3b4c55cd0c5f'), (9337, 'e15f8374-58a2-41f8-ace2-17f6d19efe49'), (18046, '4c76e605-c6d5-4738-ae77-d6e0407ebc5b'), (24704, 'c6d8c610-53c9-47f6-821c-69ce50caa756'), (4741, '210088c4-7fef-45fb-b3bd-ba5759a63329'), (23687, 'a74310e1-5e6b-478b-a97a-505cac3cf243'), (22667, 'c092ea1f-6a88-43d6-8613-7b1c7d39696e'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (24727, '710d9250-ca94-45f1-a999-cbc01b7e8ff7'), (13465, 'cbcbfaf3-ee38-4ff2-8f58-74eca846f9a2'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (9381, '1695844a-c3fc-40f7-8e86-4f2d6eea7efa'), (9383, '5987ba42-1ed1-474f-b447-872e7f6c8277'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (9390, '8ca789f9-6a64-4dfd-a067-8a38701e9237'), (9903, '584836fd-d59e-43e0-93a4-4436c108b340'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (5815, '7a89c479-9823-4c0f-a0de-e3ecf304e920'), (7351, '974ac7b4-c4c2-4761-8654-0816e4e2c955'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (21195, 'faa2b5eb-cd0a-4c23-a187-337ef7e890bd'), (21197, '3b2fbad2-3fdb-4725-9c19-a5b8f139a1a0'), (5328, '7e28df2f-1656-43a9-b36b-adab211aeb45'), (19664, '2872ed56-b851-405f-8601-3748f47a00e1'), (5842, '3b64b1f4-3187-44e6-a1b6-2814799742b2'), (21205, '6bda238b-e50a-4ace-95e5-877b1ae72029'), (5335, 'f1e2bdbb-8a30-4d40-9a37-04b75cc51d54'), (20186, 'e4800925-3dd9-4604-b5d2-755f623ef3b9'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (20703, '185d614c-743f-4b53-9078-c44a17e5c1ae'), (7393, '69b3bd6a-2eb1-4f94-84eb-2ecd7239847c'), (31458, '772a8786-3e64-4959-84fa-931ce6254e05'), (7395, 'e6cbeb63-c998-433d-b0b3-afb65bf27b67'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (20717, 'ceb57f22-5344-4bc1-8c33-80214f9440a7'), (26885, 'f2bb8a8e-0f3c-4545-9014-c725c069e776'), (7942, '14990c0d-4b9f-4a48-b72f-ad5c83124c6f'), (23303, 'c8340867-9c9b-4274-adb6-873aeaef3797'), (23306, 'ceda3f28-4ba3-4ca6-84a7-a4b914fd8d4f'), (3339, '6c368a7d-8543-44b3-9993-4674ebdec341'), (3342, '40de6e0e-cd3f-4ac4-8b93-e0a427a98e4d'), (28433, '593c1267-44ab-4a34-8d86-ac17d65ea9d6'), (28948, '35cfc8f5-5fbd-4311-bc92-d1f421c75890'), (7957, 'e79d5025-a9c4-432e-b94c-57adc02cbeba'), (21788, '76f89282-5e29-480a-9fb9-107b45af559d'), (21789, 'f348a103-0156-4f06-a4aa-5eb608e899a5'), (13091, 'fd87ced8-d15d-4eb8-98f3-447a6c638420'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (25898, '73ac5dcb-df39-4bca-8d7e-42637669ea82'), (24890, 'afcbcb7c-ee42-4823-ad2c-9b6d06c0f04b'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (4438, 'c433e5f5-9651-489c-8a76-f04bf6884777'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (25442, 'c84e5d7a-448b-458c-8c4e-d72920abe394'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (31086, '7c58f058-2d25-456b-b666-b121481ab6d4'), (31604, '18698824-2c80-453c-999a-9edd11b91d34'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (11650, 'e316dc70-9979-4cbe-b6b2-414868e8e8f2'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (31110, '9726dad8-10ae-460c-94fa-9e76f693eb05'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (16789, 'dcaee1f9-3cdd-4b24-bd0d-762f75437e1f'), (17302, '10950558-fd37-474e-868f-4f3fca1d098a'), (31642, '00d96bc2-ff51-4546-bbee-81b6b62887cd'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (25519, 'a69222a9-5a13-4e58-b18b-59551968d308'), (26036, '0e8e3f85-e18c-46e7-97ab-595f1fda1603'), (21952, '138ee00c-29cc-4a4a-b56c-0e17a9529580'), (31170, '694c9ec9-5c6b-4ea4-92ba-b75bc909f395'), (11716, '5aa7f3a8-b022-4936-88ea-c22c3c501a94'), (7623, 'ebcb37fc-acf3-4241-9938-95c52730e549'), (16853, '3cb9a538-dae1-4898-8b6a-a7940cd75253'), (30171, 'c3402588-8a4f-4dcd-81b8-f2315cfaefc7'), (2528, '7119de66-fa8a-4e1c-b7c6-9258f5205fd9'), (16871, 'e66a5e45-2cf0-4d15-bcfc-0485a92e992f'), (1513, '710901b9-382c-447e-b95f-bb910a1fdce4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

[[autodoc]] pipeline

## Pipeline batching

All pipelines can use batching. This will work
whenever the pipeline uses its streaming ability (so when passing lists or `Dataset` or `generator`).

```python
from transformers import pipeline
from transformers.pipelines.pt_utils import KeyDataset
import datasets

dataset = datasets.load_dataset(""imdb"", name=""plain_text"", split=""unsupervised"")
pipe = pipeline(""text-classification"", device=0)
for out in pipe(KeyDataset(dataset, ""text""), batch_size=8, truncation=""only_first""):
    print(out)
    # [{'label': 'POSITIVE', 'score': 0.9998743534088135}]
    # Exactly the same output as before, but the content are passed
    # as batches to the model
```

If `args.dataset` contains the `gs://` identifier, TensorFlow will understand that it needs to look into a GCS bucket. Loading locally is as easy as removing the `gs://` identifier. For the rest of the data pipeline-related code, you can refer to [this section](https://github.com/huggingface/transformers/blob/474bf508dfe0d46fc38585a1bb793e5ba74fddfd/examples/tensorflow/language-modeling-tpu/run_mlm.py#L186-#L201) in the training script.
Beam Datasets

Some datasets are too large to be processed on a single machine. Instead, you can process them with [Apache Beam](https://beam.apache.org/), a library for parallel data processing. The processing pipeline is executed on a distributed processing backend such as [Apache Flink](https://flink.apache.org/), [Apache Spark](https://spark.apache.org/), or [Google Cloud Dataflow](https://cloud.google.com/dataflow).

We have already created Beam pipelines for some of the larger datasets like [wikipedia](https://huggingface.co/datasets/wikipedia), and [wiki40b](https://huggingface.co/datasets/wiki40b). You can load these normally with [`load_dataset`]. But if you want to run your own Beam pipeline with Dataflow, here is how:

1. Specify the dataset and configuration you want to process:

```
DATASET_NAME=your_dataset_name  # ex: wikipedia
CONFIG_NAME=your_config_name    # ex: 20220301.en
```

2. Input your Google Cloud Platform information:
#### Philosophy #4: You shouldn’t have to write your own data pipeline, especially for common tasks

In addition to `transformers`, a huge open repository of pre-trained models, there is also 🤗 `datasets`, a huge open repository of datasets - text, vision, audio and more. These datasets convert easily to TensorFlow Tensors and Numpy arrays, making it easy to use them as training data. Here’s a quick example showing us tokenizing a dataset and converting it to Numpy. As always, make sure your tokenizer matches the model you want to train with, or things will get very weird!

```py
from datasets import load_dataset
from transformers import AutoTokenizer, TFAutoModelForSequenceClassification
from tensorflow.keras.optimizers import Adam

dataset = load_dataset(""glue"", ""cola"")  # Simple text classification dataset
dataset = dataset[""train""]  # Just take the training split for now
```

The [`pipeline`] can also iterate over an entire dataset for any task you like. For this example, let's choose automatic speech recognition as our task:

```py
>>> import torch
>>> from transformers import pipeline

>>> speech_recognizer = pipeline(""automatic-speech-recognition"", model=""facebook/wav2vec2-base-960h"")
```

Load an audio dataset (see the 🤗 Datasets [Quick Start](https://huggingface.co/docs/datasets/quickstart#audio) for more details) you'd like to iterate over. For example, load the [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset:

```py
>>> from datasets import load_dataset, Audio

>>> dataset = load_dataset(""PolyAI/minds14"", name=""en-US"", split=""train"")  # doctest: +IGNORE_RESULT
 
Here is the question: What is a common pitfall when loading external datasets for NLP pipelines, and how can it be mitigated?","The common pitfall when loading external datasets for NLP pipelines is that the dataset may be too large to be processed on a single machine. This can be mitigated by using parallel data processing libraries such as Apache Beam, which can execute the processing pipeline on a distributed processing backend such as Apache Flink, Apache Spark, or Google Cloud Dataflow."
"What feature of the Transformers library supports its broad applicability to different frameworks (e.g., PyTorch, TensorFlow)?","[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (17412, '1d9b136f-d20c-4989-8975-80625fa8da45'), (17413, '5c272bf9-ff61-4ced-9930-7c174b92dc98'), (17415, '0542b335-d341-4e8d-8d7c-cbf9eeebae8d'), (17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (27146, 'cbb71c05-7152-43db-b473-37078bba89e2'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (3599, '9cf1589b-a7c4-4c31-a0b5-c8ee2365a3b5'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (5155, '86625bb7-792a-426a-9ade-ea98317da2e0'), (24126, 'f6d0b6d4-13c2-4d59-a59d-6b341a5a1508'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (12906, '39ba7aa1-526f-44ed-814c-779d9283deaf'), (24691, 'e2088a54-3bfc-40df-b1bd-8d4fb59815b9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (20667, 'b0b180ee-2478-48b8-9446-bf6a9feabb3b'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (9415, '78661659-8f11-4005-800b-e1d23d747852'), (214, 'ced565fd-8f3c-4e7b-a0f9-9e46e7ed2922'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (25302, 'bcf9a9f1-7a6b-4a62-87f7-68506603417d'), (17116, 'df0e2cb6-a368-4e77-894f-ce91ac16ed6c'), (6380, '6aed28bc-1ba7-45a0-9248-1d8f31471c81'), (22770, '978ad2c3-695e-4a95-82c8-b6fb9af51394'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (15608, '334237b9-d851-4edf-85f1-c4c4f52047ad'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (15625, 'abc82f6a-5afa-43a2-8de3-f016f34e7055'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (28429, '9c6fb59e-1d9f-48bd-be33-bd99c86d369d'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (3363, 'c8cd6d58-bf3c-4c0f-8cd0-6d6d28ea07b9'), (16679, '4002f15e-8a33-4cad-927c-232fe4ea7a37'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (17739, '540c412e-c6fc-4761-a366-165b00c59e31'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (340, 'f260b640-4e93-47fd-9cf4-b3ad7c90bd35'), (341, '5237d4cb-369e-42c3-9609-725e481f1c38'), (9051, '09b3a3e8-2734-459b-9dcf-d885565ee6dd'), (26975, 'a77435a6-b248-4c25-88bb-c6f596b9ed0c'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (14704, 'd152c193-d62e-412a-9a73-fad192c92a19'), (7546, '125265e4-f906-4180-bf64-53f1be1bd941'), (28539, '15ca206b-e242-4240-9c56-b957d70a1add'), (25980, '44d9eb20-ac45-46e9-b4ea-310683db7670'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (20871, 'edfcf78d-61e7-443d-8514-6db1ea1b0bca'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (20872, '21fadc48-4612-45a6-a09a-f093a8fb038d'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (24972, '9084f511-5580-4546-944c-05245a37f48b'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (22434, '8bea8922-5c12-40fa-87bf-aa092348b13f'), (5044, '3347ade6-a88b-4653-97f6-3b4beff0fa0c'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (5048, '4fd5ee84-be31-4b12-bfc2-332bc69dfa07'), (28601, 'eb9de433-f226-428a-9cca-06ee8c6b08ce'), (28602, '5b1d4ee5-f2ec-46e3-801a-8d10081798fd'), (15808, '528599cb-95fc-450e-b657-89b44f74fb95'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (15810, '26a1f21c-ebc4-40c0-8a1a-34e9a46f1698'), (6083, '34942899-e37f-425b-ae2a-278f328948fc'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (6087, '5ae43b09-0e79-4166-8a50-8540cdbdf06a'), (9163, 'd7206e44-29b4-499b-92c4-0008be204752'), (13775, '081f194c-20d7-476c-a3bf-56c80cdcc4e5'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (12257, '0dae409c-84f2-422a-ad8c-f8832627d8a9'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (7154, 'a06b9d90-8d8a-4fb6-b53c-619bed721fbb'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem.

In a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code.

In this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware.

And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you.
- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.
- **Flexibility**: At their core, all models are simple PyTorch `nn.Module` or TensorFlow `tf.keras.Model` classes and can be handled like any other models in their respective machine learning (ML) frameworks.
- **Simplicity**: Hardly any abstractions are made across the library. The ""All in one file"" is a core concept: a model's forward pass is entirely defined in a single file, so that the code itself is understandable and hackable.

This last feature makes 🤗 Transformers quite different from other ML libraries. The models are not built on modules 
that are shared across files; instead, each model has its own layers. In addition to making the models more approachable and understandable, this allows you to easily experiment on one model without affecting others.
```
</tf>
</frameworkcontent>

One particularly cool 🤗 Transformers feature is the ability to save a model and reload it as either a PyTorch or TensorFlow model. The `from_pt` or `from_tf` parameter can convert the model from one framework to the other:

<frameworkcontent>
<pt>

```py
>>> from transformers import AutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)
>>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)
```
</pt>
<tf>

```py
>>> from transformers import TFAutoModel

>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)
>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)
## The technical philosophy

In this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Transformers so that the community is aware of the design choices specific to this area.

Even though Transformers started with NLP, we support multiple modalities today, for example – vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:

- Easy model download with a single line of code with `from_pretrained()`
- Easy model upload with `push_to_hub()`
- Support for loading huge checkpoints with efficient checkpoint sharding techniques
- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))
- Initialization from model configurations
- Support for both PyTorch and TensorFlow (non-exhaustive)
- and many more
Using 🤗 `transformers` at Hugging Face

🤗 `transformers` is a library maintained by Hugging Face and the community, for state-of-the-art Machine Learning for Pytorch, TensorFlow and JAX. It provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio. We are a bit biased, but we really like 🤗 `transformers`!

## Exploring 🤗 transformers in the Hub

There are over 25,000 `transformers` models in the Hub which you can find by filtering at the left of [the models page](https://huggingface.co/models?library=transformers&sort=downloads). 

You can find models for many different tasks:
 
Here is the question: What feature of the Transformers library supports its broad applicability to different frameworks (e.g., PyTorch, TensorFlow)?","The feature of the Transformers library that supports its broad applicability to different frameworks (e.g., PyTorch, TensorFlow) is its ability to save a model and reload it as either a PyTorch or TensorFlow model, using the `from_pt` or `from_tf` parameter. This allows for flexibility and ease of use across different frameworks."
Why is licensing compliance critical when sharing models on the Hugging Face Hub?,"[(15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (19488, 'e5ffa4ca-1cb8-46ea-b311-723f83f32707'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (7725, '258a242e-844a-4498-bd2d-4c45f980f5f2'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (29235, '28f83b83-b564-4757-a1e4-3c0a127d21bd'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (12348, '57a3c79a-af62-43be-bebc-4ef70c6c1495'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (12352, '0326df1b-2a30-43eb-9f03-9fc1aa4385a1'), (28741, '50733951-b5db-4e9b-83a4-b2911af5c989'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8780, '63aad1b0-7631-43d7-927d-8cd10bcff3da'), (8781, 'aa5739fd-df24-49fc-983b-b9026f0b07d7'), (8782, '4cffb526-4c01-473c-ae6a-aa99e1991fcc'), (8783, '56426659-c214-4bcb-8f80-30aa9e0e911b'), (8784, '2689d196-69e1-4f9e-8033-4ed364ef2567'), (14927, 'cd247d85-d1ee-4e3e-b3e5-9bc9d15c4563'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (25685, 'e7551698-0040-4251-b71c-50be7bc42a95'), (24664, 'e88147dc-45fe-448b-b483-b5a91f408802'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (14944, 'cd075ce4-d02e-47be-ad35-21b38e850692'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (13457, '3eab0a1f-ba7f-4162-a614-4f6915347e14'), (19603, '561460ee-7e32-4394-b0d3-d59f4b7790dc'), (27284, '344b1139-9e69-4a8a-a5b7-593e7b089268'), (19100, '28b56063-0856-43a8-90c0-d1124a372f5a'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (30380, 'b4858a62-bfab-4374-b4e5-c287a54d33be'), (14509, '64200989-d1d6-4b32-8613-3c0ad1c88f4f'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (7355, 'fadfe698-c03f-4512-b775-f152160d9589'), (17596, 'be398fa1-fe61-4e41-ac13-876c1fb3040e'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (7873, '6685e912-4253-46db-a9e7-b7dfb8fc6141'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (29896, 'b72ba5ef-9507-48a0-91d0-2fe3e2cc33f3'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (11975, 'c977182b-d101-4944-95e4-2c832d9bf6fd'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (10461, '791d16b9-ac09-45b6-a426-0eea3d3b4063'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (24804, '23cde3d9-30d2-4fd8-84f5-8504a15e3a42'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (10992, '004cff2f-f98d-40e2-9254-bfe3183ca2f2'), (5873, '9d5439bb-61db-43f2-ba27-d82d1ce267c5'), (14577, '03a094b5-b29c-4840-a743-d2ceb896c494'), (23284, '59c5d501-a716-4200-a252-695f07ef83dc'), (21243, '4c853a7f-56ab-4489-93ba-acedd42e9107'), (11526, '994250e0-f714-40fe-b8ff-52c2610db830'), (7434, '5d226047-dfd9-4632-a2fe-7a48cd58fe3e'), (4366, 'fa03c662-5522-4732-a0ba-6b673062d9af'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (1301, '5137aed7-d723-4ecf-8874-26a7c9ecb926'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (10530, '04d72a8e-2ec5-4e32-86ed-6aad0064f400'), (26919, '5ebb5465-6f4c-4e1c-8326-5f71516e66a4'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (12099, '97456597-42ed-4914-bc4a-1146c5fc4b6f'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4449, '98291195-a6fd-4eeb-8e55-4bc54d2a1fb3'), (8549, '7ee9801f-9160-4b67-ab5e-ccacabf36b60'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2927, '4e8e5c35-d707-48b4-88b1-ca7eba35f22d'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (2940, '41b2ed97-f869-45fa-a2b1-445971ef0afd'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (22421, 'bdae70d4-1482-4a7a-aa11-6e2aa1cd81c2'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (17301, '9e130d4e-12df-4941-91c1-6e5fd07be093'), (16791, 'e1e0fd50-9bed-4d12-950d-4c3107e00575'), (11163, '395c4c3d-8c49-42dc-9fda-2e3de70b64bc'), (10655, 'b65f54ab-b114-433c-b9d8-c631d2973538'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (19927, '0fb8b493-49ef-4bd7-84fe-d2f87486e82d'), (25050, '4ea83d3e-bb4a-4ab5-b25a-c79f5c52c46a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (24042, '089a831c-4f38-4cd4-b040-5b7abcd993a9'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (5625, 'c79981e0-e8ac-454a-a1ec-1e4b2c4b1ba2'), (22012, '5c55b2ec-3284-45a4-9f86-cf514fba02e0')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
```

which display on the Hub as

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/license_other_example.png)


This challenge aims to improve the completeness of this metadata on the Hub, which will ultimately benefit all users.

In other cases, the license was previously set as `other` but given the popularity of the models, the Hugging Face Hub started to support the license officially. This is especially the case of Llama 2 models for which the `license:llama2` is now a supported license. In this case, it's best to set it directly in the Model Card metadata:

```yaml
# Example from https://huggingface.co/codellama/CodeLlama-34b-hf
---
license: llama2
---
## Why might Galleries, Libraries, Archives and Museums want to use the Hugging Face hub?

There are many different reasons why institutions want to contribute to
the Hugging Face Hub:

- **Exposure to a new audience**: the Hub has become a central destination for people working in machine learning, AI and related fields. Sharing on the Hub will help expose your collections and work to this audience. This also opens up the opportunity for further collaboration with this audience.

- **Community:** The Hub has many community-oriented features, allowing users and potential users of your material to ask questions and engage with materials you share via the Hub. Sharing trained models and machine learning datasets also allows people to build on each other's work and lowers the barrier to using machine learning in the sector.
 
Here is the question: Why is licensing compliance critical when sharing models on the Hugging Face Hub?","Licensing compliance is critical when sharing models on the Hugging Face Hub because authors can define which license applies to the work they share, and users must be able to quickly know which license applies to which model. This ensures that users are aware of any limitations or restrictions on the use of the model, and can comply with the terms of the license."
What debugging technique is recommended for resolving 'AttributeError' during model forward passes?,"[(3077, '45070278-b6c4-4645-8c0b-f2a7a699a326'), (24083, '03ad03d2-915e-4319-b7cb-4c8afd73232b'), (24084, 'f2b13125-b559-44d1-932b-c4f1d6b674fd'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (4127, '8b77edf9-5fbe-4e5a-84df-0360075e8182'), (6688, 'b9aef38c-a854-4a94-a9c4-fb3e1f95ef5e'), (1582, 'a8ee532e-7dea-4930-81ee-7fad8d3c4c60'), (19515, 'e49b8303-c291-4b3c-9e0b-8536793c3176'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (25700, '23e83777-b949-4f93-bc54-d45092bb6976'), (20069, 'c53fb95e-58c9-41b3-aa8c-8ba29cb514f0'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (23142, '8fd73c82-896f-4e49-9763-6a97ee39a709'), (23146, '787618f8-0624-4588-874a-444a599b5e9e'), (22640, '53ddd74d-fbd4-4faa-922b-0591f35c527b'), (29812, '37b531a9-377d-43be-a980-a2de1bc9c32f'), (20085, '7bcc1692-9fd3-479f-84b4-4b930e6caa83'), (1150, 'b91271f2-c6c3-489b-b0cd-5c23b6b7df5e'), (20099, '5e7fe10f-0024-4628-aa58-73dc11da2265'), (1160, 'f7757a01-ad04-440b-8307-00fc7e0285de'), (1161, '9d7a9c13-b785-4334-b67a-324ae65947d6'), (10888, '11531df8-1ac7-4a86-906d-d2dee9e00e2a'), (28810, 'facad8b8-798f-4a21-aa2a-f853d957b27c'), (1168, '0e3b941c-35b9-4bc5-b4dc-9c3309e9f562'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (1171, '542581c9-1c35-4102-be0d-d89e6a2225c6'), (1175, '140bfebc-c10e-415e-bfb0-9cbc492c2a7b'), (1176, 'bb9a042a-a3fb-46a4-831f-51906924decb'), (1178, '50ff656a-d70d-4407-8f69-09dc714a3b66'), (1188, '84476d43-a102-4354-b00a-2b9e71636e89'), (25254, '281f8259-9493-40e9-869f-f4066fb8bef7'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (1203, 'fa5804fc-2e57-4cfe-8041-e03bba3c0d2f'), (30387, '71bbd928-9b3c-4867-bb36-7014da37b81b'), (1205, '19799104-79e5-4b3b-bcc1-036b3fd80da5'), (1206, '1d985b81-a7e0-4313-b060-7cfdd799be25'), (1207, 'ba67be42-b645-475b-a449-454ae493e9ef'), (30388, '7c11aa0d-d06a-4d6a-a5b6-2ad084f9cf27'), (30906, 'd0acfe0e-7c52-44b9-a03b-f99643195c36'), (14523, '7eaad718-02e7-45b8-a664-b3c1b35d6c39'), (30908, 'f813e9a8-eab4-4c6c-b738-b58567335fa5'), (1213, 'bec86d0f-9e27-4271-bd19-fa2a88278612'), (1214, 'c6152586-1c48-41e4-975f-13183809eb4b'), (30910, '562b2c34-aa31-45cf-a703-d544e0641ad7'), (1215, '5739b2fb-8f4c-41a1-b697-0e0fd5f0a76f'), (1216, '485aadcf-fe5c-4160-a137-0b1dc3b07685'), (14524, '9edaacf8-e6a9-45da-9168-efea104fecf4'), (30909, '7990fb83-8871-47ad-b63e-08b50ef15d14'), (30916, '83870699-d55d-4da0-9257-d46016695c8c'), (30917, '963d0459-77e4-4c23-ae6f-2ef4da277f61'), (30918, '5636518a-1eb1-4077-baf0-ae21e3344396'), (10440, 'd29c2db8-5c36-4df5-b8e8-7091225e6736'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (19668, '55b764e4-abdd-42d3-aae5-2fbaedf7cc68'), (19669, 'aee04d72-b4ef-45e8-8857-bc8147c6672b'), (1238, '7a8ecc14-0e3a-4b2e-856a-9df768f253cf'), (1239, 'cbdef099-57bf-4d75-bd28-c97758db1411'), (1246, 'f1c4bb4f-9bb9-4e34-a9a3-2f65bf45e38a'), (30943, '1d6f7d97-f6d0-42cc-a25b-b2ba69a79f65'), (1248, '5162b3cb-da7d-411e-92b2-275490ed5442'), (30950, '8c43e960-5196-430d-a443-73c88cf050e0'), (22254, 'e27fface-fa4d-4917-ac05-c99b3208e99f'), (12022, 'ca590bdd-6993-45bf-bc5c-6216ff9e670d'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (7936, 'a5dbf1a5-4346-46c1-ac10-c332139056bc'), (7937, '72e80afc-d1e0-4a0f-85ab-e5658b60f5d1'), (20250, '06ee8af0-01dd-4c97-aae3-5ddee389cb4b'), (16155, '88f1863a-42df-40eb-8137-81a4119fa560'), (20252, 'b9e133f5-90ab-47a2-9af2-55d084a98c15'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (798, 'e25738d0-cf95-4ef5-a393-52b324ec4219'), (22815, '74db38e2-08ba-488a-9ea5-7bc39c8ea15d'), (16167, '2178036f-632c-47f1-bd2c-49882a635406'), (17705, 'fe0b5515-c6a2-4316-b6a4-97cb643fb598'), (16178, 'c8a23931-5f9c-441f-a223-3dbfd115498c'), (20278, '20a49944-5093-4845-b2bb-d6f91d60ee65'), (11575, '28589867-4683-4d14-8fbc-3442efb0da38'), (16186, 'd28a9bb2-1d44-4685-89a2-827e11eeb7f4'), (24379, '70fe8edf-d85c-4e28-b69c-0095888bb75c'), (20287, 'b9745a11-37a1-41f2-8092-7aad294368b2'), (5951, 'c708aa10-9c40-4846-9c1b-6b3f3bcd3981'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (30530, 'e5008ac9-7f83-4083-a94d-69eb3dc6e0ca'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (19781, '9f2a2233-03db-481e-90ac-428d888ec1f5'), (4426, '8099d9dc-e246-4cba-ab73-f704c0327af2'), (4427, '3a4777b5-58f7-4c71-994f-d317fee8f1db'), (14185, '5fbf3e82-047c-476d-b8e6-ef4f1870121a'), (17266, 'dc87794e-ee0f-42ab-ae4c-f8864c485519'), (30583, '70683c9c-bcb8-4652-9328-13d870fad3be'), (17793, '8e853aa7-59db-4aaf-b04c-fe7fa8b49fcd'), (11662, '928cd162-1e95-4a12-9fd8-d4d850d43a48'), (11663, '5ef12e91-f3a0-4bd0-a8e9-bccb08282751'), (11664, 'df5f018c-98db-46c8-9e1c-59df03701ef0'), (20881, '33e801e1-f227-425d-a8d8-54759c599993'), (16275, '74585de3-90e9-44d0-973d-2d2fd8ced55b'), (20896, '54b807b5-1a34-48ec-80e9-686c9508556d'), (20899, 'e77570b8-6051-4cdf-b2d5-bce5f86ecf00'), (20900, '1482385c-4de4-4a6a-abb9-eccea1d3fb5e'), (20906, '4c0585c7-5b68-4b5f-820b-740cca4e0187'), (20907, '789c7a86-81f9-481b-a930-b5e65873ccf1'), (20908, 'ee5a5b82-d78b-40de-b0b2-ed026ae576d3'), (28590, '0882d2dc-76a2-4d07-be9a-e555af910496'), (3502, '18a2ff4c-5b6b-451c-ad35-a358c425930b'), (12208, '21c7841f-1c61-47aa-a203-fcef1d8034e2'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (25527, '6370c66c-4f80-4bb8-aa8e-1c55bf8f87da'), (25532, '04fba18e-d8fb-4bbf-8469-407f3b8a3ca5'), (25536, '218027de-680d-422f-b703-435d44a91ce5'), (25537, 'c395a1e1-9e7b-45e2-9950-903e9a31b1f8'), (25026, '9e3a9873-ca79-49a5-a14e-a0cfd408d99d'), (20932, '79a5d60b-5251-49a9-a1d1-cc6eebb75e0d'), (25541, 'c23dff23-c24b-406b-aff6-016be08df943'), (20939, '8ede0b48-613d-4eb4-ace4-a4fa4749ce80'), (21979, '16fe4c5a-614e-4bf2-8c5d-dd6054dea3f9'), (4067, 'db389377-70fa-4887-84a7-4ea544f7efea'), (30181, 'cb4670d6-2bec-4922-9873-7da6829bc6df'), (31210, '99578f19-a57b-424c-958f-b852e6b00a5b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

Next, regarding the debugging strategy, there are generally a few from which to choose from:

- Decompose the original model into many small testable components and run a forward pass on each of those for
  verification
- Decompose the original model only into the original *tokenizer* and the original *model*, run a forward pass on
  those, and use intermediate print statements or breakpoints for verification

Again, it is up to you which strategy to choose. Often, one or the other is advantageous depending on the original code
base.

If the original code-base allows you to decompose the model into smaller sub-components, *e.g.* if the original
code-base can easily be run in eager mode, it is usually worth the effort to do so. There are some important advantages
to taking the more difficult road in the beginning:
```

Next, regarding the debugging strategy, there are generally a few from
which to choose from:

-   Decompose the original model into many small testable components and
    run a forward pass on each of those for verification
-   Decompose the original model only into the original *tokenizer* and
    the original *model*, run a forward pass on those, and use
    intermediate print statements or breakpoints for verification

Again, it is up to you which strategy to choose. Often, one or the other
is advantageous depending on the original code base.

If the original code-base allows you to decompose the model into smaller
sub-components, *e.g.*, if the original code-base can easily be run in
eager mode, it is usually worth the effort to do so. There are some
important advantages to taking the more difficult road in the beginning:
```

Next, regarding the debugging strategy, there are generally a few from
which to choose from:

-   Decompose the original model into many small testable components and
    run a forward pass on each of those for verification
-   Decompose the original model only into the original *tokenizer* and
    the original *model*, run a forward pass on those, and use
    intermediate print statements or breakpoints for verification

Again, it is up to you which strategy to choose. Often, one or the other
is advantageous depending on the original code base.

If the original code-base allows you to decompose the model into smaller
sub-components, *e.g.*, if the original code-base can easily be run in
eager mode, it is usually worth the effort to do so. There are some
important advantages to taking the more difficult road in the beginning:
## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]

Although the `pipeline` is great for most applications where you need to quickly generate predictions, sometimes you'll need to access the model's logits (say, if you have some custom post-processing that you'd like to apply). To see what can go wrong in this case, let's first grab the model and tokenizer from our `pipeline`:

```python
tokenizer = reader.tokenizer
model = reader.model
```

Woohoo, it worked! Let's recap what you've just learned:

- The error messages in Python are known as _tracebacks_ and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem.
- If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs.
- If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue.
- The `huggingface_hub` 
// 🤗 Hub?
library provides a suite of tools that you can use to interact with and debug repositories on the Hub.

Now that you know how to debug a pipeline, let's take a look at a trickier example in the forward pass of the model itself.

## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]
 
Here is the question: What debugging technique is recommended for resolving 'AttributeError' during model forward passes?","The provided context does not specifically mention 'AttributeError' or provide a recommended debugging technique for resolving it during model forward passes. The context discusses general debugging strategies for models, such as decomposing the original model into smaller testable components or into the original tokenizer and model, and using intermediate print statements or breakpoints for verification. However, it does not provide a specific solution for 'AttributeError' during model forward passes."
How does the Datasets library handle data stored remotely?,"[(2561, 'a241eea3-9d38-4b32-9dc5-392dd9f97ef0'), (24583, '3e20db8d-779d-4bed-9130-2436441e7181'), (7179, 'f7a83da8-fb6a-4d58-92b8-88c2a5406418'), (15376, '18b4fd51-c4a0-4806-9a8c-66bd3fb7b887'), (26641, '972af779-bf9e-4865-bebd-97def3d5dd6d'), (23575, '3cedbab4-4057-4eca-9e31-44fc411ca9a2'), (23065, 'd98cc50f-6b46-43db-a019-c4f0aea3d979'), (8224, '44baca0d-26b7-42a0-9b85-bb757038c40e'), (31276, 'bd8aa051-fb07-44dd-a1a4-82bc82c72e46'), (20531, '58733ce5-d6e4-4037-be5a-f82e81ea407f'), (23605, 'cb4e48ed-d022-4aac-ae79-8303f065e1e8'), (20543, 'b5986ba7-30ec-4955-ba6c-72d1afd89b5b'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (25673, 'eea35395-f70f-4040-b8c6-591cadd77b52'), (5707, 'f1652f24-131f-48b0-a83d-0e7049f1189e'), (27735, '2e00db33-c75a-4cb0-b29e-68bc4acce90a'), (24156, 'cfb1685e-60cf-4fb0-a3ef-a2470dbb5881'), (24157, 'befedd3e-173a-4c86-9a3f-4d163de16f99'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (11877, '6269ad60-72d2-4e18-81da-010b5682cff4'), (6766, 'e35d1260-f910-44b2-a82d-648bc59cb455'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (11393, '9930410b-3008-41e2-9bc0-7dec7e568ed0'), (130, '426de6ec-a6b0-421b-8201-d87a7c67d0be'), (129, '76f26c3c-c4f8-4956-9bd7-e5dc0c2aef83'), (4741, '210088c4-7fef-45fb-b3bd-ba5759a63329'), (11398, 'c896d600-5e2c-467c-87a5-e393c3572337'), (3734, '7bbadbb5-638d-46a5-981d-f293cc3e6d0a'), (21657, 'caaddc26-338c-4b5f-89ab-bc044b94afe4'), (24734, 'f3f60e0e-fe06-4732-8b9a-79db264b56f1'), (24735, '6083df3a-9308-4afa-8346-92f15fcc248a'), (24736, 'dbfea546-1d21-471b-a468-ca263a9274fc'), (5801, 'e6df3fe7-f709-49e1-88a7-c4beb3a53bc9'), (15020, '37820be1-34ef-4e05-8b4f-880db34fb455'), (22713, '31320a45-af6a-4672-b522-8677e668387b'), (5313, 'e3673df9-36cc-476d-b755-3bf1fb595ebf'), (5327, '7bb56183-8dbd-4678-987c-44526c61ed91'), (5328, '7e28df2f-1656-43a9-b36b-adab211aeb45'), (5329, '08880c56-ac91-49cf-9b42-a59277ab2e37'), (9936, '9c48fcdd-c609-463f-ad7e-442feef8c8d5'), (31446, 'c4e8f3c8-49c2-4215-8868-959cf5b4ecbd'), (5337, '6ac0cb69-eb3f-4938-b193-a28613826b62'), (7386, '1e75d6ef-eb2a-47b6-8bf4-a1960ff737f8'), (7389, 'b481fda8-b5de-4d04-8ec6-e36a9f1f4039'), (24798, 'e0f48390-3fe1-4f09-a5e1-dd6f48593b5a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (20703, '185d614c-743f-4b53-9078-c44a17e5c1ae'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (20704, 'c152cb8b-269f-4a6c-abcf-20254a409de0'), (7395, 'e6cbeb63-c998-433d-b0b3-afb65bf27b67'), (13028, '916bc188-accd-498f-9b24-b64e40ed6c6a'), (17128, 'b8f3678a-4561-4c83-b4eb-b6617ffedb76'), (23274, '20ba3cf8-d2ff-4e5f-8b8b-323410a96fc3'), (17130, '881bf517-ab0a-4a1f-a757-ecf13e903b3c'), (3312, '287caacd-a3a6-4b79-9882-1292d43e4c5b'), (6384, '690dfc1e-5a31-46cf-bdce-a46b5a2b71a7'), (13043, '29a53aa4-5747-4d42-81cc-a011a8d9e69b'), (15603, '171bdfc7-bd9a-42f7-bbfc-146da4232795'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (23306, 'ceda3f28-4ba3-4ca6-84a7-a4b914fd8d4f'), (30989, '8ab696e1-efbe-45bd-8dab-02c487fa3171'), (8986, '4d8d186e-ecd5-4250-be5f-bba02de56f71'), (24347, '10d68075-9bb2-41df-a0e5-357873db4b4e'), (21789, 'f348a103-0156-4f06-a4aa-5eb608e899a5'), (21790, 'ad9580c8-18dd-4498-9dc7-b2dd03de689c'), (21791, '4f692aeb-3d3d-4422-9f32-e5a2985e2585'), (22304, '3897446b-d474-4a3d-9db5-5b91924cce5e'), (26399, 'ec38dc22-9378-4870-8e6b-47fe916cdfb0'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (26398, '2e0951bf-7007-4f4e-88f6-750d405238ad'), (21797, '70fd8a17-39ef-492b-9662-a96111502aab'), (24364, '268352af-6a3c-483f-8481-c7fecd64ff92'), (22317, '0daa3d28-92e3-492b-965c-e4ad098bc076'), (15669, '0256580b-34ce-4e4a-88aa-42403eb7a050'), (3385, 'd1c40df7-f6d3-4137-bd79-ca1352f9aa57'), (9538, 'cd1c44cf-83e6-42b7-b1cd-753b168bf055'), (835, '90be4731-3d87-4691-9bc4-3013cc2baba5'), (834, 'a18cfcba-27e0-41eb-b126-fe98d29828f6'), (1354, '63a9f049-ffdf-469e-a0ec-63eb7a813f54'), (15181, 'fb7c5366-41d1-41cb-9f21-6d3b57cf1e94'), (14165, 'f8e81797-b080-4a41-9f82-86eea16eabe8'), (9559, 'a8284fc7-f6b2-4800-baa1-e5b290b17b91'), (7010, 'a1650e86-e351-4bde-bd80-a0933a6b7661'), (7012, 'cf6d1616-0228-41da-ba47-d641e8747df1'), (15217, '77662209-3d2c-443f-8e8c-3085e7ecc6bf'), (7029, '00a05e41-17b7-4145-9888-ba014f3c3a94'), (29564, '68c48bc2-1c96-4e9d-ad94-26a1ae6dab76'), (31110, '9726dad8-10ae-460c-94fa-9e76f693eb05'), (31111, '15a82996-f78b-40ac-9919-f6f0307c152b'), (31112, 'ca7ab14c-5b6d-4313-b660-ca64236d08bf'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (31114, '8b46ee59-f8a3-45ee-a0cf-e6a579991e29'), (8587, '5dfac67b-30fa-4e8d-b899-f2fb32fac46c'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (24975, '84ec31e0-6e0d-40d2-aeed-7c845e768cb2'), (8591, 'da53a557-9d2c-49b1-976b-aaa341d7ad09'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (27557, '7dffbab4-38e8-4f8b-a105-0944ff6fb066'), (26031, 'f7a849c8-18fd-4d0a-8b63-6ae8789656a4'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (12742, 'f43f21cc-4dab-4dab-ac40-694072cdbeb9'), (7625, '68f9e1d3-8747-4f48-ab13-1d6a4a4baffb'), (5578, 'a60914f3-4328-43c1-b670-4b7d01e2fd74'), (12748, '7e2f7e69-f15b-4724-b46e-41d6bb2a1491'), (16850, '8d4c8a3e-04e1-47df-82a1-1b7d7af6968c'), (16851, '0edad950-cf69-4d0c-8d21-bba90d48a71f'), (16854, '2f5366a7-967e-48fa-983b-e0965294b7f4'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (16855, '566c4392-0a1c-4d30-aee7-cd3c019e10ea'), (16861, '214b51ca-08fc-4df6-918d-c7e539d37ce9'), (16865, '8458e40d-8dcc-4644-85d2-3e073e283055'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (20465, 'a8ced92a-342e-466e-8c4b-8769775c1b58'), (20467, 'd0074f05-bf62-4e31-acfd-177faf27318a'), (13814, 'd8be2885-e7f0-4d64-973b-c7c6e554c60d'), (13819, 'b5bed38d-29b0-4d78-84a3-3a189d2995bc'), (12798, '3d5604e9-6c18-43b1-8c57-99abd2a1973c')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: aving and reloading a dataset. In this video we'll take a look saving a dataset in various formats, and explore the ways to reload the saved data. When you download a dataset, the processing scripts and data are stored locally on your computer. The cache allows the Datasets library to avoid re-downloading or processing the entire dataset every time you use it. The data is stored in the form of Arrow tables whose location can be found by accessing the dataset's cache_files attribute. In this example, we've downloaded the allocine dataset from the Hugging Face Hub and you can see there are three Arrow files stored in the cache, one for each split. But in many cases, you'll want to save your dataset in a different location or format. As shown in the table, the Datasets library provides four main functions to achieve this. You're probably familiar with the CSV and JSON formats, both of which are great if you want to save small to medium-sized datasets. But if your dataset is huge, you'll
To handle these large datasets, the Datasets library is built on two core features: the Apache Arrow format and a streaming API. Arrow is designed for high-performance data processing and represents each table-like dataset with an in-memory columnar format. As you can see in this example, columnar formats group the elements of a table in consecutive blocks of RAM and this unlocks fast access and processing. Arrow is great at processing data at any scale, but some datasets are so large that you can't even fit them on your hard disk. For these cases, the Datasets library provides a streaming API that allows you to progressively download the raw data one element at a time. The result is a special object called an IterableDataset that we'll see in more detail soon. Let's start by looking at why Arrow is so powerful. The first feature is that it treat every dataset as a memory-mapped file. Memory mapping is a mechanism that maps a portion of a file or an entire file on disk to a chunk of
<Youtube id=""HyQgpJTkRdE""/>

## Working with local and remote datasets[[working-with-local-and-remote-datasets]]

🤗 Datasets provides loading scripts to handle the loading of local and remote datasets. It supports several common data formats, such as:

|    Data format     | Loading script |                         Example                         |
| :----------------: | :------------: | :-----------------------------------------------------: |
|     CSV & TSV      |     `csv`      |     `load_dataset(""csv"", data_files=""my_file.csv"")`     |
|     Text files     |     `text`     |    `load_dataset(""text"", data_files=""my_file.txt"")`     |
| JSON & JSON Lines  |     `json`     |   `load_dataset(""json"", data_files=""my_file.jsonl"")`    |
| Pickled DataFrames |    `pandas`    | `load_dataset(""pandas"", data_files=""my_dataframe.pkl"")` |
oading a custom dataset. Although the Hugging Face Hub hosts over a thousand public datasets, you'll often need to work with data that is stored on your laptop or some remote server. In this video we'll explore how the Datasets library can be used to load datasets that aren’t available on the Hugging Face Hub. As you can see in this table, the Datasets library provides several in-built scripts to load datasets in several formats. To load a dataset in one of these formats, you just need to provide the name of the format to the load_dataset function, along with a data_files argument that points to one or more filepaths or URLs. To see this in action, let's start by loading a local CSV file. In this example, we first download a dataset about wine quality from the UCI machine learning repository. Since this is a CSV file, we then specify the csv loading script. This script needs to know where our data is located, so we provide the filename as part of the data_files argument. The CSV
The dataset is generated with a Python generator, which doesn't load all the data in memory. As a result, the generator can handle large datasets. However, before the generated samples are flushed to the dataset file on disk, they are stored in an `ArrowWriter` buffer. This means the generated samples are written by batch. If your dataset samples consumes a lot of memory (images or videos), then make sure to specify a low value for the `DEFAULT_WRITER_BATCH_SIZE` attribute in [`DatasetBuilder`]. We recommend not exceeding a size of 200 MB.

## Maintaining integrity

To ensure a dataset is complete, [`load_dataset`] will perform a series of tests on the downloaded files to make sure everything is there. This way, you don't encounter any surprises when your requested dataset doesn't get generated as expected. [`load_dataset`] verifies:
 
Here is the question: How does the Datasets library handle data stored remotely?","According to the context, the Datasets library handles data stored remotely through its streaming API, which allows for progressive downloading of raw data one element at a time, resulting in a special object called an IterableDataset. This feature is useful for handling large datasets that cannot fit on a local hard disk."
What is a unique benefit of using PatchTST for self-supervised time series pretraining?,"[(5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (5135, '8bb2ea1c-369d-4db0-ab7a-127366db9831'), (24600, 'd44ed870-3d04-435e-a54a-4e35c0e1d296'), (21027, 'ae9699a9-7d18-4dd8-91fe-c31dd77b8e6f'), (22063, 'b6a4b9c4-c683-4973-b991-8c2cb24c7519'), (19002, 'b4b151aa-98d0-40e4-847b-3faecb435a73'), (25147, 'a2669d4a-4881-4334-8052-9102cf1a1249'), (25150, 'c125fa87-ac9a-4bb7-9926-8dc62c5fb983'), (19014, '28a449dd-bfb5-4851-90ff-3b2cd97264cc'), (25162, '946c9ec3-717f-4307-aad1-37aac459e5a1'), (29776, 'e3203540-4467-4975-8a4a-3a14658f4f60'), (25171, 'da15400c-495a-4c04-a4f7-bc0e717aa6a8'), (19540, 'b1b16778-0b99-4188-bc83-514da8f56532'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (29800, '21cb357b-9e31-4411-852e-3c3ce9bdb3e7'), (29805, '02c5425b-9b35-42ac-93c9-203e9cd34734'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (21123, '2e262a5c-5f46-42e7-980c-481e0e6534c6'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (21128, '1b0739a8-867b-4a25-bf9e-966b397ff990'), (12937, '292ea5f5-80e1-456b-86e0-34464796bb4b'), (21129, '30b7dc88-1489-428b-b8cf-8404a7481499'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (15004, '5d426919-bf3f-42f7-b626-5dd6e9e94acc'), (20649, 'b99dfbb1-3bca-4177-9b1e-d01ea82d9060'), (15028, '86c91e1c-d5df-4053-b7f8-f475c47edcf1'), (13505, 'f66bf4af-bf2d-468b-bc25-83e2c5c904ba'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (11465, '2352d31d-4cb6-43cc-a668-77747f9ff8db'), (25290, '16b47a24-aff2-46e9-8b5f-87234f7699d4'), (16079, '526ce690-4958-4079-a0ce-055d3e006d6a'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (18138, 'bc713553-0249-4017-81c9-8b7d7b301725'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (9445, 'b6bd1183-24ee-43fb-9575-54230c8c4579'), (25327, '5e7e4a85-eccd-4ed0-b6db-bafdf08facce'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (13556, 'f047eb0d-f7be-4bf3-925a-566417d8ed33'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (254, 'eb6fe67a-123d-43fd-9e73-96693204d7dc'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (26379, '1ccd1f17-c6b7-48d1-9592-f50598ebe298'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (26383, 'ae72b8dd-bf79-46fb-a35d-9e01211070c3'), (12049, '9ebd4171-a0ce-40d1-9883-c37ead9490d1'), (787, '17f95d66-cbff-4264-9f2e-0d77da353822'), (792, '999bbe02-98fa-4c04-8bcd-c94434420a04'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (5407, '083fe81d-625c-4b43-86c3-15a5b61f9c71'), (28960, '1040c198-e0d5-401e-a9b4-fc013110dcd0'), (5411, '4fafe081-7327-4280-bc98-64f9f75d9966'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (23869, '26cd5348-6284-4c3b-96a3-3840b630697b'), (4421, '95a1272e-1f19-494f-96d5-f36f9a919a0a'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15696, 'f5c82a25-56d1-4160-a926-0d697826a6bb'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (8021, '2eb3eb4c-195e-4237-b854-70714e8cde11'), (6997, '35d0066b-d1f2-45f1-a295-d792e0dda90d'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (22877, '79c5fcb1-22aa-4ce1-be25-33a02d2b8e03'), (4966, '42e714b3-cfee-4118-a976-588eded30493'), (4967, '43778b66-3e71-4fd3-a9bf-f2cf850f713d'), (1898, 'd0cd0bd7-ad19-4c35-a562-ddc9c9499a18'), (1899, 'f888ee25-af78-4996-8147-194908c2b4b0'), (1900, '9ac81dc8-63e8-4bb5-b1a2-66f999fdf9c9'), (25457, 'bfcd5d3f-52bb-4950-9929-e559851e2d65'), (18803, 'e7da8b1d-636a-4b36-887e-45035013d1e5'), (23941, '4552e7c6-32c1-47d7-9c48-1cd66703f9a7'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (2964, '2e5d286b-3b13-48e8-895f-ec256ffed18c'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (11684, '9cfcfbdb-6441-4284-8ef3-d14780da146f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (4530, '006e7cf7-47b3-4211-9fbb-c2a986881355'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (22456, '55e20e3e-6f8f-413c-8a0f-663f7d1df98d'), (9144, '3be21646-2d02-41ae-b3a2-d0cf74a4654a'), (29634, 'd756cc17-3381-4c73-99de-56f9d281f772'), (29143, 'bc13a857-206c-4b91-b966-9c4913882054'), (29146, '2c98ac16-543f-4c54-b75d-62f161c19dd7'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (14320, '18b2a378-fe08-4e21-bc4a-6b7b9c5ad71d'), (31218, '453bb283-ba88-4589-860e-7e2e4fa22e63'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (9205, '813fcccf-2923-465b-ab73-2cfe6cc521fd'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
2D interpolation of the pre-trained position embeddings, according to their location in the original image.
- The best results are obtained with supervised pre-training, which is not the case in NLP. The authors also performed
  an experiment with a self-supervised pre-training objective, namely masked patched prediction (inspired by masked
  language modeling). With this approach, the smaller ViT-B/16 model achieves 79.9% accuracy on ImageNet, a significant
  improvement of 2% to training from scratch, but still 4% behind supervised pre-training.
*The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et
 
Here is the question: What is a unique benefit of using PatchTST for self-supervised time series pretraining?","Based on the context, a unique benefit of using PatchTST for self-supervised time series pretraining is that it can attend longer history, which is not explicitly mentioned as a benefit of PatchTSMixer or other models."
What common debugging tool can assist in interpreting stack traces effectively?,"[(7168, '310c37ce-9a1b-4396-babb-63976d8a23d8'), (7169, 'e1c4104c-5431-4c27-9a3e-aef70bd80fa5'), (24066, '952c921e-525d-4b47-9343-9bcee49f6f61'), (24067, '157c15d9-4820-40cb-9366-e7b656d413a7'), (24068, '70ef38f1-9271-4e67-b292-a311c28fb5c9'), (24070, '371ec714-93b3-4dad-a32b-b9909a7cb278'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (31258, '7f34d34d-e04a-4ab0-9cf6-5d23de269e84'), (28, 'e85e3119-1ad5-46de-8468-54238b531847'), (4637, '437d2b41-0e03-45cf-8918-97ac6e3f3b81'), (17443, 'bd891b6b-f323-49c6-b503-c3e499ebcdd1'), (27683, 'c32ad975-8cb8-4b0f-a698-353380f47db2'), (11303, '5eadded2-f72a-41da-a938-6bf03f630f7e'), (11304, '87595938-5fb8-42be-9a54-704aaa91dd48'), (6699, '58ca5c49-6634-4ac6-b530-96b323e1f021'), (6700, '15379fc5-6b8b-43ad-b0c4-76dff124bcbe'), (6707, 'd1444353-2bc3-4e99-8b60-361b06b19fbc'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (16451, 'ef3d262a-39ec-48e4-9a8c-6e3feaa26040'), (6723, '01ac9d03-828c-47a2-95fc-a16de8fc474e'), (30278, '9a49a14a-c17e-40eb-b72a-5621d602f5b0'), (6731, 'a4c86bc7-662c-4b5d-9cbb-3967e69de023'), (19536, 'fbbacd1f-478e-4c12-96f1-94f962921780'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (6749, '81c8a3fc-8f17-4481-92ab-f3d126518cc2'), (19549, '09c34333-c237-43a8-9e29-0dda3e054d55'), (6752, '5064067c-8a7f-4f23-ae5f-0b016bc7ca69'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (6756, '66d0932f-7334-4856-b0d9-2c287e50b5f6'), (16486, '44b89854-89ae-4b45-9b98-62b3d9edc5a7'), (9832, 'bdc6bea1-7f20-4300-aae9-3f7ea03a6783'), (1159, 'ed1c0f2d-3594-4151-ba80-832c5099d69b'), (1168, '0e3b941c-35b9-4bc5-b4dc-9c3309e9f562'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (10387, 'fd029e80-f142-427a-b5b0-aab172a1ebe3'), (20117, '344e2a72-849d-4598-a5f5-e5a7a605e227'), (30878, 'a2aa9838-7a0f-4e8a-903a-f685abf3448d'), (30879, '2ac848c0-3793-4511-8a97-46726b48dae5'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (1205, '19799104-79e5-4b3b-bcc1-036b3fd80da5'), (1207, 'ba67be42-b645-475b-a449-454ae493e9ef'), (5306, '9ab1539d-84b1-4f98-833d-f4707d58c3ae'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (14524, '9edaacf8-e6a9-45da-9168-efea104fecf4'), (30908, 'f813e9a8-eab4-4c6c-b738-b58567335fa5'), (30910, '562b2c34-aa31-45cf-a703-d544e0641ad7'), (1213, 'bec86d0f-9e27-4271-bd19-fa2a88278612'), (14523, '7eaad718-02e7-45b8-a664-b3c1b35d6c39'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (30916, '83870699-d55d-4da0-9257-d46016695c8c'), (30919, 'ec3af9bb-c9be-4eca-813b-d91f994dd8af'), (19655, '6f5971e7-8354-4d7e-aec5-3ee6c1e98e7b'), (12999, '8c0a8acc-d1d0-4543-ae8f-69c312cd7840'), (6862, '0fb37f9d-1ebb-42b9-95ac-1cd17654ef37'), (2777, 'e5b7258c-391c-4fc5-b42b-ecccc1585117'), (20186, 'e4800925-3dd9-4604-b5d2-755f623ef3b9'), (14557, '419c5c32-2b03-40c2-9cf6-ed174b2b64d9'), (6369, '2cd13d66-4109-4355-a19d-7b7cb03135e6'), (22774, '7bea6ef0-2f31-426f-a550-040a93f64ded'), (12026, '0c59c0a7-bb17-41b9-ac73-98736a9b573e'), (11027, '65243695-8d6f-4826-9b62-78e22d96536f'), (23849, '4484ca3c-0f23-423b-b0f2-bbe12951cc16'), (20273, '812f2b38-d4e9-4406-a021-ce5f9734d8c0'), (24372, '4f97df48-3209-488e-a375-0f041a0fc31f'), (11579, 'e2b354ec-02aa-4b95-9146-bb9e9a359a73'), (19261, '828a7548-bebe-48da-bd43-915ef22bbb9b'), (22340, '2ae2fc34-8a2b-4041-853c-00202430737a'), (29510, '16398d72-8143-4b3f-9011-1330c903d4b0'), (20298, 'cf07b1e2-6cae-4114-989b-151be05e8ba7'), (4969, '277ff2a3-6bfe-49ef-9e45-1583b28de893'), (13690, '02b73842-811f-4da8-8c9c-dbb8545cab6e'), (13696, 'b5412eb0-f2c5-4077-b7f1-cce90588570b'), (27524, 'bc1716c1-4f72-45b3-8221-5f45a4dfa573'), (17796, 'ef4db9a3-d486-43f2-8c7c-a26573cc88ea'), (7570, '30cef6d0-64c6-47bd-8d5c-6aeb6012e5a8'), (7571, 'a17fc819-2d0a-44f4-8d2d-62dff5461fbe'), (7573, '41bb37f3-e799-43d1-acf8-d9687b7f6f7e'), (20896, '54b807b5-1a34-48ec-80e9-686c9508556d'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (20898, '52c8c849-3c8c-475d-89ee-582dfad7ac0b'), (20900, '1482385c-4de4-4a6a-abb9-eccea1d3fb5e'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (25002, '22fbe702-faab-4fce-b201-61d82588743f'), (20906, '4c0585c7-5b68-4b5f-820b-740cca4e0187'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (16301, '77400005-3388-49a3-86df-4cd1d768c650'), (12205, '70c5332b-6b84-438d-beff-bca04129bd86'), (16299, 'e76f46a8-1a2f-4f3d-a804-bdce969dbb81'), (25009, 'e191215c-3849-428b-93c3-94c92cc197bb'), (6578, '3015fd28-83cd-4b3a-84a3-76782d312101'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (25010, 'bc1b31d0-5d61-4c93-b5df-6b9cd91fcd26'), (25011, 'e16ec568-e896-44e7-95b7-22fd47a75a61'), (16308, 'e100c0a7-79a3-4834-8539-b23456ac7b3a'), (16309, '2135aa54-c407-499c-b2a2-0189de3220a5'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (16310, '4d0340ca-2008-4c33-93d4-bf10fff43a0f'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (6587, '93e5fc34-4543-4839-b5b3-c7bf2b203c3e'), (19388, '141d4b90-0e49-4309-98b8-fb6890009430'), (6589, '3ed8fde9-70c5-4727-bffd-b1343bd706cf'), (1470, 'b2d459cf-a7d0-4834-8979-cc59e274af59'), (25023, '69d90df0-d143-46ab-8898-0ca3e27188d8'), (25024, '24213a1a-72e6-48ed-b43d-8b482efb632d'), (14777, '4164c6cd-2dbc-4873-b6de-934479f5e6ae'), (25026, '9e3a9873-ca79-49a5-a14e-a0cfd408d99d'), (25539, '19301fff-8b06-445d-b54f-970e21aa7bc1'), (7613, '2f0d0b7d-35df-48b0-a562-dfc0a78e5974'), (25541, 'c23dff23-c24b-406b-aff6-016be08df943'), (30152, '5ade3db8-c692-48c8-b807-fb6a9c7027cd'), (30153, 'e761ff90-a3a2-4d6a-bc33-92f64c872625'), (30154, '5e9431bd-c8d9-4fb9-ba40-fb86a51de0bc'), (30667, '5ab35ed2-c28e-4c60-be97-eb6e05d3cce4'), (10701, 'bbd5a305-164e-4ed1-afba-5cdf2aeca8ce'), (10702, '12c8b32e-bc12-4269-9240-fea9ee3c4b99'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (3047, '613dfbaa-b77d-455c-a74c-88e0a04c24de'), (3048, '8a8682d5-da84-45a3-aa2a-a9304b4e7571'), (1516, '18d0dd47-15ad-4d34-865a-9277bc2db97c'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (13295, 'ef2895be-813f-42aa-bbcd-b68c45fc6027'), (14326, '394d8e4b-4b37-40df-975b-71f00ac59dbc'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (7166, '22b64645-5204-4309-9f1d-a01650ba218e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

Here you will get a huge number of frames dumped - as many as there were forward calls in your model, so it may or may
not what you want, but sometimes it can be easier to use for debugging purposes than a normal debugger. For example, if
a problem starts happening at batch number 150. So you can dump traces for batches 149 and 150 and compare where
numbers started to diverge.

You can also specify the batch number after which to stop the training, with:

```python
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3], abort_after_batch_num=3)
```
```

Woohoo, it worked! Let's recap what you've just learned:

- The error messages in Python are known as _tracebacks_ and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem.
- If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs.
- If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue.
- The `huggingface_hub` 
// 🤗 Hub?
library provides a suite of tools that you can use to interact with and debug repositories on the Hub.

Now that you know how to debug a pipeline, let's take a look at a trickier example in the forward pass of the model itself.

## Debugging the forward pass of your model[[debugging-the-forward-pass-of-your-model]]
to go a little deeper to debug the problem. Fortunately, you can use the Python debugger at any time you get an error in a Jupyter Notebook by typing %debug in any cell. When executing that cell, you go to the very bottom of the traceback where you can type commands and you can type commands. The first two commands you should learn are u and d (for up and down), which allow you to go up in the Traceback or down. Going up twice, we get to the point the error was reached. The third command to learn is p, for print. It allows you to print any value you want. For instance here, we can see the value of return_tensors or batch_outputs to try to understand what triggered the error. The batch outputs dictionary is a bit hard to see, so let's dive into smaller pieces of it. Inside the debugger you can not only print any variable but also evaluate any expression, so we can look independently at the inputs or labels. Those labels are definitely weird: they are of various size, which we can
problem. Fortunately, you can use the Python debugger quite easily in a terminal by launching your script with python -m pdb instead of just python. When executing that command, you are sent to the first instruction of your script. You can run just the next instruction by typing n, or continue to the error by directly typing c. Once there, you go to the very bottom of the traceback, and you can type commands. The first two commands you should learn are u and d (for up and down), which allow you to go up in the Traceback or down. Going up twice, we get to the point the error was reached. The third command to learn is p, for print. It allows you to print any value you want. For instance here, we can see the value of return_tensors or batch_outputs to try to understand what triggered the error. The batch outputs dictionary is a bit hard to see, so let's dive into smaller pieces of it. Inside the debugger you can not only print any variable but also evaluate any expression, so we can look
```

### Specific batch absolute min and max value tracing

The same debugging class can be used for per-batch tracing with the underflow/overflow detection feature turned off.

Let's say you want to watch the absolute min and max values for all the ingredients of each `forward` call of a given
batch, and only do that for batches 1 and 3. Then you instantiate this class as:

```python
debug_overflow = DebugUnderflowOverflow(model, trace_batch_nums=[1, 3])
```

And now full batches 1 and 3 will be traced using the same format as the underflow/overflow detector does.

Batches are 0-indexed.

This is helpful if you know that the program starts misbehaving after a certain batch number, so you can fast-forward
right to that area. Here is a sample truncated output for such configuration:
 
Here is the question: What common debugging tool can assist in interpreting stack traces effectively?",The common debugging tool that can assist in interpreting stack traces effectively is the Python debugger.
How does using JSON Lines benefit dataset preprocessing in NLP pipelines?,"[(13825, 'f1446f89-52a4-4aaf-ab1a-9b92f5010ce7'), (19459, 'cb31efe8-a229-4e7b-9ab4-f850276a68c2'), (1540, '6087e6c1-790b-400f-af65-2510f2275487'), (19463, '72f624a9-b037-428d-9837-de3484ab1799'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (11792, 'e3cb7b34-455d-4c33-be90-a3547abf420d'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (6186, '7d25cfc6-c134-45b5-b7ac-649d42e1558c'), (8235, 'e0e09663-4604-4e22-b4bb-bb1496379185'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (8239, '601c3e59-2039-4a07-a2ff-f1e97ecadabd'), (6192, 'ce855971-8e34-465c-bc05-66c10ee3ec5a'), (6193, '64c18491-4a9d-47a6-9c49-3936a488647c'), (20023, '75c63aa3-1c17-4390-90d8-15e465f0a22d'), (25145, '20079c45-5e6f-4adf-8f78-0c23c277da08'), (24126, 'f6d0b6d4-13c2-4d59-a59d-6b341a5a1508'), (17476, '622f9bf0-744a-4fe6-b4e7-eeebdc82d918'), (3679, '8e31b4d7-de14-44b3-be8a-7236eb150661'), (13422, '7fcc282e-51e4-4714-887d-b6075a5d4b55'), (7280, 'f968b214-cb48-463f-882e-be4af04c57b3'), (7282, 'b97fe108-9df7-46b1-a671-c7446a0ce2cc'), (8819, '2703836a-9d2a-4d76-90f6-34224a5780b1'), (4742, '5857a040-9dd1-4080-acf9-bddc350100ea'), (4743, 'b64a40eb-e329-46ff-b747-342002c50922'), (24204, '24972dc6-4053-46d8-93be-df0faad47b39'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (13979, 'de950627-bfa4-44a7-aded-c6a93c8ee195'), (13980, 'a1b5a2e5-b7b6-4914-b2db-b07eb318ca48'), (18590, 'ac6b59ce-1adb-4f5d-b5cc-890731055b35'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (9381, '1695844a-c3fc-40f7-8e86-4f2d6eea7efa'), (11435, '35ee7661-d4c6-4384-bb77-e40ec77986b8'), (9390, '8ca789f9-6a64-4dfd-a067-8a38701e9237'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (7352, 'af9cca0a-f231-47a9-b943-dc040ea91c49'), (24763, '81ad470e-e933-4a27-9979-d29627c04cf1'), (5309, '1063e5a8-266b-4339-bd57-f04a35f278f7'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (5320, 'b954f493-8fa4-4de0-b1f8-801bcad1c1a4'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (21197, '3b2fbad2-3fdb-4725-9c19-a5b8f139a1a0'), (19664, '2872ed56-b851-405f-8601-3748f47a00e1'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (21205, '6bda238b-e50a-4ace-95e5-877b1ae72029'), (28885, '11eba754-446a-42b5-b962-5c9bc4531b7d'), (28890, 'b1ae8ca7-d69b-4326-a81e-655300398e52'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (15581, '8a808209-2556-4ec5-bd15-34e03c26c682'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (31458, '772a8786-3e64-4959-84fa-931ce6254e05'), (24293, 'ee8be06f-b07e-40de-81fc-6549ce8b8cdc'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (15592, '481a6c32-3e99-4c07-a468-dbe480fef1b4'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (7942, '14990c0d-4b9f-4a48-b72f-ad5c83124c6f'), (3342, '40de6e0e-cd3f-4ac4-8b93-e0a427a98e4d'), (3346, '032420bb-66aa-4f3e-b636-fe7b4d17188b'), (21788, '76f89282-5e29-480a-9fb9-107b45af559d'), (8989, '28551de5-f06d-4b91-b2c9-f1fdcc62c353'), (13091, 'fd87ced8-d15d-4eb8-98f3-447a6c638420'), (26931, '3bc791e3-879f-48fa-a4a0-c5c426d6b558'), (28982, '50351ec7-ad8b-43e7-a669-2087f3d54e21'), (15672, '3c536d00-c1fc-410b-a634-96a25e8778ee'), (24890, 'afcbcb7c-ee42-4823-ad2c-9b6d06c0f04b'), (25914, 'd343a79e-38fb-4fce-9fcc-0d1e23c73b40'), (24892, '87b31d2d-afa8-4e15-8262-9ef33723c313'), (24891, '4d510e18-9e28-413e-a0cf-1a455cdee512'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (19788, '7899cbd2-e4be-4403-a2ea-4ddf86fdf820'), (27474, 'ef7b2c34-eec3-4ddb-8f32-6a85eff1331d'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (3925, '12548579-f219-484d-b120-fbfedb7bd71f'), (4438, 'c433e5f5-9651-489c-8a76-f04bf6884777'), (3926, '950bfb92-9345-4f9a-afbc-1e4393cf4f64'), (18265, 'f0ca8e07-29ec-4f8d-a2a7-1484fd1b7959'), (26969, '9feb9ebe-24da-4312-bfc6-1ea3e3d83468'), (26970, '56e38e53-1af9-4cec-beb0-886be736d495'), (3932, '40509c62-9846-48d2-8d6f-a3635dd9263f'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (31586, '4351b921-b4f2-4a0a-a126-07b9ddb21e7f'), (18276, '3c255798-f281-4027-9f8e-004acb7710ef'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (18286, '0aa69d94-c6a8-4e6a-a558-be1d6abd7fdb'), (31598, 'd3e6a577-47b5-4ed6-83a7-e22fb2d3c6c5'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (31604, '18698824-2c80-453c-999a-9edd11b91d34'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (24962, '5276ff68-1983-46e4-8f9b-e33eb6dc153d'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (9610, 'b094bdc0-3783-4e56-aa01-2bb05d7808ef'), (400, 'd8436d7a-0504-4d3e-8007-c5431d2d3b62'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (19870, '359a7e1c-0599-4279-9610-c730b74ad737'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (26018, 'f342cf73-c386-4093-99cd-e639ba2efb08'), (31141, '246a8167-3468-46a7-97ea-8eba392ffd1c'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (13219, 'f4a481fd-9e87-4669-93a5-4a2aedc5d286'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (5039, '52341562-e420-4ebb-baa7-503d3b27ab6e'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (17327, '99cc7628-65c7-4491-ae85-b9a882fc399d'), (26034, '8be7f373-dafd-44e1-9781-709da1732757'), (26036, '0e8e3f85-e18c-46e7-97ab-595f1fda1603'), (6102, '3165e349-5926-4468-ac0c-1f9d8959ed1d'), (16856, 'c092e317-6d17-4c0f-a54c-70332d7e4094'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (8186, '66d4be7e-e0f6-4443-b832-97821546eb78'), (25598, '8c2cf4ad-64ee-430a-8e85-335f69c1caf8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: `run_translation.py` is a lightweight examples of how to download and preprocess a dataset from the [🤗 Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.

For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets#json-files
and you also will find examples of these below.


## With Trainer

Here is an example of a translation fine-tuning with a MarianMT model:

```bash
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path Helsinki-NLP/opus-mt-en-ro \
    --do_train \
    --do_eval \
    --source_lang en \
    --target_lang ro \
    --dataset_name wmt16 \
    --dataset_config_name ro-en \
    --output_dir /tmp/tst-translation \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
```

### Data preprocessing
In the rest of this tutorial we will be using [CodeParrot](https://huggingface.co/codeparrot/codeparrot-small) model and data as an example.

The training data requires some preprocessing. First, you need to convert it into a loose json format, with one json containing a text sample per line. If you're using 🤗 [Datasets](https://huggingface.co/docs/datasets/index), here is an example on how to do that (always inside Megatron-LM folder):
```python
from datasets import load_dataset

train_data = load_dataset('codeparrot/codeparrot-clean-train', split='train')
train_data.to_json(""codeparrot_data.json"", lines=True)
```

The data is then tokenized, shuffled and processed into a binary format for training using the following command:
```bash
pip install nltk
cd Megatron-LM
python tools/preprocess_data.py \
       --input codeparrot_data.json \
       --output-prefix codeparrot \
       --vocab vocab.json \
       --dataset-impl mmap \
       --tokenizer-type GPT2BPETokenizer \
       --merge-file merges.txt \
       --json-keys content \
       --workers 32 \
       --chunk-size 25 \
       --append-eod
```

The data is then tokenized, shuffled and processed into a binary format for training using the following command:
```bash
#if nltk isn't installed
pip install nltk
python tools/preprocess_data.py \
       --input codeparrot_data.json \
       --output-prefix codeparrot \
       --vocab vocab.json \
       --dataset-impl mmap \
       --tokenizer-type GPT2BPETokenizer \
       --merge-file merges.txt \
       --json-keys content \
       --workers 32 \
       --chunk-size 25 \
       --append-eod
`run_summarization.py` is a lightweight example of how to download and preprocess a dataset from the [🤗 Datasets](https://github.com/huggingface/datasets) library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.

For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets#json-files
and you also will find examples of these below.

## With Trainer

Here is an example on a summarization task:
```bash
python examples/pytorch/summarization/run_summarization.py \
    --model_name_or_path t5-small \
    --do_train \
    --do_eval \
    --dataset_name cnn_dailymail \
    --dataset_config ""3.0.0"" \
    --source_prefix ""summarize: "" \
    --output_dir /tmp/tst-summarization \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --predict_with_generate
 
Here is the question: How does using JSON Lines benefit dataset preprocessing in NLP pipelines?","Using JSON Lines benefits dataset preprocessing in NLP pipelines by allowing for efficient and flexible data processing. JSON Lines is a format where each line of a file contains a single JSON object, making it easy to process large datasets in a streaming fashion. This format enables:

1. **Efficient data processing**: JSON Lines allows for parallel processing of data, making it faster and more efficient than processing a single large JSON file.
2. **Flexibility in data processing**: JSON Lines enables easy data manipulation, such"
What is the best approach to debug an ML pipeline while ensuring compliance with licensing requirements for datasets?,"[(29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (29201, 'cdbabce2-0910-47ac-93ec-2dc35c8b6b91'), (29202, 'a6666314-edd1-47cc-8628-335a50fe419d'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (30739, 'fda68c03-224d-4b2c-8385-15822bc392ef'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (19986, 'a48956b7-e680-4f75-8fd7-9bd7d7bf8e21'), (19988, '082a0597-a1f2-44ad-84e9-05168e283d72'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (13339, 'f046b9a1-daad-440b-8fa6-1a8de946d278'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (13857, 'eee70593-20c9-48e9-8ff9-15a701fce582'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (14382, 'f0a3cc11-b2fc-4b21-a76e-c0d46a55834a'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (28723, 'c23b045d-e37d-4922-ab87-54a26f4802b3'), (7735, '73b16f91-3f71-40bd-9699-6896aa1dd3b5'), (12354, 'fd8028df-5620-45c4-b917-7cb54c1b6133'), (9282, '91edce5b-dc46-47e2-85e4-33cf1d17dab7'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (19053, 'ae1e94a1-3807-46ed-85c6-ccce72cc33aa'), (1159, 'ed1c0f2d-3594-4151-ba80-832c5099d69b'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (20117, '344e2a72-849d-4598-a5f5-e5a7a605e227'), (10923, '48a8b0e4-0e9b-4f01-b296-83212b9301c1'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (24752, '3798bf75-5a2c-4b7a-9800-c75a034bfae1'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (20153, 'dcf1b3ac-5649-402f-a8af-0e99997d1e7b'), (697, 'a79771f8-9f63-4615-8faf-cca019cc8939'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (20671, '817cbafe-ab8a-4498-88c1-de98146869d4'), (20672, 'e610e274-c761-4856-9c20-1c74ba007cdf'), (11457, 'b6ea4cc8-fd8c-4df2-b3a6-eaa7852e8372'), (21187, 'd9dcef43-8125-461e-9c4f-131925ad7523'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16582, 'a5a53226-6dca-425c-afa6-94c7b4adf300'), (29896, 'b72ba5ef-9507-48a0-91d0-2fe3e2cc33f3'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (17609, '4cdc2094-1a54-40b7-9a6e-05ea32af5ee7'), (28880, 'c5576ba8-d702-4f10-b06a-87380bcb7a9f'), (24277, 'ed65358e-23d7-460f-9fcd-87b89b8b726b'), (21205, '6bda238b-e50a-4ace-95e5-877b1ae72029'), (17623, '9ffafb20-3956-436d-ae73-eb7132a07981'), (28889, 'a03452b3-d0fc-4ca5-a961-53dc3439cfee'), (732, 'd42b2bda-6ad0-4591-845a-4dd23901ca1a'), (28893, 'fc3d92d3-71f0-4842-97ff-435385c1ab3f'), (28894, '62d52bf8-8829-4280-8d8e-044fe915ed0b'), (12022, 'ca590bdd-6993-45bf-bc5c-6216ff9e670d'), (17658, '9d5fe55e-4bcc-4604-991c-a59dd47f9d33'), (7942, '14990c0d-4b9f-4a48-b72f-ad5c83124c6f'), (3337, '86cf17f6-cc33-43f4-8fc6-628d03c4520f'), (3339, '6c368a7d-8543-44b3-9993-4674ebdec341'), (16146, 'e100c4ec-db20-4e52-93d6-f9535f7f90b8'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (21797, '70fd8a17-39ef-492b-9662-a96111502aab'), (21799, 'eef8b3e0-a59f-4579-9fbe-b2fff100f9c0'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (11575, '28589867-4683-4d14-8fbc-3442efb0da38'), (2873, 'b3811c6d-da6f-4c9f-8885-cc9b2dbf1bc7'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (20298, 'cf07b1e2-6cae-4114-989b-151be05e8ba7'), (4427, '3a4777b5-58f7-4c71-994f-d317fee8f1db'), (4442, '274884d8-70a8-4a04-900c-0d6dc0e00410'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (31586, '4351b921-b4f2-4a0a-a126-07b9ddb21e7f'), (18276, '3c255798-f281-4027-9f8e-004acb7710ef'), (28517, '83d1dded-982b-449c-9af4-e5fecc3aac5b'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (31584, '08909b15-59c3-48af-b6fc-5206707e70f1'), (28522, 'd5bd37ca-d3b3-41a0-b14f-2a82a1f0daa1'), (31601, 'f5490662-af6a-4c95-883f-c30838703a4e'), (29689, 'deb041f0-74a4-4cfd-8792-9dd1e899718d'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (14725, 'f93b2fc4-f0b6-42bb-9b39-5f3d69f3047b'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (5509, '509fc88e-4d96-444c-8ea2-74d4ce6ee3d0'), (19857, '894d909b-b9a8-4c18-8789-710e8a2fd282'), (19867, '88ae508e-cedb-469a-b846-e494beb34b6b'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (2469, '24b5a6c3-9526-4bd4-b85f-cf7733f0a28b'), (17326, '2891f89d-3318-435c-816d-b7537b0b5338'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (25049, 'cf8bc526-5eeb-4029-aba2-8ec814ad0167'), (12765, 'a9a75a73-70f1-4e42-a8a3-2f8413929d72'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (1516, '18d0dd47-15ad-4d34-865a-9277bc2db97c'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (24560, 'ae4f8022-24f0-469e-9cb5-358e58b5773d'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (5625, 'c79981e0-e8ac-454a-a1ec-1e4b2c4b1ba2'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (5627, '79dad456-91d9-4b8a-8282-4c0a0cf743eb')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""L-WSwUWde1U""/>

The problem when you encounter an error in `trainer.train()` is that it could come from multiple sources, as the `Trainer` usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.
!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the ""License""); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an ""AS IS"" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
-->

# Pipelines

Pipelines provide a simple way to run state-of-the-art diffusion models in inference by bundling all of the necessary components (multiple independently-trained models, schedulers, and processors) into a single end-to-end class. Pipelines are flexible and they can be adapted to use different schedulers or even model components.
- **Open:** these licenses allow royalty free access and flexible downstream use and re-distribution of the licensed material, and distribution of any derivatives of it.

- **Responsible:** OpenRAIL licenses embed a specific set of restrictions for the use of the licensed AI artifact in identified critical scenarios. Use-based restrictions are informed by an evidence-based approach to ML development and use limitations which forces to draw a line between promoting wide access and use of ML against potential social costs stemming from harmful uses of the openly licensed AI artifact. Therefore, while benefiting from an open access to the ML model, the user will not be able to use the model for the specified restricted scenarios.
If indeed ML models and software/source code are different artifacts, why is the former released under open source licenses? The answer is easy, open source licenses have become the de facto standard in software-related markets for the open sharing of code among software communities. This ""open source"" approach to collaborative software development has permeated and influenced AI development and licensing practices and has brought huge benefits. Both open source and Open & Responsible AI licenses (""OpenRAIL"") might well be complementary initiatives.

**Why don't we design a set of licensing mechanisms inspired by movements such as open source and led by an evidence-based approach from the ML field?** In fact, there is a new set of licensing frameworks which are going to be the vehicle towards open and responsible ML development, use and access: Open & Responsible AI Licenses ([OpenRAIL](https://www.licenses.ai/blog/2022/8/18/naming-convention-of-responsible-ai-licenses)).
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""N9kO52itd0Q""/>

The problem when you encounter an error in `model.fit()` is that it could come from multiple sources, as training usually brings together a lot of things that you've been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `model.fit()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):
 
Here is the question: What is the best approach to debug an ML pipeline while ensuring compliance with licensing requirements for datasets?","The best approach to debug an ML pipeline while ensuring compliance with licensing requirements for datasets is to manually go through the entire pipeline to identify where the error occurs. This involves checking the dataset, data batching, model code, loss function, optimizer, and metric to determine the source of the error. Additionally, it is essential to review the licensing requirements for the datasets used in the pipeline to ensure compliance. This may involve checking the terms of the license, such as open source or OpenRAIL licenses, to"
Which combination of techniques ensures optimal time-series forecasting with PatchTST?,"[(517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (520, 'b101edb2-f02a-4bd8-b69e-0e3ba6a11f4d'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (12298, '75d1fbe7-d1d3-4868-b940-d25e740f5bdd'), (6209, 'dc759306-d7ce-4e78-b89d-4681bc9dd25d'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (6212, '9d2539d2-4c9d-4ea5-8dad-8105ad0d656e'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29771, '54c867d1-dd3a-4a9e-97f5-dac70011ef4b'), (29772, 'e9f0dc72-4849-4954-ab57-e595cbc0c79d'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (20570, '5dd4252e-a29c-4f30-ae23-7b7de46bb5a0'), (29818, '02cc33d8-0c79-466e-ba0c-bdadb265c65f'), (29820, '49d6a04e-36ec-4a38-adbf-50f525d423b1'), (29821, '84d9d326-f3f5-4a05-b9fc-f14b56bf8280'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (22657, '9268b17f-2fc5-4ae4-8106-8039de92a69b'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (17031, '3f233fb1-a6a0-4c1c-8465-4333aaaee216'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (9877, '02d00698-e099-4595-9b5b-915a102c8c31'), (20632, 'bc02c575-f28b-4824-b171-9d79cdbbccb2'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (4765, '289b3e37-708b-4282-af84-e4f1c3501749'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11467, '99d6488b-6726-4a42-866c-9569f978641d'), (1739, 'daa73787-386c-4e77-9c96-580d96b227fd'), (18126, '7975e5be-9bee-4c7f-a5d1-dbbcdbdbc120'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (13544, '1da8af21-fa53-45bb-b392-40172dafefba'), (749, 'aadabaa8-059f-4e53-bb4a-defc70cc04cc'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (28911, '179dfbeb-a989-4498-b4fb-6fac44b1aa4a'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (242, '2eca8361-c9f3-4bbe-8cd0-710465ac0d1e'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (765, '65b17d57-f43a-42e2-a6d4-b96ae347324c'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5379, '287c1ed6-130b-4784-8132-3f5287c3471c'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5396, '18d8ed78-6549-4720-9c12-5ae48fb89fc8'), (14105, '30d56fe4-77e8-4044-a195-afe51b4ed84a'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (805, '45825ed8-8f85-40ed-a174-19eeebd97cf7'), (807, 'ce4fda9f-c07a-4ca5-8538-9f8ededb01e2'), (808, '2232e5cb-cb93-4ea7-8c40-d51528355f6c'), (5419, 'ca377909-1f4d-48de-af71-01684b43fc59'), (812, 'ecabe7c7-05c4-4cfa-9e6e-c24df8d129b4'), (814, '3c86feb6-9865-4995-82a9-c9e1bafa680f'), (815, '00347210-f522-4fa2-a819-98958034885b'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15696, 'f5c82a25-56d1-4160-a926-0d697826a6bb'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (9062, '51850a4f-1794-457b-acdc-6dda09458247'), (10101, '93f0483d-8fce-49ba-8617-9adcebcc9265'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (29586, '67cb4809-8361-4ee5-9dd4-cb31eef9b1bf'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (18356, '31d586a8-3709-4998-8ff5-6d7466228a8f'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (23993, '1ab46630-fc89-4db9-8991-e991994175e0'), (5053, '276ed589-b825-4d36-bdc1-a6568aa8af45'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# PatchTSMixer

## Overview

The PatchTSMixer model was proposed in [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) by Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong and Jayant Kalagnanam.
existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components, we significantly enhance the learning capability of simple MLP structures, outperforming complex Transformer models with minimal computing usage. Moreover, TSMixer's modular design enables compatibility with both supervised and masked self-supervised learning methods, making it a promising building block for time-series Foundation Models. TSMixer outperforms state-of-the-art MLP and Transformer models in forecasting by a considerable margin of 8-60%. It also outperforms the latest strong benchmarks of Patch-Transformer models (by 1-2%) with a significant reduction in memory and runtime (2-3X).*
*TSMixer is a lightweight neural architecture exclusively composed of multi-layer perceptron (MLP) modules designed for multivariate forecasting and representation learning on patched time series. Our model draws inspiration from the success of MLP-Mixer models in computer vision. We demonstrate the challenges involved in adapting Vision MLP-Mixer for time series and introduce empirically validated components to enhance accuracy. This includes a novel design paradigm of attaching online reconciliation heads to the MLP-Mixer backbone, for explicitly modeling the time-series properties such as hierarchy and channel-correlations. We also propose a Hybrid channel modeling approach to effectively handle noisy channel interactions and generalization across diverse datasets, a common challenge in existing patch channel-mixing methods. Additionally, a simple gated attention mechanism is introduced in the backbone to prioritize important features. By incorporating these lightweight components,
 
Here is the question: Which combination of techniques ensures optimal time-series forecasting with PatchTST?","Based on the context, the combination of techniques that ensures optimal time-series forecasting with PatchTST is:

1. Segmentation of time series into subseries-level patches
2. Channel-independence, where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series
3. Patching design, which retains local semantic information, reduces computation and memory usage, and allows the model to attend longer history.

These techniques, as described in the abstract, enable"
What is the most effective way to integrate datasets into Transformer workflows while maintaining ethical standards?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (19481, 'cf242eeb-0067-4382-864e-8b376d0da920'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (29224, 'a1b49036-83a8-4545-96ac-718a57c76b4b'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (24636, '34c928bf-700f-4de6-b158-b590ff679457'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (1601, '06cbfead-420b-465d-a07c-566ed263f552'), (13892, '18f9d2dd-b24d-40df-bf4b-70633772beaa'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (26202, 'a3e63e78-75c5-4e73-a5b3-110285882551'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (26207, 'dd7cd3b1-658f-4b98-9830-c5742411682f'), (26212, '8d5de3f2-1ac9-40c8-b7e9-607000e02cd5'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (12912, '426a098c-d467-4adb-a3a9-bb78cb34af6b'), (6774, '9e32e928-42a2-4f8a-8751-39ce7e917839'), (1183, '83219e89-2f3e-4e38-8e76-34ba4626dc2d'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (28846, 'a95944ca-cb3f-47e9-a1ca-670e4f9f9fea'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (17088, '17e59b2c-7682-45e9-abb8-a88720517b2c'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (16618, 'e909832c-f075-4840-be99-5ebc6f8d3b8e'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (19187, '3903562f-59a1-4286-92d6-9666ba3e9bd1'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (13064, '589f6f88-a4f1-45d4-863a-1d5eee88fa12'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28424, '118d80b7-d822-4e36-87a3-b04479f5676e'), (4363, '3a3e4a17-7092-49fb-a163-1e382dffbc6d'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (781, '12fec9ca-f24a-44df-9ea5-9cd201ac15a5'), (16142, 'c771e38b-cfa7-4d59-96c0-b3151c87a39d'), (10015, '39ee57c9-083a-4eae-9edd-a5123097c744'), (15142, '10c62a15-e400-4c35-93a8-017a7ab8b3d6'), (17709, 'badeb458-a406-47a3-a49d-2636f6cdef1e'), (17720, '944ded38-82f0-4442-8aed-7cb4d8402fe8'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (15726, 'ab5f73f9-6717-4fea-9073-c6abfe535dbe'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (5534, '3e4707b0-83e3-4137-b771-207785927e14'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (22434, '8bea8922-5c12-40fa-87bf-aa092348b13f'), (22435, '2cb4720c-dc7b-487b-8244-1eb135abdcb2'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8626, 'b2adf91d-3907-4c74-a7a7-bbcc199cea59'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (28601, 'eb9de433-f226-428a-9cca-06ee8c6b08ce'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (22476, '6a8f51fd-9677-457f-8034-63e0566b51c3'), (16844, '14804510-d68b-43ef-b630-fea72e07be9a'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (1499, '1faf2c53-b46d-4aec-a233-dd712314dc97'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (4065, '40ca2d84-7561-4754-bbb3-490ed78468fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Transformers have proven to be extremely efficient for a wide range of functions, including feature extraction, text generation, sentiment analysis, translation and many more. Models like BERT are widely used by Graphcore customers in a huge array of applications including cybersecurity, voice call automation, drug discovery, and translation.

Optimizing their performance in the real world requires considerable time, effort and skills that are beyond the reach of many companies and organizations. In providing an open-source library of transformer models, Hugging Face has directly addressed these issues. Integrating IPUs with HuggingFace also allows developers to leverage not just the models, but also datasets available in the HuggingFace Hub.
Given a dataset, most dataloaders follow the following recipe:

1. Fetch data (e.g. where your JPEG images are stored on disk)
2. The CPU reads encoded images
3. The CPU decodes images
4. The CPU applies image transformations to augment images
5. Finally, images are sent to devices (although this is usually not done by the dataloader itself)

Instead of doing the whole process on CPU and send ready-to-train data to devices, a more efficient workflow would be to send encoded images to devices first and then perform image decoding and augmentations:

1. Same as before
2. Same as before
3. Encoded images are sent to devices
4. Devices decode images
5. Devices apply image transformations to augment images
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
As a final note, we recommend making use of Hub's dataset functionality even when working with private datasets. The Hugging Face Hub, Transformers, and Datasets are flawlessly integrated, which makes it trivial to use them in combination when training models.

In addition, the Hugging Face Hub offers:

-   [A dataset viewer for every dataset](https://huggingface.co/datasets/amazon_reviews_multi)
-   [Easy demoing of every model using widgets](https://huggingface.co/docs/hub/models-widgets)
-   [Private and Public models](https://huggingface.co/docs/hub/repositories-settings)
-   [Git version control for repositories](https://huggingface.co/docs/hub/repositories-getting-started)
-   [Highest security mechanisms](https://huggingface.co/docs/hub/security)


### Finding a suitable model

Having decided on the task and the dataset that best describes our use case, we can now look into choosing a model to be used.
In [Chapter 7](/course/chapter7), we'll put all of this to good use as we take a deep dive into the core NLP tasks that Transformer models are great for. Before jumping ahead, though, put your knowledge of 🤗 Datasets to the test with a quick quiz!
 
Here is the question: What is the most effective way to integrate datasets into Transformer workflows while maintaining ethical standards?","The most effective way to integrate datasets into Transformer workflows while maintaining ethical standards is to use the Hugging Face Hub's dataset functionality, even when working with private datasets. This allows for seamless integration with Transformers and Datasets, making it trivial to use them in combination when training models, while also providing features such as dataset viewer, easy demoing of models, private and public models, git version control, and highest security mechanisms."
How can time-series datasets be prepared for use in Transformer-based models like PatchTST?,"[(517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (18438, '46782e4c-a0e6-4646-81b0-50890671322d'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (5135, '8bb2ea1c-369d-4db0-ab7a-127366db9831'), (5136, 'd9840db6-0933-4364-81bb-b7ab396b1651'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29776, 'e3203540-4467-4975-8a4a-3a14658f4f60'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (29785, '0648b88f-f28f-478b-a1f2-87949481cff2'), (29786, '945fd8e4-77cb-409e-af9d-4d7171e7ea93'), (29791, 'daadbbdc-5083-4b37-be0d-583734c74458'), (29792, '68aa8771-8ebc-47c4-96a6-3071c24003d1'), (29795, 'e8acfe88-7634-4f4d-a180-dbea58d5a556'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (29828, '4a689bdc-aef9-4849-9b09-6a5b97979b6c'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (17029, '48fc79d9-063a-4c64-b9a0-98a6773c1208'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11463, '20088ef2-37e8-4def-98fa-8c9ab021a5f1'), (11465, '2352d31d-4cb6-43cc-a668-77747f9ff8db'), (11466, '183d3a88-55ea-4c58-a7b7-12ea26f360de'), (11467, '99d6488b-6726-4a42-866c-9569f978641d'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (23784, 'ed5ad04f-6d3c-413f-a4b9-4419c92834de'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (4845, '940b1611-e095-4f8e-8f1c-183c7c7752cd'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (764, '803a240a-32a6-4cfb-885a-8813241a5488'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5380, 'c3402ba2-db82-486f-b86b-a5f292b965d1'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (779, 'c84b16d0-6361-44a8-b2c9-cc03a7a33748'), (780, '6344f2d1-6815-49e5-bc6e-11d86a1bb2cf'), (782, '6279502b-8279-4967-bca2-1610f71071a5'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (5401, 'fabf08d1-3eb0-4c54-abc0-8267df388968'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (1817, '1c818452-3fbe-49d9-a499-ac6ab04ee510'), (18206, '8db1e991-a927-49a1-bf19-bfdc54622518'), (5422, 'c12a80a5-4e72-4a35-b1ea-7113b01b5c00'), (5426, '5052dd1e-66fa-4818-9c3d-a9efed8343c7'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (13623, '61a8c7a1-9ad3-4d9a-aa84-06e57daeda3a'), (17720, '944ded38-82f0-4442-8aed-7cb4d8402fe8'), (322, 'f79309af-2846-48e5-a187-020a8da015fe'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15696, 'f5c82a25-56d1-4160-a926-0d697826a6bb'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (9144, '3be21646-2d02-41ae-b3a2-d0cf74a4654a'), (9145, 'a9a8d501-4863-4964-b90e-1c9b79a97a29'), (10185, '7824c364-4a2f-4032-a5e8-10723b216cbe'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example. 

The current model assumes the presence of a date-time together with the time series values, which might not be the case for every time series in the wild. See for instance neuroscience datasets like the one from [WOODS](https://woods-benchmarks.github.io/). Thus, one would need to generalize the current model to make some inputs optional in the whole pipeline.

Finally, the NLP/Vision domain has benefitted tremendously from [large pre-trained models](https://arxiv.org/abs/1810.04805), while this is not the case as far as we are aware for the time series domain. Transformer based models seem like the obvious choice in pursuing this avenue of research and we cannot wait to see what researchers and practitioners come up with!
- Similar to other models in the library, [`TimeSeriesTransformerModel`] is the raw Transformer without any head on top, and [`TimeSeriesTransformerForPrediction`]
adds a distribution head on top of the former, which can be used for time-series forecasting. Note that this is a so-called probabilistic forecasting model, not a
point forecasting model. This means that the model learns a distribution, from which one can sample. The model doesn't directly output values.
- [`TimeSeriesTransformerForPrediction`] consists of 2 blocks: an encoder, which takes a `context_length` of time series values as input (called `past_values`),
and a decoder, which predicts a `prediction_length` of time series values into the future (called `future_values`). During training, one needs to provide
pairs of (`past_values` and `future_values`) to the model.
- In addition to the raw (`past_values` and `future_values`), one typically provides additional features to the model. These can be the following:
As time series researchers will know, there has been a lot of interest in applying Transformer based models to the time series problem. The vanilla Transformer is just one of many attention-based models and so there is a need to add more models to the library.

At the moment nothing is stopping us from modeling multivariate time series, however for that one would need to instantiate the model with a multivariate distribution head. Currently, diagonal independent distributions are supported, and other multivariate distributions will be added. Stay tuned for a future blog post that will include a tutorial.

Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example.
 
Here is the question: How can time-series datasets be prepared for use in Transformer-based models like PatchTST?","Based on the context, time-series datasets can be prepared for use in Transformer-based models like PatchTST by segmenting the time series into subseries-level patches, which are served as input tokens to the Transformer. Each channel should contain a single univariate time series that shares the same embedding and Transformer weights across all the series. This patching design allows for local semantic information to be retained in the embedding, reduces computation and memory usage of the attention maps, and enables the model to attend to longer"
Which approach best resolves dataset-related errors in an ML pipeline?,"[(17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (30730, 'e011abf6-7dd0-49d9-bdb0-7971bf51aba6'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (29201, 'cdbabce2-0910-47ac-93ec-2dc35c8b6b91'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (30746, '63d7f2f1-c139-46db-85bb-2cb86c04e01c'), (13339, 'f046b9a1-daad-440b-8fa6-1a8de946d278'), (17435, '02482aac-2fce-4d55-9561-bb7cb09f6b7d'), (3611, '5acb0ab4-672d-47da-9ea1-6cdc6cb1d8db'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (22560, '41060592-3553-4a4b-872a-3e8ae06dd33a'), (13344, '8351ae83-b363-4005-b8b5-26db3adae749'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (8239, '601c3e59-2039-4a07-a2ff-f1e97ecadabd'), (13362, 'f7780c87-c11b-4abf-842f-a2fa567045b9'), (18488, 'd75be24a-7b2f-45c2-b82a-35db1d9e237d'), (7229, '2f99d43f-c4bc-428b-bf1d-34c40438e64f'), (9282, '91edce5b-dc46-47e2-85e4-33cf1d17dab7'), (25697, '6b8af185-ee70-45c0-9d9a-b9a8b0f6d7dd'), (20070, 'cfe2f990-f0d5-4bf3-9304-f884cba7e6a3'), (20071, '163a9ba3-cb9e-4a4f-bdd6-07ceaa145535'), (23142, '8fd73c82-896f-4e49-9763-6a97ee39a709'), (20075, '29516309-c7f6-48f9-9710-db5e67efea2f'), (1150, 'b91271f2-c6c3-489b-b0cd-5c23b6b7df5e'), (2689, 'f1e3bc45-e447-43b5-ba36-51abd28111e5'), (1159, 'ed1c0f2d-3594-4151-ba80-832c5099d69b'), (19079, 'f3bedfe8-7295-452a-b97a-76a688901be8'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (20109, 'c1cf2a53-efd5-4487-a825-14bd70fd3cf8'), (1169, 'a200349e-4188-4cbd-827c-64549db1ed40'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (11936, '80e0ccf0-574c-4683-9ea0-81265e58a8b9'), (11945, '37f87402-6ca5-4ac1-8e8b-1ba183125fc0'), (9386, 'b44012b3-7cb4-480a-b300-7c69ebc48a10'), (6317, '56ffe729-9ef7-4575-b631-fdfdf483ee5f'), (24750, 'd8b7af25-fac0-472d-8d54-deda8884bddf'), (7351, '974ac7b4-c4c2-4761-8654-0816e4e2c955'), (24250, 'cd31588b-6899-41c3-b7a1-d6ec27934933'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (16576, '02456d8c-7662-44df-894b-be28d7834936'), (16577, '65b23733-c969-4f5c-815e-b4d8c66dc8a9'), (16582, 'a5a53226-6dca-425c-afa6-94c7b4adf300'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (6859, '93632aec-a5b2-4214-a02d-264a1c961147'), (21205, '6bda238b-e50a-4ace-95e5-877b1ae72029'), (7387, '49e59733-bddf-4e92-be0e-5013437fa51b'), (7397, 'e1ff602a-b5f3-4ecd-be3a-6e21ca34b728'), (7398, '383d361a-826c-44cb-a553-5def932b4a1a'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (12022, 'ca590bdd-6993-45bf-bc5c-6216ff9e670d'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (3339, '6c368a7d-8543-44b3-9993-4674ebdec341'), (3342, '40de6e0e-cd3f-4ac4-8b93-e0a427a98e4d'), (16146, 'e100c4ec-db20-4e52-93d6-f9535f7f90b8'), (17175, 'a0d830f7-cded-4564-acf5-276d18511164'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (20251, '5bf24271-ee00-40cb-bf63-a4d2e0d73a81'), (20252, 'b9e133f5-90ab-47a2-9af2-55d084a98c15'), (14110, '0460530a-535c-4c70-a78d-72af3548e3d5'), (20255, 'e4f72617-0d68-4257-a017-b9389b18e11b'), (13091, 'fd87ced8-d15d-4eb8-98f3-447a6c638420'), (21799, 'eef8b3e0-a59f-4579-9fbe-b2fff100f9c0'), (20264, '60a638d9-b685-4dcf-8430-ebcf2c52b960'), (10544, '27baad9c-7401-4c06-96f3-6846c762d56a'), (11575, '28589867-4683-4d14-8fbc-3442efb0da38'), (29497, '09f25b4f-6e5b-42c1-9f25-79a6d7c74f64'), (20287, 'b9745a11-37a1-41f2-8092-7aad294368b2'), (20289, 'a41ebeed-c447-46c3-9db5-981ea2b9259b'), (20290, 'c24ca7c6-c884-4476-add4-545fba5cd666'), (4427, '3a4777b5-58f7-4c71-994f-d317fee8f1db'), (20304, '17c77f39-33fb-44d1-9c3d-ba8f6556e075'), (28505, 'dc37ccb3-6fee-474b-a35b-6a331cfa196c'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (18276, '3c255798-f281-4027-9f8e-004acb7710ef'), (28517, '83d1dded-982b-449c-9af4-e5fecc3aac5b'), (31590, '619ccde5-a220-428a-8098-7201457c49da'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (26014, '960c2e57-dc37-4bae-9c70-0448415e6410'), (12192, 'e8178d2a-59a2-4f9a-9381-891b9fe71553'), (26018, 'f342cf73-c386-4093-99cd-e639ba2efb08'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (26028, 'f8cc1383-70d9-453f-a623-628d8cbafd47'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (16307, '1118b74f-12f5-4e76-aaae-98b715118010'), (21948, 'e1dd1005-94e8-4562-b9c3-6ba611211942'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (31170, '694c9ec9-5c6b-4ea4-92ba-b75bc909f395'), (11716, '5aa7f3a8-b022-4936-88ea-c22c3c501a94'), (23503, '3b5d2d8b-76f5-4f37-957a-813df4470815'), (25049, 'cf8bc526-5eeb-4029-aba2-8ec814ad0167'), (15330, '9f43616d-6dfb-4651-ae47-169eef44cbd0'), (30179, '2ce3afe5-1f88-459a-8f8e-f954df770550'), (16871, 'e66a5e45-2cf0-4d15-bcfc-0485a92e992f'), (3048, '8a8682d5-da84-45a3-aa2a-a9304b4e7571'), (13802, '54b46bfe-ed8a-4198-9b58-bf91ffa494d0'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (24560, 'ae4f8022-24f0-469e-9cb5-358e58b5773d'), (29689, 'deb041f0-74a4-4cfd-8792-9dd1e899718d')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: #### **3. What’s a common mistake you see people make when trying to integrate ML into Engineering?**
Using an overpowered ML technique on a small problem dataset is one common mistake I see people making in integrating ML into Engineering.  Deep Learning, for example, is moving AI and ML to heights unimagined in such a short period, but it may not be one’s best method for solving a problem, depending on your problem space.  Often more straightforward methods work just as well or better when working with small training datasets on limited hardware.
The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):

```py
from datasets import load_dataset
import evaluate
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
)

raw_datasets = load_dataset(""glue"", ""mnli"")

model_checkpoint = ""distilbert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


def preprocess_function(examples):
    return tokenizer(examples[""premise""], examples[""hypothesis""], truncation=True)


tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""L-WSwUWde1U""/>

The problem when you encounter an error in `trainer.train()` is that it could come from multiple sources, as the `Trainer` usually puts together lots of things. It converts datasets to dataloaders, so the problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Then it takes a batch of data and feeds it to the model, so the problem could be in the model code. After that, it computes the gradients and performs the optimization step, so the problem could also be in your optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `trainer.train()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.
## Debugging the training pipeline[[debugging-the-training-pipeline]]

<Youtube id=""N9kO52itd0Q""/>

The problem when you encounter an error in `model.fit()` is that it could come from multiple sources, as training usually brings together a lot of things that you've been working on up until that point. The problem could be something wrong in your dataset, or some issue when trying to batch elements of the datasets together. Or it could be something wrong in the model code, or your loss function or optimizer. And even if everything goes well for training, something could still go wrong during the evaluation if there is a problem with your metric.

The best way to debug an error that arises in `model.fit()` is to manually go through this whole pipeline to see where things went awry. The error is then often very easy to solve.

To demonstrate this, we will use the following script that (tries to) fine-tune a DistilBERT model on the [MNLI dataset](https://huggingface.co/datasets/glue):
## Addressing Bias throughout the ML Development Cycle

Ready for some practical advice yet? Here we go 🤗

There is no one single way to develop ML systems; which steps happen in what order depends on a number of factors including the development setting (university, large company, startup, grassroots organization, etc…), the modality (text, tabular data, images, etc…), and the preeminence or scarcity of publicly available ML resources. However, we can identify three common stages of particular interest in addressing bias. These are the task definition, the data curation, and the model training. Let’s have a look at how bias handling may differ across these various stages.

<p align=""center"">
 <br>
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/ethics_soc_2/img_pipeline.png"" alt=""The Bias ML Pipeline by Meg"" width=""500"" />
    <em>The Bias ML Pipeline by <a href=""https://huggingface.co/meg"">Meg</a></em>
</p>
 
Here is the question: Which approach best resolves dataset-related errors in an ML pipeline?","The best approach to resolve dataset-related errors in an ML pipeline is to manually go through the whole pipeline to see where things went awry. This involves checking the dataset, data loading, model code, optimizer, and metric to identify the source of the error."
How does PatchTST achieve efficiency in long-term time-series forecasting?,"[(517, '1afcd64d-cccc-4621-b605-312f71df43b6'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (519, '3b09d707-e071-4ac0-a094-b668c9fe1697'), (520, 'b101edb2-f02a-4bd8-b69e-0e3ba6a11f4d'), (521, 'e92b8394-4768-4350-8dc4-c0a57aa6e56b'), (12298, '75d1fbe7-d1d3-4868-b940-d25e740f5bdd'), (24592, '1bc68948-b978-48cf-aee4-932675f8db96'), (6209, 'dc759306-d7ce-4e78-b89d-4681bc9dd25d'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (6211, '829e40da-b92f-45c7-9ba7-fc66be14b35f'), (6212, '9d2539d2-4c9d-4ea5-8dad-8105ad0d656e'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29771, '54c867d1-dd3a-4a9e-97f5-dac70011ef4b'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29786, '945fd8e4-77cb-409e-af9d-4d7171e7ea93'), (20570, '5dd4252e-a29c-4f30-ae23-7b7de46bb5a0'), (29818, '02cc33d8-0c79-466e-ba0c-bdadb265c65f'), (22651, 'ead7a63c-a6d5-446d-8807-99d33418b4ba'), (29820, '49d6a04e-36ec-4a38-adbf-50f525d423b1'), (29821, '84d9d326-f3f5-4a05-b9fc-f14b56bf8280'), (4735, 'c6b8a21e-4a90-4025-9b24-99a6bbf289ba'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (4738, '5afe5001-eafb-4d59-a275-b9b9b8dbf606'), (4739, 'ecc9ae5a-c094-45bf-b400-de5929639143'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29829, '42638958-2c87-4709-9690-fee2637cfab4'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (17031, '3f233fb1-a6a0-4c1c-8465-4333aaaee216'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (22657, '9268b17f-2fc5-4ae4-8106-8039de92a69b'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29828, '4a689bdc-aef9-4849-9b09-6a5b97979b6c'), (9877, '02d00698-e099-4595-9b5b-915a102c8c31'), (20633, 'd89df931-c3cd-4216-9be5-637cc5af0154'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (11466, '183d3a88-55ea-4c58-a7b7-12ea26f360de'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (4827, '5f17dd29-ed7c-4d6d-a7b0-799598ffae24'), (4828, '3c6a1391-8173-44ba-8e15-9bc3f915bb93'), (749, 'aadabaa8-059f-4e53-bb4a-defc70cc04cc'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (751, '67603cec-9c52-4005-8adb-f0899c41f242'), (753, '98e3f81d-5038-4507-afb9-bfc0f6735837'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (765, '65b17d57-f43a-42e2-a6d4-b96ae347324c'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (5378, '45c18aac-df8d-468a-a6eb-a02ad673236f'), (5379, '287c1ed6-130b-4784-8132-3f5287c3471c'), (5381, '90d6952d-8f17-49a9-b076-9ae23d582769'), (5382, 'df27d4fd-9541-4ebc-a46e-52e46fdf5ee9'), (774, 'e2833191-1067-4180-a284-3e7092c768cb'), (1800, 'd16c8c4e-9a1a-4c83-9237-3dca3cd9a5fc'), (5383, '496f6221-9c05-4fdb-b452-3aa7c5a8b963'), (5387, 'f4e3df67-9c81-416c-94fa-53706d07085e'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5396, '18d8ed78-6549-4720-9c12-5ae48fb89fc8'), (14105, '30d56fe4-77e8-4044-a195-afe51b4ed84a'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (5923, 'd508a7d3-3d71-4f96-9bcd-951254088aab'), (805, '45825ed8-8f85-40ed-a174-19eeebd97cf7'), (807, 'ce4fda9f-c07a-4ca5-8538-9f8ededb01e2'), (808, '2232e5cb-cb93-4ea7-8c40-d51528355f6c'), (5419, 'ca377909-1f4d-48de-af71-01684b43fc59'), (812, 'ecabe7c7-05c4-4cfa-9e6e-c24df8d129b4'), (5420, '19d2ca4f-3e1d-4789-848e-5363e52ea2e9'), (814, '3c86feb6-9865-4995-82a9-c9e1bafa680f'), (815, '00347210-f522-4fa2-a819-98958034885b'), (5422, 'c12a80a5-4e72-4a35-b1ea-7113b01b5c00'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (5428, '61a96b3a-5dd6-407e-9050-392a2584d46e'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (5431, '738c4cb8-084a-4b96-84e3-c429db12fb03'), (15692, '5077bcbf-1067-4b3a-970d-76f47377dcad'), (15693, '48d73b9f-7ee2-4af2-bab0-324c5bafaf86'), (15694, '87125726-fe11-4d17-9bed-a537495bbb1b'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (15697, 'd992d311-aebc-4a7d-af2a-af386b1d777b'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (29586, '67cb4809-8361-4ee5-9dd4-cb31eef9b1bf'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (9125, '76843201-d6ad-4f2b-81d0-a469028a0e1c'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (17857, '640d96b2-19b8-40ad-a0f5-1a0a7b13b9d1'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (18419, '4182631f-f9d3-483a-8e48-c274482dcdaa'), (5116, '99173b97-15f5-40a7-af2e-9551d8088826')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
PatchTSMixer is a lightweight time-series modeling approach based on the MLP-Mixer architecture. In this HuggingFace implementation, we provide PatchTSMixer's capabilities to effortlessly facilitate lightweight mixing across patches, channels, and hidden features for effective multivariate time-series modeling. It also supports various attention mechanisms starting from simple gated attention to more complex self-attention blocks that can be customized accordingly. The model can be pretrained and subsequently used for various downstream tasks such as forecasting, classification and regression.


The abstract from the paper is the following:
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM から) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. から公開された研究論文 [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google から) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu から公開された研究論文: [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777)
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google から) Jason Phang, Yao Zhao, and Peter J. Liu から公開された研究論文: [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347)
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM 에서 제공)은 Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam.의 [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf)논문과 함께 발표했습니다.
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (Google 에서) Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu 의 [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) 논문과 함께 발표했습니다.
1. **[PEGASUS-X](https://huggingface.co/docs/transformers/model_doc/pegasus_x)** (Google 에서) Jason Phang, Yao Zhao, Peter J. Liu 의 [Investigating Efficiently Extending Transformers for Long Input Summarization](https://arxiv.org/abs/2208.04347) 논문과 함께 발표했습니다.
1. **[PatchTSMixer](https://huggingface.co/docs/transformers/model_doc/patchtsmixer)** ( IBM Research से) Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. द्वाराअनुसंधान पत्र [TSMixer: Lightweight MLP-Mixer Model for Multivariate Time Series Forecasting](https://arxiv.org/pdf/2306.09364.pdf) के साथ जारी किया गया
1. **[PatchTST](https://huggingface.co/docs/transformers/model_doc/patchtst)** (IBM से) Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, Jayant Kalagnanam. द्वाराअनुसंधान पत्र [A Time Series is Worth 64 Words: Long-term Forecasting with Transformers](https://arxiv.org/pdf/2211.14730.pdf) के साथ जारी किया गया
1. **[Pegasus](https://huggingface.co/docs/transformers/model_doc/pegasus)** (from Google) released with the paper [PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization](https://arxiv.org/abs/1912.08777) by Jingqing Zhang, Yao Zhao, Mohammad Saleh and Peter J. Liu.
 
Here is the question: How does PatchTST achieve efficiency in long-term time-series forecasting?","According to the context, PatchTST achieves efficiency in long-term time-series forecasting through its patching design, which has three-fold benefits:

1. Local semantic information is retained in the embedding.
2. Computation and memory usage of the attention maps are quadratically reduced given the same look-back window.
3. The model can attend longer history.

This allows PatchTST to improve long-term forecasting accuracy significantly when compared with other Transformer-based models."
What role does dataset preprocessing play in successful Transformer model fine-tuning?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (15420, '8ac3193e-579b-4ffe-9257-4fbbd1a77881'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (2647, 'a9b33316-4844-4d79-ace2-5d36fae71281'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (26207, 'dd7cd3b1-658f-4b98-9830-c5742411682f'), (26212, '8d5de3f2-1ac9-40c8-b7e9-607000e02cd5'), (15972, '0b384a9e-35c6-4a62-b202-426be4dd17da'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (3724, '1c0e1b75-0bd9-45a1-af24-e9cf468d539c'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (6355, '9cf0e1e3-ca85-4c73-afc6-a71c99755175'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (19187, '3903562f-59a1-4286-92d6-9666ba3e9bd1'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (5876, '6c565e09-ac75-4392-891f-2a1bbddd3dd9'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (17166, 'f8edb48c-3037-4068-b3cc-1bc5a12bd5d3'), (7443, '9f6d74aa-75e8-4d1e-b0f7-7135867538b0'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (25883, 'f01a6bd7-35e0-4d59-b1e1-8879b0662972'), (25884, '585808ef-bcbd-4bda-9dc1-376342a66a99'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (15659, '54679008-9ef1-4235-9d99-b2a9666e3d13'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (4916, 'eef334ce-c96c-41d6-8320-5bb2ac306f38'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (17740, '278dd567-d102-4857-990f-de19046c1e3c'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7000, '8d065847-3189-4122-97bf-d861398ddd06'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (7534, '4354b918-223f-4fd0-92ea-e59bd0b20f6b'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (22399, '8c31bbea-69a2-4f83-afb3-796762dba928'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (5534, '3e4707b0-83e3-4137-b771-207785927e14'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (6083, '34942899-e37f-425b-ae2a-278f328948fc'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (22485, '13852224-673a-4353-9524-8a0123f2dcc8'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (24057, 'd292faa1-de56-402c-878a-e2d243e5db03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
{/if}

For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results. 

However, there are a few cases where you'll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!
### Preprocessing
Graph transformer frameworks usually apply specific preprocessing to their datasets to generate added features and properties which help the underlying learning task (classification in our case).
Here, we use Graphormer's default preprocessing, which generates in/out degree information, the shortest path between node matrices, and other properties of interest for the model. 
 
```python
from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator

dataset_processed = dataset.map(preprocess_item, batched=False)
```

It is also possible to apply this preprocessing on the fly, in the DataCollator's parameters (by setting `on_the_fly_processing` to True): not all datasets are as small as `ogbg-molhiv`, and for large graphs, it might be too costly to store all the preprocessed data beforehand. 

## Model

### Loading
Here, we load an existing pretrained model/checkpoint and fine-tune it on our downstream task, which is a binary classification task (hence `num_classes = 2`). We could also fine-tune our model on regression tasks (`num_classes = 1`) or on multi-task classification.
```python
from transformers import GraphormerForGraphClassification

model = GraphormerForGraphClassification.from_pretrained(
    ""clefourrier/pcqm4mv2_graphormer_base"",
    num_classes=2, # num_classes for the downstream task 
    ignore_mismatched_sizes=True,
)
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
 
Here is the question: What role does dataset preprocessing play in successful Transformer model fine-tuning?","According to the context, dataset preprocessing plays a crucial role in successful Transformer model fine-tuning. It helps generate added features and properties that aid the underlying learning task, such as classification. In the case of Graphormer, preprocessing generates in/out degree information, the shortest path between node matrices, and other properties of interest for the model. Proper preprocessing can boost the performance of downstream tasks, especially when dealing with domain-specific data that may contain rare tokens."
Which method ensures ethical compliance when using custom datasets in NLP workflows?,"[(13312, 'e35c48e0-463f-491b-a0bc-f4d5b85237df'), (13313, 'd5c6a86f-c6b1-4ea0-9a8e-0fed12888c3b'), (21510, 'c46191e4-71b9-4418-af1a-b816bb6ad8a0'), (13321, '10328f23-c6d0-4eca-a172-090f97ccf6a3'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (13328, 'c42aa481-5886-433e-af56-03e6055e90aa'), (13331, '633f850b-58db-489a-b905-3eddfa0fcbe5'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (12314, 'e385ad95-96ab-4d32-aa88-01c826234363'), (13339, 'f046b9a1-daad-440b-8fa6-1a8de946d278'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (26651, 'f1b12a30-cfeb-4c01-a3a4-7f8d4640303f'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (13348, '32abce73-16cb-4b17-9dc7-423f77e23e65'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (29224, 'a1b49036-83a8-4545-96ac-718a57c76b4b'), (5673, '96383ca4-e892-40fe-a6bf-43344f625329'), (5674, '04606b96-cd85-41c8-93df-3a3dd54c72b6'), (5676, '32a6ebd5-20be-4722-b90f-1cdb4aca4fe5'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5678, 'eb23d76d-3e4d-4635-945d-829b6d3c08bd'), (8239, '601c3e59-2039-4a07-a2ff-f1e97ecadabd'), (5682, 'fb18c66c-cdd7-47e6-a4bf-022dc4f0f8af'), (24628, '7a34ad72-3bc7-47cd-a7b7-91eec037cfe1'), (1601, '06cbfead-420b-465d-a07c-566ed263f552'), (29252, '7191173d-f4d2-46ab-b63d-00b253aa0edc'), (23111, 'ce25fab8-4a21-467c-acb9-c565c4d93e4e'), (29261, '1715bea8-d32b-4d78-b7b9-eff4094c9c6d'), (29262, '5dfbb43a-ae42-4a9d-a676-5a72a99fe9c8'), (29264, 'c55f34a1-f978-4720-895f-18a6151edf17'), (29265, '47595a18-5b82-4980-91ef-b2cc34ee15b8'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (7870, '610d1ee4-f410-4065-85e0-0d2bbb6941e3'), (7869, 'cd9ca43c-5b32-45ea-bebe-dc5cbbb52e37'), (5824, '15cf9465-09ed-4fd6-9a0d-9322bf3b46ac'), (17107, '32ba865a-f6ba-4f9d-9218-821244d7569c'), (29397, 'dcf5399f-1b24-4591-9bad-114ee181f811'), (29399, '25ed7cc3-3bd3-437e-88be-1863a11ee6c3'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (7398, '383d361a-826c-44cb-a553-5def932b4a1a'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (4361, '69a44467-ab86-4ff5-bd68-f4fcb4287814'), (4362, 'a3614fd5-f69b-44a3-90b0-01f9eb078f1a'), (4363, '3a3e4a17-7092-49fb-a163-1e382dffbc6d'), (4364, '7ac2e616-e1bd-4528-887e-a7bea1a9693f'), (15127, '58cfa5d1-aa1f-4671-909f-1abfdb9b7292'), (6960, 'c1946ea0-6bb7-4f1c-9021-5a997b1fd87a'), (2870, '8630c5af-b3e7-4d83-b67d-8cca87440b11'), (29502, 'b92461f5-c7c7-4ab3-b930-a6ccd8804c23'), (29503, '8d8f1ab2-d503-4300-a5cb-65e057381f71'), (14155, '95c08218-8442-42e3-940d-90bafd8c884a'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (1359, '1ee49ddb-9952-4cfc-b4ed-2acfce41628f'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (2921, 'e87ad01f-4e33-407d-baa9-acd9bdb513a9'), (2925, '4022af0b-f26c-4d58-aa84-a072968a500b'), (5485, '78e3c74d-b4e4-464a-b49a-eab745dc5023'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (2929, '8f107d2b-bd58-4ee8-9812-a79c2f815cb2'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (27561, 'ca47c575-982f-4676-ae78-827fcad2783e'), (8618, 'b72c1623-10fb-4fdc-99e7-31e030260a85'), (8619, 'd9485217-089e-41ee-8fdb-86766c17aadf'), (8620, '9f78056e-5ab0-4773-b7a2-0ff1437ffe6c'), (8621, '8f4f50a9-43b1-4ce4-94ea-b087a4a4a34a'), (27566, '228f8e78-7d1b-4627-8417-c05cbd941d34'), (8624, '339598bc-e183-4af9-9283-072e1969ff78'), (8625, '53733ef6-753f-4090-914f-eb6b37582487'), (8626, 'b2adf91d-3907-4c74-a7a7-bbcc199cea59'), (27569, 'b29d78ff-795c-4f5a-b4c3-4d89384bc1ad'), (21948, 'e1dd1005-94e8-4562-b9c3-6ba611211942'), (29629, '6616af81-5a7a-45be-b96a-4aefa6ba382a'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (21949, '3403214f-3950-4911-9222-b83ecb47fb03'), (21952, '138ee00c-29cc-4a4a-b56c-0e17a9529580'), (29633, '470b577b-aa9a-4eb1-8a92-21446ac23695'), (21951, '358a38e1-9865-45a7-bb11-f4d56f194e8e'), (29635, '1703fceb-c153-43cb-9e28-9f71223b471c'), (7108, '7e8be3a4-3d9e-4572-b072-eaacb2c1a9a1'), (29636, 'aef337fa-5cf2-4587-b3bd-be14dc33c597'), (7110, 'ace1a408-647b-4149-a9c1-b6a9a032d552'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (21459, 'beb9ef98-dada-47ef-930b-876f832af55f'), (21460, 'f64ebd76-efa7-434b-9335-86e1cef9b251'), (21461, '2153cb8b-d5cc-4559-8c58-93aaabad7f62'), (1499, '1faf2c53-b46d-4aec-a233-dd712314dc97'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (27101, 'a0b5c992-ca35-43fb-8604-666943c50f3c'), (1502, 'be3626fb-0c72-415e-99b7-8b512a97ad59'), (1500, '958de4d0-417a-4556-b8cb-9391406533c5'), (27104, 'e925e094-c16c-49a8-8dfd-34ed58849a27'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (1510, 'a7df3897-3c02-4187-b2c6-88844cd91c0a'), (1513, '710901b9-382c-447e-b95f-bb910a1fdce4'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (25072, '756040d5-e13a-41c7-b379-2fee2efdc6d4'), (24560, 'ae4f8022-24f0-469e-9cb5-358e58b5773d'), (28665, 'cb67a532-878d-42fe-b5b6-d35a88bc3843'), (28667, '3238cf32-0487-42f8-b216-f189d8ad0e32'), (28668, '1ff5f30d-efd5-4282-8a5f-b6f1516d26fc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: BigCode expanded upon this work by implementing novel techniques for the code domain and building The Stack with compliance as a core principle, such as commercially permissible license filtering, consent mechanisms (developers can [easily find out if their code is present and request to be opted out](https://huggingface.co/spaces/bigcode/in-the-stack) of the dataset), and extensive documentation and tools to inspect the [source data](https://huggingface.co/datasets/bigcode/the-stack-metadata), and dataset improvements (such as [deduplication](https://huggingface.co/blog/dedup) and [PII removal](https://huggingface.co/bigcode/starpii)).
So that really became my shift into ethical AI work.

### In what applications is data ethics most important?

**Meg:** Human-centric technology that deals with people and identity (face recognition, pedestrian recognition). In NLP this would pertain more to the privacy of individuals, how individuals are talked about, and the biases models pick up with regards to descriptors used for people.

### How can ML teams be more aware of harmful bias?

**Meg:** A primary issue is that these concepts haven't been taught and most teams simply aren’t aware. Another problem is the lack of a lexicon to contextualize and communicate what is going on.

For example:
- This is what marginalization is
- This is what a power differential is
- Here is what inclusion is
- Here is how stereotypes work

Having a better understanding of these pillars is really important.
As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.

Data sources, data governance, management of copyrighted data are just a few of the most important compliance areas to consider. BigScience, the older cousin and inspiration for BigCode, addressed these areas in working groups before they were broadly recognized by the draft AI EU Act, and as a result was [graded as most compliant among Foundational Model Providers in a Stanford CRFM study](https://crfm.stanford.edu/2023/06/15/eu-ai-act.html).
Now that we have a great pre-trained model for financial data, the next step is to fine-tune it using our own data for doing sentiment analysis!  

So, we first upload a [custom dataset for sentiment analysis](https://huggingface.co/datasets/FinanceInc/auditor_sentiment) that we built internally with the team to our Private Hub. This dataset has several thousand sentences from financial news in English and proprietary financial data manually categorized by our team according to their sentiment. This data contains sensitive information, so our compliance team only allows us to upload this data on our own servers. Luckily, this is not an issue as we run the Private Hub on our own AWS instance.

Then, we use [AutoTrain](https://huggingface.co/autotrain) to quickly fine-tune the FinBert model with our custom sentiment analysis dataset. We can do this straight from the datasets page on our Private Hub:
In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.

In the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.

## Compliance as a Core Principle

As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.
 
Here is the question: Which method ensures ethical compliance when using custom datasets in NLP workflows?","According to the context, ensuring ethical compliance when using custom datasets in NLP workflows involves several methods, including:

1. Commercially permissible license filtering
2. Consent mechanisms (allowing developers to easily find out if their code is present and request to be opted out)
3. Extensive documentation and tools to inspect the source data
4. Dataset improvements (such as deduplication and PII removal)
5. Data governance and management of copyrighted data
6. Running the Private Hub on own"
What is the significance of licensing considerations when sharing Transformer models trained on custom datasets?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (17412, '1d9b136f-d20c-4989-8975-80625fa8da45'), (24074, 'c7902235-64ce-4d97-b746-5682c76f72ec'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (16405, '35bf25f4-bda1-4a53-8c87-7c4c58181fa4'), (16406, '57321316-4910-41b2-9525-559cf4584035'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (24090, 'a57879a5-a168-4658-8957-ba1cefeb8b49'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (12350, '694bafe7-d167-4c4c-b795-264ca6e0a421'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (12351, '3507bacd-33e0-47cf-a2a6-154bda1acd90'), (26177, 'f49670a1-d2e0-4dbe-9b78-0b59ee600cda'), (12356, 'd426880d-8fc8-4f06-9e8d-f072e54790f4'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (13937, 'edfad0ce-4e2e-4e9b-ac33-041d619a5911'), (13455, 'f0b6cad7-53a6-427e-8bec-1909b4d63763'), (12947, '8890d1af-9df8-4229-a9ce-37479fb8c9d3'), (1186, '50598bfb-1eff-44c7-a14f-8886c016a045'), (30886, '54b36145-f8cd-4fc5-98d8-547220fe27b3'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (29893, '16331caf-1cac-4314-a989-642e9e54a5da'), (29896, 'b72ba5ef-9507-48a0-91d0-2fe3e2cc33f3'), (9416, '28859eb3-1961-4028-8c62-23786a3bd60d'), (10957, '85f6d5dc-fae6-4c38-8c5b-e31e0ee02af3'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (6889, 'f9703e3f-789b-4d40-bb13-0dcb8de3863a'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (28424, '118d80b7-d822-4e36-87a3-b04479f5676e'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (23844, 'c14d0dd8-b543-44ba-b5a9-290396c2a5d2'), (8488, 'b63dba89-ad6e-4edd-a90b-3c654aac9306'), (5928, '46517678-0861-4d86-b92b-eb731ae44060'), (29487, '2321da63-59f7-4fbb-b56d-00810837d618'), (28979, '89c0d9c3-1678-4fa1-be87-cd2090709597'), (28980, '2b0c07ed-129d-48ce-a54a-c5be8281aba6'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (5978, '46c8cc0b-9403-4474-9d60-2b2578a424c6'), (4442, '274884d8-70a8-4a04-900c-0d6dc0e00410'), (4443, 'f66e1230-dd7a-4bce-a009-823af2dfd2e7'), (4445, '1ccd8c8c-66b1-4de6-a2a2-330cad4fc3ea'), (4446, '40d3cf72-6dd6-404d-a125-86a85d113da0'), (4447, '6415bec3-cb2b-4a91-8cdd-ab53e4f49c28'), (4448, '932ed82a-d4aa-4fbc-a48a-08662f7d50dd'), (26975, 'a77435a6-b248-4c25-88bb-c6f596b9ed0c'), (4450, 'dc7b49d7-ee04-4ffc-b3b0-b7d323a5fc2d'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (4453, 'd9a5aacd-1007-4a1b-91e0-7c5e48bc2fe6'), (4456, '20964420-7c99-46f9-9479-8cfd6d42a9fa'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (5487, 'e1a11602-a6aa-4600-b152-18a272a1212b'), (2928, '69808eb1-3f12-4f14-935d-c957dff75a53'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (17798, '7ae25397-da2a-4397-9fa8-858e74f29d9b'), (22410, '13ecbc63-29cf-46c7-9168-10a91b424aaf'), (24972, '9084f511-5580-4546-944c-05245a37f48b'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (27048, 'e3873222-b7e8-4f6f-9732-ae991af045de'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (2505, '4ea66460-be58-4cb2-8c33-19f80091b8a7'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (17363, '509f9576-4f0e-4f9e-a4f6-a2956a1427f9'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (27106, '24b7e8d8-2615-43be-9e15-89597784071f'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (27635, 'b26adb5f-a6fa-44e8-8763-0e70df9a1d17'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
All these releases a) included model weights (under varyingly open licenses) and b) had good performance for models on the smaller side (between 3B and 70B parameters), and therefore, they were instantly adopted by the community. Almost all of these models use the decoder transformer architecture, with various tweaks (ALiBi or RoPE, RMS pre-normalization, SwiGLU), as well as some changes to the attention functions (Flash-Attention, GQA, sliding windows) and different code base implementations to optimize for training or inference speed. These tweaks are likely to affect the performance and training speed to some extent; however, as all the architectures have been released publicly with the weights, the core differences that remain are the training data and the licensing of the models.
The 🤗 Transformers library comes with a vanilla probabilistic time series Transformer model, simply called the [Time Series Transformer](https://huggingface.co/docs/transformers/model_doc/time_series_transformer). In the sections below, we'll show how to train such a model on a custom dataset.


## Set-up Environment

First, let's install the necessary libraries: 🤗 Transformers, 🤗 Datasets, 🤗 Evaluate,  🤗 Accelerate and [GluonTS](https://github.com/awslabs/gluonts).

As we will show, GluonTS will be used for transforming the data to create features as well as for creating appropriate training, validation and test batches.


```python
!pip install -q transformers

!pip install -q datasets

!pip install -q evaluate

!pip install -q accelerate

!pip install -q gluonts ujson
Industry and academia make enormous contributions to advancing Document AI. There are a wide assortment of models and datasets available for data scientists to use. However, licensing can be a non-starter for building an enterprise solution. Some well-known models have restrictive licenses that forbid the model from being used for commercial purposes. Most notably, Microsoft's [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv2) and [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) checkpoints cannot be used commercially. When you start a project, we advise carefully evaluating the license of prospective models. Knowing which models you want to use is essential at the outset, since that may affect data collection and annotation. A table of the popular models with their licensing license information is at the end of this post. 
</div>
    </div>
        </div>
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with 🤗 Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>
 
Here is the question: What is the significance of licensing considerations when sharing Transformer models trained on custom datasets?","The significance of licensing considerations when sharing Transformer models trained on custom datasets is that some well-known models have restrictive licenses that forbid the model from being used for commercial purposes. Therefore, it is essential to carefully evaluate the license of prospective models before using them, as it may affect data collection and annotation."
How can Transformer workflows be debugged for scalability in time-series applications?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (518, 'a8f63379-dd7c-4c62-9b2d-f4795d89ce19'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (8741, '2cf8ec78-79aa-438c-a89d-ba35d135aad7'), (24126, 'f6d0b6d4-13c2-4d59-a59d-6b341a5a1508'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (6210, '8a3e81ae-0567-4699-95cd-a3cd6352fa67'), (29770, 'b896d6b5-900c-42e6-b9d9-290b9d8386ea'), (29773, 'c64bee15-2897-4793-a41c-2b82ee8bc6f9'), (29774, '88217dad-aa3a-4714-b805-409e665d0b5c'), (29776, 'e3203540-4467-4975-8a4a-3a14658f4f60'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (29778, '1f256083-cf84-4197-8b1e-d8ba92d6d6e7'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (29795, 'e8acfe88-7634-4f4d-a180-dbea58d5a556'), (4223, '386573d6-bd83-4bd5-824c-7fbdd3122bad'), (4736, 'afcc5e44-d756-43a8-8155-7c87e0ed122e'), (4737, '309bfb2c-6688-4515-b2bb-2dc083d12e72'), (20610, '74ae985b-2c9e-46f3-97de-6141443eb744'), (29827, '59070c57-b6e3-4e41-8ce8-c3b3fd2f9719'), (29828, '4a689bdc-aef9-4849-9b09-6a5b97979b6c'), (29830, 'c74b8adc-af53-45ce-966f-5d3af1a485b6'), (29831, '655c1aa0-c4da-426c-9447-0ff89fd367d6'), (12948, 'de99c19f-cfde-4076-a099-a2a5d6202e05'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (30895, 'd9909539-e9b8-4ba7-a169-17d2b6a76435'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (1204, '7aa299dc-2c67-463e-bfeb-a1c0a507a538'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (30907, 'e7bb832e-a5c6-45ac-a8c0-877bbf550efe'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (20160, 'c0c6fd6f-69be-4907-9b87-052041804e6c'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (11461, '1ad416a1-838a-44c7-9268-ba0f5b1b2769'), (11462, '512b1434-7b89-4606-bbf2-3136d1f0469b'), (4805, '1a0e20e3-e212-4264-9bfd-e4e7946b8237'), (11463, '20088ef2-37e8-4def-98fa-8c9ab021a5f1'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (726, 'f827742a-dd3e-48c2-97d8-d431bce2569f'), (23784, 'ed5ad04f-6d3c-413f-a4b9-4419c92834de'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (750, '8163df18-be33-4a44-a490-af6ed50d784c'), (1778, '3490636d-9d3d-4b08-ae72-cce708271da3'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (17652, '0cde7a3f-eece-4d99-9338-d127b6c4bc95'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (18166, 'a8d91e83-c3dd-495e-b080-1a04810755d0'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (5371, 'd4eead91-0a32-479b-9f05-fe58ec4939b1'), (7932, 'b99512aa-f8fc-45ad-ae42-7b543455a8d7'), (5372, '38f70772-7d8f-438f-9ee1-a16c417a0503'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (5374, '1f140cd2-b30d-496a-a494-0de5f298d4fb'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (5377, '76274653-e3d1-4019-923a-221ed6eee4b2'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (5380, 'c3402ba2-db82-486f-b86b-a5f292b965d1'), (779, 'c84b16d0-6361-44a8-b2c9-cc03a7a33748'), (782, '6279502b-8279-4967-bca2-1610f71071a5'), (13583, 'c9016d1d-7f32-42f6-b5d9-5b5a14b67a53'), (5394, '9633b12f-2202-491f-9275-47e115f181e0'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (5401, 'fabf08d1-3eb0-4c54-abc0-8267df388968'), (282, '97b448d7-4ccb-4ad6-a71b-63b305a26e7a'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (5430, '8805cf95-19ee-40e9-b687-d326cddebad0'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (15695, 'f75a3f96-a485-474b-bc0c-6f975c930b94'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (9102, '7268a3c1-7e71-48c1-b8da-189d8ce279ad'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (10142, 'a6b23e0e-5e7e-46b7-b4fc-2e5d77193a4f'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (20897, '66ea23a2-cf1d-4055-af3d-f6bddfd77673'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (10167, '3a3180ff-f81e-4772-b16e-32d7e49b1921'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (6083, '34942899-e37f-425b-ae2a-278f328948fc'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (18396, 'b5693089-3b5a-4564-bdff-66625f511757'), (5093, '019d72f2-0658-49a3-a857-2371dbbe28d3'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Secondly, a Transformer helps us to train on time series data which might contain thousands of time points. It might not be feasible to input *all* the history of a time series at once to the model, due to the time- and memory constraints of the attention mechanism. Thus, one can consider some appropriate context window and sample this window and the subsequent prediction length sized window from the training data when constructing batches for stochastic gradient descent (SGD). The context sized window can be passed to the encoder and the prediction window to a *causal-masked* decoder. This means that the decoder can only look at previous time steps when learning the next value. This is equivalent to how one would train a vanilla Transformer for machine translation, referred to as ""teacher forcing"".
As time series researchers will know, there has been a lot of interest in applying Transformer based models to the time series problem. The vanilla Transformer is just one of many attention-based models and so there is a need to add more models to the library.

At the moment nothing is stopping us from modeling multivariate time series, however for that one would need to instantiate the model with a multivariate distribution head. Currently, diagonal independent distributions are supported, and other multivariate distributions will be added. Stay tuned for a future blog post that will include a tutorial.

Another thing on the roadmap is time series classification. This entails adding a time series model with a classification head to the library, for the anomaly detection task for example.
*We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning
*Many real-world applications require the prediction of long sequence time-series, such as electricity consumption planning. Long sequence time-series forecasting (LSTF) demands a high prediction capacity of the model, which is the ability to capture precise long-range dependency coupling between output and input efficiently. Recent studies have shown the potential of Transformer to increase the prediction capacity. However, there are several severe issues with Transformer that prevent it from being directly applicable to LSTF, including quadratic time complexity, high memory usage, and inherent limitation of the encoder-decoder architecture. To address these issues, we design an efficient transformer-based model for LSTF, named Informer, with three distinctive characteristics: (i) a ProbSparse self-attention mechanism, which achieves O(L logL) in time complexity and memory usage, and has comparable performance on sequences' dependency alignment. (ii) the self-attention distilling
Firstly, we will provide empirical evidence that **Transformers are indeed Effective for Time Series Forecasting**. Our comparison shows that the simple linear model, known as _DLinear_, is not better than Transformers as claimed. When compared against equivalent sized models in the same setting as the linear models, the Transformer-based models perform better on the test set metrics we consider.
 
Here is the question: How can Transformer workflows be debugged for scalability in time-series applications?","Based on the context, it can be inferred that debugging Transformer workflows for scalability in time-series applications involves considering the following strategies:

1. **Appropriate context window**: Selecting an appropriate context window and sampling it along with the subsequent prediction length sized window from the training data when constructing batches for stochastic gradient descent (SGD).

2. **Causal masking**: Applying causal masking to the decoder, ensuring it only looks at previous time steps when learning the next value, equivalent to ""teacher forcing"" in"
"How can you debug a Gradio image component for a segmentation demo, ensuring optimal interactivity and fixed rendering?","[(28161, '6a39374d-d9fd-4229-8372-ce6d65514142'), (29189, '883ee1a2-0ee3-4a54-9e8d-f665671f3504'), (4103, '69742828-77bd-4856-a8aa-a0025c5d7383'), (10253, 'e0fdcfc2-6e25-4d57-8dea-e42c925a1e2c'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (28223, '1242aa7b-fd2d-4b3a-91e7-050096d89d60'), (28225, 'f8194a95-f679-4274-b75b-4af2eed31b7c'), (28232, 'edcdb710-1075-4024-937a-e1e9b421cd61'), (28235, '830b922e-76ec-42e3-a191-77ac60d2456d'), (28236, 'ff029be9-b5d9-47d9-8b0a-e79b53b6a8ea'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (28243, '0bf9268d-f628-4160-b2fb-5d484fbd603f'), (2143, '6e81dc66-340f-4578-890b-b72e0440f793'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (2148, 'f1cfa6b4-070a-4df1-aebc-c4c0063f5916'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (2160, 'aab3ec8f-f715-464b-9c91-039450b42ea1'), (10864, '9971a567-07ea-4560-98d4-eede4b1d186b'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (2177, 'e4f20765-678d-400f-acda-3fd9e844e57d'), (14466, '5bdeece5-5451-4254-98c5-22dbafed0ff0'), (4740, 'a3172a98-921d-49f4-98ff-7a1f3290f6af'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (28297, '7a0b2105-7a16-4c1e-9c64-d09c6b7d1025'), (2191, 'd67da064-a30b-46ee-84b4-6a5d395c7270'), (2193, '6fbfd651-6963-445c-81c8-31b14275444a'), (2196, 'aa4313fe-e229-40bb-88bd-e16975e7fdf4'), (28312, 'f0ba4fda-06cc-4261-84f3-c36ed70b5c42'), (2201, 'f63d9476-9b03-480b-81dc-56db1f33810e'), (27811, '6351e475-b7a2-4ee3-88f7-ce204ff277fb'), (6820, '296a6e9a-0b6c-450c-8983-f33bcfc0a165'), (2214, '025e63ca-22c5-4fa2-9b8e-44065d8d2701'), (29868, 'a87fd676-df82-4ca4-b1c7-c7cce13dd5ce'), (3756, '1a1f79f6-d46a-4043-97ae-61320836b7ce'), (27833, '5e4e9e7a-1313-4bac-92f6-d813db5fa660'), (27841, 'c37d24ce-3d99-4ab8-9016-ad2a6d6c1e39'), (2242, 'd59c776c-ce85-4340-b4b9-7f708f83dabb'), (27844, 'fb6d0216-35d2-4c4e-adff-45ae67110b4c'), (2245, '47834d58-902b-4e38-853e-427121d538ab'), (2246, 'b2714547-7d53-41a2-ade0-0dc1989a2476'), (2249, 'b6c12764-f869-4323-a0ab-efd1cbcb6ee1'), (2250, '0dadf097-d697-4513-ac48-4dadd82077e9'), (2251, '49c92cce-066a-48de-9511-6f677fedd988'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (2271, 'c495e660-5754-46e0-b5f7-0c3e3008a2a8'), (2276, 'ee12d392-f2b2-4380-bf78-b1cfb00624cc'), (8420, 'f005d893-d4fc-411d-96ea-a435bfa37288'), (2287, '6b30a70b-fcb5-42bf-a607-1f8aaa309ffb'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (6403, '6b72a701-c1b5-46a7-bcb8-a543df789c1f'), (26889, '739be90a-f89d-445a-be17-3ab3af2a79d6'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (15640, '31d2f322-0056-4e5f-86ac-5092c9d1a8ae'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (2349, 'fa79adea-d3d6-4744-8ccb-83a17393cd8e'), (2351, 'fa81d4f4-fc0f-4558-8324-eab056c116db'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (2358, '61c00040-18d9-47df-b54f-732d42c2c5f9'), (2361, '2ac95a08-0360-4a96-a4eb-29f9cd1a7333'), (2362, '1cfbdc0e-9682-4844-b541-ff932ae3ea3a'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (2369, '2d8765ce-950f-4812-be38-74a3ca44c156'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (13130, 'f6f02375-3da9-4427-ae24-c6969b183c8a'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (3934, '24000b88-1568-44be-a5d6-7fe94664b817'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (9574, 'd0df761f-dfb3-4f51-b012-2b14e04c4cfd'), (28017, '29c87561-6f4c-4afc-adfd-b3054e4a4dad'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (28022, '1928178a-06a4-4630-9d61-6e3b44f3411c'), (2423, '702ce4ca-cbbd-43a9-b982-c55bba1a9f2b'), (28034, 'f2bbf0f1-2ac1-454f-b05c-9985ae9ec0d2'), (2438, 'f22a2019-b1e6-4e51-811f-fa5cbf156208'), (1937, '9e067ec3-9c36-4419-9560-eb68ac22179f'), (28051, '2e9d8676-4090-4ac4-ac08-acae65f2d32d'), (28065, 'bf7e5a2e-ed4b-41c2-97cb-11e1738eaa0f'), (28067, 'cbe3fc74-edd1-421c-971e-a62ec9c187ed'), (3493, '8a1040ce-1b60-4e24-90e9-cb4e7127b718'), (28070, '69c3ce65-8bee-4b77-bb91-93ad624a7734'), (1959, 'eb3f07af-df5e-484b-ad45-9c039094f773'), (28075, 'bc307f87-d3b6-4e90-95d8-df896b01cb89'), (1967, 'e50667e5-7a0c-461c-aff5-9a06cf3ab823'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (1970, 'af7a3664-be46-4edc-ab17-ca264d4a0b29'), (28088, '86a70b0f-cff2-4271-8ac0-c271371d3049'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (28116, '3ef5a8e2-3d00-4ed8-a2bd-1e8553e1ac00'), (9173, '185d824b-9a3c-4154-8bc6-d6d5af0833a6'), (28119, '1dcccb98-51b9-49bd-81ec-c5d5c62bea87'), (28120, '0e15845c-8278-4ae1-8b41-403e6698afe5'), (28123, '1e0f48d2-52b2-4fe6-a624-509a98f026ae'), (28124, '6ddae567-fe9b-4244-91c4-0025ccc9f1a2'), (28125, '24144982-b2b1-4f85-a299-ae52b3b80ed5'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (23021, 'fd440ad8-ad63-44c2-81b0-e2aa3b03e430'), (28145, 'fcb1311b-654e-41fa-8c68-2302815e01ca'), (28150, '0149c2ee-29ac-43be-8e3e-704a364356a8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Gradio Demo: image_segmentation
### Simple image segmentation using gradio's AnnotatedImage component.
        


```
!pip install -q gradio 
```
### Full Changelog:

- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https://github.com/aliabd) in [PR 3866](https://github.com/gradio-app/gradio/pull/3866)
- Fix error in running `gr.themes.builder()` by [@deepkyu](https://github.com/deepkyu) in [PR 3869](https://github.com/gradio-app/gradio/pull/3869)
- Fixed a JavaScript TypeError when loading custom JS with `_js` and setting `outputs` to `None` in `gradio.Blocks()` by [@DavG25](https://github.com/DavG25) in [PR 3883](https://github.com/gradio-app/gradio/pull/3883)
- Fixed bg_background_fill theme property to expand to whole background, block_radius to affect form elements as well, and added block_label_shadow theme property by [@aliabid94](https://github.com/aliabid94) in [PR 3590](https://github.com/gradio-app/gradio/pull/3590)

### Contributors Shoutout:

No changes to highlight.

## 3.27.0

### New Features:

###### AnnotatedImage Component
### Full Changelog:

- Add DESCRIPTION.md to image_segmentation demo by [@aliabd](https://github.com/aliabd) in [PR 3866](https://github.com/gradio-app/gradio/pull/3866)
- Fix error in running `gr.themes.builder()` by [@deepkyu](https://github.com/deepkyu) in [PR 3869](https://github.com/gradio-app/gradio/pull/3869)
- Fixed a JavaScript TypeError when loading custom JS with `_js` and setting `outputs` to `None` in `gradio.Blocks()` by [@DavG25](https://github.com/DavG25) in [PR 3883](https://github.com/gradio-app/gradio/pull/3883)
- Fixed bg_background_fill theme property to expand to whole background, block_radius to affect form elements as well, and added block_label_shadow theme property by [@aliabid94](https://github.com/aliabid94) in [PR 3590](https://github.com/gradio-app/gradio/pull/3590)

### Contributors Shoutout:

No changes to highlight.

## 3.27.0

### New Features:

###### AnnotatedImage Component
Gradio Demo: annotatedimage_component


```
!pip install -q gradio 
```


```
import gradio as gr
import pathlib
from PIL import Image
import numpy as np
import urllib.request


source_dir = pathlib.Path(__file__).parent

urllib.request.urlretrieve(
  'https://gradio-builds.s3.amazonaws.com/demo-files/base.png',
   str(source_dir / ""base.png"")
)
urllib.request.urlretrieve(
    ""https://gradio-builds.s3.amazonaws.com/demo-files/buildings.png"",
    str(source_dir / ""buildings.png"")
)

base_image = Image.open(str(source_dir / ""base.png""))
building_image = Image.open(str(source_dir / ""buildings.png""))

# Create segmentation mask
building_image = np.asarray(building_image)[:, :, -1] > 0

with gr.Blocks() as demo:
    gr.AnnotatedImage(
        value=(base_image, [(building_image, ""buildings"")]),
        height=500,
    )

demo.launch()
```
1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)
2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)
3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)
4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)
5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=""compact"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.
 
Here is the question: How can you debug a Gradio image component for a segmentation demo, ensuring optimal interactivity and fixed rendering?","Based on the provided context, here's a comprehensive answer to the question:

To debug a Gradio image component for a segmentation demo, ensuring optimal interactivity and fixed rendering, you can follow these steps:

1. **Verify the image loading**: Ensure that the images are loaded correctly using `urllib.request.urlretrieve` and `Image.open`. Check if the images are downloaded and opened successfully.

2. **Check the segmentation mask creation**: Verify that the segmentation mask is created correctly using `np.asarray"
What approach best evaluates a question-answering system using SQuAD metrics?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (8705, 'b08718b7-b11c-4e45-a67b-4cb8bdf07270'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (19467, '8be7a151-f10b-4b06-9623-13ac201be523'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (21574, '4ce5b749-0aaa-48d2-8faa-7b2c1beec632'), (26182, '96aace9f-0a6a-4f49-afec-d24a3aa1912a'), (20060, 'ce186691-06ac-411b-93cf-30eb955e0e0f'), (23168, 'd2dc356a-69f4-4cf2-bff3-df262a358222'), (23169, 'd0722de5-d7fc-4a8f-ae0b-edec0a069987'), (24707, 'b83268da-8092-42eb-96fd-1c3b065d1862'), (3206, 'e262cb9b-233a-420d-b20c-ccbc6b0e9095'), (1167, '39a4a830-b657-4ddb-a114-122b57b5f0f9'), (29337, '5d51df0e-4f08-4f4a-ac00-293e1f0b356d'), (29340, 'ab71f4d8-6dc6-42d0-822a-0484f4c731fb'), (18076, 'b3534aa0-9092-4738-a2bf-2bbb3efe8f95'), (9898, '35b6c8c8-71d9-411c-86e9-3c96522d65db'), (23725, 'd1867780-5efa-411f-ae04-2da3db2b0e4a'), (29871, '2ca65b39-0c9f-4558-bc9a-484b76359cd2'), (29874, 'e938808e-a9ad-48f7-b403-2f13ed1f6c56'), (698, '100204eb-4cc8-4afa-8045-f237e1c30995'), (3286, '2e7f139b-09ad-48d3-bf0a-1813c7260d24'), (3287, '94a9e9d5-ba43-44c1-a818-53826dc9982e'), (17624, 'a191bdec-c60a-438c-9768-1f1de4decef0'), (15066, '3546ba18-55ee-4e8b-99f2-475e43dca42c'), (20189, '8bf4e38a-2076-41ce-8eb6-944757a03db3'), (733, '5721b5a3-1c47-4743-bc71-45e81848d38c'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (15075, '5f627d6a-cdb5-4546-a07d-c8bf72389612'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (15592, '481a6c32-3e99-4c07-a468-dbe480fef1b4'), (15593, 'd41ecdb5-1abb-4bbd-a4dd-adf3baf6ee42'), (17659, 'eeb61970-def6-4e18-9276-066bb91dd22c'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (10499, 'b8eebff5-9a09-4f24-a60a-982ca9d90281'), (17163, '91671e10-cb42-42c8-80d7-529032a2f2a4'), (7950, 'f61203be-255c-4c5e-926c-dcf16d1724f4'), (7951, 'b309f09d-7e43-4961-9836-3b705f22816b'), (17176, '4369c63e-51da-4f67-bc64-ddac28c46ca5'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (7462, '89751b8c-b5f1-404f-944f-a5eed44725ae'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (7466, 'bdbba006-58e9-4ae7-87f4-f1b2fb95017a'), (1324, 'ace8d742-d7ef-4a2a-a21f-d2453744f07f'), (7469, '5e4eeb28-721e-4fa6-bb7f-b2ef23f884ee'), (1325, '6cc495c0-da56-4861-829a-d2523aa5d044'), (7471, '533c05d0-2794-409c-9e72-c7265e0258db'), (31025, '45c966d3-3c0f-48cd-9fc5-0fe756439304'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (13112, 'fa72104c-7e7f-477f-a099-a607098c4571'), (9015, '6e28a1fc-953b-4f1b-8b44-61d598ff2f39'), (2872, '293fe2c4-cdc4-4355-a91f-7b007acd74d5'), (13117, '058e07d2-d17f-4303-a0e9-7fe2a5c19611'), (25406, '4f4cb473-5b84-46a2-ac48-b8512fb4b2d7'), (15167, '1b9ae140-8223-4870-bb50-abbc78ae7a0d'), (15166, '9b3c8ab6-5a5b-42f2-88b5-253bc50ca795'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (13123, 'cc8204dc-e828-4d26-968c-2d91d034ccba'), (15168, '62e7a0be-933e-4e8b-afec-55689b396bc8'), (15173, '4a741b1f-fbb9-47c1-84a2-95ca374ec46e'), (15169, '8ef04162-a554-46ac-a86d-a6072d67d9e4'), (24899, 'd5261e5b-d3f0-43d7-aac1-57db8255c358'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (19788, '7899cbd2-e4be-4403-a2ea-4ddf86fdf820'), (6477, 'c6d1f800-e6ad-43b9-8a32-911ce5083a43'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (23888, '5efd6f99-8edf-425d-bf42-fbca2966a6f1'), (12116, 'bb60fc61-08eb-484a-98d5-0aa3d2715f1c'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (12120, '0bbac7aa-9cb9-46e0-89f9-cf746b68cc80'), (7513, '0c07cf85-6aef-484a-beb0-fb65a8a61b89'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (12124, 'f2d8ff59-c9b5-4ccb-b0a9-5488b6acd532'), (1889, 'ff5fff0f-84f6-4097-870a-81b81a13b28b'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (23399, '8c4d5380-a5bc-4827-8694-d2ad31217a3d'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (23403, '9b397e8f-f497-439e-8279-5d67e64c42c8'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (24432, '65160c4e-e76b-478e-af89-46847fcd223f'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (3443, '974c1801-170d-4f66-9f80-84ecc9d6086f'), (2934, '07953283-5664-45cf-b67e-58683c7ff0ea'), (3961, 'efa02508-c542-4ad1-8b61-608f4b1415de'), (7550, '7d55e630-3187-47ea-a06e-4ac35f9a8265'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (3464, '089c63fe-b29d-425a-a5a7-00926ec79c45'), (24467, '4ddbf49c-138b-42ef-8062-54f8e2303ce9'), (28571, '255382e0-8ccf-4b19-85b2-5503370b8f36'), (25505, 'fedd29d3-5600-4087-a101-167964776b87'), (5030, 'd65cd4d7-7315-45e1-a2d7-60c432346eb0'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (28604, '3f1941c1-775f-425b-9ca7-73abb8786647'), (2503, '2e1ff638-b3dd-42ac-9810-3e463ba23b97'), (22472, 'b0c10efd-f97e-48f8-92b6-3a85661ce3aa'), (29137, '5482f963-0400-4c03-b3c5-f9f03ae4c29f'), (22481, 'dc81fdad-ba23-418c-90b4-2a4a24a952dc'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (6102, '3165e349-5926-4468-ac0c-1f9d8959ed1d'), (22492, 'b126ec4b-308e-463e-9f6b-675a86eb01d5'), (22493, '3d5e18a5-b466-426f-9e1f-8e5332bcb405'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (8694, 'e750c432-a8e4-4a5f-bd8f-08dc728b2779'), (11766, '05925ab7-8b14-4726-8033-184a90d67a01'), (8699, '815c3f58-57b7-42ae-9fe8-1c414ae5624e'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: --
title: SQuAD v2
emoji: 🤗 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 2 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.

  SQuAD2.0 combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions 
  written adversarially by crowdworkers to look similar to answerable ones.
  To do well on SQuAD2.0, systems must not only answer questions when possible, but also
  determine when no answer is supported by the paragraph and abstain from answering.
---

# Metric Card for SQuAD v2
## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from evaluate import load
squad_metric = load(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
# Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
--
title: SQuAD
emoji: 🤗 
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.19.1
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
  This metric wrap the official scoring script for version 1 of the Stanford Question Answering Dataset (SQuAD).

  Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by
  crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span,
  from the corresponding reading passage, or the question might be unanswerable.
---

# Metric Card for SQuAD

## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad).
 
Here is the question: What approach best evaluates a question-answering system using SQuAD metrics?","The approach that best evaluates a question-answering system using SQuAD metrics is to use the official scoring script for SQuAD, which takes two files or two lists of question-answers dictionaries as inputs: one with the predictions of the model and the other with the references to be compared to. This allows the system to not only answer questions when possible but also determine when no answer is supported by the paragraph and abstain from answering."
Which method ensures optimal Q-learning implementation in a hands-on setup?,"[(26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (26655, '14ea53e1-bcb7-4126-b80b-09759ba5a722'), (26656, '7cd01870-b453-47fe-b8b4-83d5c97bd9da'), (17966, '0b146d2b-e740-4ab1-861b-c17317d4c9b8'), (7732, '8d07afb9-9121-46ff-a877-120dcac45b78'), (27702, '768fdc11-37ba-43a8-8a96-80545bcb82bf'), (7734, '85ca7829-223b-4021-a48e-c1331212e356'), (27706, '81c17562-7f3d-40fd-944d-ee0b54114838'), (27721, '3f3795c6-7158-4cce-964f-83b9d8df21b4'), (27722, 'c667aeb9-e35c-4791-8c36-ac06162bf37e'), (21604, 'ef6f4014-2b68-45df-b66a-a23331cdcc1b'), (14438, 'adc4d77e-0477-455a-8002-6bde28537b35'), (26727, 'aa34abd9-5ca0-479f-8fe5-8f5a215baf20'), (26728, '7b671840-3675-4abf-895d-c4fe995d2dfd'), (28782, 'f832e002-e981-4806-b6b5-7743c24c81db'), (28783, '972d27cc-6972-4574-8746-66f122afd7aa'), (28784, '351f6005-a2b3-4b17-abf5-a5a07d1bf877'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (17525, '7500c161-c38b-4986-ae2a-ce026b6c10f7'), (17527, 'bd457bd8-f4ad-4b21-9f6c-2cc379928d75'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17533, 'f2d73255-0ee5-4a2b-a2a4-b10e4d2c0612'), (17534, 'e8a226a9-ce5d-412c-b8bb-1a951a1857fb'), (17539, '54748e65-182a-4269-bd1f-6a11d3a531d1'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (144, 'ad9a2442-d5bf-479e-bf4a-b911cfbe1dd2'), (17552, 'e6dc2f24-99a1-4fd8-a073-c0a439c5d184'), (17557, '0f6b36da-2ed0-49e5-b5d2-09ced93bbc46'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (3742, '3ef31faa-4839-44d2-adc1-0d293e09d39a'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (8871, '370b7369-f26d-4340-86fd-357c2065b987'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (25263, 'd7a4626e-6223-485c-95a7-4e2cd4cb33b1'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25265, '177935f5-c8bc-4c5a-9cf0-07eefafb6b83'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25270, '5bbe44fe-46d8-484e-8cdc-15d0afcee3ee'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25273, 'c22e1441-c578-412a-a88e-227ee5847492'), (17585, '57d5dfc1-963c-4fe2-8c61-5ac6acc65e15'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25281, 'a825bb91-b46e-497a-b2bf-52f559ad86d8'), (25282, 'c5f90a36-0447-48df-ba63-aaba6325e62e'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (5869, 'bb6cdbd7-bbcf-493e-8250-030bab25db6c'), (27373, '1cf527a1-db9e-45b4-affe-fab3bfe9777e'), (27379, '88aea9d0-3b36-4f80-986f-5040b1529bab'), (27380, '5efa5370-b16f-4fd9-a3b0-12f7fa4ce693'), (17163, '91671e10-cb42-42c8-80d7-529032a2f2a4'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (3377, '156b12d0-1aa0-4557-a4ca-1e911e59c896'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (22371, '21051c79-d40f-41ae-ad3c-c20ce6cd2470'), (20325, '8e25c4d9-f41e-426e-a372-9465025605ad'), (20326, '9d94613c-da69-433f-a73d-136ad258d459'), (20327, 'db43986d-7e00-4d86-8a36-11e8aaa8e37e'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (20329, '32e96d05-f2b2-42de-bd87-cde71e890e69'), (20331, 'ad781e21-34b5-465a-b44f-1d0ef976b671'), (28525, '60cdc2d0-c35a-420d-bc30-64e0ac98ea5a'), (28526, '99ae54b8-b8c0-49a8-b122-3be8abec3e42'), (20335, '24699ce0-932d-4081-a7d8-ed55918efde0'), (20336, 'd2582ee2-dd1b-40d8-ab3e-09b77460db3b'), (28527, '9dd374d0-f99b-4b6b-9b15-5f4ce6382aa4'), (20337, '1ac41f0f-b53f-41f2-adc7-29016e775aba'), (28533, 'd3ab6409-d782-41a1-83d1-a46f07b48b6e'), (28534, '3e54a2ff-2d80-46f7-870f-7a3153c2036b'), (20343, '5f332e1f-fff2-4a93-becc-b7ee2e2e97ce'), (20344, 'edac2d6a-a444-44cc-b455-7144e89a1f09'), (20345, '90c786f2-9de0-48b8-b056-36c300207419'), (20342, '63ba5e06-4514-4380-a011-5c55dede8208'), (28535, 'c4e1adc6-dc57-4146-96e1-ff983b95b866'), (20854, '110af335-df5a-4138-bd7b-a0ef1308d837'), (23944, 'fd1a1aa9-c058-47ff-ae6d-f4e2de03a110'), (23946, '0d096575-2d3c-4976-93fc-0610e2da9ae0'), (23947, '925054c6-05b5-4bda-a736-9a6f6991a7ae'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (3481, '4c7ec93f-67c6-4662-83f9-0ef6f0461760'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (14774, 'd85cfc80-67be-498d-9904-3d83a553094a'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (14776, '25ac1795-a4d1-45b8-837a-29dc4ccf5415'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (962, 'c68008e9-dc91-4db0-a2f0-89a793040fff'), (27085, 'ee8bb765-44a6-4187-9e25-b70671fe43b7'), (13781, '315f918c-0798-4c74-8867-003400d6c986')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Introducing Q-Learning [[q-learning]]
## What is Q-Learning? [[what-is-q-learning]]

Q-Learning is an **off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end of this unit.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us **the value of each state or each state-action pair.**
- *TD approach:* **updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-function**, an **action-value function** that determines the value of being at a particular state and taking a specific action at that state.
So now, before diving on Q-Learning, let's summarise what we just learned:

We have two types of value-based functions:

- State-Value function: outputs the expected return if **the agent starts at a given state and acts accordingly to the policy forever after.**
- Action-Value function: outputs the expected return if **the agent starts in a given state, takes a given action at that state** and then acts accordingly to the policy forever after.
- In value-based methods, **we define the policy by hand** because we don't train it, we train a value function. The idea is that if we have an optimal value function, we **will have an optimal policy.**

There are two types of methods to learn a policy for a value function:
Glossary 

This is a community-created glossary. Contributions are welcome!

- **Deep Q-Learning:** A value-based deep reinforcement learning algorithm that uses a deep neural network to approximate Q-values for actions in a given state. The goal of Deep Q-learning is to find the optimal policy that maximizes the expected cumulative reward by learning the action-values.

- **Value-based methods:** Reinforcement Learning methods that estimate a value function as an intermediate step towards finding an optimal policy.

- **Policy-based methods:** Reinforcement Learning methods that directly learn to approximate the optimal policy without learning a value function. In practice they output a probability distribution over actions.
## **Introducing Q-Learning**
### **What is Q-Learning?**

Q-Learning is an **off-policy value-based method that uses a TD approach to train its action-value function:**

- *Off-policy*: we'll talk about that at the end of this chapter.
- *Value-based method*: finds the optimal policy indirectly by training a value or action-value function that will tell us **the value of each state or each state-action pair.**
- *Uses a TD approach:* **updates its action-value function at each step instead of at the end of the episode.**

**Q-Learning is the algorithm we use to train our Q-Function**, an **action-value function** that determines the value of being at a particular state and taking a specific action at that state.

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/Q-function.jpg"" alt=""Q-function""/>
  <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption>
</figure>
Mid-way Recap [[mid-way-recap]]

Before diving into Q-Learning, let's summarize what we've just learned.

We have two types of value-based functions:

- State-value function: outputs the expected return if **the agent starts at a given state and acts according to the policy forever after.**
- Action-value function: outputs the expected return if **the agent starts in a given state, takes a given action at that state** and then acts accordingly to the policy forever after.
- In value-based methods, rather than learning the policy, **we define the policy by hand** and we learn a value function. If we have an optimal value function, we **will have an optimal policy.**

There are two types of methods to learn a policy for a value function:
 
Here is the question: Which method ensures optimal Q-learning implementation in a hands-on setup?","Based on the context, the answer is not explicitly stated. The context provides information about Q-Learning, its characteristics, and types of value-based functions, but it does not mention a specific method that ensures optimal Q-learning implementation in a hands-on setup. Therefore, I cannot provide a concise and relevant answer to the question."
How can interactivity be improved in a Gradio-based Q-learning demo?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (28185, 'd648591c-3383-45e8-ae71-83b504e2b8bd'), (26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (16416, '1b6290e2-d918-4435-af5a-004a7c4401ca'), (16417, '0d19a33d-48bf-4fe6-9bc5-fb1aa7a04d3d'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (17986, '0cd1ad02-8722-4792-b0cd-f52c7abe1a5d'), (26702, '0dfd4542-43d5-4695-ae1b-65d6f6900675'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (12888, 'a883e339-6701-4caa-8202-06ce6b64d595'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (19965, '12831356-5f20-4ce7-950a-36abb8c5c787'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (26729, 'cc0355de-d3e7-4ff3-b9ac-1a956918ed8f'), (10863, 'd34b0609-c441-46f8-90ed-ae0defa50563'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (10869, 'b9b21277-67db-4138-9f1a-6ff6485d6dea'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (1145, 'abe44225-c493-44fb-82d3-494085e90422'), (25213, 'a685a826-ac2d-4ea1-abcd-a005d3666f4a'), (28286, '39470b50-3713-4d1b-a80d-3e82d6cdc0b3'), (8832, '5b58679b-9e79-45bd-b609-a9cfd637d06a'), (14465, '83b269a3-f505-41d3-82ff-1eb665eca1a9'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (18584, '815482b7-2249-4e35-a25e-18ccc16fd17b'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17067, '3bf582fa-4398-4a72-b777-790828b0ac34'), (200, '6a028e03-7235-4604-ae1a-4720d632dd52'), (24789, '6151fc10-9ec2-41ab-8af7-99a5aa43b076'), (5340, '78b2cae9-72fa-40f1-bbee-becd9000ade1'), (3293, '5f819fce-e9c7-4e44-9ca4-987d88b67b7c'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (22781, 'b13ec4bc-84fa-45ba-b0de-a9b045c4ba59'), (22780, 'edd8b962-01b2-4f07-9fe7-d76733c4cb6b'), (10493, '3ec8bf92-41e9-40b7-9f00-f8ed0724086b'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (16641, '29e13c06-e204-4a93-a6ac-03bf2fc934e4'), (2311, '2ddd9274-7512-4bdd-8e03-f54c56c6fdfd'), (16648, 'df4f4a13-501d-4c10-b9c0-49460f41f042'), (19213, '840e22ee-ba80-4c36-bc85-aee69aeb8196'), (15118, '93b598bf-d31e-4208-b48c-3ec7e363e751'), (4369, 'b6c4ae76-37bf-4efd-8b0d-7759406950e3'), (4372, '15104665-06f8-4513-9003-1763299b31bb'), (7956, 'd4ff1d21-f881-466a-b017-db4aa36a263d'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (15640, '31d2f322-0056-4e5f-86ac-5092c9d1a8ae'), (18714, 'fe897da9-d479-4ace-87b0-7b4588248f07'), (11037, 'e2587233-f088-45f5-84c5-cb20fd903daa'), (2855, 'a45109fc-b17f-4704-b636-3746515d9806'), (18728, '70353f36-5ed1-42b1-97fc-404e6f7df31a'), (2857, '38ec415f-811a-4083-9a14-e8b784be9054'), (2860, 'cd0fbf96-6263-4efc-9922-f67f4cce3a51'), (10032, '913e4e64-ed64-4da8-8450-9db05def7664'), (31546, 'beee1c56-1a47-432b-803d-8d5a9bbef9d3'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (8003, 'd276b6ee-69fb-4b8d-bf50-690c97270d08'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (31058, '63892ddc-34fa-42d7-b5f1-86a45aa445fb'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (19290, 'bdaea1c7-fd99-40b6-873e-6221088c435a'), (3934, '24000b88-1568-44be-a5d6-7fe94664b817'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (29030, 'ed348923-8671-4bc9-a0d6-a4245a724358'), (2412, '0c57a0cf-6f0b-46fa-be21-bd4c2d35ff2c'), (882, '2fadaf7f-4477-49fb-816e-b563239244f5'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (20856, '4f9550f1-181e-4522-88e9-c20941c7cb4f'), (11138, '3fda2fc9-6e91-43f4-a37d-d843d5ba7494'), (31622, 'c012b457-a421-4af8-92d1-38172f9768b8'), (19345, '481aa977-d7fa-4428-b152-084d47a8343d'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14247, '99b38e8f-68a6-4442-a93a-76803370e58f'), (7589, 'f4422a78-4905-4239-b7ef-f84981f84f3f'), (14249, '3bde59f7-ba94-4ec5-8768-6195057f7e50'), (31658, '9e35ef87-1099-4e1b-85c7-47560654a07e'), (14251, '93d814d8-d816-46e7-baa5-72a0dd060c27'), (14252, '77a63f90-12e6-4253-993b-4d2cad182d17'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (22445, '66ff92b2-7481-4b66-9cdf-5e8e7bced9b0'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (4028, 'c98b654b-2b16-424d-a288-12544a686cb5'), (21438, 'f486d89f-3657-4546-a293-d0f7bb46e794'), (21441, '437d81e7-4ab7-4143-8a89-e25826d44a56'), (968, '88e52945-9235-4271-97ab-7879699bc94e'), (5579, '57e6f3ee-cf0e-4625-9bf7-346b8f5823ae'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (13794, '3d2170c8-cc85-48a0-9180-b22d135a5f1f'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (29670, 'ccfff15f-115c-4de7-b620-659b6be0a991'), (29673, '5fa6188c-04bd-4733-bf74-5e5ee58334e3'), (29674, '81aa5082-4e7c-42b6-8684-491f31bb23aa'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29678, '978c5788-4b70-4882-ad94-79625d10e12f'), (29685, '618f64da-b777-4baa-a84c-3aeed9bd4704'), (30204, '0a5ab1df-1323-4d95-a90c-58d37c79519a'), (23037, '2846a21e-0855-4189-8474-0dbc618dee36')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
The 4 Kinds of Gradio Interfaces

So far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.

It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:
We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

## Sharing Your Demo

What good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return ""Hello "" + name + ""!""

demo = gr.Interface(fn=greet, inputs=""textbox"", outputs=""textbox"")
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter 🚀
Gradio Demo: dashboard
### This demo shows how you can build an interactive dashboard with gradio. Click on a python library on the left hand side and then on the right hand side click on the metric you'd like to see plot over time. Data is pulled from HuggingFace Hub datasets.
        


```
!pip install -q gradio plotly
```


```
# Downloading files from the demo repo
import os
!wget -q https://github.com/gradio-app/gradio/raw/main/demo/dashboard/helpers.py
```
We'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).

### Sharing Your Demo

What good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:

```python
import gradio as gr

def greet(name):
    return ""Hello "" + name + ""!""

demo = gr.Interface(fn=greet, inputs=""textbox"", outputs=""textbox"")
    
demo.launch(share=True)  # Share your demo with just 1 extra parameter 🚀
 
Here is the question: How can interactivity be improved in a Gradio-based Q-learning demo?","Based on the context, interactivity in a Gradio-based Q-learning demo can be improved by setting a `max_size` that is reasonable given expectations of how long users might be willing to wait for a prediction, and by writing the function to accept batches of inputs, which can increase parallelism and make the demo more efficient."
What best describes the integration of SQuAD metrics into a semantic segmentation workflow?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (9, 'ee938445-5e5d-4e33-b4a3-e17998c36b16'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (9756, 'e069aae1-679b-40e3-b27a-e8572cc4f72c'), (24607, '8f241013-9385-4355-bc80-ae8a6405c9ad'), (15395, '71a93765-78eb-42ad-b0a5-3203d5f8995b'), (15396, 'fdb0d317-8122-48fd-96db-d9486a8d2e55'), (9765, 'eb810505-9351-4111-b534-654b31082470'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (29743, '1628f56d-c18c-4c91-bdb8-dc690e6399dc'), (15409, '7d8ffb8d-f529-4f54-84fb-8ded62e8620b'), (26173, 'd7a2c16f-0893-4feb-91f4-3c5cb8b37930'), (3148, 'eb17b7df-c261-4c87-a068-9db283ce4f8e'), (21587, '94da7690-487a-4b9a-9209-2eb5a9a7fd51'), (20060, 'ce186691-06ac-411b-93cf-30eb955e0e0f'), (6767, '3f1e722c-81ec-4b6b-be92-d562c3b1f267'), (6768, '8898d7a0-b088-46fa-947b-e4d751b10926'), (21634, '59dd1789-1f01-48bf-8eb3-110a3171dd59'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (21639, 'f89e686f-ed68-4e75-a452-e5832f6fd820'), (21640, 'abe405b2-ba14-4e03-9ad6-1596d753124c'), (6801, 'a8d073e8-4092-47e2-b7f0-c61300a489a7'), (6807, '28b4e64e-1fdd-4cc1-8122-d34fbd3cfa1f'), (19127, 'bd45afb5-2ff0-41f6-9042-d4e2ad620dae'), (25788, '80465c07-5351-4512-b9d3-c7df4bbf4fd7'), (25795, '8a88bc05-c2cf-41f8-aec4-fd247e336b17'), (25796, 'f457e45a-0754-41ca-a0a6-2fdb1f0068a1'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (4816, 'a25658ec-cc9b-40ea-b2ab-ffd276670359'), (11477, 'c5876285-5fa1-4172-9c57-bef923092cfa'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (11479, 'fc956e8d-5dd1-4658-b204-0eb9c0902da0'), (11482, '20a475f8-c4d1-4a54-95c7-08202215095b'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (11485, '1ac012be-e3dd-4b0c-b474-6637ab42aabc'), (19170, '880d968f-6f41-4d99-ba94-11e54bb9b625'), (9955, '7ee7bc27-6bae-4837-bd55-d680c5e56127'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (19172, 'eb42355c-cae2-49a8-aa2b-8ce6e5bb9a88'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (19175, '556ab34a-7f05-49af-962c-ab0872458367'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (30975, '63c59737-5943-46ba-b976-f82d30ce8680'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (3334, 'be61aaba-10f0-4007-a548-48ae184f321f'), (16142, 'c771e38b-cfa7-4d59-96c0-b3151c87a39d'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (27424, '340dce7f-8d94-42f2-b250-66cee43d3a67'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (27446, 'a836a2b1-6614-4c59-bc09-62ded216179c'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (4922, '72fa3aeb-aff1-4f00-ad30-1abc8348f40a'), (4924, 'deedfb1d-fc7e-43ef-b276-351a4b81d5eb'), (4927, '62061aab-3f5b-42e2-a42a-b553dce5a502'), (4928, 'f552f0dc-5036-401b-8670-a271611c255a'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (10563, '138c4559-9b2e-43b7-a3bb-746c045905f0'), (4933, '2682eeec-2f87-45ae-823c-e45571425b9f'), (4934, 'a508bee7-e229-42e0-8c2a-b8b117a54fc3'), (13129, '39d1b188-cb2a-4d4c-b760-cc5e51bb538d'), (4944, '61aa7427-e3a9-48ae-b6cb-d54b354c21ee'), (12116, 'bb60fc61-08eb-484a-98d5-0aa3d2715f1c'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (12124, 'f2d8ff59-c9b5-4ccb-b0a9-5488b6acd532'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (27494, 'f762b4f5-1fab-45e9-84a8-da6c3a184d50'), (23399, '8c4d5380-a5bc-4827-8694-d2ad31217a3d'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (2930, '1cdeeab8-de41-4c2c-99e9-580b9279f768'), (22925, '5c22144f-4b1e-4566-8088-264be9c9149a'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (20405, 'e3b986a7-6a6e-4258-a2b4-940d463071ec'), (20407, 'd2ed85aa-27cd-4080-ae18-59d3f37d4d0f'), (16827, '9548d4e8-62bb-403e-b4b0-a233ed540940'), (13246, '7d3c0151-5723-45ae-9577-08964effc903'), (28609, '9c3549b2-3301-4ee5-a616-66788b24cd8f'), (22467, '099da8b2-e81c-406e-af44-0cda9eee86ff'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (21959, 'e07e46db-da0b-45dc-98f8-c77c3ec3b081'), (4566, 'cb029d8d-8fc2-4dea-a947-d50b975b7925'), (21466, 'f6d14c74-029a-49c7-abc6-cad4fba7ad91'), (7132, 'a6493562-d0d0-430d-82c0-7db219a71c05'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (21469, '70c3e6d0-0def-4824-b02a-c50d18bdfe9b'), (5599, 'dd67de1c-aeff-4a2f-8d93-77772f57bfcd'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5601, '098dde3d-a5c2-4abc-bdf3-f35ebe913ba2'), (5602, '983f13dc-846c-463b-b63f-5b68c866bb0b'), (25573, '340d60ab-d024-458a-ba51-ed5bc11a806f'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (28655, '35c2c419-d612-4a35-bc16-bde5d7aaf7c9'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (28659, '8af7827d-abea-40a9-8154-cb7dd5e9b2ef'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (8695, '1bfbe4f7-517a-43ad-93d5-ab487e7ddfbd'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

Next, we'll define a function that computes the evaluation metric we want to work with. Because we're doing semantic segmentation, we'll use the [mean Intersection over Union (mIoU)](https://huggingface.co/spaces/evaluate-metric/mean_iou), directly accessible in the [`evaluate` library](https://huggingface.co/docs/evaluate/index). IoU represents the overlap of segmentation masks. Mean IoU is the average of the IoU of all semantic classes. Take a look at [this blogpost](https://www.jeremyjordan.me/evaluating-image-segmentation-models/) for an overview of evaluation metrics for image segmentation.

Because our model outputs logits with dimensions height/4 and width/4, we have to upscale them before we can compute the mIoU.


```python
import torch
from torch import nn
import evaluate

metric = evaluate.load(""mean_iou"")
Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
# Metric Card for SQuAD v2

## Metric description
This metric wraps the official scoring script for version 2 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad_v2).

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

SQuAD 2.0 combines the 100,000 questions in SQuAD 1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering.

## How to use 

The metric takes two files or two lists - one representing model predictions and the other the references to compare them to.
supervision and without any pixel-level annotations, GroupViT learns to group together semantic regions and successfully transfers to the task of semantic segmentation in a zero-shot manner, i.e., without any further fine-tuning. It achieves a zero-shot accuracy of 52.3% mIoU on the PASCAL VOC 2012 and 22.4% mIoU on PASCAL Context datasets, and performs competitively to state-of-the-art transfer-learning methods requiring greater levels of supervision.*
## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from evaluate import load
squad_metric = load(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
 
Here is the question: What best describes the integration of SQuAD metrics into a semantic segmentation workflow?","The SQuAD metrics are not integrated into the semantic segmentation workflow. The SQuAD metrics are for evaluating reading comprehension models on the Stanford Question Answering Dataset (SQuAD), whereas the semantic segmentation workflow is using the mean Intersection over Union (mIoU) metric to evaluate the model's performance on image segmentation tasks."
What is the best method to evaluate a FrozenLake Q-learning agent?,"[(26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (14882, '852649be-111b-45c0-a6b9-59df28eb5286'), (17961, '3a3a9bdc-e3cb-41c3-9672-90e1f2cfdbbb'), (7732, '8d07afb9-9121-46ff-a877-120dcac45b78'), (7733, 'a2362124-f3cb-4cc4-9b53-12b93d771951'), (7734, '85ca7829-223b-4021-a48e-c1331212e356'), (27702, '768fdc11-37ba-43a8-8a96-80545bcb82bf'), (27706, '81c17562-7f3d-40fd-944d-ee0b54114838'), (21570, '59e82945-6577-401b-a9d4-b39d18e1ff2b'), (21572, '3e8b8e08-4ae8-4d17-ad2d-2ea9eb7c37ee'), (27721, '3f3795c6-7158-4cce-964f-83b9d8df21b4'), (27722, 'c667aeb9-e35c-4791-8c36-ac06162bf37e'), (30817, 'ce6db1b0-14b2-4588-be54-92b24764e498'), (21091, '87783093-9756-4756-8b30-173539c965ca'), (13925, 'f4d3df0e-cb78-480b-988d-5e680ddbd5e6'), (26727, 'aa34abd9-5ca0-479f-8fe5-8f5a215baf20'), (26728, '7b671840-3675-4abf-895d-c4fe995d2dfd'), (13927, '38fd6452-7b8a-4923-aeb8-b117f1211ee6'), (28782, 'f832e002-e981-4806-b6b5-7743c24c81db'), (28784, '351f6005-a2b3-4b17-abf5-a5a07d1bf877'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (17525, '7500c161-c38b-4986-ae2a-ce026b6c10f7'), (17527, 'bd457bd8-f4ad-4b21-9f6c-2cc379928d75'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (17530, 'c9b2877b-b331-401f-9cf7-6145e2ebf403'), (17533, 'f2d73255-0ee5-4a2b-a2a4-b10e4d2c0612'), (17534, 'e8a226a9-ce5d-412c-b8bb-1a951a1857fb'), (17539, '54748e65-182a-4269-bd1f-6a11d3a531d1'), (17541, '066f6d02-943c-42b4-bb34-3177430a52d6'), (17545, '9309792d-a91d-4967-9d4e-76d19030c9c2'), (17550, 'f7ea19ba-2641-4b2f-89db-c031f1abbd53'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (144, 'ad9a2442-d5bf-479e-bf4a-b911cfbe1dd2'), (8850, 'f618b39d-a6d6-423b-b81f-c9e4b252f663'), (17556, '919013bb-f304-449e-b11b-8027a2022684'), (17557, '0f6b36da-2ed0-49e5-b5d2-09ced93bbc46'), (17558, '2bd70e53-9495-4328-911c-02296b863f4b'), (17560, '4985bb51-18be-411a-b1d0-3fd7d5abb568'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (17572, 'b665d2df-6e85-4589-a245-3eceefb2bfc6'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (8871, '370b7369-f26d-4340-86fd-357c2065b987'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (163, '12b31373-c096-4126-8d61-c0cee6b23bce'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (14506, 'ab0aebcb-84ed-4c62-8106-95d67fa96ab5'), (17583, '0dbe2699-71f1-4305-a75b-eaca59468638'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25265, '177935f5-c8bc-4c5a-9cf0-07eefafb6b83'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (25263, 'd7a4626e-6223-485c-95a7-4e2cd4cb33b1'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25270, '5bbe44fe-46d8-484e-8cdc-15d0afcee3ee'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25281, 'a825bb91-b46e-497a-b2bf-52f559ad86d8'), (25282, 'c5f90a36-0447-48df-ba63-aaba6325e62e'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (22745, '178ef776-3f46-4d08-bfbe-38fccf619c70'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (17182, 'e35b0873-45e6-4904-800e-d69da7501701'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (1320, 'd5a5263a-a570-474e-8d13-92d45d7a6e95'), (1339, 'a10dddfb-e0c1-4342-9d4b-4cfa501b83b4'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (167, 'bed696f3-f53a-41f2-8793-1fe84ebe7310'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (20325, '8e25c4d9-f41e-426e-a372-9465025605ad'), (20326, '9d94613c-da69-433f-a73d-136ad258d459'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (20329, '32e96d05-f2b2-42de-bd87-cde71e890e69'), (28526, '99ae54b8-b8c0-49a8-b122-3be8abec3e42'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (24435, '4d1f7268-cba2-40e0-afce-b5a9799a5792'), (20854, '110af335-df5a-4138-bd7b-a0ef1308d837'), (20342, '63ba5e06-4514-4380-a011-5c55dede8208'), (28534, '3e54a2ff-2d80-46f7-870f-7a3153c2036b'), (20345, '90c786f2-9de0-48b8-b056-36c300207419'), (23944, 'fd1a1aa9-c058-47ff-ae6d-f4e2de03a110'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (23946, '0d096575-2d3c-4976-93fc-0610e2da9ae0'), (23947, '925054c6-05b5-4bda-a736-9a6f6991a7ae'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (3481, '4c7ec93f-67c6-4662-83f9-0ef6f0461760'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (23974, '44af3921-c4f0-41f8-b0ec-0e3f805863e1'), (14773, 'eddfb9b5-65fd-4a2d-a578-118281f8c55e'), (14776, '25ac1795-a4d1-45b8-837a-29dc4ccf5415'), (11716, '5aa7f3a8-b022-4936-88ea-c22c3c501a94'), (29639, 'ad7bf7d2-d8c6-47bf-9e9c-6c68383ad368'), (13787, '3d112122-a48e-416f-9e0d-1d80749c9c5a'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: We'll also **implement our first RL agent from scratch**, a Q-Learning agent, and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to **go from the starting state (S) to the goal state (G)** by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi: where our agent will need **to learn to navigate** a city to **transport its passengers from point A to point B.**


<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif"" alt=""Environments""/>

Concretely, we will:

- Learn about **value-based methods**.
- Learn about the **differences between Monte Carlo and Temporal Difference Learning**.
- Study and implement **our first RL algorithm**: Q-Learning.
```

## Evaluate our Q-Learning agent 📈

- Usually, you should have a mean reward of 1.0
- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex.

```python
# Evaluate our Agent
mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f""Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}"")
We'll also **implement our first RL agent from scratch**: a Q-Learning agent and will train it in two environments:

1. Frozen-Lake-v1 (non-slippery version): where our agent will need to **go from the starting state (S) to the goal state (G)** by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi will need **to learn to navigate** a city to **transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/envs.gif"" alt=""Environments""/>
</figure>

This unit is divided into 2 parts:
<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/70_deep_rl_q_part1/two_parts.jpg"" alt=""Two Parts""/>
</figure>

In the first part, we'll **learn about the value-based methods and the difference between Monte Carlo and Temporal Difference Learning.**

And in the second part, **we'll study our first RL algorithm: Q-Learning, and implement our first RL Agent.**
So, in the second part, we’ll **study Q-Learning**, **and implement our first RL agent from scratch**, a Q-Learning agent, and will train it in two environments:

1. Frozen Lake v1 ❄️: where our agent will need to **go from the starting state (S) to the goal state (G)** by walking only on frozen tiles (F) and avoiding holes (H).
2. An autonomous taxi 🚕: where the agent will need **to learn to navigate** a city to **transport its passengers from point A to point B.**

<figure class=""image table text-center m-0 w-full"">
  <img src=""assets/73_deep_rl_q_part2/envs.gif"" alt=""Environments""/>
</figure>

This unit is fundamental if you want to be able to work on Deep Q-Learning (Unit 3).

So let’s get started! 🚀

- [Introducing Q-Learning](#introducing-q-learning)
  - [What is Q-Learning?](#what-is-q-learning)
  - [The Q-Learning algorithm](#the-q-learning-algorithm)
  - [Off-policy vs. On-policy](#off-policy-vs-on-policy)
- [A Q-Learning example](#a-q-learning-example)
```

We're now ready to code our Q-Learning algorithm 🔥

# Part 1: Frozen Lake ⛄ (non slippery version)

## Create and understand [FrozenLake environment ⛄]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)
---

💡 A good habit when you start to use an environment is to check its documentation

👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/

---

We're going to train our Q-Learning agent **to navigate from the starting state (S) to the goal state (G) by walking only on frozen tiles (F) and avoid holes (H)**.

We can have two sizes of environment:

- `map_name=""4x4""`: a 4x4 grid version
- `map_name=""8x8""`: a 8x8 grid version


The environment has two modes:

- `is_slippery=False`: The agent always moves **in the intended direction** due to the non-slippery nature of the frozen lake (deterministic).
- `is_slippery=True`: The agent **may not always move in the intended direction** due to the slippery nature of the frozen lake (stochastic).
 
Here is the question: What is the best method to evaluate a FrozenLake Q-learning agent?","The best method to evaluate a FrozenLake Q-learning agent is to calculate the mean reward of the agent. Usually, you should have a mean reward of 1.0."
Which combination optimizes Gradio demos for semantic segmentation?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (8195, '861f0b53-114b-45d7-a496-19803646eaf0'), (10253, 'e0fdcfc2-6e25-4d57-8dea-e42c925a1e2c'), (530, '283699ed-c384-40ef-bd5c-44fb732077a8'), (28185, 'd648591c-3383-45e8-ae71-83b504e2b8bd'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (16430, '66815a93-d7da-4188-9272-6bbfdf32ae6b'), (7741, '404ecfe6-cf93-4fff-b68d-6c5d1f3e1871'), (11838, 'e4ab66e3-f6c3-4b1e-a385-2e87d7ec1d07'), (27724, '97cf97fc-7c5f-4d7b-9e58-02224ba44e30'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (28243, '0bf9268d-f628-4160-b2fb-5d484fbd603f'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (17500, '952950af-f44a-43ea-828b-79daa8e88544'), (28255, 'bcb424b2-729f-4991-87ac-45ef5ce77853'), (16479, 'f8325aa8-886a-43ff-9c7c-96bb8c00df22'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (10864, '9971a567-07ea-4560-98d4-eede4b1d186b'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (1144, 'a663b646-8b6a-43f0-8527-660d724e1849'), (28281, '1f26f38e-955f-43c3-b484-cf828c48fd85'), (28282, '464f484f-7d78-406d-9b75-a994dc1cba25'), (25213, 'a685a826-ac2d-4ea1-abcd-a005d3666f4a'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (4740, 'a3172a98-921d-49f4-98ff-7a1f3290f6af'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (28305, '35e22005-7747-4988-a75c-23a2275641c0'), (2201, 'f63d9476-9b03-480b-81dc-56db1f33810e'), (31387, '47d6c03e-2d5b-4713-9953-0a20c8fa6bbf'), (2203, '1daeda64-b42f-49a8-913c-f245e97796d6'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (27817, 'a9c81310-0c86-4115-9626-68c493091cda'), (27841, 'c37d24ce-3d99-4ab8-9016-ad2a6d6c1e39'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (9422, '14ddfa55-b16d-426e-93f3-e72ae34972f0'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (6403, '6b72a701-c1b5-46a7-bcb8-a543df789c1f'), (2311, '2ddd9274-7512-4bdd-8e03-f54c56c6fdfd'), (24842, '0d694989-93f1-48d3-b256-e21d25ab982b'), (24331, 'e99463c9-3fff-46a2-98cd-b973d53da958'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (30999, '53998aea-d5c1-4b05-87c7-09e519c96443'), (15640, '31d2f322-0056-4e5f-86ac-5092c9d1a8ae'), (22318, '04f5bd94-17e7-41e7-8caa-f844242aacf8'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (6973, '1b3ca299-1a5b-4ef1-913a-2012d57029c8'), (2369, '2d8765ce-950f-4812-be38-74a3ca44c156'), (10049, '54ef31a1-53bb-423e-9653-c5f902ed05ab'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (13124, 'aea52821-30f4-464d-97eb-a126eed5e070'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18247, '74315d00-318c-4e2d-84b1-cb46200e84ce'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (13129, '39d1b188-cb2a-4d4c-b760-cc5e51bb538d'), (2381, '25022ddc-db5e-48bf-a2bf-918a00646b76'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (22866, 'ddec1804-5a36-4f25-9429-414ca69c8abd'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (20310, '03e6bed2-053e-46f3-83c9-e9e52ffe5997'), (19290, 'bdaea1c7-fd99-40b6-873e-6221088c435a'), (3934, '24000b88-1568-44be-a5d6-7fe94664b817'), (5474, '1158d6f0-fad9-4bbb-babf-d6178165afc7'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (23395, '0d90b5c9-4b7b-4a57-86dc-af8d7fcb6c9c'), (2407, '59755c96-9597-4369-8901-6b6346cf8892'), (2408, '3c930812-0a9f-46fb-92ef-42ccc975ebc5'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (28536, 'a61182a7-0b78-447e-b85b-2fed9d53fc77'), (20857, '1561c4d2-349a-4a35-b8b9-2422553f4829'), (2431, 'd82501d2-94be-47fd-843d-2fab52c53bb9'), (24974, 'f1c0e348-09df-4e2b-b7ca-2b4104b24b23'), (1943, 'd18071fb-d551-4b15-bdda-1775319ff90a'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (9629, '996348a9-c581-4d4a-aa01-4f8cccfad6ae'), (15781, '6a546fd4-6a50-4e31-8789-c9e292a36bec'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (7589, 'f4422a78-4905-4239-b7ef-f84981f84f3f'), (28075, 'bc307f87-d3b6-4e90-95d8-df896b01cb89'), (28077, '880f9923-936e-43dd-bb07-042c35cd6eb6'), (1967, 'e50667e5-7a0c-461c-aff5-9a06cf3ab823'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (13746, '0bb29eea-90a5-4fd6-8177-0ccbbf32e132'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (4028, 'c98b654b-2b16-424d-a288-12544a686cb5'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (7652, '74e1f9d5-9ae3-4a7e-a167-e5947691f653'), (14314, 'a3691777-5101-400c-a9e4-34a9b9c6c7b2'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Gradio Demo: text_analysis
### This simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation.
        


```
!pip install -q gradio spacy
```
his simple demo takes advantage of Gradio's HighlightedText, JSON and HTML outputs to create a clear NER segmentation.
Gradio Demo: image_segmentation
### Simple image segmentation using gradio's AnnotatedImage component.
        


```
!pip install -q gradio 
```
Gradio Demo: Echocardiogram-Segmentation


```
!pip install -q gradio -f https://download.pytorch.org/whl/torch_stable.html numpy matplotlib wget torch torchvision   
```


```
# Downloading files from the demo repo
import os
!wget -q https://github.com/gradio-app/gradio/raw/main/demo/Echocardiogram-Segmentation/img1.jpg
!wget -q https://github.com/gradio-app/gradio/raw/main/demo/Echocardiogram-Segmentation/img2.jpg
```
imple image segmentation using gradio's AnnotatedImage component.
 
Here is the question: Which combination optimizes Gradio demos for semantic segmentation?","Based on the context, the combination that optimizes Gradio demos for semantic segmentation is:

`!pip install -q gradio -f https://download.pytorch.org/whl/torch_stable.html numpy matplotlib wget torch torchvision`

This combination installs Gradio along with PyTorch, NumPy, Matplotlib, and other necessary libraries, which are required for semantic segmentation tasks."
How can reinforcement learning workflows benefit from Gradio enhancements?,"[(28163, '5b05c947-965d-452b-b6a2-11871c6ca3de'), (27658, 'd78d6628-059b-4669-aa0e-3ec08cbd5db3'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (2075, '57697ebc-669f-4aa3-8585-4c1388c93630'), (5661, 'd8772dad-7e28-4ef1-bdae-a1b7fda9c18c'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (28212, '5e9daa1d-4aac-4b33-b060-98ba4480ba10'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (28219, '53ccc20d-0caf-44e6-ba84-9d617960607b'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (28261, 'bfc07af5-45ac-4bb9-85a4-5ee400369c7a'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29292, '4214c4f3-080e-4daf-807b-01ee54650589'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (28279, '835b3d07-8075-4019-a72c-9d9253024dd9'), (25210, '2697df78-2558-4fe9-b9f1-6f730389b8e0'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7803, '6a89c97a-46b0-4481-8f91-7c30ad2193d6'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (23189, '36e981bb-328e-4d35-a16b-5facd733cb56'), (25241, '8ffca09f-8a35-494f-b9e1-c50b4ebd9632'), (2208, 'ba593b1c-6f5d-49c6-b377-4b55c72484e4'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (21674, 'd50e3027-ded0-476c-98f4-2bde7ec4db1b'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (27351, '472b1c8c-b7ef-4926-8092-42cba02c3d9c'), (14042, '6d5f441d-7e53-469a-af95-3fdc35f8fa6b'), (2273, 'a6326c4f-6dff-4e5d-bdf4-9a5fcac51933'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (2285, '5b7edfae-d1bb-4c5d-8d8f-83b616f05d0d'), (2289, '7dedbe04-fd86-4b28-9140-a527061b8400'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (23807, '9d7cdc35-737d-4b2c-9e58-0c6898cdfc31'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (24842, '0d694989-93f1-48d3-b256-e21d25ab982b'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (2338, '573a62da-e270-46be-9b10-d3f2a1466060'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2345, '3bd3225a-2064-438c-80c1-e672696ab2a2'), (27949, 'f9f3c1af-030d-42eb-b727-680bb2457e6d'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (2387, '7c12efc3-2c67-4d4e-b835-f6c066184a68'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (20826, '03cab1e0-bceb-43ed-b419-ce42e8f8c4eb'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (2405, 'c11b172e-c44e-453b-802d-3a9e7e40954e'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (12161, 'b0547742-b229-4a52-aaf4-073b9efc9a8a'), (2464, '54179c43-bf4a-4c14-b098-b5140e0a1a4b'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (12720, '43bba8c3-1827-429c-a918-da99a6efbc71'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (28082, '8c86f9c1-60c8-40ed-8084-d82bfb3fc00a'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (27066, '580b5cc7-1b78-434c-b69f-5d96e6bd54a9'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29666, 'd518d011-4a11-4759-922a-8c4af66d1d89'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (5613, '5730d53c-cfc2-46cd-8958-07e68d2c19e5'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30192, '0dd45130-aa3c-4726-bab7-30afeed79b5b'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28147, '5d201cfe-6314-4484-a0ec-2c2bbc19b283'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd'), (28159, '82652842-43af-40f0-9e66-fbd1e5adac72')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Paradoxically, setting a `max_size` can often improve user experience because it prevents users from being dissuaded by very long queue wait times. Users who are more interested and invested in your demo will keep trying to join the queue, and will be able to get their results faster.

**Recommendation**: For a better user experience, set a `max_size` that is reasonable given your expectations of how long users might be willing to wait for a prediction.

### The `max_batch_size` parameter in events

Another way to increase the parallelism of your Gradio demo is to write your function so that it can accept **batches** of inputs. Most deep learning models can process batches of samples more efficiently than processing individual samples.
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!

## 0.1.0

### Features

- [#5005](https://github.com/gradio-app/gradio/pull/5005) [`f5539c76`](https://github.com/gradio-app/gradio/commit/f5539c7618e31451420bd3228754774da14dc65f) - Enhancement: Add focus event to textbox and number component. Thanks [@JodyZ0203](https://github.com/JodyZ0203)!

### Fixes
## Introduction

Tabular data science is the most widely used domain of machine learning, with problems ranging from customer segmentation to churn prediction. Throughout various stages of the tabular data science workflow, communicating your work to stakeholders or clients can be cumbersome; which prevents data scientists from focusing on what matters, such as data analysis and model building. Data scientists can end up spending hours building a dashboard that takes in dataframe and returning plots, or returning a prediction or plot of clusters in a dataset. In this guide, we'll go through how to use `gradio` to improve your data science workflows. We will also talk about how to use `gradio` and [skops](https://skops.readthedocs.io/en/stable/) to build interfaces with only one line of code!

### Prerequisites

Make sure you have the `gradio` Python package already [installed](/getting_started).

## Let's Create a Simple Interface!
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
```

**Try it out**: You can see this example running in [this Hugging Face Static Space](https://huggingface.co/spaces/abidlabs/gradio-lite-classify), which lets you host static (serverless) web applications for free. Visit the page and you'll be able to run a machine learning model without internet access!

## Benefits of Using `@gradio/lite`

### 1. Serverless Deployment
The primary advantage of @gradio/lite is that it eliminates the need for server infrastructure. This simplifies deployment, reduces server-related costs, and makes it easier to share your Gradio applications with others.

### 2. Low Latency
By running in the browser, @gradio/lite offers low-latency interactions for users. There's no need for data to travel to and from a server, resulting in faster responses and a smoother user experience.
 
Here is the question: How can reinforcement learning workflows benefit from Gradio enhancements?","The context does not provide a direct answer to how reinforcement learning workflows can benefit from Gradio enhancements. The context primarily discusses Gradio's features, improvements, and benefits in the context of tabular data science workflows, with no mention of reinforcement learning. Therefore, it is not possible to provide a comprehensive answer to this question based on the provided context."
What makes Q-learning scalable for complex environments?,"[(26653, '44e22b81-b52a-4025-a3c8-b48592fc9a64'), (26654, '042ecf39-629a-44e3-be8d-e471b2b8640a'), (26655, '14ea53e1-bcb7-4126-b80b-09759ba5a722'), (26656, '7cd01870-b453-47fe-b8b4-83d5c97bd9da'), (17966, '0b146d2b-e740-4ab1-861b-c17317d4c9b8'), (7732, '8d07afb9-9121-46ff-a877-120dcac45b78'), (7733, 'a2362124-f3cb-4cc4-9b53-12b93d771951'), (7734, '85ca7829-223b-4021-a48e-c1331212e356'), (27722, 'c667aeb9-e35c-4791-8c36-ac06162bf37e'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (21604, 'ef6f4014-2b68-45df-b66a-a23331cdcc1b'), (14438, 'adc4d77e-0477-455a-8002-6bde28537b35'), (26727, 'aa34abd9-5ca0-479f-8fe5-8f5a215baf20'), (26728, '7b671840-3675-4abf-895d-c4fe995d2dfd'), (28782, 'f832e002-e981-4806-b6b5-7743c24c81db'), (28784, '351f6005-a2b3-4b17-abf5-a5a07d1bf877'), (28787, '340b023c-f3c5-4db1-bd39-92e99e7c3fc3'), (17525, '7500c161-c38b-4986-ae2a-ce026b6c10f7'), (23160, 'a5d757ab-f68a-4fa9-80c4-258f56886fc2'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (17532, 'ae190843-2669-417f-bcc6-7b17066f1ded'), (17533, 'f2d73255-0ee5-4a2b-a2a4-b10e4d2c0612'), (17534, 'e8a226a9-ce5d-412c-b8bb-1a951a1857fb'), (17539, '54748e65-182a-4269-bd1f-6a11d3a531d1'), (143, 'eea555e3-9de1-41c4-b90c-9b068d045134'), (144, 'ad9a2442-d5bf-479e-bf4a-b911cfbe1dd2'), (17552, 'e6dc2f24-99a1-4fd8-a073-c0a439c5d184'), (17557, '0f6b36da-2ed0-49e5-b5d2-09ced93bbc46'), (17560, '4985bb51-18be-411a-b1d0-3fd7d5abb568'), (8347, '6aa8880a-8695-4c23-b28b-def12d9eea77'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (8866, '7746f015-9eaf-4510-a881-4e62c32c21b1'), (8867, '6b349e82-c59a-470a-9dea-41ac2e88e1b5'), (8868, '928a0250-c8cc-4327-847f-79f3912cdb2a'), (8869, '1c711ca5-0c70-4cfd-b3f2-8f9a0fc4fda9'), (8870, '63855897-4b1a-4af7-9fee-9f592898fc84'), (8871, '370b7369-f26d-4340-86fd-357c2065b987'), (8872, '77af9de5-5cd4-41ea-84d5-cff9004ac7ca'), (8873, '661b5fe6-ef90-47bf-94d9-4b14826e826d'), (8874, '3cabe335-5230-42b5-8102-570ba914abde'), (8876, 'd4dfdc57-f700-4e20-acc7-592f16b88ccf'), (17584, '6c90256a-ecc4-4594-8498-330768d6a8de'), (25265, '177935f5-c8bc-4c5a-9cf0-07eefafb6b83'), (25266, '8a9d67a4-973b-45dd-a057-7d8f0ecab024'), (17585, '57d5dfc1-963c-4fe2-8c61-5ac6acc65e15'), (25268, 'd2faf7c7-de78-4150-8442-df2c4d374e6d'), (25269, '3baaf0bf-5a5a-4591-a2b1-92d9bee6979e'), (25270, '5bbe44fe-46d8-484e-8cdc-15d0afcee3ee'), (25264, 'a647ba14-4bf0-4f84-b7ec-dec8e85d8fd1'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (25272, '183d10b4-a909-471c-9c70-73c86252f116'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (25281, 'a825bb91-b46e-497a-b2bf-52f559ad86d8'), (25282, 'c5f90a36-0447-48df-ba63-aaba6325e62e'), (2768, '4e4170b6-fcbc-44c3-aa44-53377196e1e4'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (5869, 'bb6cdbd7-bbcf-493e-8250-030bab25db6c'), (27373, '1cf527a1-db9e-45b4-affe-fab3bfe9777e'), (5872, '16f36727-d1a9-4d89-9ff9-364a903634fa'), (11507, '97835531-9aad-446d-b001-fcb29b66d609'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (21807, 'b3346506-6c4a-48ac-9d75-c1258f287641'), (20803, 'd90f7eb1-7beb-4503-9c0f-a958a651a4dc'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (20325, '8e25c4d9-f41e-426e-a372-9465025605ad'), (20326, '9d94613c-da69-433f-a73d-136ad258d459'), (20327, 'db43986d-7e00-4d86-8a36-11e8aaa8e37e'), (20328, 'cd5fbafb-9ecd-4b1a-988b-f679747dbaa7'), (20329, '32e96d05-f2b2-42de-bd87-cde71e890e69'), (20330, '307f8dbe-c15b-4989-a66b-e9a437fb3480'), (20331, 'ad781e21-34b5-465a-b44f-1d0ef976b671'), (28525, '60cdc2d0-c35a-420d-bc30-64e0ac98ea5a'), (28526, '99ae54b8-b8c0-49a8-b122-3be8abec3e42'), (20334, '667dc4e1-4197-4c21-ae58-e797af402ed0'), (20336, 'd2582ee2-dd1b-40d8-ab3e-09b77460db3b'), (20335, '24699ce0-932d-4081-a7d8-ed55918efde0'), (28529, 'f43bdd02-a29e-4b4f-a712-94cf31ec3551'), (28527, '9dd374d0-f99b-4b6b-9b15-5f4ce6382aa4'), (20337, '1ac41f0f-b53f-41f2-adc7-29016e775aba'), (28531, 'f3ff6d8a-4d00-45e1-bba1-8468c7b59936'), (28534, '3e54a2ff-2d80-46f7-870f-7a3153c2036b'), (28535, 'c4e1adc6-dc57-4146-96e1-ff983b95b866'), (20343, '5f332e1f-fff2-4a93-becc-b7ee2e2e97ce'), (20345, '90c786f2-9de0-48b8-b056-36c300207419'), (20344, 'edac2d6a-a444-44cc-b455-7144e89a1f09'), (20854, '110af335-df5a-4138-bd7b-a0ef1308d837'), (20342, '63ba5e06-4514-4380-a011-5c55dede8208'), (23946, '0d096575-2d3c-4976-93fc-0610e2da9ae0'), (23947, '925054c6-05b5-4bda-a736-9a6f6991a7ae'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (3481, '4c7ec93f-67c6-4662-83f9-0ef6f0461760'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (23963, '81281aae-4625-4dcf-a6de-74108ef7e080'), (23972, '21ae0098-6a87-484b-ae9a-d0ad1b9f1230'), (14773, 'eddfb9b5-65fd-4a2d-a578-118281f8c55e'), (14774, 'd85cfc80-67be-498d-9904-3d83a553094a'), (14775, 'fbd3866f-6df1-4f19-bdd5-dd2c6bd9441f'), (14776, '25ac1795-a4d1-45b8-837a-29dc4ccf5415'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (962, 'c68008e9-dc91-4db0-a2f0-89a793040fff'), (11714, '81248f40-6e7d-4225-8017-1c87a3d5c64a'), (27085, 'ee8bb765-44a6-4187-9e25-b70671fe43b7'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (13782, 'cc4cbb46-9352-4787-bd3e-b5aec0c09d4e'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: From Q-Learning to Deep Q-Learning [[from-q-to-dqn]]

We learned that **Q-Learning is an algorithm we use to train our Q-Function**, an **action-value function** that determines the value of being at a particular state and taking a specific action at that state.

<figure>
  <img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function.jpg"" alt=""Q-function""/>
</figure>

The **Q comes from ""the Quality"" of that action at that state.**

Internally, our Q-function is encoded by **a Q-table, a table where each cell corresponds to a state-action pair value.** Think of this Q-table as **the memory or cheat sheet of our Q-function.**

The problem is that Q-Learning is a *tabular method*. This becomes a problem if the states and actions spaces **are not small enough to be represented efficiently by arrays and tables**. In other words: it is **not scalable**.
Q-Learning worked well with small state space environments like:
```

## Evaluate our Q-Learning agent 📈

- Usually, you should have a mean reward of 1.0
- The **environment is relatively easy** since the state space is really small (16). What you can try to do is [to replace it with the slippery version](https://www.gymlibrary.dev/environments/toy_text/frozen_lake/), which introduces stochasticity, making the environment more complex.

```python
# Evaluate our Agent
mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)
print(f""Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}"")
<figure class=""image table text-center m-0 w-full""> <img src=""assets/73_deep_rl_q_part2/Q-function.jpg"" alt=""Q-function""/> <figcaption>Given a state and action, our Q Function outputs a state-action value (also called Q-value)</figcaption> </figure>

The **Q comes from ""the Quality"" of that action at that state.**

Internally, our Q-function has **a Q-table, a table where each cell corresponds to a state-action pair value.** Think of this Q-table as **the memory or cheat sheet of our Q-function.**

The problem is that Q-Learning is a *tabular method*. Aka, a problem in which the state and actions spaces **are small enough to approximate value functions to be represented as arrays and tables**. And this is **not scalable**.

Q-Learning was working well with small state space environments like:

- FrozenLake, we had 14 states.
- Taxi-v3, we had 500 states.

But think of what we're going to do today: we will train an agent to learn to play Space Invaders using the frames as input.
_____________________________________________________________________
Congrats 🥳, you've just implemented, trained, and uploaded your first Reinforcement Learning agent.

Understanding Q-Learning is an **important step to understanding value-based methods.**

In the next Unit with Deep Q-Learning, we'll see that while creating and updating a Q-table was a good strategy — **however, it is not scalable.**

For instance, imagine you create an agent that learns to play Doom.

<img src=""https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png"" alt=""Doom""/>

Doom is a large environment with a huge state space (millions of different states). Creating and updating a Q-table for that environment would not be efficient.

That's why we'll study Deep Q-Learning in the next unit, an algorithm **where we use a neural network that approximates, given a state, the different Q-values for each action.**
But as we'll see, producing and updating a **Q-table can become ineffective in large state space environments.**

So in this unit, **we'll study our first Deep Reinforcement Learning agent**: Deep Q-Learning. Instead of using a Q-table, Deep Q-Learning uses a Neural Network that takes a state and approximates Q-values for each action based on that state.

And **we'll train it to play Space Invaders and other Atari environments using [RL-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)**, a training framework for RL using Stable-Baselines that provides scripts for training, evaluating agents, tuning hyperparameters, plotting results, and recording videos.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif"" alt=""Environments""/>

So let’s get started! 🚀
 
Here is the question: What makes Q-learning scalable for complex environments?","According to the context, Q-learning is not scalable for complex environments. In fact, it is stated that Q-learning is a tabular method that becomes a problem when the state and action spaces are not small enough to be represented efficiently by arrays and tables, making it not scalable. It is mentioned that Q-learning works well with small state space environments, but it becomes inefficient when dealing with large state space environments, such as Doom or Space Invaders. Therefore, it is Deep Q-Learning, which"
How can SQuAD metrics improve segmentation model performance evaluations?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (3, 'a958fa67-0263-40fe-aaa1-f05fdc6884b3'), (4, '6772e5c6-cd4d-4a56-84fa-c161ac6651b6'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (1560, '9c881267-9303-4686-81be-1ac7f7fd886c'), (1563, '7eb98c85-2153-41ac-b24e-c57402004544'), (24604, '0385f67b-6e32-4932-a53b-9211bd2aa9ed'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (24607, '8f241013-9385-4355-bc80-ae8a6405c9ad'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (13361, '2a46555c-7f28-4592-ba1c-b08638649b90'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (15924, '0fa9c0d3-9d36-4b4c-9711-b6faf189ad89'), (6200, '0a8f0e5f-ea71-4a51-9143-e020ffa286f2'), (21587, '94da7690-487a-4b9a-9209-2eb5a9a7fd51'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (9862, '33a9f610-ef4f-48c2-8df9-388fa0fd2a8f'), (9863, 'a9841602-0157-4f97-8e6f-19b4d1415529'), (21640, 'abe405b2-ba14-4e03-9ad6-1596d753124c'), (6801, 'a8d073e8-4092-47e2-b7f0-c61300a489a7'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (6310, '74a4cbde-53ef-459b-948d-593f322581a5'), (11437, 'bcf6c108-66d8-4c82-8e0b-90a0f98d8602'), (6319, '71e77401-a57c-4f0f-9fe7-add6c21b71a4'), (11951, 'e2e3c3c6-9b1e-41a4-9614-e5c2e001ff14'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (5316, 'ba980fc9-284b-4847-8d3d-2e1c0abbda6a'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (7907, 'cb3c07b7-bee9-4236-b390-7bf897dbd84d'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (7909, 'dd794d88-5bc7-4f34-a73a-8e22216ed4c4'), (3304, '918998b6-1cb0-4e26-8d59-d80c01cd2975'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (31475, 'b1f00834-8c6d-48f1-8a7f-6d5a0d9c3e52'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (1277, 'e44228be-c67f-41da-82d1-29c99a785385'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (26880, '439720fa-abdc-4492-afe7-ad647d8bd1e5'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (7939, 'c7dea0da-4b72-46f5-b615-3cc4c517e29e'), (7943, 'dd28dc4a-920f-4fc9-b713-5719ff38f1a8'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (8487, '023439b8-4419-4fa3-8b19-956261474fbd'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (13117, '058e07d2-d17f-4303-a0e9-7fe2a5c19611'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (10563, '138c4559-9b2e-43b7-a3bb-746c045905f0'), (10562, '1d9e4375-4a2b-43f6-b9a1-03eb4a2d05b8'), (21831, '42729a1e-e5c7-400b-86f3-8e2c67aaf58e'), (8011, '3742404d-4578-469e-9ebc-84b9395496ff'), (4944, '61aa7427-e3a9-48ae-b6cb-d54b354c21ee'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (12120, '0bbac7aa-9cb9-46e0-89f9-cf746b68cc80'), (7515, '0b6ef7a3-1204-44ab-b960-52455f130eca'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (12124, 'f2d8ff59-c9b5-4ccb-b0a9-5488b6acd532'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (23399, '8c4d5380-a5bc-4827-8694-d2ad31217a3d'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (2934, '07953283-5664-45cf-b67e-58683c7ff0ea'), (23425, 'bae40a82-a442-41f3-a45b-e024f39cc6ca'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (12198, 'c617332f-ef8f-477a-803c-147aed04f986'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (12203, 'aa81f46c-8e78-41dd-bd4b-57fdf62c8164'), (30638, 'bb43e4ba-f6ab-478d-8c31-6535c42d1276'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (2511, '9d4f9ab1-abb1-483e-84e9-4cd82481acea'), (4559, '5a7e2589-6026-4893-8bd9-e030899ea13a'), (29137, '5482f963-0400-4c03-b3c5-f9f03ae4c29f'), (4566, 'cb029d8d-8fc2-4dea-a947-d50b975b7925'), (17879, '0febef04-6ab5-44f6-bc89-962863e3aea9'), (17880, 'b0df342e-5867-45e5-95f6-3db08814a8c4'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (9694, 'd82a3bcb-3b3a-47f4-b946-77d14978e196'), (29149, 'da3811b9-e3d4-4955-80f4-1f74e90996bf'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5601, '098dde3d-a5c2-4abc-bdf3-f35ebe913ba2'), (5602, '983f13dc-846c-463b-b63f-5b68c866bb0b'), (15330, '9f43616d-6dfb-4651-ae47-169eef44cbd0'), (25573, '340d60ab-d024-458a-ba51-ed5bc11a806f'), (15336, '807f8cec-83e0-450b-aa1a-a12265b5545d'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (17910, '47d873f7-c1c4-4475-8be6-1e33f1bae1e1'), (17911, '8ab220f8-70fb-4fc3-b2e2-b2441991c61f'), (17912, '6c000270-627d-44ae-bac1-9ab0640951e5'), (8694, 'e750c432-a8e4-4a5f-bd8f-08dc728b2779'), (29690, 'bec5489a-802b-4c40-a81e-19c57753d246'), (8699, '815c3f58-57b7-42ae-9fe8-1c414ae5624e'), (7670, 'd97a96a2-3c50-4cf6-b418-52fae15a092d'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: If you are evaluating your model on a benchmark dataset like the ones mentioned above, you can use its dedicated evaluation metric. Make sure you respect the format that they require. For example, to evaluate your model on the [SQuAD](https://huggingface.co/datasets/squad) dataset, you need to feed the `question` and `context` into your model and return the `prediction_text`, which should be compared with the `references` (based on matching the `id` of the question) :
```

Again, that's rather good considering that according to [its paper](https://arxiv.org/abs/1910.01108v2) DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset.

{#if fw === 'pt'}

Now let's put everything we just did in a `compute_metrics()` function that we will use in the `Trainer`. Normally, that `compute_metrics()` function only receives a tuple `eval_preds` with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won't be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.

The `compute_metrics()` function groups the same steps as before; we just add a small check in case we don't come up with any valid answers (in which case we predict an empty string).

{:else}
```

Note that evaluation results include both the requested metric, and information about the time it took to obtain predictions through the pipeline.

<Tip>

The time performances can give useful indication on model speed for inference but should be taken with a grain of salt: they include all the processing that goes on in the pipeline. This may include tokenizing, post-processing, that may be different depending on the model. Furthermore, it depends a lot on the hardware you are running the evaluation on and you may be able to improve the performance by optimizing things like the batch size.

</Tip>

### Evaluate multiple metrics

With the [`combine`] function one can bundle several metrics into an object that behaves like a single metric. We can use this to evaluate several metrics at once with the evaluator:

```python
import evaluate
```

## Evaluate

Including a metric during training is often helpful for evaluating your model's performance. You can quickly load a evaluation method with the 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) library. For this task, load the [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) metric (see the 🤗 Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) to learn more about how to load and compute a metric):

```py
>>> import evaluate

>>> accuracy = evaluate.load(""accuracy"")
```

Then create a function that passes your predictions and labels to [`~evaluate.EvaluationModule.compute`] to calculate the accuracy:

```py
>>> import numpy as np


>>> def compute_metrics(eval_pred):
...     predictions = np.argmax(eval_pred.predictions, axis=1)
...     return accuracy.compute(predictions=predictions, references=eval_pred.label_ids)
## Metric description
This metric wraps the official scoring script for version 1 of the [Stanford Question Answering Dataset (SQuAD)](https://huggingface.co/datasets/squad). 

SQuAD is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.

## How to use 

The metric takes two files or two lists of question-answers dictionaries as inputs : one with the predictions of the model and the other with the references to be compared to:

```python
from evaluate import load
squad_metric = load(""squad"")
results = squad_metric.compute(predictions=predictions, references=references)
 
Here is the question: How can SQuAD metrics improve segmentation model performance evaluations?","The SQuAD metric can improve segmentation model performance evaluations by providing a dedicated evaluation metric specifically designed for the SQuAD dataset, which is a reading comprehension dataset consisting of questions posed on Wikipedia articles. This metric allows for a more accurate evaluation of a model's performance on this specific task, by comparing the model's predictions with the references (correct answers) based on matching the id of the question. This can help to identify areas where the model needs improvement and provide a more comprehensive understanding of its performance"
What is the best way to handle modular reinforcement learning workflows in Gradio demos?,"[(2560, '07bd0560-dadd-40f2-85d8-1d96772bbd22'), (12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (530, '283699ed-c384-40ef-bd5c-44fb732077a8'), (28185, 'd648591c-3383-45e8-ae71-83b504e2b8bd'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (16417, '0d19a33d-48bf-4fe6-9bc5-fb1aa7a04d3d'), (24609, '86e2b73e-43f9-4512-b8a2-0326f78967cf'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (16430, '66815a93-d7da-4188-9272-6bbfdf32ae6b'), (28239, '8b1cf917-5eba-4e74-9915-392dfba1eb57'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (9307, '3c6a18bb-91b0-4500-b20a-d84b9418c42c'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (7266, 'f5c4435e-d192-4727-9e6d-35acf4404166'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (27756, '8f9de8cd-281f-441a-b692-3d1b132a6728'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (6259, 'e1dad236-a338-4128-821c-38c369e6a47e'), (28276, 'f4a66bc9-205e-4eb3-8d50-50eb389d8352'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (28279, '835b3d07-8075-4019-a72c-9d9253024dd9'), (1145, 'abe44225-c493-44fb-82d3-494085e90422'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (28295, 'f058a9ae-4712-49e5-ae33-6ce9fa46a4df'), (23189, '36e981bb-328e-4d35-a16b-5facd733cb56'), (18584, '815482b7-2249-4e35-a25e-18ccc16fd17b'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (17067, '3bf582fa-4398-4a72-b777-790828b0ac34'), (17068, 'e8d4188d-34a3-4d81-96d0-85488e9eb25a'), (24789, '6151fc10-9ec2-41ab-8af7-99a5aa43b076'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (2265, 'e4798614-c262-4b6e-9925-1fc464ef7715'), (27874, 'c5f217f1-0f49-40f8-9556-3a6535594065'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (27897, '00ee70f5-c8bc-45d4-9a8b-c379cafc9c97'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (16641, '29e13c06-e204-4a93-a6ac-03bf2fc934e4'), (2311, '2ddd9274-7512-4bdd-8e03-f54c56c6fdfd'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (15639, '706242a1-e193-4ad4-910e-4830e33c736a'), (15640, '31d2f322-0056-4e5f-86ac-5092c9d1a8ae'), (11037, 'e2587233-f088-45f5-84c5-cb20fd903daa'), (18728, '70353f36-5ed1-42b1-97fc-404e6f7df31a'), (2365, '05851a72-a5b9-4a96-af4d-3f6e2557ef22'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18247, '74315d00-318c-4e2d-84b1-cb46200e84ce'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (20310, '03e6bed2-053e-46f3-83c9-e9e52ffe5997'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (2402, 'bf619499-284f-4b2f-9463-83cf285ec6ed'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (2405, 'c11b172e-c44e-453b-802d-3a9e7e40954e'), (2415, 'a1d515ce-a0f1-4910-a9cd-e2f1170cd93b'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (2421, 'b38bf045-1333-4093-8e43-d0f3720e5f10'), (31614, 'f1246781-19ff-4a9f-a5ce-1fe184da4c47'), (5524, 'bb1d7e3e-0981-435e-af37-33e2588f6b6b'), (14246, '0f2d11c8-f421-456b-9b96-20b5112a8538'), (14249, '3bde59f7-ba94-4ec5-8768-6195057f7e50'), (14251, '93d814d8-d816-46e7-baa5-72a0dd060c27'), (14252, '77a63f90-12e6-4253-993b-4d2cad182d17'), (14253, '375f1c07-fcab-4817-be5a-6b53126120ed'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (7095, '46239874-f414-40b3-bf11-c8ec6587d46c'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (23997, 'f5eb3def-edf7-4dfa-b630-80e186c92aed'), (13247, '7151a6c0-468c-45c7-9cb8-418c3953caaf'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (2000, '91dbaac5-8de3-4453-8b6e-299bba78ef2b'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (29670, 'ccfff15f-115c-4de7-b620-659b6be0a991'), (2023, 'd20a455f-f235-41b7-a1c5-5b15b10c5065'), (29673, '5fa6188c-04bd-4733-bf74-5e5ee58334e3'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (28139, '6e24e6fe-e632-4da7-b359-4a0216b57e3c'), (29678, '978c5788-4b70-4882-ad94-79625d10e12f'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The 4 Kinds of Gradio Interfaces

So far, we've always assumed that in order to build an Gradio demo, you need both inputs and outputs. But this isn't always the case for machine learning demos: for example, _unconditional image generation models_ don't take any input but produce an image as the output.

It turns out that the `gradio.Interface` class can actually handle 4 different kinds of demos:
The `input` and `output` arguments take one or more Gradio components. As we'll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/components) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications. 

> [!TIP]
 > For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`""textbox""`) or an instance of the class (`gr.Textbox()`).

If your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.
Now that we studied the theory behind PPO, the best way to understand how it works **is to implement it from scratch.** 
      
Implementing an architecture from scratch is the best way to understand it, and it's a good habit. We have already done it for a value-based method with Q-Learning and a Policy-based method with Reinforce.

So, to be able to code it, we're going to use two resources:
- A tutorial made by [Costa Huang](https://github.com/vwxyzjn). Costa is behind [CleanRL](https://github.com/vwxyzjn/cleanrl), a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features.
- In addition to the tutorial, to go deeper, you can read the 13 core implementation details: [https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

Then, to test its robustness, we're going to train it in 2 different classical environments:
Introduction to Gradio[[introduction-to-gradio]]

<CourseFloatingBanner
    chapter={9}
    classNames=""absolute z-10 right-0 top-0""
/>

In this chapter we will be learning about how to build **interactive demos** for your machine learning models.

Why build a demo or a GUI for your machine learning model in the first place? Demos allow:

- **Machine learning developers** to easily present their work to a wide audience including non-technical teams or customers
- **Researchers** to more easily reproduce machine learning models and behavior
- **Quality testers** or **end users** to more easily identify and debug failure points of models
- **Diverse users** to discover algorithmic biases in models

We'll be using the Gradio library to build demos for our models. Gradio allows you to build, customize, and share web-based demos for any machine learning model, entirely in Python.

Here are some examples of machine learning demos built with Gradio:
$demo_hello_world_4

Type your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.

Tip: When developing locally, you can run your Gradio app in <strong>hot reload mode</strong>, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in <code>gradio</code> before the name of the file instead of <code>python</code>. In the example above, you would type: `gradio app.py` in your terminal. Learn more about hot reloading in the <a href=""https://www.gradio.app/guides/developing-faster-with-reload-mode"">Hot Reloading Guide</a>.


**Understanding the `Interface` Class**

You'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.
 
Here is the question: What is the best way to handle modular reinforcement learning workflows in Gradio demos?","The context does not provide a direct answer to the question about handling modular reinforcement learning workflows in Gradio demos. The context primarily focuses on explaining the different kinds of Gradio interfaces, their components, and how to build demos for machine learning models. It does not provide specific information on handling modular reinforcement learning workflows in Gradio demos. Therefore, it is not possible to provide a concise and relevant answer to the question based on the provided context."
How can SQuAD metrics be adapted for evaluating QA systems in multimodal applications?,"[(8704, 'fade4df4-61f5-4d6c-b524-29a508622f3e'), (3, 'a958fa67-0263-40fe-aaa1-f05fdc6884b3'), (4, '6772e5c6-cd4d-4a56-84fa-c161ac6651b6'), (17925, '5414c780-2b24-471e-b343-9f5788f10a02'), (8, '70902519-1a01-4e55-97a6-1615da342966'), (10, '44f856e1-ab1a-4bcb-b2a7-bebf2dcc1ba6'), (11, 'e8935af1-6860-4651-b6ac-50c19ef3b5d1'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (11798, 'a10d0762-9d23-4bf4-bfed-d1bae45ad2bc'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (24607, '8f241013-9385-4355-bc80-ae8a6405c9ad'), (24135, 'cc34900d-f265-4078-a275-d6c67eec2d85'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (24140, '20e6a0cf-1c4c-49cd-a66f-68346da0b020'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (20060, 'ce186691-06ac-411b-93cf-30eb955e0e0f'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (8807, '170b9534-cae9-40e8-bbe4-468c9b1e40f0'), (9862, '33a9f610-ef4f-48c2-8df9-388fa0fd2a8f'), (6802, '816d52f3-ceec-436a-b89e-8e94e93f67cd'), (6295, 'ad2bcd7b-0c46-4c91-a469-ccfda1a76ae2'), (29336, 'f04db5fb-1ac5-4f06-affc-512b050f79c1'), (6296, '664fff80-d879-4fbe-8833-23d6ab76309c'), (6297, '36c57f53-6f6e-4221-9e62-46a6d817f8d1'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (21666, '139e3ac3-a662-445f-86e2-abdce40d3b6a'), (6310, '74a4cbde-53ef-459b-948d-593f322581a5'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (16055, '1c40cffb-366c-48a0-b2e8-3d3a31a0feda'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (3286, '2e7f139b-09ad-48d3-bf0a-1813c7260d24'), (7897, 'b49e9eed-7307-41a6-ace9-39f2f03a63c0'), (7899, '4be9bd0d-6bc1-46da-ac3e-67bbeba3a58b'), (19676, '6be519b5-bd03-4b2b-9582-e4c28595603a'), (20189, '8bf4e38a-2076-41ce-8eb6-944757a03db3'), (7902, '93a6f76b-0134-468f-95ff-37a1b2fe2a0e'), (7903, '978777fb-120e-4b9d-9d3e-176818f1e218'), (7901, '17026dcd-41fe-493c-8a87-b0ddc5ea48cb'), (9954, '1ab66d52-77d3-4952-b03c-71962ff5126a'), (15075, '5f627d6a-cdb5-4546-a07d-c8bf72389612'), (28902, '4cf0d944-f2d5-4362-a6e5-6b48fe1f6e6a'), (9958, '20edf0a1-57c5-4952-9cd7-83ba28261ddb'), (7913, '18d40909-e83c-4c62-9cee-03e8a507f58f'), (14574, '5aefda82-e413-49b2-b371-18ab17e282df'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (7941, 'b6948520-45f2-4b5f-aff0-55432a46fb26'), (7950, 'f61203be-255c-4c5e-926c-dcf16d1724f4'), (17178, 'e73b7c7b-0f56-409f-a053-6462a4e9a794'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (7469, '5e4eeb28-721e-4fa6-bb7f-b2ef23f884ee'), (7471, '533c05d0-2794-409c-9e72-c7265e0258db'), (31026, '03ac0a44-5359-4071-8b23-753e0ba02a05'), (3378, '035baeac-20c5-4b81-a77c-8414a9ad5e13'), (13111, 'be476784-d73d-4f1c-9d4d-83f003c97e02'), (13112, 'fa72104c-7e7f-477f-a099-a607098c4571'), (13117, '058e07d2-d17f-4303-a0e9-7fe2a5c19611'), (15166, '9b3c8ab6-5a5b-42f2-88b5-253bc50ca795'), (15167, '1b9ae140-8223-4870-bb50-abbc78ae7a0d'), (15168, '62e7a0be-933e-4e8b-afec-55689b396bc8'), (13121, '1620026c-a383-40e3-966b-aba9126d81f8'), (13122, '2e691ff5-03d7-4629-b985-ff472fdb1903'), (17728, 'a585681c-5f98-4669-9248-eb15e6452e4b'), (10563, '138c4559-9b2e-43b7-a3bb-746c045905f0'), (29003, '7ee00c11-ba51-4469-8529-287c6c866ba3'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (6477, 'c6d1f800-e6ad-43b9-8a32-911ce5083a43'), (14158, '379977d9-63d1-45cb-8211-1708ee66a615'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (12116, 'bb60fc61-08eb-484a-98d5-0aa3d2715f1c'), (12117, '7aeeb859-3ec1-4e26-b3a4-2b69eff33c55'), (12120, '0bbac7aa-9cb9-46e0-89f9-cf746b68cc80'), (12123, 'd38099ac-47c2-44ee-9bed-c421c97f5448'), (12124, 'f2d8ff59-c9b5-4ccb-b0a9-5488b6acd532'), (7515, '0b6ef7a3-1204-44ab-b960-52455f130eca'), (1889, 'ff5fff0f-84f6-4097-870a-81b81a13b28b'), (23396, '43eeded4-deea-40ba-8133-1223664f670e'), (23399, '8c4d5380-a5bc-4827-8694-d2ad31217a3d'), (23402, 'd35eb133-24dc-4cdf-a260-6efaaa80b665'), (23403, '9b397e8f-f497-439e-8279-5d67e64c42c8'), (13164, 'cfa9ac8e-fabc-426b-833c-cea587d2f308'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (24432, '65160c4e-e76b-478e-af89-46847fcd223f'), (11121, '13995e0d-c226-469f-8290-6b8f2ddff184'), (3443, '974c1801-170d-4f66-9f80-84ecc9d6086f'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (20866, '5c75cf2e-fd56-4e04-9099-a0346e84d978'), (10632, 'fb591bd6-f603-45a5-be34-48c85736dd6f'), (31131, '66823a48-2ce4-4006-999f-30cce2128e89'), (25505, 'fedd29d3-5600-4087-a101-167964776b87'), (12198, 'c617332f-ef8f-477a-803c-147aed04f986'), (29098, '013504cb-b775-4134-a17d-f8c00016693f'), (12203, 'aa81f46c-8e78-41dd-bd4b-57fdf62c8164'), (4529, '795ebffd-b6ef-42a5-ba9b-9d869516b585'), (437, '81e7f812-ddf5-4bc6-b6ba-81c2fb0d5b0c'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (14787, 'e59233cd-e754-4a11-a878-ab859959dd58'), (11717, 'b8fb5249-ce5f-4240-bb63-03d4a5635d97'), (29639, 'ad7bf7d2-d8c6-47bf-9e9c-6c68383ad368'), (2511, '9d4f9ab1-abb1-483e-84e9-4cd82481acea'), (29137, '5482f963-0400-4c03-b3c5-f9f03ae4c29f'), (2519, 'ea219184-5b7d-4421-b252-b5bd2876d98e'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (9694, 'd82a3bcb-3b3a-47f4-b946-77d14978e196'), (9695, '748e38b2-3433-4e28-b53a-0e09c6449b8a'), (30686, 'ce9f60be-25b2-4a8c-ab73-5dc4df24d5f9'), (8692, '0b55bbf6-f9da-44f5-b802-1a27c9554a04'), (8693, '898a35ba-ec24-4c7d-b234-995b71644073'), (8694, 'e750c432-a8e4-4a5f-bd8f-08dc728b2779'), (29690, 'bec5489a-802b-4c40-a81e-19c57753d246'), (8699, '815c3f58-57b7-42ae-9fe8-1c414ae5624e'), (23038, '4aaa658b-2b87-4e34-ad80-4a1a9ae444e6'), (8703, 'ccbf0bbe-3f3b-4575-b00b-1b7f23cec3b2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

We can now leverage the [map](https://huggingface.co/docs/datasets/v2.1.0/en/process#map) function of [datasets](https://huggingface.co/docs/datasets/index) to iterate over the validation set of squad 2 and run prediction for each data point. Therefore we write a `evaluate` helper method which uses our pipelines and applies some transformation to work with the [squad v2 metric.](https://huggingface.co/metrics/squad_v2)

*This can take quite a while (1.5h)*
Note that if your dataset contains samples with no possible answers (like SQuAD version 2), you need to pass along 
the flag `--version_2_with_negative`.

The following example applies post-training dynamic quantization on a DistilBERT fine-tuned on the SQuAD1.0 dataset.

```bash
python run_qa.py \
    --model_name_or_path distilbert-base-uncased-distilled-squad \
    --dataset_name squad \
    --quantization_approach dynamic \
    --do_eval \
    --output_dir /tmp/quantized_distilbert_squad
In this example, we use QDQBERT model to do quantization on SQuAD task, including Quantization Aware Training (QAT), Post Training Quantization (PTQ) and inferencing using TensorRT.

Required:
- [pytorch-quantization toolkit](https://github.com/NVIDIA/TensorRT/tree/master/tools/pytorch-quantization)
- [TensorRT >= 8.2](https://developer.nvidia.com/tensorrt)
- PyTorch >= 1.10.0

## Setup the environment with Dockerfile

Under the directory of `transformers/`, build the docker image:
```

Again, that's rather good considering that according to [its paper](https://arxiv.org/abs/1910.01108v2) DistilBERT fine-tuned on SQuAD obtains 79.1 and 86.9 for those scores on the whole dataset.

{#if fw === 'pt'}

Now let's put everything we just did in a `compute_metrics()` function that we will use in the `Trainer`. Normally, that `compute_metrics()` function only receives a tuple `eval_preds` with logits and labels. Here we will need a bit more, as we have to look in the dataset of features for the offset and in the dataset of examples for the original contexts, so we won't be able to use this function to get regular evaluation results during training. We will only use it at the end of training to check the results.

The `compute_metrics()` function groups the same steps as before; we just add a small check in case we don't come up with any valid answers (in which case we predict an empty string).

{:else}
### Fine-tuning BERT on SQuAD1.0

The [`run_qa.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/question-answering/run_qa.py) script
allows to fine-tune any model from our [hub](https://huggingface.co/models) (as long as its architecture has a `ForQuestionAnswering` version in the library) on a question-answering dataset (such as SQuAD, or any other QA dataset available in the `datasets` library, or your own csv/jsonlines files) as long as they are structured the same way as SQuAD. You might need to tweak the data processing inside the script if your data is structured differently.
 
Here is the question: How can SQuAD metrics be adapted for evaluating QA systems in multimodal applications?","The context does not provide a direct answer to the question of how SQuAD metrics can be adapted for evaluating QA systems in multimodal applications. The context primarily focuses on fine-tuning DistilBERT on the SQuAD dataset, using the `datasets` library, and applying post-training dynamic quantization. It does not discuss adapting SQuAD metrics for multimodal applications. Therefore, I cannot provide a comprehensive answer to this question based on the provided context."
What approach optimizes debugging Gradio image components for reinforcement learning scenarios?,"[(4103, '69742828-77bd-4856-a8aa-a0025c5d7383'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (4119, 'b7a0eee1-d5ca-4ce1-b308-ee2deffe6bce'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (28206, '5c9e1c1e-9e04-4cac-a433-415ea2c4f738'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28232, 'edcdb710-1075-4024-937a-e1e9b421cd61'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (28239, '8b1cf917-5eba-4e74-9915-392dfba1eb57'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (28243, '0bf9268d-f628-4160-b2fb-5d484fbd603f'), (2143, '6e81dc66-340f-4578-890b-b72e0440f793'), (16479, 'f8325aa8-886a-43ff-9c7c-96bb8c00df22'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (28278, 'f42859f6-7a51-470b-83f5-219dd5b33fa5'), (28279, '835b3d07-8075-4019-a72c-9d9253024dd9'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (28284, 'bfc33107-d72c-472b-95c4-03530bf3a56d'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (2177, 'e4f20765-678d-400f-acda-3fd9e844e57d'), (4740, 'a3172a98-921d-49f4-98ff-7a1f3290f6af'), (28297, '7a0b2105-7a16-4c1e-9c64-d09c6b7d1025'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (2191, 'd67da064-a30b-46ee-84b4-6a5d395c7270'), (28305, '35e22005-7747-4988-a75c-23a2275641c0'), (2195, 'f3236d15-e59b-4821-ba48-d9fe7ac5b8b1'), (6820, '296a6e9a-0b6c-450c-8983-f33bcfc0a165'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (2214, '025e63ca-22c5-4fa2-9b8e-44065d8d2701'), (3756, '1a1f79f6-d46a-4043-97ae-61320836b7ce'), (29868, 'a87fd676-df82-4ca4-b1c7-c7cce13dd5ce'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (27841, 'c37d24ce-3d99-4ab8-9016-ad2a6d6c1e39'), (2242, 'd59c776c-ce85-4340-b4b9-7f708f83dabb'), (27844, 'fb6d0216-35d2-4c4e-adff-45ae67110b4c'), (2245, '47834d58-902b-4e38-853e-427121d538ab'), (2251, '49c92cce-066a-48de-9511-6f677fedd988'), (27852, '81090f0f-86b7-4c58-9fe0-a796307b1748'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (14046, '066f129e-a91c-4c41-9a89-7bd254f42b28'), (8420, 'f005d893-d4fc-411d-96ea-a435bfa37288'), (2282, '5640e425-05ef-492c-9a00-c8280de383d6'), (27887, '69311dd3-e6f7-4e37-a620-9ae98462dcfd'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (26889, '739be90a-f89d-445a-be17-3ab3af2a79d6'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (2332, 'f0d81b41-881d-4e9d-8dd9-5911dcb7e594'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (2358, '61c00040-18d9-47df-b54f-732d42c2c5f9'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (2365, '05851a72-a5b9-4a96-af4d-3f6e2557ef22'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (6973, '1b3ca299-1a5b-4ef1-913a-2012d57029c8'), (2369, '2d8765ce-950f-4812-be38-74a3ca44c156'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (2404, '171352f9-77b3-42e3-806b-084befe2e91b'), (2405, 'c11b172e-c44e-453b-802d-3a9e7e40954e'), (2410, '9ae572e8-8387-48a6-b43e-2d6abf2e6560'), (28017, '29c87561-6f4c-4afc-adfd-b3054e4a4dad'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (2423, '702ce4ca-cbbd-43a9-b982-c55bba1a9f2b'), (2431, 'd82501d2-94be-47fd-843d-2fab52c53bb9'), (28051, '2e9d8676-4090-4ac4-ac08-acae65f2d32d'), (28065, 'bf7e5a2e-ed4b-41c2-97cb-11e1738eaa0f'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (28069, 'f3e15b8f-8e96-4ae3-b6a4-4a3dd439bec4'), (3493, '8a1040ce-1b60-4e24-90e9-cb4e7127b718'), (20394, '5ee7db72-d4a7-43e0-9cc0-f5da6aab5d5c'), (1967, 'e50667e5-7a0c-461c-aff5-9a06cf3ab823'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (1970, 'af7a3664-be46-4edc-ab17-ca264d4a0b29'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (28088, '86a70b0f-cff2-4271-8ac0-c271371d3049'), (1978, 'e6aa00b1-e118-4a20-9aea-377dc102cb43'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (28116, '3ef5a8e2-3d00-4ed8-a2bd-1e8553e1ac00'), (28119, '1dcccb98-51b9-49bd-81ec-c5d5c62bea87'), (28125, '24144982-b2b1-4f85-a299-ae52b3b80ed5'), (2013, '8247fa80-5295-40ba-80ca-5ab308278cc0'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28156, '6d381f66-497f-44f0-aa3b-5b19a1b2b548')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Let's test what you learned in this chapter!

### 1. What can you use Gradio to do?

<Question
	choices={[
        {
			text: ""Create a demo for your machine learning model"",
			explain: ""With a few lines of python code you can generate a demo for your ML model using our library of pre-built components."",
			correct: true
		},
		{
			text: ""Share your machine learning model with others"",
			explain: ""Using the <code>share=True</code> parameter in the launch method, you can generate a share link to send to anyone."",
            correct: true
		},
		{
			text: ""Debug your model"",
			explain: ""One advantage of a gradio demo is being able to test your model with real data which you can change and observe the model's predictions change in real time, helping you debug your model."",
			correct: true
		},
		{
			text: ""Train your model"",
			explain: ""Gradio is designed to be used for model inference, AFTER your model is trained."",
		}
	]}
/>

### 2. Gradio ONLY works with PyTorch models
1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)
2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)
3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)
4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)
5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=""compact"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.
1. Fix typo in guide image path by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2357](https://github.com/gradio-app/gradio/pull/2357)
2. Raise error if Blocks has duplicate component with same IDs by [@abidlabs](https://github.com/abidlabs) in [PR 2359](https://github.com/gradio-app/gradio/pull/2359)
3. Catch the permission exception on the audio component by [@Ian-GL](https://github.com/Ian-GL) in [PR 2330](https://github.com/gradio-app/gradio/pull/2330)
4. Fix image_classifier_interface_load demo by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2365](https://github.com/gradio-app/gradio/pull/2365)
5. Fix combining adjacent components without gaps by introducing `gr.Row(variant=""compact"")` by [@aliabid94](https://github.com/aliabid94) in [PR 2291](https://github.com/gradio-app/gradio/pull/2291) This comes with deprecation of the following arguments for `Component.style`: `round`, `margin`, `border`.
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
 
Here is the question: What approach optimizes debugging Gradio image components for reinforcement learning scenarios?","The context does not provide a specific approach that optimizes debugging Gradio image components for reinforcement learning scenarios. The question seems to be out of scope of the provided context, which primarily discusses the capabilities of Gradio and its various features. Therefore, I cannot provide a concise and relevant answer based on the given context."
How can multimodal systems benefit from combining QA metrics and rendering improvements?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (18434, 'd59e6b88-c090-403a-9dfa-aab7f0b5b516'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (12334, '84c0ef55-03b6-40a3-892b-1499c9a28ee7'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (11332, '08558caa-7583-4086-a5e8-4938ce4a5a55'), (22086, '6f92aef8-fd03-45fb-8920-1fc2364e4e13'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (3150, '7fba3ceb-aac9-41ed-b1c8-75ecead46d84'), (11344, '614a7745-0caa-4be3-9ac1-c8116d68a14b'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (21078, 'c0d06505-c17f-42f2-9cdc-ece4e30fd5da'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (21089, '9e8976c4-72d9-49eb-9b17-5078f33abec4'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (14438, 'adc4d77e-0477-455a-8002-6bde28537b35'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (21109, '0626c7f9-dffb-4329-86ab-21a3d6000823'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (4237, 'aa571318-cb21-42dc-96d5-9247c29a9ac5'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (22169, '1eefa675-c3d2-4400-8957-842b99fce9c4'), (6297, '36c57f53-6f6e-4221-9e62-46a6d817f8d1'), (22171, 'e6e94240-1f1e-4271-8c60-d6ce80b13dbc'), (6310, '74a4cbde-53ef-459b-948d-593f322581a5'), (28846, 'a95944ca-cb3f-47e9-a1ca-670e4f9f9fea'), (6322, '75f4fc4f-937c-467c-ae41-0d25256d3b8f'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (9958, '20edf0a1-57c5-4952-9cd7-83ba28261ddb'), (28902, '4cf0d944-f2d5-4362-a6e5-6b48fe1f6e6a'), (21230, 'cd52268c-23fc-439d-affa-c5c374ffb57a'), (21234, '53ed4e4a-e74b-4f4b-a1a7-eeee2b356164'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (23813, 'e529e90a-7751-49cc-8418-262a566b79d9'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (19218, '29f57a47-3df1-4b88-a202-974f96482787'), (18202, '3299c4a6-a3ea-461c-82a7-0fafcfde3c68'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (15646, '6f1d3cc0-3c8a-4c86-abf4-65e1bf901355'), (27423, 'cdb54c20-dca4-48f4-96a7-f000fa351601'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (16174, '7aeaf6a6-cf85-46b0-af0b-f0ab92bc6103'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (13619, '3db87f08-ff12-4890-b601-12a7fa3ea0f5'), (19261, '828a7548-bebe-48da-bd43-915ef22bbb9b'), (318, '860b95b8-d28b-4042-8c68-1fb6320b87c1'), (19262, 'cb4dea97-cfcf-4d5d-827a-c683ab6dcaf2'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (27466, 'bfb56804-3d91-45c6-b17f-8d15d5d3a201'), (28495, 'b74287be-53a5-421e-9ec1-c670c53f7941'), (3931, '0c9ddccb-4952-4be4-a2a0-e4b22afc3eee'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (13165, 'e430f4ce-b52f-4aae-831c-0df1d9c6f697'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (4975, 'aa4eff3e-d52b-4069-bf80-f16bc832f50e'), (24433, 'bebc2a4b-2754-442d-8616-0f6f184afc3c'), (9073, '814aa859-5cdc-4add-85c5-eefdb37f0d28'), (24434, 'bb7afb46-7718-4546-95b2-cc62410b73cb'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (893, 'd7a5a15a-3c14-47bd-ad8f-f78e904ff293'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (7072, '9b1f498b-8529-4819-b080-b212dcd54acb'), (10145, 'ce96ae5b-5345-4742-bb66-c625e53c53e0'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (7073, 'a58afddf-a5d9-4a17-b197-423b25fb7aac'), (10149, '717ac025-7b4c-4c12-a316-fbdbf28787d5'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (1464, 'a4466f5a-2fe9-454b-ae5c-94c51477452d'), (18367, '6a784930-de82-4a31-a463-40d44eda4423'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (5064, '442cd4f9-dd4a-42f0-bdbd-31fcb1c45bd1'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (2510, '7d4dc0bf-199e-49ec-8219-cc474ae04cc7'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (6612, '66c2ed9f-c626-47f7-842e-40752f739bb2'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (9695, '748e38b2-3433-4e28-b53a-0e09c6449b8a'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29161, '5c1a0615-2746-4f25-8cae-56a17b9d371d'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
# Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA


LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. 
Our [LLM.int8 blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) showed how the techniques in the [LLM.int8 paper](https://arxiv.org/abs/2208.07339) were integrated in transformers using the `bitsandbytes` library.
As we strive to make models even more accessible to anyone, we decided to collaborate with bitsandbytes again to allow users to run models in 4-bit precision. This includes a large majority of HF models, in any modality (text, vision, multi-modal, etc.). Users can also train adapters on top of 4bit models leveraging tools from the Hugging Face ecosystem. This is a new method introduced today in the QLoRA paper by Dettmers et al. The abstract of the paper is as follows:
That's where models like [LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut) come into play. By incorporating not only text but also visual information, these models can dramatically increase accuracy. For comparison, on [RVL-CDIP](https://huggingface.co/datasets/rvl_cdip), an important benchmark for document image classification, a BERT-base model achieves 89% accuracy by using the text. A [DiT](https://huggingface.co/docs/transformers/main/en/model_doc/dit) (Document Image Transformer) is a pure vision model (i.e., it does not take text as input) and can reach 92% accuracy. But models like [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut), which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how
The abstract from the paper is the following:

*Large multimodal models (LMM) have recently shown encouraging progress with visual instruction tuning. In this note, we show that the fully-connected vision-language cross-modal connector in LLaVA is surprisingly powerful and data-efficient. With simple modifications to LLaVA, namely, using CLIP-ViT-L-336px with an MLP projection and adding academic-task-oriented VQA data with simple response formatting prompts, we establish stronger baselines that achieve state-of-the-art across 11 benchmarks. Our final 13B checkpoint uses merely 1.2M publicly available data, and finishes full training in ∼1 day on a single 8-A100 node. We hope this can make state-of-the-art LMM research more accessible. Code and model will be publicly available*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava_architecture.jpg""
alt=""drawing"" width=""600""/>
##### Various performance improvements

These improvements will be particularly beneficial to large applications.

- Rather than attaching events manually, they are now delegated, leading to a significant performance improvement and addressing a performance regression introduced in a recent version of Gradio. App startup for large applications is now around twice as fast.
- Optimised the mounting of individual components, leading to a modest performance improvement during startup (~30%).
- Corrected an issue that was causing markdown to re-render infinitely.
- Ensured that the `gr.3DModel` does re-render prematurely.

Thanks [@pngwn](https://github.com/pngwn)!
 
Here is the question: How can multimodal systems benefit from combining QA metrics and rendering improvements?","The context does not provide a direct answer to the question. The text discusses the benefits of multimodal models in Document AI use cases, the integration of 4-bit quantization and QLoRA to make LLMs more accessible, and various performance improvements in Gradio. However, it does not mention combining QA metrics and rendering improvements in multimodal systems. Therefore, it is not possible to provide a concise and relevant answer to the question based on the provided context."
What is the best approach to scale reinforcement learning workflows using Gradio modularity?,"[(27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (2070, '160640cc-4bbf-4460-b6e4-3a19e64c6542'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (2075, '57697ebc-669f-4aa3-8585-4c1388c93630'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (24609, '86e2b73e-43f9-4512-b8a2-0326f78967cf'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (10794, '87f2e6f8-9266-4bcc-b6e8-da2a99bb7c9d'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (16467, '150cbe1c-540f-4183-be08-4ec28ca54c99'), (16470, '3f9dd7b6-87af-49cf-acf6-54fc8d156aaa'), (16475, 'd43596fa-ef7b-4a1c-adfd-7c4ada0ed26c'), (16476, 'dcd1d7e8-9348-4ba0-9b2c-d908799181c4'), (28256, '1f56cf28-a428-4f73-89b5-298c07083ac8'), (28258, '0f9b5df9-a291-442a-98d9-847dea25a433'), (7266, 'f5c4435e-d192-4727-9e6d-35acf4404166'), (27748, '63beb359-67e7-495a-aae1-dcdf197acc34'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (27756, '8f9de8cd-281f-441a-b692-3d1b132a6728'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (6259, 'e1dad236-a338-4128-821c-38c369e6a47e'), (6266, '829c4c3b-b0fd-456c-bd7c-f2346f83d626'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (6791, 'caea27fd-5810-4f6f-afa1-5befc362cced'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (17039, 'fc8e9c32-094c-4ab9-ac63-c84f0f0f4a6a'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (21147, 'dd1291ef-03a7-4a6a-91f6-3ea791acefdb'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (27863, '608821e0-6cac-4415-9f3c-b2d77fec5bc2'), (6380, '6aed28bc-1ba7-45a0-9248-1d8f31471c81'), (21228, '4e0af35d-a2a3-44be-8a60-2006257963bc'), (18672, '51d56682-3787-49c9-b694-c8ccc94be302'), (23793, 'd299b3ef-1a11-4d70-ac3d-3a50eb09a074'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (6397, 'a42d6ffe-a12b-4bd0-b1ee-ef817d09a213'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (11037, 'e2587233-f088-45f5-84c5-cb20fd903daa'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (18728, '70353f36-5ed1-42b1-97fc-404e6f7df31a'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (27944, '0354f351-4139-4091-b7fd-ae882e056b91'), (27949, 'f9f3c1af-030d-42eb-b727-680bb2457e6d'), (30519, 'e988d4c0-44ec-4a7f-a800-615f96ae0da2'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (16715, 'e3eb35de-68db-4c66-b9da-4c622cd55f2a'), (2382, '7e5deb8c-4602-4330-a0cc-8c7b139190d0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (2384, '9ec415ef-d6bb-4467-a453-d75a4b9ee1ee'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (2388, '2abe42c6-c544-4d06-b7c4-4e93d78d1f97'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (29041, '65c5b774-f53f-4108-8196-a6d821cfb1ec'), (29042, '0b59b28a-d5c0-48d6-9ef0-2b3e32c1a65b'), (30584, '72eded47-82e5-4d94-aff8-ea0caf3f4dc7'), (24458, 'f7480eb5-a90b-4bb4-b923-1a0adb0171ff'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (5524, 'bb1d7e3e-0981-435e-af37-33e2588f6b6b'), (24477, '108827d3-9954-417d-a4ca-4f275ac82faf'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (16315, '670f661d-5822-4779-aa29-436258e97b09'), (23997, 'f5eb3def-edf7-4dfa-b630-80e186c92aed'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (1989, '29a9c889-c7ff-4c28-9d9b-cf94ad5cb758'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Generalization in Reinforcement Learning

Generalization plays a pivotal role in the realm of Reinforcement Learning. While **RL algorithms demonstrate good performance in controlled environments**, the real world presents a **unique challenge due to its non-stationary and open-ended nature**.

As a result, the development of RL algorithms that stay robust in the face of environmental variations, coupled with the capability to transfer and adapt to uncharted yet analogous tasks and settings, becomes fundamental for real world application of RL.

If you're interested to dive deeper into this research subject, we recommend exploring the following resource:

- [Generalization in Reinforcement Learning by Robert Kirk](https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html): this comprehensive survey provides an insightful **overview of the concept of generalization in RL**, making it an excellent starting point for your exploration.
Now that we studied the theory behind PPO, the best way to understand how it works **is to implement it from scratch.** 
      
Implementing an architecture from scratch is the best way to understand it, and it's a good habit. We have already done it for a value-based method with Q-Learning and a Policy-based method with Reinforce.

So, to be able to code it, we're going to use two resources:
- A tutorial made by [Costa Huang](https://github.com/vwxyzjn). Costa is behind [CleanRL](https://github.com/vwxyzjn/cleanrl), a Deep Reinforcement Learning library that provides high-quality single-file implementation with research-friendly features.
- In addition to the tutorial, to go deeper, you can read the 13 core implementation details: [https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)

Then, to test its robustness, we're going to train it in 2 different classical environments:
ONNX Runtime is a cross-platform inference and training machine-learning accelerator. It makes live Gradio demos with ONNX Model Zoo model on Hugging Face possible.

ONNX Runtime inference can enable faster customer experiences and lower costs, supporting models from deep learning frameworks such as PyTorch and TensorFlow/Keras as well as classical machine learning libraries such as scikit-learn, LightGBM, XGBoost, etc. ONNX Runtime is compatible with different hardware, drivers, and operating systems, and provides optimal performance by leveraging hardware accelerators where applicable alongside graph optimizations and transforms. For more information please see the [official website](https://onnxruntime.ai/).

## Setting up a Gradio Demo for EfficientNet-Lite4
Introduction [[introduction]]

One of the most critical tasks in Deep Reinforcement Learning is to **find a good set of training hyperparameters**.

<img src=""https://raw.githubusercontent.com/optuna/optuna/master/docs/image/optuna-logo.png"" alt=""Optuna Logo""/>

[Optuna](https://optuna.org/) is a library that helps you to automate the search. In this Unit, we'll study a **little bit of the theory behind automatic hyperparameter tuning**. We'll first try to optimize the parameters of the DQN studied in the last unit manually. We'll then **learn how to automate the search using Optuna**.
## What’s next?

In the coming weeks and months, we plan on supporting other tools from the ecosystem:

- Integrating **[RL-baselines3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo)**
- Uploading **[RL-trained-agents models](https://github.com/DLR-RM/rl-trained-agents)** into the Hub: a big collection of pre-trained Reinforcement Learning agents using stable-baselines3
- Integrating other Deep Reinforcement Learning libraries
- Implementing Convolutional Decision Transformers For Atari
- And more to come 🥳

The best way to keep in touch is to **[join our discord server](https://discord.gg/YRAq8fMnUG)** to exchange with us and with the community.

## References

[1] Chen, Lili, et al. ""Decision transformer: Reinforcement learning via sequence modeling."" *Advances in neural information processing systems* 34 (2021).
 
Here is the question: What is the best approach to scale reinforcement learning workflows using Gradio modularity?","The context does not provide a direct answer to the question about scaling reinforcement learning workflows using Gradio modularity. The context discusses generalization in reinforcement learning, implementing PPO from scratch, and using Optuna for hyperparameter tuning, but it does not mention Gradio modularity or scaling reinforcement learning workflows. Therefore, I cannot provide a comprehensive answer to the question based on the provided context."
How can segmentation workflows improve using reinforcement learning insights?,"[(6144, '34a43938-8523-4217-9140-ca800a80ae2b'), (6147, '5a0a9bce-c74d-4d84-9cbd-c2b32a3e4e57'), (5128, '44db1975-a88d-43b3-b960-a2139eebbdc6'), (16907, 'ceef7a09-4dca-4a74-aa2b-0b0690a74f0d'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (9765, 'eb810505-9351-4111-b534-654b31082470'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (29743, '1628f56d-c18c-4c91-bdb8-dc690e6399dc'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (12852, 'b241f9da-f9a4-4802-9ba7-8b3acc58a446'), (12860, '37a4e1ea-7dc1-4432-8c7d-bcf671aaa7cd'), (26174, '821e6c05-2784-49a5-bd59-cc0fbe84bc9b'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30297, '1ea23f14-b6ce-4790-86cb-ad9dc50aaa2f'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (21595, '5efa160a-36d2-4b83-aa17-269b32dddfc6'), (30298, '4e59aebc-b2c4-4323-9471-a87bce987887'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (3168, 'd58e31d4-b4b6-4cc9-8587-4ad1e36d4985'), (22112, '72ebbfd3-6f7e-4e49-aeee-7692ef491d72'), (21602, '5ec88213-8b61-4e6e-a1bd-16c55ccd23cb'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (21607, 'b8f10d9a-6a0f-4bc2-9750-2fbc7550343f'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (22648, 'b652fc6a-23a8-44f7-9f33-2d0433295fc6'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (21640, 'abe405b2-ba14-4e03-9ad6-1596d753124c'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (174, '3afa87b4-1cf0-4362-96ad-7ef17bd6382d'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd'), (19127, 'bd45afb5-2ff0-41f6-9042-d4e2ad620dae'), (25784, '7cb641f4-1b19-4387-a006-38511a58f088'), (25788, '80465c07-5351-4512-b9d3-c7df4bbf4fd7'), (21186, '31cd25b9-75f3-43f6-b771-f47d73dafddd'), (14019, '1e028933-046d-4cab-bd8a-f607e4e474e9'), (25795, '8a88bc05-c2cf-41f8-aec4-fd247e336b17'), (25796, 'f457e45a-0754-41ca-a0a6-2fdb1f0068a1'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (20698, 'f52ebae7-79a4-4743-b8ee-714263eac884'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (20701, '04553fa1-c85e-4533-9f15-8a7240e55fc3'), (18142, '6432e78a-9379-42f5-ae76-b4e21cea4701'), (11490, '3eefa38e-fe5a-4ba7-b750-a5646ee7bb1b'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (4838, '60a144be-ec71-4f14-b20a-c312206f9649'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (3831, '320e50a7-2690-4798-86c2-921ef51a4cac'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (13560, '838f91f6-9316-4ade-94c0-ca8e99840812'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (30975, '63c59737-5943-46ba-b976-f82d30ce8680'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (258, 'ccf8ce5c-aeab-4f18-8d26-8afda967080a'), (3334, 'be61aaba-10f0-4007-a548-48ae184f321f'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (13615, '55b24d4d-ba4d-4442-a442-5c26f4d22693'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (27446, 'a836a2b1-6614-4c59-bc09-62ded216179c'), (314, '5b769cd0-05b9-407b-9d89-41edc442354b'), (22850, '660de6ac-c1ce-49bd-a434-4ddb5c3001b7'), (4930, '275b1eee-812a-47cd-be0f-53ecc35949e0'), (4935, '197b4fb3-4955-4228-8a30-8afc077b0038'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (30558, 'df0c3742-0b5f-44b2-a4c9-00979aafc11f'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (31617, 'c3283dce-ca20-4784-bdcf-43a5c3a84bbf'), (31618, '58b8c823-f817-4d5b-893b-11132d6df26f'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22925, '5c22144f-4b1e-4566-8088-264be9c9149a'), (22926, 'f448c8d4-9e0f-4837-af00-f4aa2144feaa'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (22934, '61d6db46-73e3-4b28-ae7e-592a144a4fcf'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (20401, '199f2f58-c978-4f4b-a9d9-cafcea17b82a'), (20402, 'ada5bb0d-ddbf-4bc3-923c-a6eea7de05d4'), (9137, 'c1ba5932-857e-4f99-b9d9-24fd7e3673da'), (20405, 'e3b986a7-6a6e-4258-a2b4-940d463071ec'), (13242, 'd0ca70d9-6af4-4c7e-81d5-aef3d71eb98c'), (13246, '7d3c0151-5723-45ae-9577-08964effc903'), (960, '49e7d917-8586-4f18-83aa-46158d6fd9cd'), (28609, '9c3549b2-3301-4ee5-a616-66788b24cd8f'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (7132, 'a6493562-d0d0-430d-82c0-7db219a71c05'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (21469, '70c3e6d0-0def-4824-b02a-c50d18bdfe9b'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (23523, '6af90060-dd6b-4db4-8bc4-ec3b30f02425'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

As you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of 
fine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art 
results. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same 
model variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total 
trainable parameters and in less time.

If you wish to use this example and improve the results, here are some things that you can try:

* Increase the number of training samples.
* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).
* Try different values for the arguments available in `LoraConfig`.
* Tune the learning rate and batch size.
Conclusion


**Congrats on finishing this unit**! There was a lot of information.
And congrats on finishing the tutorial. You've just coded your first Deep Reinforcement Learning agent from scratch using PyTorch and shared it on the Hub 🥳.

Don't hesitate to iterate on this unit **by improving the implementation for more complex environments** (for instance, what about changing the network to a Convolutional Neural Network to handle
frames as observation)?

In the next unit, **we're going to learn more about Unity MLAgents**, by training agents in Unity environments. This way, you will be ready to participate in the **AI vs AI challenges where you'll train your agents
to compete against other agents in a snowball fight and a soccer game.**

Sound fun? See you next time!

Finally, we would love **to hear what you think of the course and how we can improve it**. If you have some feedback then please 👉  [fill this form](https://forms.gle/BzKXWzLAGZESGNaE9)
- [Improving Generalization in Reinforcement Learning using Policy Similarity Embeddings](https://blog.research.google/2021/09/improving-generalization-in.html?m=1)
```

## Some additional challenges 🏆

The best way to learn **is to try things on your own**! As you saw, the current agent is not doing great. As a first suggestion, you can train for more steps. But also try to find better parameters.

In the [Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard) you will find your agents. Can you get to the top?

Here are some ideas to climb up the leaderboard:
* Train more steps
* Try different hyperparameters by looking at what your classmates have done 👉 https://huggingface.co/models?other=reinforce
* **Push your new trained model** on the Hub 🔥
* **Improving the implementation for more complex environments** (for instance, what about changing the network to a Convolutional Neural Network to handle
frames as observation)?

________________________________________________________________________
One limitation of most image segmentation models is that they only work with a fixed list of categories. For example, you cannot simply use a segmentation model trained on oranges to segment apples. To teach the segmentation model an additional category, you have to label data of the new category and train a new model, which can be costly and time-consuming. But what if there was a model that can already segment almost any kind of object, without any further training? That’s exactly what [CLIPSeg](https://arxiv.org/abs/2112.10003), a zero-shot segmentation model, achieves.
 
Here is the question: How can segmentation workflows improve using reinforcement learning insights?","The context does not provide a direct answer to the question of how segmentation workflows can improve using reinforcement learning insights. The context primarily discusses fine-tuning a semantic segmentation model with LoRa technique and deep reinforcement learning agents, but it does not explicitly mention how reinforcement learning can improve segmentation workflows. Therefore, it is not possible to provide a concise and relevant answer based on the provided context."
How can Gradio's modularity enhance multimodal reinforcement learning setups?,"[(12291, 'd5de9495-9838-4fab-98b0-0be846279bb7'), (27654, 'b251137d-d7d4-4fa5-be14-69555f6edf11'), (27659, '6d1c133a-710f-4e98-b2f3-66cfe80ce508'), (29196, '62ef49b9-9682-4414-af41-c88d9e1734ef'), (1037, 'ee236590-8d9d-4d1a-b76d-0cee9e154cdf'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (5656, '48889bbd-e147-43c1-9fed-511928271239'), (16415, '5102d298-58e8-416e-93b8-8fdc11615282'), (16416, '1b6290e2-d918-4435-af5a-004a7c4401ca'), (23079, 'd8d7522d-2458-4cb1-8bf1-20c0919ffa71'), (23080, '75d86ad5-e6d5-4e7b-9c90-e62adf96782a'), (23081, 'ee379356-f44c-4aa7-90b8-c610cd212722'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (27690, '6665c35c-ec76-46cf-b648-a349634c1c60'), (16426, '5b4e1e59-8d4c-41c6-86bd-80663aaba033'), (3119, '6a16877a-d058-484e-adf4-1e3130b324ae'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (3124, 'a3659303-0679-45cb-955d-d39a2f021b62'), (22585, 'edb214bd-f40c-4147-b630-0356d08fb6af'), (30778, 'a7cd9083-5e20-47c7-aff0-726923c16ad8'), (17980, 'e29ba4fa-fb2c-4785-b843-f02e109715ad'), (11836, '617b0947-c33e-41fe-897f-200765fc0fee'), (7742, 'a8c2a09b-3def-40a3-b4cb-2099d5f618c1'), (28240, '4035c01a-4aa2-417f-b523-d52485a27026'), (12889, '160c0b44-cd15-410c-bb6e-219ac158df55'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (27749, '36029bd7-ee3e-4a5f-aa4d-6ad3bf7b43cf'), (28262, '8a459067-4295-4f36-9434-ea65140715dc'), (9322, 'e8e8248a-6631-497d-a676-e17adbcee881'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (6254, 'd9fa13e2-a767-4d01-b533-d75868d06809'), (6255, '681fb439-0816-4462-948f-833ea8fbc7a9'), (6256, 'ffe4ca49-03b4-4810-94bd-6cccf7565388'), (25206, '9db87039-b249-40f0-a7a4-0f0a0ffad669'), (7799, '90107451-0822-4205-96bc-e19ca4488723'), (6266, '829c4c3b-b0fd-456c-bd7c-f2346f83d626'), (25211, '70426de3-d1ea-4941-9f6a-d330e09b60bf'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (4235, '681eb163-d8c1-47e3-be3d-645f57170f9d'), (22159, '17799602-fc0b-4ba1-ba09-4ace810a37c6'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (17061, '8236feee-2266-435c-9a8e-1f0c96cfbbfc'), (6826, '8aa2b833-292f-452f-a64e-c2b51b5ee38a'), (21675, 'f912b25a-5222-43ff-86a2-960b77ef8b41'), (21676, 'ffbe5cb7-0805-4291-8bf3-60dec72837ba'), (6831, '7ac1e106-868d-48b1-9a06-cfe591bee3e7'), (3764, '1d566ff1-0723-4b99-8df5-4ed497879ffa'), (22735, '83b4c533-1cd1-40c7-ba24-0e1cd2864f7f'), (15574, '3027817e-bff0-4f98-b280-898dc7515a2d'), (28389, '298bdf93-edee-48fb-94e3-1cf5585de1f2'), (28390, '39c5e061-d8dd-498b-a12f-8cc7f00ea6fd'), (2285, '5b7edfae-d1bb-4c5d-8d8f-83b616f05d0d'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (28407, '7274244c-531e-4ac2-af5d-255338d3f15f'), (16633, '2be04bc8-2a16-4193-acba-a4458e5c3368'), (16634, '6f1a4254-3071-4a9a-bec7-b76924e783ea'), (21754, '87b73ba0-88c4-4302-9951-5e5c45977bc5'), (16635, '409724c1-2138-41ad-8ba3-5f41b03ecc65'), (22780, 'edd8b962-01b2-4f07-9fe7-d76733c4cb6b'), (16639, '7009fc2d-ec8e-4632-83c4-60325dc25b35'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (15638, '4ab80b8b-3588-4236-a85f-25535e9cc861'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (4891, 'f9ecfb76-9f53-4b23-a8a3-a9b3357cde0a'), (12061, '088d44f7-6d57-47e8-bc79-ebf1f9b0e362'), (22813, '2ccd25e9-2fd3-4dee-9049-bbfb56eb3451'), (28446, '3004c35d-d9ea-427e-9a30-be1316497fec'), (23332, 'c84bd7fc-8be2-4db7-a4b6-ecba8420a3a9'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (23337, '6e20cb65-6726-441c-88b7-a6672caeb2a6'), (6973, '1b3ca299-1a5b-4ef1-913a-2012d57029c8'), (2366, '1174c2ad-26a9-4b06-b3e8-46a85f56cdf4'), (18242, 'a7179ccb-11b1-4187-8e41-305d86db021f'), (18244, 'd81aa300-5f1d-4039-940e-b6a43e999e2b'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (10054, 'c4f973e1-af44-4c0e-b4e6-70eb4198d313'), (18252, '65831a7f-cf17-4f94-829f-9632ff58a378'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (9552, '2fb5d785-7b4f-4cfe-8610-41fb486eba8a'), (9553, '7bf59ff6-d752-4eb0-9dc8-8aacda143835'), (17746, 'c2165130-5602-4551-afbb-1ace9bc4aa41'), (24916, 'da097562-0433-4341-8051-878ade17cbc6'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (5481, 'ba71a34e-5274-4266-bf75-5885ee20973d'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (24458, 'f7480eb5-a90b-4bb4-b923-1a0adb0171ff'), (24483, '024f64b0-5c8c-4ef6-b6a8-052ad25d717a'), (14245, '35bd9389-fa00-4736-b142-8e91a4b39287'), (14254, 'b23c2dce-9681-4ae0-8bb8-09f6d0757352'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (14256, '0a4cfdad-30ef-4391-80ac-97d920ce2976'), (12721, '26fbe79d-0795-4bcf-a444-8f56c357cc4c'), (14258, '6667bcf7-db4f-4153-8157-fb8ed11ecb89'), (11185, '91d6bcb4-003e-4935-a022-88d5dd293dc8'), (13751, '65fc6bad-ea60-4022-a81a-aeee8811fa1d'), (27067, 'c7913caf-83a2-4e0c-b2a6-4afc40eeb3bb'), (15803, '4d5385bd-2349-4a1a-8011-0a7ff542f2cc'), (23996, '4e71d9e3-ccf3-4d58-bdff-72c433ef9348'), (12739, '6cc8e2a4-dc70-44f1-ac92-984307404b7c'), (23495, '4c4dc9a6-b797-4ed4-a8d0-581f8ccc38b8'), (24009, '65541691-dd2c-4d52-ad58-e96067945c0a'), (3537, '9a71c7df-5460-4001-8821-388bee41536d'), (9170, '76d735e8-88bd-47aa-b8f1-d4d24b1129a0'), (9171, '3e57eea4-ca15-42c1-88ee-00a7cc4ff780'), (5592, 'f99b11e3-95f5-4489-a25c-3f6ec638d4cb'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (11744, '35e9d691-1085-49f8-9f6c-9a9fb3cbc426'), (29667, 'e602cdac-15d3-4250-b608-d55b803ae560'), (24037, 'd81b75dc-9e08-4ebb-b5b5-4892c8afbb15'), (5608, 'a7a2b842-db8e-4097-857c-0f4dca7d024b'), (29675, '000019b3-8a78-4906-88f7-f83866a7e7db'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (29677, '719c6c87-6233-4425-9b62-74cb004f3d99'), (27115, '69cf4e86-d85f-4cbc-ad45-b73133bc467d'), (27631, 'c0cf877f-6e3c-4643-9be5-31c5cba23ab0'), (30193, 'b4abc783-19f0-466e-b8f0-7941d569c619'), (28159, '82652842-43af-40f0-9e66-fbd1e5adac72')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: For example, Gradio lets you load multiple models in _parallel_ (imagine you want to compare 4 different text generation models from Hugging Face to see which one is the best for your use case):

![GIF of Gradio 2.0](./assets/22_gradio/recording-22.gif)

Or put your models in _series_. This makes it easy to build complex applications built from multiple machine learning models. For example, here we can build an application to translate and summarize Finnish news articles in 3 lines of code:

![GIF of Gradio 2.0](./assets/22_gradio/recording-24.gif)

You can even mix multiple models in _series_ compared to each other in _parallel_ (we’ll let you try that yourself!). To try any of this out, just install Gradio (`pip install gradio`) and pick a Hugging Face model you want to try. Start building with Gradio and Hugging Face 🧱⛏️
```


Tip: Change the filepaths so that they correspond to files on your machine. Also, if you are running in development mode, make sure the files are located in the top level of your custom component directory.

## Part 5 - Deploying and Conclusion

Let's build and deploy our demo with `gradio cc build` and `gradio cc deploy`!

You can check out our component deployed to [HuggingFace Spaces](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot) and all of the source code is available [here](https://huggingface.co/spaces/freddyaboulton/gradio_multimodalchatbot/tree/main/src).

See you in the next installment of this series!
![GIF of Gradio 2.0](./assets/22_gradio/recording-20.gif)

By default, this uses HuggingFace’s hosted Inference API (you can supply your own API key or use the public access without an API key), or you can also run `pip install transformers` and run the model computations locally if you’d like.

Do you want to customize the demo? You can override any of the default parameters of the [Interface class](https://gradio.app/docs) by passing in your own parameters:


![GIF of Gradio 2.0](./assets/22_gradio/recording-21.gif)




**_But wait, there’s more!_** With 10,000 models already on Model Hub, we see models not just as standalone pieces of code, but as lego pieces that can be **composed and mixed** to create more sophisticated applications and demos. 

For example, Gradio lets you load multiple models in _parallel_ (imagine you want to compare 4 different text generation models from Hugging Face to see which one is the best for your use case):
Build a Custom Multimodal Chatbot - Part 1

This is the first in a two part series where we build a custom Multimodal Chatbot component.
In part 1, we will modify the Gradio Chatbot component to display text and media files (video, audio, image) in the same message.
In part 2, we will build a custom Textbox component that will be able to send multimodal messages (text and media files) to the chatbot.

You can follow along with the author of this post as he implements the chatbot component in the following YouTube video!

<iframe width=""560"" height=""315"" src=""https://www.youtube.com/embed/IVJkOHTBPn0?si=bs-sBv43X-RVA8ly"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"" allowfullscreen></iframe>

Here's a preview of what our multimodal chatbot component will look like:

![MultiModal Chatbot](https://gradio-builds.s3.amazonaws.com/assets/MultimodalChatbot.png)
Here's a preview of what our multimodal chatbot component will look like:

![MultiModal Chatbot](https://gradio-builds.s3.amazonaws.com/assets/MultimodalChatbot.png)


## Part 1 - Creating our project

For this demo we will be tweaking the existing Gradio `Chatbot` component to display text and media files in the same message.
Let's create a new custom component directory by templating off of the `Chatbot` component source code.

```bash
gradio cc create MultimodalChatbot --template Chatbot
 
Here is the question: How can Gradio's modularity enhance multimodal reinforcement learning setups?","Gradio's modularity can enhance multimodal reinforcement learning setups by allowing users to load multiple models in parallel or in series, making it easy to build complex applications from multiple machine learning models. This modularity enables the composition and mixing of models to create more sophisticated applications and demos, which can be particularly useful in multimodal reinforcement learning setups where multiple models may need to be combined to process different types of data (e.g., text, images, audio)."
What is the best method for ensuring rendering fixes support RL-based segmentation workflows?,"[(4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (5128, '44db1975-a88d-43b3-b960-a2139eebbdc6'), (23572, '2de7f6ad-6c86-4678-bfbe-52a6a52e7949'), (12310, '51bc5584-b4bf-49cc-8c34-9e8af7e277d7'), (9757, '6fff0206-6c7c-4ec9-bae9-4ec88388616d'), (5661, 'd8772dad-7e28-4ef1-bdae-a1b7fda9c18c'), (9758, 'd58a44d2-3d7d-4d1b-8add-dfc7cda56c76'), (15393, '4f5d36f4-af1e-4e87-9447-f4bd71ac7487'), (15396, 'fdb0d317-8122-48fd-96db-d9486a8d2e55'), (9765, 'eb810505-9351-4111-b534-654b31082470'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (14384, 'fc019d04-8e11-4d2c-a0e1-8c8c8356ba37'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (30776, 'a5497ed7-9df9-460a-9ab8-8fffde7862ad'), (9284, 'b612daf8-a308-4ac1-a05d-d1af0da0a26b'), (587, 'd2ef4b9b-f115-4853-823e-4f5394d3d34c'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (20631, 'bffff36f-f170-4a81-9084-165694e82cc9'), (28312, 'f0ba4fda-06cc-4261-84f3-c36ed70b5c42'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (2203, '1daeda64-b42f-49a8-913c-f245e97796d6'), (4802, '41283247-db41-4748-a320-18738bac8c73'), (25796, 'f457e45a-0754-41ca-a0a6-2fdb1f0068a1'), (15558, '7340e3bf-b4c9-4123-a0cf-22c29f90717b'), (2249, 'b6c12764-f869-4323-a0ab-efd1cbcb6ee1'), (2250, '0dadf097-d697-4513-ac48-4dadd82077e9'), (715, 'a26ff4d2-acc5-48d5-a681-8537dd00b8b8'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (27351, '472b1c8c-b7ef-4926-8092-42cba02c3d9c'), (4826, 'f0958c86-ccd2-4cda-a032-0da6e036cf9e'), (740, 'b602239e-194a-474c-b7c5-7e805aeeedb6'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (17641, '912c2692-c674-4352-8d5a-792ba243de66'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (1775, 'b79bc5e0-231b-499e-a75f-be29512f9b98'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (18163, '54bb91f4-2781-4f40-8d92-13aa27630ec2'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (30975, '63c59737-5943-46ba-b976-f82d30ce8680'), (8959, '09aa42ad-193e-471f-80c7-dac2fb56a568'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (17666, 'b02b8b8a-fa20-4d12-94a9-c6d27cd0880f'), (18187, '792df508-2887-4c52-a0dd-ce849cee43ba'), (13580, 'b6a0460c-619e-49aa-8a9a-cc442cd9e4e3'), (18198, '9dabcd1b-1b21-4c5b-9457-90245c5b751a'), (279, 'a555b2d9-fe32-4a3a-b4b0-a28651c853be'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (8996, '2cfc69fd-58d5-4ede-ba16-0ce14d0215c1'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (13604, 'fe5f6c05-1601-4a5c-b151-6f19ea59c483'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (302, '3262b558-4660-418e-ad78-46906b82015f'), (13615, '55b24d4d-ba4d-4442-a442-5c26f4d22693'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (314, '5b769cd0-05b9-407b-9d89-41edc442354b'), (4922, '72fa3aeb-aff1-4f00-ad30-1abc8348f40a'), (28482, '3e98ed61-e405-42ac-8bd4-63da353dcace'), (1350, 'cdc3a9e8-a518-4730-a8ff-f721d7a45f20'), (26449, '9a9839ac-996e-4da7-8de0-7b2c0e2c1495'), (26452, '10b60c66-17b5-41d7-b872-1ecf45394bad'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (27492, 'bb58504b-576d-405b-b209-554ad924b1a8'), (31619, '15b09247-3696-4abe-b6ab-82b08cf31da4'), (2438, 'f22a2019-b1e6-4e51-811f-fa5cbf156208'), (9099, '19c52cd7-45ad-4005-a796-341d0e580413'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (27555, 'd90f51b7-b3a3-4746-828e-0d44f6fe834b'), (27556, 'd2700c56-e517-4a1b-bd0a-b8e812422bc8'), (9123, 'fa2c28c1-1c7a-4fbd-9f71-0208a67d4014'), (10153, '98362b25-6ed1-4f78-a411-6a8704d25529'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (28077, '880f9923-936e-43dd-bb07-042c35cd6eb6'), (15280, 'fd14bd2f-86be-41cd-a741-ac4438504319'), (9137, 'c1ba5932-857e-4f99-b9d9-24fd7e3673da'), (10165, 'f75e3fbd-0542-4741-8751-d1cf44ccf031'), (20405, 'e3b986a7-6a6e-4258-a2b4-940d463071ec'), (1463, '53fe3144-fd3d-468c-9412-86dd033d0631'), (20407, 'd2ed85aa-27cd-4080-ae18-59d3f37d4d0f'), (13246, '7d3c0151-5723-45ae-9577-08964effc903'), (28609, '9c3549b2-3301-4ee5-a616-66788b24cd8f'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (18393, '173808a6-4668-4801-844f-b58b1173a4ca'), (26587, '32fb3dde-e808-425b-838b-bc450d234324'), (28124, '6ddae567-fe9b-4244-91c4-0025ccc9f1a2'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (28123, '1e0f48d2-52b2-4fe6-a624-509a98f026ae'), (5090, '63922f2b-1c9a-493b-bbea-c9ecfdcca480'), (29666, 'd518d011-4a11-4759-922a-8c4af66d1d89'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (4079, 'be431428-66c6-491f-8c57-30b998a6e9d3'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (18417, '9c79ae80-999f-435f-853a-cb6873e2863b'), (28659, '8af7827d-abea-40a9-8154-cb7dd5e9b2ef'), (28660, '976239e6-d40b-4494-9ddc-ec24713753df'), (19447, 'bf6ee45f-ab8e-4e41-a43b-91a17678e167'), (5114, '5ac7776e-1bc2-465d-aad3-dba3f747ec34'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: - To get the final segmentation, depending on the task, you can call [`~OneFormerProcessor.post_process_semantic_segmentation`] or [`~OneFormerImageProcessor.post_process_instance_segmentation`] or [`~OneFormerImageProcessor.post_process_panoptic_segmentation`]. All three tasks can be solved using [`OneFormerForUniversalSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.
- One can use [`MaskFormerImageProcessor`] to prepare images for the model and optional targets for the model.
- To get the final segmentation, depending on the task, you can call [`~MaskFormerImageProcessor.post_process_semantic_segmentation`] or [`~MaskFormerImageProcessor.post_process_panoptic_segmentation`]. Both tasks can be solved using [`MaskFormerForInstanceSegmentation`] output, panoptic segmentation accepts an optional `label_ids_to_fuse` argument to fuse instances of the target object/s (e.g. sky) together.
## Universal image segmentation

Luckily, since around 2020, people started to come up with models that can solve all 3 tasks (instance, semantic and panoptic segmentation) with a unified architecture, using the same paradigm. This started with [DETR](https://huggingface.co/docs/transformers/model_doc/detr), which was the first model that solved panoptic segmentation using a ""binary mask classification"" paradigm, by treating ""things"" and ""stuff"" classes in a unified way. The key innovation was to have a Transformer decoder come up with a set of binary masks + classes in a parallel way. This was then improved in the [MaskFormer](https://huggingface.co/docs/transformers/model_doc/maskformer) paper, which showed that the ""binary mask classification"" paradigm also works really well for semantic segmentation.
```

As you can see, the results are far from perfect, however, this example is designed to illustrate the end-to-end workflow of 
fine-tuning a semantic segmentation model with LoRa technique, and is not aiming to achieve state-of-the-art 
results. The results you see here are the same as you would get if you performed full fine-tuning on the same setup (same 
model variant, same dataset, same training schedule, etc.), except LoRA allows to achieve them with a fraction of total 
trainable parameters and in less time.

If you wish to use this example and improve the results, here are some things that you can try:

* Increase the number of training samples.
* Try a larger SegFormer model variant (explore available model variants on the [Hugging Face Hub](https://huggingface.co/models?search=segformer)).
* Try different values for the arguments available in `LoraConfig`.
* Tune the learning rate and batch size.
```

If you take a look at the [uploaded prediction on
Segments.ai](https://segments.ai/admin-tobias/clipseg/samples/71a80d39-8cf3-4768-a097-e81e0b677517/ground-truth),
you can see that it\'s not perfect. However, you can manually correct
the biggest mistakes, and then you can use the corrected dataset to
train a better model than CLIPSeg.

<figure class=""image table text-center m-0 w-9/12"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Thumbnails of the final segmentation labels on Segments.ai."" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/123_clipseg-zero-shot/segments-thumbs.png""></medium-zoom>
</figure>

## Conclusion

CLIPSeg is a zero-shot segmentation model that works with both text and image prompts. The model adds a decoder to CLIP and can segment almost anything. However, the output segmentation masks are still very low-res for now, so you’ll probably still want to fine-tune a different segmentation model if accuracy is important.
 
Here is the question: What is the best method for ensuring rendering fixes support RL-based segmentation workflows?","There is no answer to this question based on the provided context. The context discusses image segmentation models, their architectures, and fine-tuning techniques, but it does not mention rendering fixes or RL-based segmentation workflows. Therefore, it is not possible to determine the best method for ensuring rendering fixes support RL-based segmentation workflows based on the provided information."
How can modular reinforcement learning improve segmentation performance evaluations?,"[(6146, 'd950b985-7fb5-46b2-9b05-0af6419fd745'), (6147, '5a0a9bce-c74d-4d84-9cbd-c2b32a3e4e57'), (6149, 'c49a7f6f-f774-4d7a-8ff4-176a9bbc89ed'), (1037, 'ee236590-8d9d-4d1a-b76d-0cee9e154cdf'), (10253, 'e0fdcfc2-6e25-4d57-8dea-e42c925a1e2c'), (9755, '5e041675-0a59-4588-b425-33a48fe0a077'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (29230, '454b7dff-bb2f-4e45-a707-2d84f949af29'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (12852, 'b241f9da-f9a4-4802-9ba7-8b3acc58a446'), (12859, '490a120a-9a4a-4266-b754-fcd3a64022ed'), (7740, '0b59319b-a4fe-475c-81b5-7609c67c8e9c'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (15939, '718490ff-eb0f-4285-a8a6-b096d2e3ca36'), (15940, '700dacdb-b5b3-49d9-93f9-479d38c003a9'), (15941, 'b55be617-b94e-461c-9aef-0ae569761bf9'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30291, 'a243f586-f6c1-4087-830f-30f59b90fe84'), (30804, '8058faaf-3301-4b40-916f-dd971333e242'), (27221, '3a0769e6-1ce2-4b41-9dbd-f9d0c6e8b04a'), (30294, 'dd8b16c3-fc89-4239-9882-3817703274f1'), (30292, '523f2e0b-8c7c-4d43-81b4-0bf745ad3379'), (30296, '958b0307-01d7-499e-8757-68692a7592a1'), (30297, '1ea23f14-b6ce-4790-86cb-ad9dc50aaa2f'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (21595, '5efa160a-36d2-4b83-aa17-269b32dddfc6'), (30293, '5d3a40e3-d4ac-44dd-978e-75cf25bfcb48'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30302, '4b7094d4-6aaf-42db-a006-4357c743ab41'), (30815, '4369e466-66c7-4eb4-b9cf-6dd77a047b40'), (30814, 'e42c82a0-50b1-4bce-847f-64b3fafb8038'), (21601, 'c196ce11-4041-40b9-8dc2-3f78645ca29e'), (21602, '5ec88213-8b61-4e6e-a1bd-16c55ccd23cb'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (21600, '9af1cc4c-21f2-4e01-81b1-d37162920ffb'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (21606, '136ec334-4b38-4957-a9a2-8b75cf91a5d1'), (21607, 'b8f10d9a-6a0f-4bc2-9750-2fbc7550343f'), (21608, '6d09190c-d1bb-44f6-9ff0-a0b2d6817266'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (21634, '59dd1789-1f01-48bf-8eb3-110a3171dd59'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (17034, '3900ad5e-267d-4c2f-bc71-54582b7591c6'), (23706, 'f26700ae-f00f-44bc-9c45-e17d6aeb4ab5'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (174, '3afa87b4-1cf0-4362-96ad-7ef17bd6382d'), (19127, 'bd45afb5-2ff0-41f6-9042-d4e2ad620dae'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (11483, '53fe2dd4-7446-402a-af18-0684ab5a5a47'), (19170, '880d968f-6f41-4d99-ba94-11e54bb9b625'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (6913, 'af173c92-a2cb-498e-8886-b34ca01dc05f'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (22813, '2ccd25e9-2fd3-4dee-9049-bbfb56eb3451'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (21802, '758235c8-efff-4748-9307-88fe98eabe23'), (20785, '857a3846-1162-4548-b0eb-3485b15ed280'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (22850, '660de6ac-c1ce-49bd-a434-4ddb5c3001b7'), (1351, 'b16150a3-12f9-4f52-8d3e-e0a43373b5c5'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (30558, 'df0c3742-0b5f-44b2-a4c9-00979aafc11f'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (13165, 'e430f4ce-b52f-4aae-831c-0df1d9c6f697'), (30298, '4e59aebc-b2c4-4323-9471-a87bce987887'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22926, 'f448c8d4-9e0f-4837-af00-f4aa2144feaa'), (31631, 'ff89793b-da5c-481a-97bb-e3d16785ec5a'), (5016, '2c7de3aa-8b01-440b-8b78-5d6d18c84a7d'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (8600, '269c11e4-e332-4535-8735-0da74c456141'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (20402, 'ada5bb0d-ddbf-4bc3-923c-a6eea7de05d4'), (959, 'a0c212d4-11c9-4064-b109-ffa186c14f64'), (13248, '60d563de-39e5-4163-89e6-86fa1da80be6'), (13249, 'f48db77e-e072-4406-ae6f-94f4b6862f54'), (960, '49e7d917-8586-4f18-83aa-46158d6fd9cd'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (964, '65fb34e4-bfe0-4990-b85f-bf4e0f542e42'), (13785, 'b8dd6060-ebfc-430a-ba77-3c51dae3fc40'), (7132, 'a6493562-d0d0-430d-82c0-7db219a71c05'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (21472, '4ebcad3d-c890-48ce-b30c-d05fb9a7166b'), (5601, '098dde3d-a5c2-4abc-bdf3-f35ebe913ba2'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (21481, '9356a740-eb3f-4eec-975e-bd0b77cbd195'), (21483, '5887ee51-0550-47b1-a9c9-bc00327706c9'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (28656, 'f5d4f38a-54a5-48d5-a0a0-0ae9ce8eaa77'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```
### Evaluating the model

In order to evaluate the model, we need some additional information; the mean and standard deviation of the states that were used during training. Fortunately, these are available for each of the checkpoint’s [model card](https://huggingface.co/edbeeching/decision-transformer-gym-hopper-expert) on the Hugging Face Hub! 

We also need a target return for the model. This is the power of return conditioned Offline Reinforcement Learning: we can use the target return to control the performance of the policy. This could be really powerful in a multiplayer setting, where we would like to adjust the performance of an opponent bot to be at a suitable difficulty for the player. The authors show a great plot of this in their paper!
Generalization in Reinforcement Learning

Generalization plays a pivotal role in the realm of Reinforcement Learning. While **RL algorithms demonstrate good performance in controlled environments**, the real world presents a **unique challenge due to its non-stationary and open-ended nature**.

As a result, the development of RL algorithms that stay robust in the face of environmental variations, coupled with the capability to transfer and adapt to uncharted yet analogous tasks and settings, becomes fundamental for real world application of RL.

If you're interested to dive deeper into this research subject, we recommend exploring the following resource:

- [Generalization in Reinforcement Learning by Robert Kirk](https://robertkirk.github.io/2022/01/17/generalisation-in-reinforcement-learning-survey.html): this comprehensive survey provides an insightful **overview of the concept of generalization in RL**, making it an excellent starting point for your exploration.
* [Gilbert et al. (2022)](https://huggingface.co/papers/2204.10817) propose **Reward Reports for Reinforcement Learning** systems, recognizing the dynamic nature of ML systems and the need for documentation efforts to incorporate considerations of post-deployment performance, especially for reinforcement learning systems.
* [Goel et al. (2021)](https://huggingface.co/papers/2101.04840) develop **Robustness Gym**, an evaluation toolkit for testing several aspects of deep neural networks in real-world systems, allowing for comparison across evaluation paradigms. 
* Through the [ABOUT ML project](https://partnershiponai.org/workstream/about-ml/) ([Raji and Yang, 2019](https://huggingface.co/papers/1912.06166)), the Partnership on AI is coordinating efforts across groups of stakeholders in the machine learning community to develop comprehensive, scalable documentation tools for ML systems.
We show that MBRL outperforms other methods for deep reinforcement learning (RL) and deep convolutional
neural networks (CNNs) by a factor of at least 2. In addition, we show that, compared to CNNs, MBRL
performs better in two-dimensional (2D) and three-dimensional (3D) cases.
----------------------------------------------------------------------------------------------------
domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.*
 
Here is the question: How can modular reinforcement learning improve segmentation performance evaluations?","The context does not provide a direct answer to how modular reinforcement learning can improve segmentation performance evaluations. The discussion on reinforcement learning is focused on generalization, offline reinforcement learning, and robustness, but it does not explicitly connect to segmentation performance evaluations. The section on segmentation appears to be unrelated to reinforcement learning and discusses a model called OneFormer for image segmentation tasks. Therefore, it is not possible to deduce an answer to the question based on the provided context."
What makes rendering improvements crucial for multimodal segmentation demos?,"[(5128, '44db1975-a88d-43b3-b960-a2139eebbdc6'), (10253, 'e0fdcfc2-6e25-4d57-8dea-e42c925a1e2c'), (11287, 'a536c7d1-0fcb-4f4c-8ff8-e204c220d9f2'), (15899, '9605f71a-24d0-47f8-83b8-41f85bd0d5f9'), (15901, '7f0d2609-8a6a-48fd-969a-3abbc22f383d'), (15405, '0f488188-1f44-4119-a5c7-4677aadc18ef'), (561, '187180dc-60a2-4d0b-8b3d-ecf187941bd1'), (18430, '36a4cf8d-615d-4664-9e74-f6327bbb72fb'), (587, 'd2ef4b9b-f115-4853-823e-4f5394d3d34c'), (3148, 'eb17b7df-c261-4c87-a068-9db283ce4f8e'), (11345, '24539c36-0220-4657-a439-8a9880b8094b'), (30802, '29278ad4-3abc-4d93-9603-14c1b27a60e5'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (3168, 'd58e31d4-b4b6-4cc9-8587-4ad1e36d4985'), (21602, '5ec88213-8b61-4e6e-a1bd-16c55ccd23cb'), (14947, 'd35aeb84-5aef-4870-9278-f4fadc4041e5'), (20580, 'd3179689-1789-4157-83a3-9dfab3333b49'), (20068, 'd2d9fdee-7033-4cda-a724-d9c2e219656c'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (14975, '66700e6c-33b1-40fc-aede-27c4961007ad'), (6790, '928f2a5a-78fe-4871-8cdd-ba2b926ec649'), (3210, '95831a12-9e50-4a68-b8b8-4b4585e6eb0e'), (20631, 'bffff36f-f170-4a81-9084-165694e82cc9'), (20643, 'd1871a39-b9f8-41c2-8661-80c13f31418c'), (677, '44aea0e0-14ad-4f24-8006-33bfd0e9b710'), (4775, 'dece872c-e754-4f2b-b021-07367964cf41'), (20148, '204a890a-e9d0-4a29-9919-40ed0c2b5ff3'), (25784, '7cb641f4-1b19-4387-a006-38511a58f088'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (4802, '41283247-db41-4748-a320-18738bac8c73'), (22220, 'ce994d8b-46c5-4aec-af8e-5f03224f51aa'), (1749, '22147376-467e-46ca-bd5a-e6e7fd40222e'), (11478, '2300ff5b-8339-404f-821e-444682c445c9'), (18136, 'ea71868e-38ba-42e9-9d5c-3ac4745b0a68'), (4826, 'f0958c86-ccd2-4cda-a032-0da6e036cf9e'), (19173, 'a4446067-88e5-40cc-a875-f47ee7a0f9f6'), (4838, '60a144be-ec71-4f14-b20a-c312206f9649'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (1775, 'b79bc5e0-231b-499e-a75f-be29512f9b98'), (19184, 'd05aa2be-ca0f-4956-a55d-6ce988a2ae43'), (13554, 'f239a4cb-1368-4161-bb6c-d4d76b74b26d'), (18163, '54bb91f4-2781-4f40-8d92-13aa27630ec2'), (23287, '29e5f8af-f9b4-47ce-84d1-7a3cd7791dfc'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (252, '1d1458f5-0779-4f41-8eab-579cf8943202'), (1789, 'c5b52700-22e4-4c95-b920-96904f5a68fc'), (18177, 'd4e0c705-3de4-4216-a3ae-83c0e11dfa4c'), (6403, '6b72a701-c1b5-46a7-bcb8-a543df789c1f'), (18187, '792df508-2887-4c52-a0dd-ce849cee43ba'), (13580, 'b6a0460c-619e-49aa-8a9a-cc442cd9e4e3'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (24339, 'bc00f956-dd91-4b6d-a59a-7b7567c85c23'), (18198, '9dabcd1b-1b21-4c5b-9457-90245c5b751a'), (279, 'a555b2d9-fe32-4a3a-b4b0-a28651c853be'), (13594, '92550864-e4ab-4457-bbb2-500e3701308a'), (13604, 'fe5f6c05-1601-4a5c-b151-6f19ea59c483'), (293, '9eab52bc-efd8-46aa-9ce1-6a7b11c3b774'), (4903, '6095ce46-c484-40c2-9887-31d8075f7af3'), (9001, '1c57d7da-eb78-49f0-bc98-77ea66ec1f79'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (302, '3262b558-4660-418e-ad78-46906b82015f'), (13615, '55b24d4d-ba4d-4442-a442-5c26f4d22693'), (10543, '595b18a2-b752-4644-b8bf-9f5028af4359'), (10542, '6cf5c516-c66b-4848-a5f4-b8b0ae2bab1c'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (314, '5b769cd0-05b9-407b-9d89-41edc442354b'), (4930, '275b1eee-812a-47cd-be0f-53ecc35949e0'), (18245, '28688e6c-a826-48ad-8c3c-e6f5ebd8ed30'), (3402, '4dc94cdc-9499-4f9e-b678-d2f3f5a62202'), (26449, '9a9839ac-996e-4da7-8de0-7b2c0e2c1495'), (26452, '10b60c66-17b5-41d7-b872-1ecf45394bad'), (4951, '7b7c6e79-a660-42b0-88a4-9edd64e3b870'), (3933, 'ded8e577-ef5c-43b5-9c1d-44aff955b0bf'), (13165, 'e430f4ce-b52f-4aae-831c-0df1d9c6f697'), (9072, '7a516a26-4603-45f9-b69a-f48eb9d9c67e'), (891, '2e36c17c-4d8b-4447-9771-1316e6eaf105'), (893, 'd7a5a15a-3c14-47bd-ad8f-f78e904ff293'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (19339, 'bf3697af-335f-4f36-bbbe-8873cd28d428'), (9099, '19c52cd7-45ad-4005-a796-341d0e580413'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (9113, 'ebd0fed7-b505-42b8-bfad-b7c31a585708'), (22943, '17b477fa-3d53-4e7c-960a-195da8ecfc2c'), (27554, 'ec5bc3ba-1f5d-4d02-8f10-3ae6e1bd96e7'), (9123, 'fa2c28c1-1c7a-4fbd-9f71-0208a67d4014'), (13731, '121de4cf-204b-4183-8bb7-4053b4f062df'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (9137, 'c1ba5932-857e-4f99-b9d9-24fd7e3673da'), (20402, 'ada5bb0d-ddbf-4bc3-923c-a6eea7de05d4'), (10165, 'f75e3fbd-0542-4741-8751-d1cf44ccf031'), (1463, '53fe3144-fd3d-468c-9412-86dd033d0631'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (18366, '8a7e2dee-f14d-4ea1-819d-851a6affb887'), (10177, 'ec293404-bbef-49a7-bb4f-935bed906eca'), (22468, '8b38d677-f152-45c5-bbfa-64b1c1a7a30c'), (5063, 'f4cfdc74-4cbc-4213-a3fb-a400b432dc40'), (1481, 'e4150511-83cc-45b5-a991-ecb59d6f6b6c'), (18393, '173808a6-4668-4801-844f-b58b1173a4ca'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (7133, '3ca3af98-00c3-4ab5-bdf1-69dc04498342'), (5090, '63922f2b-1c9a-493b-bbea-c9ecfdcca480'), (18407, 'a0e0fda7-1e4a-4d52-95bc-719a3f5678d5'), (5104, 'c7298af0-3cd0-4756-a07b-1e0315189121'), (18417, '9c79ae80-999f-435f-853a-cb6873e2863b'), (19447, 'bf6ee45f-ab8e-4e41-a43b-91a17678e167'), (5114, '5ac7776e-1bc2-465d-aad3-dba3f747ec34'), (26620, 'cc50a00e-0492-4785-8035-69e31ebb8d8b'), (26621, '0fb3e8f4-b67a-4eb4-85bb-d759d00c69b2'), (26622, 'b28e4c35-8797-4756-be0c-d31d539e4af0')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: domain (semantic, instance, and panoptic segmentation) within a single multi-task training process. Secondly, we introduce a task token to condition our model on the task at hand, making our model task-dynamic to support multi-task training and inference. Thirdly, we propose using a query-text contrastive loss during training to establish better inter-task and inter-class distinctions. Notably, our single OneFormer model outperforms specialized Mask2Former models across all three segmentation tasks on ADE20k, CityScapes, and COCO, despite the latter being trained on each of the three tasks individually with three times the resources. With new ConvNeXt and DiNAT backbones, we observe even more performance improvement. We believe OneFormer is a significant step towards making image segmentation more universal and accessible.*
*We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with
lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel
hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding,
thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution
differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from
different layers, and thus combining both local attention and global attention to render powerful representations. We
show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our
approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
1. Improvements in NeRF-to-mesh and mesh generation. As we've seen, current generation models are similar to photogrammetry in that they require a lot of work to produce game-ready assets. While this is useful in some scenarios, like creating realistic high-fidelity assets, it's still more time-consuming than making low-poly assets from scratch, especially if you're like me and use an ultra-low-poly art style.
2. New rendering techniques that allow NeRFs to be rendered directly in-engine. While there have been no official announcements, one could speculate that [NVIDIA](https://www.nvidia.com/en-us/omniverse/) and [Google](https://dreamfusion3d.github.io/), among others, may be working on this.

Of course, only time will tell. If you want to keep up with advancements as they come, feel free to follow me on [Twitter](https://twitter.com/dylan_ebert_). If there are new developments I've missed, feel free to reach out!
Segmentation tasks are helpful in self-driving vehicles to create a pixel-level map of the world around them so they can navigate safely around pedestrians and other vehicles. It is also useful for medical imaging, where the task's finer granularity can help identify abnormal cells or organ features. Image segmentation can also be used in ecommerce to virtually try on clothes or create augmented reality experiences by overlaying objects in the real world through your camera.

```py
>>> from transformers import pipeline

>>> segmenter = pipeline(task=""image-segmentation"")
>>> preds = segmenter(
...     ""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg""
... )
>>> preds = [{""score"": round(pred[""score""], 4), ""label"": pred[""label""]} for pred in preds]
>>> print(*preds, sep=""\n"")
{'score': 0.9879, 'label': 'LABEL_184'}
{'score': 0.9973, 'label': 'snow'}
{'score': 0.9972, 'label': 'cat'}
 
Here is the question: What makes rendering improvements crucial for multimodal segmentation demos?","The context does not provide a direct answer to this question. The discussion on rendering improvements is related to NeRF-to-mesh and mesh generation, which is not directly connected to multimodal segmentation demos. The context does mention image segmentation and its applications, but it does not explain why rendering improvements are crucial for multimodal segmentation demos. Therefore, I cannot provide a concise and relevant answer to this question based on the provided context."
How can Transformers be applied to multimodal tasks such as visual question answering and audio classification?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (21532, '0c371312-bf0e-4f80-a268-6d03b0c8ab3f'), (30241, '53763b36-d979-4447-9d75-c844d9251995'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (27688, 'a073f2c4-979d-44a2-9ead-1fd732d3029a'), (28715, 'aaa48597-36d7-4f2f-8b37-6eafd713a6bf'), (14910, '5cb12218-60e4-49e4-b76c-dd87fd9f469b'), (24130, '0bf14987-ba3e-4959-8f52-26aa0bf90f9f'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (25704, '436067a1-a6a4-42f8-917c-fb3822de75ac'), (25708, '0a161716-5f3c-45c5-aac2-4eb719f697f4'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (14982, '1c8219eb-d67d-4e86-8dbf-0845ac9a253e'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (5800, '974ae270-e2f0-42c9-a6a9-2f92b2b880f6'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (24298, '1aa312da-1a6c-44c4-9f8e-72755f23c204'), (4849, 'c4a3a5ac-8550-403f-80ec-5cfb0aa62c18'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (2811, '7a203781-c1a2-43a6-ad0b-ce893ad7bd5b'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (17158, 'fb9984a1-c183-455d-9942-b8dbeb04c544'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (1821, '41436a46-8833-4205-8350-5b9954f8c11d'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (23859, 'e47fd052-4097-479b-95f3-f65bc65a3c2c'), (23860, 'afa6eb69-b861-463e-94f7-a8cffe60467e'), (23861, '780f7ace-a5db-41f7-899f-c6d125531fab'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (5436, '3eecd54e-6656-44ff-b0a3-3c487b30dc3f'), (4415, 'b9a9a85e-e4e0-4cb7-b45f-4ec40e2891b6'), (14655, 'c5adb19a-7ebd-4d40-a31d-55a6f8a1b978'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (14657, 'd5903c56-f92a-451c-b8e1-27c7ee957814'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (3921, '6311b8eb-e903-45b5-bc7c-c24e69a96ce8'), (3922, '6f3ca0aa-6e55-4248-8281-52387a0ce0a2'), (3923, 'e87b7120-b4af-4c3f-9448-479a9d54a9d7'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (11114, '933fa587-6033-469c-b633-a24147cec80c'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (7546, '125265e4-f906-4180-bf64-53f1be1bd941'), (22918, '201f6149-0a67-49ea-81bd-e07891bdb9c1'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (5530, '89aa49e3-2bb4-41a3-8831-f5755770ebde'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (5035, '83a015d9-5914-4516-b56d-1bd17eb3d24e'), (30130, '701c9d28-b4a0-4d27-8f27-eef55e68d62c'), (30131, '238a09bf-3422-4cda-b19b-b8b9886a02c8'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (7123, '5550e6f5-759d-4aef-9d97-83b53144cc4b'), (7124, 'dc82a8b4-22d9-469e-838d-0257251354eb'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (10194, 'e573dcfe-a4f3-435f-9afe-101265a292e5'), (6616, 'c8496f16-33f0-4cf4-a35c-3dc3f0fefa1b'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (13277, 'b5287569-e8e1-4084-8fef-d21dbf2bb03f'), (24549, 'e159577d-48a3-4a97-b848-68e78c570951'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (11760, '3e12a8e7-b157-4867-8416-88fc207b2fdc'), (8176, '2cc6d0c0-3a2d-4ebc-8266-4d2ee6bf3300'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: 📝 **Natural Language Processing**: text classification, named entity recognition, question answering, language modeling, summarization, translation, multiple choice, and text generation.<br>
🖼️ **Computer Vision**: image classification, object detection, and segmentation.<br>
🗣️ **Audio**: automatic speech recognition and audio classification.<br>
🐙 **Multimodal**: table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers support framework interoperability between PyTorch, TensorFlow, and JAX. This provides the flexibility to use a different framework at each stage of a model's life; train a model in three lines of code in one framework, and load it for inference in another. Models can also be exported to a format like ONNX and TorchScript for deployment in production environments.
In Audio:
- [Automatic Speech Recognition with Wav2Vec2](https://huggingface.co/facebook/wav2vec2-base-960h)
- [Keyword Spotting with Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)
- [Audio Classification with Audio Spectrogram Transformer](https://huggingface.co/MIT/ast-finetuned-audioset-10-10-0.4593)

In Multimodal tasks:
- [Table Question Answering with TAPAS](https://huggingface.co/google/tapas-base-finetuned-wtq)
- [Visual Question Answering with ViLT](https://huggingface.co/dandelin/vilt-b32-finetuned-vqa)
- [Zero-shot Image Classification with CLIP](https://huggingface.co/openai/clip-vit-large-patch14)
- [Document Question Answering with LayoutLM](https://huggingface.co/impira/layoutlm-document-qa)
- [Zero-shot Video Classification with X-CLIP](https://huggingface.co/docs/transformers/model_doc/xclip)


## 100 projects using Transformers
```


## Processor

For models that support multimodal tasks, 🤗 Transformers offers a processor class that conveniently wraps processing classes such as a feature extractor and a tokenizer into a single object. For example, let's use the [`Wav2Vec2Processor`] for an automatic speech recognition task (ASR). ASR transcribes audio to text, so you will need a feature extractor and a tokenizer.

Create a feature extractor to handle the audio inputs:

```py
>>> from transformers import Wav2Vec2FeatureExtractor

>>> feature_extractor = Wav2Vec2FeatureExtractor(padding_value=1.0, do_normalize=True)
```

Create a tokenizer to handle the text inputs:

```py
>>> from transformers import Wav2Vec2CTCTokenizer

>>> tokenizer = Wav2Vec2CTCTokenizer(vocab_file=""my_vocab_file.txt"")
```

Combine the feature extractor and tokenizer in [`Wav2Vec2Processor`]:

```py
>>> from transformers import Wav2Vec2Processor

>>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)
<h3 align=""center"">
    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>
</h3>

<h3 align=""center"">
    <a href=""https://hf.co/course""><img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png""></a>
</h3>

🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.

These models can be applied on:

* 📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.
* 🖼️ Images, for tasks like image classification, object detection, and segmentation.
* 🗣️ Audio, for tasks like speech recognition and audio classification.
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
 
Here is the question: How can Transformers be applied to multimodal tasks such as visual question answering and audio classification?","According to the context, Transformers can be applied to multimodal tasks such as visual question answering and audio classification through various models and processors. For example, in Multimodal tasks, Transformers support:

* Visual Question Answering with ViLT
* Audio Classification with Audio Spectrogram Transformer
* Table Question Answering with TAPAS
* Zero-shot Image Classification with CLIP
* Document Question Answering with LayoutLM
* Zero-shot Video Classification with X-CLIP

Additionally, the processor"
"What is the best approach to adapt models across PyTorch, TensorFlow, and JAX?","[(17414, 'c6d055b8-3841-4b16-9578-69e160b961ba'), (17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (17417, '6ef43f68-8306-418b-b698-6d8f7cc6ab7d'), (27146, 'cbb71c05-7152-43db-b473-37078bba89e2'), (25612, '01868de2-601f-4b56-9680-7fff3bd713aa'), (18957, '193a51f9-a0f7-47e2-a2d4-3cd5a9594dd6'), (17934, 'a346e6f1-8dc6-4546-85c3-dac1062bf6ff'), (3600, 'a5251b45-4980-4841-a1da-d832c0aa4e34'), (17426, '5faf29be-8c8f-40c3-b527-f840fdfa1f9c'), (17427, '30ad9bbf-82b5-4e3f-a73b-b07045263764'), (17429, '393ddd8a-1619-4d0d-afa0-f1d8f5566ed5'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (17439, '220bb0f1-e5a5-4c21-ba59-845839d83ad3'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (30760, '4eb04640-0bf5-4f77-820a-caa4aa28ce24'), (30761, '683724e2-567a-46f0-9254-202ca2f88a96'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (7216, 'b5472bb0-d98a-42de-8982-9dc298abcd5e'), (29236, 'c8256ce9-e984-4533-a82f-3f08a2d65e83'), (9282, '91edce5b-dc46-47e2-85e4-33cf1d17dab7'), (17994, '34680928-da37-47ee-90c9-5404760066a2'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (13958, 'ed200096-8374-40a5-9473-b55aae1e78f8'), (5768, '6cc58ea0-db3c-4c3e-a419-0131c79cf6c3'), (31377, 'de68661c-8134-474b-a6ba-4c51686c8d6b'), (19612, 'b15310a0-0755-4662-b35d-093f3683e01a'), (1181, '2e574621-b2e7-4ffb-ad69-7aa65d1e16fb'), (1191, 'ece7abdd-1376-495b-8daa-d664f2a27dae'), (30891, 'ce0c26e5-9ecb-4d1c-ac24-560c5974dda4'), (24754, '5671e7e5-7069-4f2f-a3cf-9a06fa6383f2'), (30393, '55b00024-7156-47f0-9b24-0469fd804c45'), (1213, 'bec86d0f-9e27-4271-bd19-fa2a88278612'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (30916, '83870699-d55d-4da0-9257-d46016695c8c'), (23749, 'ebbc939a-6402-4c02-9891-1531598d8076'), (23750, 'c2856f93-0376-48a3-8544-fd75e08e618a'), (21194, '31079bba-cf47-4457-a079-280a5ea0fa59'), (1227, '96984733-267c-43b9-8d96-8ba7a3af1838'), (22226, '66bae662-348e-4bb7-ba75-056a15252a52'), (14035, 'c1939c78-2288-4ec5-9074-a8606c2ef8f7'), (25302, 'bcf9a9f1-7a6b-4a62-87f7-68506603417d'), (727, '1a71cdae-fde1-416d-b879-fd79101fe6df'), (6370, '5d4d5a4e-c039-411b-b725-afbb79ba51aa'), (6371, '4d927d59-5f00-42ca-a1e6-802916ff0aeb'), (7917, '766d65a2-485e-4909-89b4-afc2cdd40c54'), (19696, '43e599c7-2548-47c5-9039-cfa92c630506'), (5875, '867d8aee-620e-43be-b8d9-287612f7a6cb'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (22772, 'd0b704d9-ea88-403d-8797-758ba929ebd4'), (17653, 'a8e94114-6620-4796-9731-d48f51986131'), (30455, '30b49335-6092-4e0b-876d-b396ad058840'), (11515, '6f898609-5b85-4751-9ca0-550a6dffe32f'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (28429, '9c6fb59e-1d9f-48bd-be33-bd99c86d369d'), (28430, '432626a7-db6d-4b84-8276-b46a0483c396'), (29460, 'f654c347-e7a6-4470-85a0-5dfbbd1cfa56'), (21785, '0f42515e-4921-41d6-a8f2-18c62768982e'), (21786, 'c4b02c4e-9c4d-437d-ba8f-fa2e3ed2ba61'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (3358, '8564a156-b378-477d-87d0-31501018c920'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (16168, '8720f9c5-bd5a-4961-b1b7-7dd1d69cb57d'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (8492, '37b69803-2f18-4f43-850e-e3c81aa1cb27'), (16176, 'a7578109-e271-40f8-aa02-f89b5dddab55'), (6963, '999901f5-1f09-4c48-81c5-c208d0036338'), (16184, '08e02bad-54da-4851-82ae-02218bac7497'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (16191, '9fbef11c-ebf4-4a39-99e1-dddb5fd7c23f'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (16201, '74a34dcf-6a1b-4918-a45a-b5f73a7e5d91'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (9051, '09b3a3e8-2734-459b-9dcf-d885565ee6dd'), (352, '756c346a-bd77-440b-b6a9-1fabbf0c457d'), (353, 'aeabca25-387b-4b0c-bd33-e034666b36e9'), (354, '2a268797-e79e-4063-843a-7cbe133cb03e'), (21349, 'c9e6dbb0-3689-456f-b44d-2e633cd150ad'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (13176, 'a57502e1-52a5-41dc-aebb-b7cb7f00763d'), (22394, 'b66b7946-772b-43b6-8e7d-b5388eaf7cf6'), (7547, '836d3b3f-ca29-4139-a241-bd2400998d51'), (14727, '680f190f-eca8-4942-bebc-9212cd7caa87'), (15762, '4e4f666b-dc03-4e9c-8dfe-2ccbb5a5c56b'), (11666, 'c66a5c93-defd-4c55-889b-7d595384e150'), (3994, 'dbdd091c-d899-497d-a2d9-65da22be66ef'), (6561, 'ee35c81c-c680-4dfc-852b-7c11aae2a074'), (2469, '24b5a6c3-9526-4bd4-b85f-cf7733f0a28b'), (20906, '4c0585c7-5b68-4b5f-820b-740cca4e0187'), (30648, '6aa08f8a-dd96-49a3-8e65-360c5e8fd924'), (20922, 'ff6bc297-0637-4274-9753-aa484a8303ad'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (19907, '4a8a0729-332c-401f-b9cf-aedad05cb3f3'), (26052, '38fb2a66-8180-4fb1-9f0b-65a441b0b6a6'), (9163, 'd7206e44-29b4-499b-92c4-0008be204752'), (16843, '926901bf-b2bd-4869-8ac4-10833c3d0dd1'), (3026, 'd16d30d5-b4b9-4fe2-9108-038403019bd9'), (21972, '85cbe3b6-6131-4a4a-9548-070d1c3b8c2d'), (21973, '7d6045ed-e6e9-40c3-88be-46c6b780d3eb'), (13280, '2c3ad26d-8418-4d01-a881-e5dd6be54e3f'), (31719, '9413008c-5af5-444b-ba79-f5ba91409f43'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (31721, '0d565372-c33c-4322-b1e2-cd0ad577cbf0'), (27116, 'ee44caad-d368-4c27-b4ad-92f672f5e3da'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31738, '7d8861cd-9646-4d02-9213-734c5cbe5597'), (7163, '1f39bf74-f944-44bc-ac1d-a46df327488e'), (7165, '962f5c10-b98a-4400-9f01-67971ed8db89')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: There is a tendency among PyTorch engineers (picture me staring darkly across the open-plan office here) to see this as a problem to be overcome; their goal is to figure out how to make TensorFlow get out of their way so they can use the low-level training and data-loading code they’re used to. This is entirely the wrong way to approach TensorFlow! Keras is a great high-level API. If you push it out of the way in any project bigger than a couple of modules you’ll end up reproducing most of its functionality yourself when you realize you need it.

As refined, respected and highly attractive TensorFlow engineers, we want to use the incredible power and flexibility of cutting-edge models, but we want to handle them with the tools and API we’re familiar with. This blogpost will be about the choices we make at Hugging Face to enable that, and what to expect from the framework as a TensorFlow programmer.

### Interlude: 30 Seconds to 🤗
```

Now the `FlaxMLPModel` will have a similar interface as PyTorch or Tensorflow models and allows us to attach loaded or randomly initialized weights to the model instance.
To learn more about each of the supported schemes, please have a look at one of the resources shared below. Please also have a look at the appropriate sections of the documentation.

Note also that the details shared below are only valid for `PyTorch` models, this is currently out of scope for Tensorflow and Flax/JAX models.

## Table of contents

- [Resources](#resources)
- [Comparing bitsandbytes and auto-gptq](#Comparing-bitsandbytes-and-auto-gptq)
- [Diving into speed benchmarks](#Diving-into-speed-benchmarks)
- [Conclusion and final words](#conclusion-and-final-words)
- [Acknowledgements](#acknowledgements)

## Resources
- Don't reinvent the wheel! More often than not, there are at least two reference implementations you should check: the
PyTorch equivalent of the model you are implementing and other TensorFlow models for the same class of problems.
- Great model implementations survive the test of time. This doesn't happen because the code is pretty, but rather
because the code is clear, easy to debug and build upon. If you make the life of the maintainers easy with your
TensorFlow implementation, by replicating the same patterns as in other TensorFlow models and minimizing the mismatch
to the PyTorch implementation, you ensure your contribution will be long lived.
- Ask for help when you're stuck! The 🤗 Transformers team is here to help, and we've probably found solutions to the same
problems you're facing.
In the remainder of this guide, you will learn what's needed to add a new TensorFlow model architecture, the
procedure to convert PyTorch into TensorFlow model weights, and how to efficiently debug mismatches across ML
frameworks. Let's get started!

<Tip>

Are you unsure whether the model you wish to use already has a corresponding TensorFlow architecture?

&nbsp;

Check the `model_type` field of the `config.json` of your model of choice
([example](https://huggingface.co/bert-base-uncased/blob/main/config.json#L14)). If the corresponding model folder in
🤗 Transformers has a file whose name starts with ""modeling_tf"", it means that it has a corresponding TensorFlow
architecture ([example](https://github.com/huggingface/transformers/tree/main/src/transformers/models/bert)).

</Tip>


## Step-by-step guide to add TensorFlow model architecture code
 
Here is the question: What is the best approach to adapt models across PyTorch, TensorFlow, and JAX?","The best approach to adapt models across PyTorch, TensorFlow, and JAX is to use the high-level API of each framework, rather than trying to overcome or push it out of the way. This allows you to leverage the power and flexibility of each framework while using the tools and API you're familiar with. Additionally, it's recommended to follow established patterns and conventions in each framework, and to seek help when needed."
How do Hugging Face pipelines simplify multimodal workflow integration?,"[(25090, '1c9734dc-9c4c-4216-b57d-1ba5098bb275'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (18489, 'c196cc2b-dfd6-4797-a458-54ad07efda99'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8269, '63e383a9-dbfa-4c9f-a286-0668a682d313'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (23159, '8df7603d-0d12-428d-8ffd-3b4c55cd0c5f'), (26745, '509a2418-a582-44fa-9c23-427ca2329092'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (10923, '48a8b0e4-0e9b-4f01-b296-83212b9301c1'), (24752, '3798bf75-5a2c-4b7a-9800-c75a034bfae1'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (9410, '9b7d8254-2d89-4787-a2de-21068114b4d5'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (21188, 'e53bd9c2-9844-411f-810d-5e35fa824c01'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (20167, '2bb84049-c72c-4886-948b-e9b8893655fb'), (17609, '4cdc2094-1a54-40b7-9a6e-05ea32af5ee7'), (30924, '44434692-dd31-4fd1-b184-cb00e1c95a5d'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (28896, '202c45a6-2f26-4d2c-acbb-40c10ed7b7b4'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (5933, 'e8977096-c2bb-4f13-9f6e-6bfd1dbeb9bf'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (14659, '55294e81-ae21-447b-8d0e-2daca375922f'), (22855, '88b3dec2-3848-4aea-9a1e-7816ddaf3236'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (23379, '463504f5-df8f-4173-b302-5da02909ff0f'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (31586, '4351b921-b4f2-4a0a-a126-07b9ddb21e7f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (14725, 'f93b2fc4-f0b6-42bb-9b39-5f3d69f3047b'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (1461, '8174cb61-99c6-4cd8-afc1-6ae0b348ac7e'), (31672, '94b34b38-3568-488b-94eb-579ac5ccf43e'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (21447, '8b7f3a13-78d2-4c1a-b32a-51b280baae78'), (21450, 'c185f639-1ee4-4cef-92bb-dc11117910af'), (6091, 'ec43fc42-f705-45a0-8110-0fb4ac955090'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
In combination with the data issues widget, the Python API offers a great way to integrate the results of existing scripts (e.g. data quality checks or model monitoring) into a scalable data inspection workflow.

## Using Spotlight on the Hugging Face hub

You can use Spotlight directly on your local NLP, audio, CV or multimodal dataset. If you would like to showcase your dataset or model results on the Hugging Face hub, you can use Hugging Face spaces to launch a Spotlight visualization for it.

We have already prepared [example spaces](https://huggingface.co/renumics) for many popular NLP, audio and CV datasets on the hub. You can simply duplicate one of these spaces and specify your dataset in the `HF_DATASET` variable.

You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.
```

## Multimodal pipeline

The [`pipeline`] supports more than one modality. For example, a visual question answering (VQA) task combines text and image. Feel free to use any image link you like and a question you want to ask about the image. The image can be a URL or a local path to the image.

For example, if you use this [invoice image](https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png):

```py
>>> from transformers import pipeline

>>> vqa = pipeline(model=""impira/layoutlm-document-qa"")
>>> vqa(
...     image=""https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png"",
...     question=""What is the invoice number?"",
... )
[{'score': 0.42515, 'answer': 'us-001', 'start': 16, 'end': 16}]
Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.

Hugging Face Infinity consists of 2 main services:
* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.
* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.

The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.

<br>
<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Product overview"" src=""assets/46_infinity_cpu_performance/overview.png""></medium-zoom>
  <figcaption>Figure 1. Infinity Overview</figcaption>
</figure>
<br>
### What was the impact of collaborating with the Hugging Face team?
The most important thing about this collaboration was making a tremendous impact on our business's scalability and our operations team's workflow. We launched our production NLP pipeline several weeks ago. Since then, we've consistently seen almost 20% of incoming messages get automatically handled by our new system. These are messages that would've created a ticket for our patient operations team before. So we've reduced a lot of low-value work from our team.
 
Here is the question: How do Hugging Face pipelines simplify multimodal workflow integration?","Hugging Face pipelines simplify multimodal workflow integration by supporting more than one modality, such as text and image, in a single pipeline. This allows for tasks like visual question answering (VQA) to be performed in a single pipeline, making it easier to integrate multimodal workflows."
Which challenge is most significant when adapting pre-trained models to new tasks?,"[(30740, '7f7e5ebd-83dc-4a7d-932f-d4395c72dedf'), (13335, 'c0f8d078-7c4a-4cc3-8b01-d07cd236de8f'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (19489, '43505cd9-a764-4a3f-8290-b2ce01a4ab4b'), (26674, 'f774fe22-eece-4e89-a2d6-d045f3827a9f'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (30781, 'ea211002-7d83-4822-87c9-010da149e4bb'), (25162, '946c9ec3-717f-4307-aad1-37aac459e5a1'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (82, '9b1467ac-93fd-4e92-ae66-0c561a9fe744'), (3155, '04ba28fe-312e-4cda-ad60-4405ce74708b'), (23636, '847215c3-b4a6-4e0a-8064-28fd730a92aa'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (10854, '3cae5d2f-8bf0-43cd-8609-1f7a2de7db58'), (12903, '44dc538d-ecf5-47f4-9862-c6179d76af59'), (12904, 'cc994569-9f3f-42ca-9c42-28f616ebe9bb'), (4205, 'c62c8c1c-0bb7-4ad7-bfce-7cd1161ad7a7'), (21112, 'b2fe447e-14ff-4d20-a842-584b4543fc26'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (13962, 'a7c31e26-ea04-4485-80fd-732f660daa25'), (3213, 'f579844e-8020-4f0c-bd34-e052f2bd02d3'), (15001, '5e52bb45-1656-4310-8d72-06c79196c8f4'), (14005, 'b5d01951-0562-4feb-9801-f08cb589fc9a'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (16573, '5bc9cd26-7570-40b5-ac99-01e496145764'), (16577, '65b23733-c969-4f5c-815e-b4d8c66dc8a9'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (9416, '28859eb3-1961-4028-8c62-23786a3bd60d'), (16587, '52d49bb8-3621-4158-a18e-a558ef51fd74'), (10450, '3d06676d-85ad-4fa3-965c-e9d2ff2a64d7'), (15583, '0bd03221-d721-44d6-b845-89e89513d7b5'), (22754, '80db5ee3-5191-4e4a-b27e-1131d8ba4b43'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (31463, '173a216a-4751-442c-968d-628b2b104933'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (5866, '4108faf4-0e48-4871-8e5b-31983d6c0129'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (5868, '01e5d2e5-9c7d-4064-910f-f3ee8f36afc7'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (5876, '6c565e09-ac75-4392-891f-2a1bbddd3dd9'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (23811, 'c772ca59-a4f0-4b08-aa7d-da263c89ceb4'), (23812, '98499772-c9a4-47d7-b198-bd18ff20d2d5'), (16135, '7571e8a8-5ff4-4358-9a88-0d57b9e70bc8'), (10508, '880f5c7d-e48d-48f5-9334-4850e1489620'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (10510, '65932cde-592e-47aa-b229-26bebb986d68'), (23822, '658e470a-87d9-4fe9-958f-16db281f71f3'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (10520, 'f89c6a86-913f-459e-8824-34dcfc05ae6d'), (29468, '17fc845b-a10d-41de-978b-b91257cc0ada'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (4918, '1e0edbe0-5591-4783-a872-6c2b258e9fd3'), (24893, 'ba24282b-69bc-4162-bc20-24e64126aaec'), (1345, '64645c76-df63-4566-9ea6-fe57d419e168'), (22850, '660de6ac-c1ce-49bd-a434-4ddb5c3001b7'), (24899, 'd5261e5b-d3f0-43d7-aac1-57db8255c358'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (17741, '28a0fb96-9ba3-48c6-a7bc-158aa6526cb7'), (6991, '01e5526c-e621-495d-a431-92b903971367'), (22351, 'de4780f5-753f-4d0a-8175-6e44c6f76bb7'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (25428, 'd391c525-320b-49a3-83dc-e6a90c266db7'), (6997, '35d0066b-d1f2-45f1-a295-d792e0dda90d'), (8021, '2eb3eb4c-195e-4237-b854-70714e8cde11'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (8023, '1dec94c2-ffe1-4e8f-a0e8-d220743cc341'), (22877, '79c5fcb1-22aa-4ce1-be25-33a02d2b8e03'), (1897, '6aba3658-1f10-4052-b9bb-ce2999ebd330'), (1898, 'd0cd0bd7-ad19-4c35-a562-ddc9c9499a18'), (1899, 'f888ee25-af78-4996-8147-194908c2b4b0'), (1900, '9ac81dc8-63e8-4bb5-b1a2-66f999fdf9c9'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (7535, '112382bf-ba59-4a80-bce9-ee6ae2715df4'), (22397, '31d21656-3ce7-4da6-b134-33cbd4345049'), (22398, '96e2d562-285e-4313-9c28-22c485856055'), (22399, '8c31bbea-69a2-4f83-afb3-796762dba928'), (17798, '7ae25397-da2a-4397-9fa8-858e74f29d9b'), (23431, 'dc646a0c-4319-4bd5-9c0d-75cf35041403'), (23432, '16d96e6e-b934-4fd6-a87b-5fdf8025aaf4'), (23433, 'e89537dc-4f68-4b71-ad4c-f7dedd2a6faa'), (24966, '6738697c-4cdb-4c6c-a5a1-2bdd4a31c06e'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (11668, 'ed1ab411-0307-41ec-b3d1-0e9dcedf48d2'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (31138, 'f0d0fdeb-800a-4c19-81fd-bca04cfc7106'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (20907, '789c7a86-81f9-481b-a930-b5e65873ccf1'), (23473, 'f50b7d19-c417-40d3-a82a-2d35ca4cc1bb'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (31161, 'e6392f6d-29c2-4b5e-99d0-0af8c82a0670'), (13759, 'e3f7ce62-98c1-4212-90c1-fc43513f4104'), (453, '88e0bd45-3e3a-40d3-b950-ecce65a9fd14'), (8135, '1a925799-28df-4126-9d1a-9278d8861e75'), (30161, '1346c908-cdd2-4afb-9f14-01eb9af69253'), (981, '34f4864e-d4b4-4f46-a7df-ca0aefbce760'), (11224, 'f888b7bb-7128-4802-8be1-00533c0c5908'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (24550, 'f8edec76-5c02-41b0-a9ee-c83e1d736419'), (25069, 'a400d317-c11c-42d5-bc72-ec9c08296b20'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (25071, '4555fe45-7cc8-438e-a7e1-ee81a5269a19'), (1521, 'f5cd0618-1edf-4c03-95d6-c14c6e10fec5'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (14326, '394d8e4b-4b37-40df-975b-71f00ac59dbc'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: -->

# PEFT configurations and models

The sheer size of today's large pretrained models - which commonly have billions of parameters - present a significant training challenge because they require more storage space and more computational power to crunch all those calculations. You'll need access to powerful GPUs or TPUs to train these large pretrained models which is expensive, not widely accessible to everyone, not environmentally friendly, and not very practical. PEFT methods address many of these challenges. There are several types of PEFT methods (soft prompting, matrix decomposition, adapters), but they all focus on the same thing, reduce the number of trainable parameters. This makes it more accessible to train and store large models on consumer hardware.
* Standard pre-trained models can be adapted to long-range inputs by simply replacing the standard self-attention with the long-range self-attention proposed in this paper and then fine-tuning on the downstream task. This avoids costly pre-training specific to long-range inputs.
1. The biggest challenges come with the operationalization and deployment of ML-trained solutions in a manner in which human operations can be replaced with minimal consequences.  We’re seeing it now with fully self-driving automobiles.   It’s challenging to automate processes with little to no fear of jeopardizing humans or processes that humans rely on.  One of the most significant examples of this phenomenon that concerns me is ML and Bias.  It is a reality that ML models trained on data containing, even if unaware, prejudiced decision-making can reproduce said bias in operation.  Bias needs to be put front and center in the attempt to incorporate ML into engineering such that systemic racism isn’t propagated into future technological advances to then cause harm to disadvantaged populations.  ML systems trained on data emanating from biased processes are doomed to repeat them, mainly if those training the ML solutions aren’t acutely aware of all forms of data present in the process
Adapt a model to a new task

Many diffusion systems share the same components, allowing you to adapt a pretrained model for one task to an entirely different task.

This guide will show you how to adapt a pretrained text-to-image model for inpainting by initializing and modifying the architecture of a pretrained [`UNet2DConditionModel`].

## Configure UNet2DConditionModel parameters

A [`UNet2DConditionModel`] by default accepts 4 channels in the [input sample](https://huggingface.co/docs/diffusers/v0.16.0/en/api/models#diffusers.UNet2DConditionModel.in_channels). For example, load a pretrained text-to-image model like [`runwayml/stable-diffusion-v1-5`](https://huggingface.co/runwayml/stable-diffusion-v1-5) and take a look at the number of `in_channels`:

```py
from diffusers import StableDiffusionPipeline

pipeline = StableDiffusionPipeline.from_pretrained(""runwayml/stable-diffusion-v1-5"", use_safetensors=True)
pipeline.unet.config[""in_channels""]
4
The capability of pre-trained language models to effectively transfer
*task-agnostic* knowledge to *task-specific* knowledge turned out to be
a great catalyst for NLU. Whereas engineers and researchers previously
had to train a language model from scratch, now publicly available
checkpoints of large pre-trained language models can be fine-tuned at a
fraction of the cost and time. This can save millions in industry and
allows for faster prototyping and better benchmarks in research.

Pre-trained language models have established a new level of performance
on NLU tasks and more and more research has been built upon leveraging
such pre-trained language models for improved NLU systems. However,
standalone BERT and GPT models have been less successful for
*sequence-to-sequence* tasks, *e.g.* *text-summarization*, *machine
translation*, *sentence-rephrasing*, etc.
 
Here is the question: Which challenge is most significant when adapting pre-trained models to new tasks?","According to the context, the most significant challenge when adapting pre-trained models to new tasks is not mentioned explicitly. However, it can be inferred that the challenges mentioned in the context, such as the operationalization and deployment of ML-trained solutions, ML and Bias, and the need for powerful GPUs or TPUs to train large pre-trained models, are significant challenges in general. But they are not specifically mentioned as the most significant challenge when adapting pre-trained models to new tasks."
How can multimodal Transformer models be evaluated for complex workflows?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (27229, '79b37341-3681-4528-9ab1-2ab8778b564d'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (18549, 'd84bf0de-2759-4c1e-9a94-cc27a2136a9b'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (13526, '594c0819-9996-42bd-8329-7cec367352a5'), (20185, '26583e89-c8f7-4107-a950-11734ddb8fbf'), (18140, '2b5de1ef-6517-4c54-b9b2-3d94550ca7a5'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (13558, '415ec308-3b10-4657-a9b2-e4f3aa21d028'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (256, '154c2da9-4985-48e3-86b8-642c818c758e'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (29487, '2321da63-59f7-4fbb-b56d-00810837d618'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (9109, '46c1eb91-a107-4d91-86e5-205812209154'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (9120, '6e6140c9-95c6-4b7e-8af0-9dfe53916a31'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (2514, '1aaf99d7-9574-4a42-9441-f4b7e0b43e08'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (18403, '89fb1534-a456-4992-b162-b64ff48680bb'), (5100, '0e2cf3f9-d5c7-4a37-9665-321547a73704'), (18414, '4bfd7618-6567-411a-9d84-140a5555b25d'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (18429, '4f938c80-c628-4d41-b604-430b07615100'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
- Abstract: Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. So, we now need a well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. I'll talk about a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically
a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically evaluate ten well-established long-range Transformer models (Reformers, Linformers, Linear Transformers, Sinkhorn Transformers, Performers, Synthesizers, Sparse Transformers, and Longformers) on LRA. LRA paves the way towards better understanding this class of efficient Transformer models, facilitates more research in this direction, and presents new challenging tasks to tackle.
promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.*
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
 
Here is the question: How can multimodal Transformer models be evaluated for complex workflows?","Multimodal Transformer models can be evaluated for complex workflows using a systematic and unified benchmark like LRA (Long-Range Arena), which consists of a suite of tasks with sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning."
"What is the primary advantage of Hugging Face’s integration across JAX, PyTorch, and TensorFlow?","[(17416, '2c9eca5a-a304-4ced-8b8f-1c79013a2105'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (17425, 'a6c4b1bc-1980-4b9d-826a-871d7430c2cc'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (17439, '220bb0f1-e5a5-4c21-ba59-845839d83ad3'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (5155, '86625bb7-792a-426a-9ade-ea98317da2e0'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (30761, '683724e2-567a-46f0-9254-202ca2f88a96'), (29236, 'c8256ce9-e984-4533-a82f-3f08a2d65e83'), (24634, '70652141-34ee-406f-816b-e497d503fd8d'), (22593, '1d7e4289-69b0-474f-a80c-712bb400aca9'), (3659, '26897d07-2459-46e9-bf92-6a9bbbe307d2'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (21076, 'f7b2eb77-c2fc-4c18-926b-38ac4c905bda'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (2656, '6fe755e9-108c-4612-a153-9b71540ab36e'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (25702, 'c8a294a5-698c-4f59-a9f4-832868f1d619'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (9840, 'b35e7b1d-c3d5-4e40-80c7-d9b3b8c7f3fe'), (31354, '50289d67-f345-42d3-bb11-7b4a1e8c5530'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21138, 'c2c5b354-94af-4fd1-bbc5-65ca22e909cb'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (21146, '52b3b9bb-9ff0-4cbb-85ea-66902b53e6a6'), (668, '249a03e0-c503-4df7-824c-7f0c50892b55'), (1181, '2e574621-b2e7-4ffb-ad69-7aa65d1e16fb'), (21151, 'c1f9f41a-2ae3-4bdd-b40e-a54327d158f4'), (1191, 'ece7abdd-1376-495b-8daa-d664f2a27dae'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (30891, 'ce0c26e5-9ecb-4d1c-ac24-560c5974dda4'), (25786, 'a3f4a2db-9125-4e13-89f2-07a0399eaa1d'), (20667, 'b0b180ee-2478-48b8-9446-bf6a9feabb3b'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (18631, '87868ed5-84c3-422f-a5d4-d23a4b6685d6'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (25302, 'bcf9a9f1-7a6b-4a62-87f7-68506603417d'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (17116, 'df0e2cb6-a368-4e77-894f-ce91ac16ed6c'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (7390, 'd82b7e48-5494-469a-b93b-42130344ea25'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (17123, 'b278f0df-995f-4b18-a7f0-c467054354da'), (22756, '583a445a-eaf6-4e7a-8f28-8707a7e1cbf7'), (24808, '6ca62949-7a64-4634-8bba-b84c7fc2521a'), (8429, '45f5650b-4b13-471b-ac60-251d69059476'), (22770, '978ad2c3-695e-4a95-82c8-b6fb9af51394'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (8971, '5a35d107-5b71-4521-8856-416fb762b753'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (28429, '9c6fb59e-1d9f-48bd-be33-bd99c86d369d'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (8469, '0ae3273f-512a-4f10-a11c-633fe9e45189'), (21785, '0f42515e-4921-41d6-a8f2-18c62768982e'), (6437, '84957027-ff15-4ac9-9366-7a35676d6fd0'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (26919, '5ebb5465-6f4c-4e1c-8326-5f71516e66a4'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (8492, '37b69803-2f18-4f43-850e-e3c81aa1cb27'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (21813, 'f41892f3-473d-4a2e-ac76-5f8c7d4613ef'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (25456, 'ac2479c5-2682-4db2-a27e-fdf59164519f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (25459, '7e9e55e8-8027-4677-b297-11f6cebd0a17'), (3957, '86a50945-a243-44e5-be1f-c80fe6f354bc'), (21366, '3f2fa721-852c-4545-b287-9c8782de1be2'), (21365, '9556f219-576e-4d37-8eb8-c37f3fb358f5'), (13176, 'a57502e1-52a5-41dc-aebb-b7cb7f00763d'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (22394, 'b66b7946-772b-43b6-8e7d-b5388eaf7cf6'), (25476, '3fcbb85d-17ea-4cf1-b253-7994b353a67e'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (20871, 'edfcf78d-61e7-443d-8514-6db1ea1b0bca'), (13196, '5f4d0098-0f25-4cd7-9d59-4037a77c492b'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (17820, 'f5b85b58-0df6-4cf9-9250-5afcef460492'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (17828, 'e0e24538-49e6-418a-8fb8-b10e6a8d00f1'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (22453, '4bad26aa-2ebe-4fb7-80b0-702de459d53c'), (31672, '94b34b38-3568-488b-94eb-579ac5ccf43e'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (6082, 'c6145d46-49ea-43dd-88bd-7d6c7daf3380'), (28614, '85bff174-da9e-4929-bdc0-904ec1a4bc99'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (9163, 'd7206e44-29b4-499b-92c4-0008be204752'), (6091, 'ec43fc42-f705-45a0-8110-0fb4ac955090'), (6093, '5d63e976-ff26-4315-b418-58387364c07e'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (31728, '5ccf1245-717b-44e0-837a-e164da6b012a'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: --
title: ""Hugging Face's TensorFlow Philosophy""
thumbnail: /blog/assets/96_tensorflow_philosophy/thumbnail.png
authors:
- user: rocketknight1
---

# Hugging Face's TensorFlow Philosophy



### Introduction


Despite increasing competition from PyTorch and JAX, TensorFlow remains [the most-used deep learning framework](https://twitter.com/fchollet/status/1478404084881190912?lang=en). It also differs from those other two libraries in some very important ways. In particular, it’s quite tightly integrated with its high-level API `Keras`, and its data loading library `tf.data`.
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples 📄
#### Suraj Patil & Patrick von Platen, Machine Learning Engineers at Hugging Face
- Talk: How to use JAX/Flax with Transformers
- Abstract: Transformers is one of the most popular open-source ML libraries and supports PyTorch, Tensorflow, and JAX/Flax. In this talk, we will explain how JAX/Flax models should be used in Transformers and compare their design in Transformers with the design of PyTorch models in Transformers. In the second part, we will give you a hands-on presentation of how a model can be trained end-to-end with the official JAX/Flax example scripts using Transformers & Datasets. Along the way, we want to give you some tips and tricks on how to best realize your project.
- Speaker info: Suraj and Patrick are part of Hugging Face’s open source team and lead the integration of JAX/Flax into Transformers.
- GitHub: https://github.com/patil-suraj & https://github.com/patrickvonplaten
## Accelerating Machine Learning from Science to Production

In addition to Hugging Face DLCs, we created a first-class Hugging Face extension to the SageMaker Python-sdk to accelerate data science teams, reducing the time required to set up and run experiments from days to minutes.

You can use the Hugging Face DLCs with the Automatic Model Tuning capability of Amazon SageMaker, in order to automatically optimize your training hyperparameters and quickly increase the accuracy of your models.

Thanks to the SageMaker Studio web-based Integrated Development Environment (IDE), you can easily track and compare your experiments and your training artifacts.

## Built-in Performance

With the Hugging Face DLCs, SageMaker customers will benefit from built-in performance optimizations for PyTorch or TensorFlow, to train NLP models faster, and with the flexibility to choose the training infrastructure with the best price/performance ratio for your workload.
To learn how to access and use the new Hugging Face DLCs with the Amazon SageMaker Python SDK, check out the guides and resources below.

> _On July 8th, 2021 we extended the Amazon SageMaker integration to add easy deployment and inference of Transformers models. If you want to learn how you can [deploy Hugging Face models easily with Amazon SageMaker](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker) take a look at the [new blog post](https://huggingface.co/blog/deploy-hugging-face-models-easily-with-amazon-sagemaker) and the [documentation](https://huggingface.co/docs/sagemaker/inference)._

---

## **Features & Benefits 🔥**

## One Command is All you Need

With the new Hugging Face Deep Learning Containers available in Amazon SageMaker, training cutting-edge Transformers-based NLP models has never been simpler. There are variants specially optimized for TensorFlow and PyTorch, for single-GPU, single-node multi-GPU and multi-node clusters.
 
Here is the question: What is the primary advantage of Hugging Face’s integration across JAX, PyTorch, and TensorFlow?","The primary advantage of Hugging Face's integration across JAX, PyTorch, and TensorFlow is not explicitly stated in the context. However, based on the information provided, it can be inferred that the integration allows for flexibility in choosing the training infrastructure with the best price/performance ratio for the workload, and provides built-in performance optimizations for each framework, enabling faster training of NLP models."
What makes Hugging Face pipelines critical for multimodal experimentation?,"[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (1046, '2efa8a05-4ce9-4c09-bfd5-107d39229dee'), (27672, '59487233-b752-42ea-aa5a-8cb853def35f'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (14875, '863f500b-8805-487f-a9d0-1a5268b9eb36'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (17609, '4cdc2094-1a54-40b7-9a6e-05ea32af5ee7'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (9952, '7b5b02be-c8e3-4b66-9ea2-d73cf414e65f'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (21804, '9912e115-c51c-4885-ad29-5a67bcb2572b'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (16716, 'd9f215fb-7aa2-4b50-a40d-d2e690dc5d03'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7060, '395b844f-bf7d-4cd0-a300-57c24b767beb'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (27040, '56f391b1-2a20-4be2-af9f-8f26721aaaf2'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (6047, '162b5640-f06a-4c27-97b1-d3c317bbb669'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (21447, '8b7f3a13-78d2-4c1a-b32a-51b280baae78'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (6091, 'ec43fc42-f705-45a0-8110-0fb4ac955090'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.

Hugging Face Infinity consists of 2 main services:
* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.
* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.

The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.

<br>
<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Product overview"" src=""assets/46_infinity_cpu_performance/overview.png""></medium-zoom>
  <figcaption>Figure 1. Infinity Overview</figcaption>
</figure>
<br>
Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!

### Finding models

Over 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.


### Widgets
An Infinity Container is designed to serve 1 Model and 1 Task. A Task corresponds to machine learning tasks as defined in the [Transformers Pipelines documentation](https://huggingface.co/docs/transformers/master/en/main_classes/pipelines). As of the writing of this blog post, supported tasks include feature extraction/document embedding, ranking, sequence classification, and token classification.

You can find more information about Hugging Face Infinity at [hf.co/infinity](https://huggingface.co/infinity), and if you are interested in testing it for yourself, you can sign up for a free trial at [hf.co/infinity-trial](https://huggingface.co/infinity-trial).

---

## Benchmark
## Why does this matter?

As we transition to this new AI paradigm, organizations will need to use their extremely valuable data to augment their AI models if they want to get the best performance within their specific domain. This will almost certainly require work in the form of data transformations, and doing this efficiently over large datasets is something Spark was designed to do. Integrating Spark with Hugging Face gives you the cost-effectiveness and performance of Spark while retaining the pipeline integration that Hugging Face provides.

## Continued Open-Source Support

We see this release as a new avenue to further contribute to the open source community, something that we believe Hugging Face does extremely well, as it has become the de facto repository for open source models and datasets. This is only the first of many contributions. We already have plans to add streaming support through Spark to make the dataset loading even faster.
 
Here is the question: What makes Hugging Face pipelines critical for multimodal experimentation?","The context does not explicitly state what makes Hugging Face pipelines critical for multimodal experimentation. The provided information discusses Hugging Face Infinity, a containerized solution for deploying end-to-end optimized inference pipelines for State-of-the-Art Transformer models, and its features, but it does not mention multimodal experimentation. Therefore, it is not possible to provide a concise and relevant answer to this question based on the provided context."
How do cross-framework APIs improve deployment flexibility for Transformer models?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (17412, '1d9b136f-d20c-4989-8975-80625fa8da45'), (24074, 'c7902235-64ce-4d97-b746-5682c76f72ec'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (16406, '57321316-4910-41b2-9525-559cf4584035'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (24631, '1cafa663-09f0-4b92-9f19-efd956944da3'), (24636, '34c928bf-700f-4de6-b158-b590ff679457'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (19525, '349b2448-2fee-4f56-89b2-5e7215220379'), (18511, '4f3566db-bced-43d1-ad6b-57ce9577de94'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (18549, 'd84bf0de-2759-4c1e-9a94-cc27a2136a9b'), (18550, '01e230dd-8782-4f9f-82af-2058aa9bd4d9'), (18571, 'e150c6a6-1333-4175-944b-28873073d9c6'), (12948, 'de99c19f-cfde-4076-a099-a2a5d6202e05'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (1195, '3e7433ba-fae2-4e13-8802-9ff9da0b8900'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (24752, '3798bf75-5a2c-4b7a-9800-c75a034bfae1'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (3769, '4d65ee65-320a-476b-a100-ef50c8da088d'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (23229, '0db85d3b-56c0-4fd4-a3fa-065cd9c9d588'), (20160, 'c0c6fd6f-69be-4907-9b87-052041804e6c'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (23748, '0ab304e4-ab01-434f-9b21-63bc07214a45'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (214, 'ced565fd-8f3c-4e7b-a0f9-9e46e7ed2922'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (26330, '7fa828c6-3c68-4d92-bd55-6a6e829e8069'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (6889, 'f9703e3f-789b-4d40-bb13-0dcb8de3863a'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (25883, 'f01a6bd7-35e0-4d59-b1e1-8879b0662972'), (6943, 'ad8c7727-69f8-472d-b29c-cffc054a6777'), (2851, '3ab3417f-d50a-4451-b44e-2d63e95b777a'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (23847, '25175e5a-8fc3-4050-87fd-bf640d60de2f'), (5933, 'e8977096-c2bb-4f13-9f6e-6bfd1dbeb9bf'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (28980, '2b0c07ed-129d-48ce-a54a-c5be8281aba6'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (18759, 'a4f9107f-b29d-42b9-adf1-a7d1b294c213'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (27498, 'b9af69b0-e49d-44fb-94c1-97f3170bdd4a'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14703, '6849a5b6-70e0-438d-9f31-71b6929312c2'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (28540, '92fc9238-353a-40ac-b3a0-fe4442102873'), (25980, '44d9eb20-ac45-46e9-b4ea-310683db7670'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (14725, 'f93b2fc4-f0b6-42bb-9b39-5f3d69f3047b'), (8585, 'b1c8ad3c-6d9c-4e46-9b2c-fb032a27a9f9'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (20887, 'e4c88d19-b6b7-484a-b21a-646c0f2984c3'), (31130, 'cfc1501b-bd1d-4469-943f-2a858e32ae23'), (15772, '65169dba-99db-485c-9bff-b711a9a78463'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (5534, '3e4707b0-83e3-4137-b771-207785927e14'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (16311, 'ff1c1d88-bc88-43e0-9d16-50398857fbc0'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (11209, '3b5ff375-2e1c-4a92-8077-43f3d30f62ae'), (16843, '926901bf-b2bd-4869-8ac4-10833c3d0dd1'), (6096, 'b1db7fca-c845-4de6-a30d-f88d2ad8689d'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (15835, '888ea0a4-ad95-4449-9478-48ba7798a983'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Quickstart

Since its 1.13 version, [PyTorch released](https://pytorch.org/blog/PyTorch-1.13-release/) the stable version of a fast path for its standard Transformer APIs that provides out of the box performance improvements for transformer-based models. You can benefit from interesting speedup on most consumer-type devices, including CPUs, older and newer versions of NIVIDIA GPUs.
You can now use this feature in 🤗 Optimum together with Transformers and use it for major models in the Hugging Face ecosystem.
Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
he Trainer API. The Transformers library provides a Trainer API that allows you to easily fine-tune transformer models on your own dataset. The Trainer class take your datasets, your model as well as the training hyperparameters and can perform the training on any kind of setup (CPU, GPU, multi GPUs, TPUs). It can also compute the predictions on any dataset, and if you provided metrics, evaluate your model on any dataset. It can also handle final data-processing such as dynamic padding as long as you provide the tokenizer or a given data collator. We will try this API on the MRPC dataset, since it's relatively small and easy to preprocess. As we saw in the Datasets overview video, here is how we can preprocess it. We do not apply padding during the preprocessing as we will use dynamic padding with our DataCollatorWithPadding. Note that we don't do the final steps of renaming/removing columns or set the format to torch tensors: the Trainer will do all of this automatically for us by
By exposing a graph with standardized operators and data types, ONNX makes it easy to
switch between frameworks. For example, a model trained in PyTorch can be exported to
ONNX format and then imported in TensorFlow (and vice versa).

Once exported to ONNX format, a model can be:
- optimized for inference via techniques such as [graph optimization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization) and [quantization](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization). 
- run with ONNX Runtime via [`ORTModelForXXX` classes](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort),
which follow the same `AutoModel` API as the one you are used to in 🤗 Transformers.
- run with [optimized inference pipelines](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines),
which has the same API as the [`pipeline`] function in 🤗 Transformers.
--
title: ""How we sped up transformer inference 100x for 🤗 API customers""
thumbnail: /blog/assets/09_accelerated_inference/thumbnail.png
---

# How we sped up transformer inference 100x for 🤗 API customers


🤗 Transformers has become the default library for data scientists all around the world to explore state of the art NLP models and build new NLP features. With over 5,000 pre-trained and fine-tuned models available, in over 250 languages, it is a rich playground, easily accessible whichever framework you are working in.

While experimenting with models in 🤗 Transformers is easy, deploying these large models into production with maximum performance, and managing them into an architecture that scales with usage is a **hard engineering challenge** for any Machine Learning Engineer.
 
Here is the question: How do cross-framework APIs improve deployment flexibility for Transformer models?","Cross-framework APIs improve deployment flexibility for Transformer models by allowing models to be trained in one framework (e.g. PyTorch) and loaded for inference in another (e.g. TensorFlow), making it easy to switch between frameworks. This is made possible by exposing a graph with standardized operators and data types through ONNX, which enables models to be exported and imported across frameworks. This flexibility allows for optimized inference via techniques such as graph optimization and quantization, and enables models to be run with ONNX"
What is the role of Hugging Face’s prebuilt models in simplifying multimodal applications?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (21012, '4252b2f2-3c37-45fc-8476-73ec6c4c1ee7'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10841, '6e53fcd9-7666-4769-ba61-2d6fab8ac52c'), (17509, '56ea90b0-f162-4bf5-8c05-654f3a183cd9'), (2663, 'efa7e63d-051b-4d0d-b130-483c5160dd59'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (20609, 'c19b3701-219d-48a8-ae4b-d6b0ad0e3b8c'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (20636, '3a97d7e3-78ab-4911-9bbf-01b10ea9777a'), (1186, '50598bfb-1eff-44c7-a14f-8886c016a045'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (30886, '54b36145-f8cd-4fc5-98d8-547220fe27b3'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (5825, '02b1a8f8-63ab-44e9-a55e-de99cee8b6dc'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (4804, '545adb55-d07e-4a08-b179-72243735bedb'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (3800, '82c0086d-96e5-4b26-a430-a265a26fdb8b'), (18136, 'ea71868e-38ba-42e9-9d5c-3ac4745b0a68'), (22752, '847c0672-9e44-445e-b14d-84817ad403ee'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (13554, 'f239a4cb-1368-4161-bb6c-d4d76b74b26d'), (1779, '920a7d81-f957-49ad-81e8-9fa4da67693a'), (14578, '326d9ca6-0615-4887-8510-011fa0f76ecf'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (14579, '8fb1461a-7247-4d33-ae8d-50335ec86543'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (252, '1d1458f5-0779-4f41-8eab-579cf8943202'), (22783, '898e6630-6e42-41f4-bf5e-7ad200e3339e'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (18184, 'a53114c0-96cb-42ef-a44e-4a6ea9778ff1'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (13601, 'd2fcf63a-6647-41a1-8530-613f53b57a88'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (18217, '4e0278ab-2929-47df-8ac5-54be5d98a007'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (299, '7f2a2f1a-f503-44f9-b87a-583789044a1f'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (13634, '63e875a9-4fad-4f80-8454-e567ece5a4ac'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (333, 'b89f73b6-20ff-473e-9c00-802ac258ce6c'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (9072, '7a516a26-4603-45f9-b69a-f48eb9d9c67e'), (16246, '3c77c431-7979-48b7-b494-42e7a252cfbb'), (22395, '76235143-e5e3-4d4b-968f-8198aaa84e79'), (16252, 'ebc3b30d-233d-43e4-b012-9f5a958063fc'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (20876, 'a784cd91-18de-4adf-a288-2724d2b961e1'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (9103, '79a8442d-c532-45de-a08c-4f82c8821d64'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (10143, 'fb24eb30-9a50-4175-84a3-36d6e8aa5ee7'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (18366, '8a7e2dee-f14d-4ea1-819d-851a6affb887'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (5063, 'f4cfdc74-4cbc-4213-a3fb-a400b432dc40'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (9174, '3767ffbd-cac9-45c5-b684-68ec257368f6'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (18397, 'fba334a1-3931-428a-ace2-8ab61c5a23c8'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (21982, '85733d75-1b4a-47c3-b481-549f15ffb07f'), (11235, '8558ce8a-5be5-4c58-91a4-e034fd8f061d'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (5094, 'ff050631-51bc-4cbe-8c50-0fcdc596f571'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (5111, 'e87c80c3-d7d3-4478-88c3-e8dc5a994da2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: **Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
Hugging Face helps enable this powerful development process by making more than 150,000 open-source models immediately available from a single source. Many of those models are specialized on domain-specific data, like the BioBERT and SciBERT models used to demonstrate [how ML can be used to spot adverse drug events](https://snorkel.ai/adverse-drug-events-how-to-spot-them-with-machine-learning/). One – or better yet, [multiple](https://snorkel.ai/combining-foundation-models-with-weak-supervision/) – specialized base models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment.

## How does Hugging Face help?
We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
- We strongly limited the number of user-facing abstractions to learn, in fact, there are almost no abstractions,
    just three standard classes required to use each model: [configuration](main_classes/configuration),
    [models](main_classes/model), and a preprocessing class ([tokenizer](main_classes/tokenizer) for NLP, [image processor](main_classes/image_processor) for vision, [feature extractor](main_classes/feature_extractor) for audio, and [processor](main_classes/processors) for multimodal inputs).
  - All of these classes can be initialized in a simple and unified way from pretrained instances by using a common
    `from_pretrained()` method which downloads (if needed), caches and
    loads the related class instance and associated data (configurations' hyperparameters, tokenizers' vocabulary,
    and models' weights) from a pretrained checkpoint provided on [Hugging Face Hub](https://huggingface.co/models) or your own saved checkpoint.
Clement Delangue, co-founder and CEO, Hugging Face

## Conclusion

Together, Snorkel and Hugging Face make it easier than ever for large companies, government agencies, and AI innovators to get value from foundation models. The ability to use Hugging Face’s comprehensive hub of foundation models means that users can pick the models that best align with their business needs without having to invest in the resources required to train them. This integration is a significant step forward in making foundation models more accessible to enterprises around the world.

_If you’re interested in Hugging Face Inference Endpoints for your company, please contact us [here](https://huggingface.co/inference-endpoints/enterprise) - our team will contact you to discuss your requirements!_
 
Here is the question: What is the role of Hugging Face’s prebuilt models in simplifying multimodal applications?","According to the context, Hugging Face's prebuilt models play a role in simplifying multimodal applications by providing a comprehensive hub of foundation models that users can pick from, without having to invest in the resources required to train them. These prebuilt models can give users a jump-start on initial predictions, prompts for improving labels, or fine-tuning a final model for deployment, making it easier to develop multimodal applications."
What strategy ensures smooth adaptation of Transformer models to new languages?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (5156, '3d5eb54f-ac2b-4444-987d-f6c1cd8201a3'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (5202, 'ffdc2e4f-ce95-48dd-8a87-2c908bbecdcf'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (30822, '1357395c-e339-49a4-9470-5057472025be'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (18550, '01e230dd-8782-4f9f-82af-2058aa9bd4d9'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1192, '48d5754e-ee39-4825-aa44-fc362b042ee3'), (30892, 'c7c7e937-8715-4300-9fb3-896a9568ab94'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (694, 'd3184d8d-7c16-43e6-9fb4-290933e9ad75'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (4796, '8f601501-25a6-4972-aa7a-aa9e072baede'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (30922, '150e7f7e-438f-472c-b0d2-d43c51ed72c0'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (17620, 'd7bec209-56a8-465d-93e3-d0e8a694edef'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (11494, '8db96499-6302-45aa-bca4-bce4bec72dee'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (6890, '08a9df1b-781c-46bb-8809-3659248867d7'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1773, 'e07fbbd8-16f9-4b35-bbb3-b1b9950618b0'), (7919, '14a01558-963d-44f8-a1fc-7a903095967b'), (18161, '661008d6-f852-47ee-a976-37ed2665fd5e'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (13578, 'cb671a15-7afc-42fd-8983-3f6a6aa4e48e'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (277, '91e3b387-1104-48c6-98d8-5d391b18159f'), (15132, '8dd8343f-285b-436f-84a7-620e9144edce'), (16681, 'fc9a2a86-bdfe-4b9d-8e65-72359a9c1b04'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (13642, '092cdf28-d7d0-42cf-9aef-cd07972bc95c'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (6989, '03087320-4dbe-4ba4-8b78-3427b35f3489'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (8017, 'f56d8d5f-9293-4c6b-8217-c1fb077fbe01'), (8020, '4658bd1d-e022-460e-9d6d-af439960931b'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (7000, '8d065847-3189-4122-97bf-d861398ddd06'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (7002, 'f6c1eb0a-744c-4cac-9ea7-db3de3ac11d6'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7006, 'ca729c38-08a8-487f-85b8-173e988c8e43'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (9097, '4aa5ab2b-ea90-4dbc-8bca-d4ab9ed22c7e'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16268, 'aebdd376-5473-4a4b-a7a2-e9a309470f69'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (20884, 'f94ec739-e144-4f11-9a61-c704eb112ba1'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (10137, 'ce88e225-95d2-48d7-9020-7d8644986a94'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (23477, '9318ef30-7329-4410-bb07-65dba4f0a1f2'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (31161, 'e6392f6d-29c2-4b5e-99d0-0af8c82a0670'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (6614, '8adaa619-97b2-446b-bc9d-86d585ad4d68'), (18391, 'f9a3fbd7-5a63-4f5d-a344-2656a7553d4d'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (5088, 'f6c7fccc-2d9b-4fb8-abb5-8bacb17044c0'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (21476, 'fc40dcb8-cfe2-4449-a2ff-c6b8962d8db5'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (18429, '4f938c80-c628-4d41-b604-430b07615100')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Up until now, we've mostly been using pretrained models and fine-tuning them for new use cases by reusing the weights from pretraining. As we saw in [Chapter 1](/course/chapter1), this is commonly referred to as _transfer learning_, and it's a very successful strategy for applying Transformer models to most real-world use cases where labeled data is sparse. In this chapter, we'll take a different approach and train a completely new model from scratch. This is a good approach to take if you have a lot of data and it is very different from the pretraining data used for the available models. However, it also requires considerably more compute resources to pretrain a language model than just to fine-tune an existing one. Examples where it can make sense to train a new model include for datasets consisting of musical notes, molecular sequences such as DNA, or programming languages. The latter have recently gained traction thanks to tools such as TabNine and GitHub's Copilot, powered by
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
<p>In 2017 a group of Google AI researchers published a paper introducing the transformer model architecture. Characterised by a novel self-attention mechanism, transformers were proposed as a new and efficient group of models for language applications. Indeed, in the last five years, transformers have seen explosive popularity and are now accepted as the de facto standard for natural language processing (NLP).</p>
<p>Transformers for language are perhaps most notably represented by the rapidly evolving GPT and BERT model families. Both can run easily and efficiently on Graphcore IPUs as part of the growing <a href=""/posts/getting-started-with-hugging-face-transformers-for-ipus-with-optimum"" rel=""noopener"" target=""_blank"">Hugging Face Optimum Graphcore library</a>).</p>
## QLoRA paper, a new way of democratizing quantized large transformer models

In few words, QLoRA reduces the memory usage of LLM finetuning without performance tradeoffs compared to standard 16-bit model finetuning. This method enables 33B model finetuning on a single 24GB GPU and 65B model finetuning on a single 46GB GPU.

More specifically, QLoRA uses 4-bit quantization to compress a pretrained language model. The LM parameters are then frozen and a relatively small number of trainable parameters are added to the model in the form of Low-Rank Adapters. During finetuning, QLoRA backpropagates gradients through the frozen 4-bit quantized pretrained language model into the Low-Rank Adapters. The LoRA layers are the only parameters being updated during training. Read more about LoRA in the [original LoRA paper](https://arxiv.org/abs/2106.09685).
 
Here is the question: What strategy ensures smooth adaptation of Transformer models to new languages?","The strategy that ensures smooth adaptation of Transformer models to new languages is transfer learning, which involves fine-tuning a pre-trained model on a new dataset. This approach is successful when applying Transformer models to most real-world use cases where labeled data is sparse."
How can Transformer models be trained effectively for multimodal applications?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (10840, '17512ae9-2273-4239-be81-2a79343a68e4'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (30392, '46096e11-0c19-408f-be8e-bf3b9aa0c761'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (9415, '78661659-8f11-4005-800b-e1d23d747852'), (9416, '28859eb3-1961-4028-8c62-23786a3bd60d'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (5324, '5eab4adf-5266-49ae-85d2-9bc2ee7519e3'), (6355, '9cf0e1e3-ca85-4c73-afc6-a71c99755175'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (4861, '79210cef-fada-44fb-a563-86b3d8b879d2'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (5398, '74d7c7d8-f6a0-48e9-b160-c391ac9a70ba'), (25884, '585808ef-bcbd-4bda-9dc1-376342a66a99'), (16674, 'a05d2b01-8fd3-48e1-8077-85f9ef33864d'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (18222, '5880432c-3d6d-4287-b3fe-745b7ed4b67f'), (23859, 'e47fd052-4097-479b-95f3-f65bc65a3c2c'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (4414, '0070b17d-3e37-4b4d-b0e4-6d764598d46e'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (17741, '28a0fb96-9ba3-48c6-a7bc-158aa6526cb7'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (338, '83c0ff2b-59c2-4ffc-bf85-93ed5abbf667'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (7002, 'f6c1eb0a-744c-4cac-9ea7-db3de3ac11d6'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (11115, 'a4f77954-5dd3-4d2c-a0a8-d2241981c438'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (22918, '201f6149-0a67-49ea-81bd-e07891bdb9c1'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (27636, '586ca969-ca44-4a60-97ef-9aadde4772b9'), (27635, 'b26adb5f-a6fa-44e8-8763-0e70df9a1d17'), (18429, '4f938c80-c628-4d41-b604-430b07615100'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Use this document as your starting point to navigate further to the methods that match your scenario.

## Training

Training large transformer models efficiently requires an accelerator such as a GPU or TPU. The most common case is where 
you have a single GPU. The methods that you can apply to improve training efficiency on a single GPU extend to other setups 
such as multiple GPU. However, there are also techniques that are specific to multi-GPU or CPU training. We cover them in 
separate sections.
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
*Recent work in language modeling demonstrates that training large transformer models advances the state of the art in
Natural Language Processing applications. However, very large models can be quite difficult to train due to memory
constraints. In this work, we present our techniques for training very large transformer models and implement a simple,
efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our
approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model
parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We
illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain
15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline
... )
[{'summary_text': ' The Transformer is the first sequence transduction model based entirely on attention . It replaces the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention . For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers .'}]
 
Here is the question: How can Transformer models be trained effectively for multimodal applications?","According to the context, Transformer models can be trained effectively for multimodal applications by using an accelerator such as a GPU or TPU, and applying techniques that improve training efficiency, such as intra-layer model parallel approach, which enables training transformer models with billions of parameters. Additionally, using multimodal Transformers that combine text and visual information together can achieve high accuracy, up to 95% in Document AI use cases."
What is the most efficient way to overcome deployment challenges for cross-framework models?,"[(17934, 'a346e6f1-8dc6-4546-85c3-dac1062bf6ff'), (13330, 'dc1de1b5-d221-482f-a382-6bb26974e488'), (29209, '719f0cf7-5b62-4d2f-9ffe-2886b383ee60'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (38, '7f1448df-4b4e-4cb0-b824-2176150e3e5b'), (9258, '3cc190db-1419-4890-96c6-6326b0493091'), (20041, '0521172d-93c3-47ab-8eb4-57b9a744de8f'), (3660, '88c885c0-729f-40d9-93d3-ff9016d1a1b1'), (18514, '2b8c705b-aa16-4343-9147-042e810a70a2'), (7766, 'bfd8ebde-f1d4-411d-8a5a-a9aef0f72162'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (7768, '224a5adb-2c8c-434b-b5b5-b6fab137f1b8'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (25178, '9c7f8ead-1eb7-4e8d-a89f-fb64c773bf7a'), (7774, '5824b2b4-f100-4de8-a61c-bf6e28cb0bf5'), (14436, 'b6c8ba09-18f8-4b76-9196-bdead257e416'), (20069, 'c53fb95e-58c9-41b3-aa8c-8ba29cb514f0'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (9858, '85fd12c3-5372-4359-a853-bd7072caa3c1'), (10389, 'a8a284e8-5f91-4b84-ae4b-bc995fb2ccc7'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (24739, 'dcc5e35f-9208-4a2c-b2ef-771fe6332967'), (23211, '3ee17899-9fcb-44fb-957a-6db593a21bcd'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (24752, '3798bf75-5a2c-4b7a-9800-c75a034bfae1'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (3769, '4d65ee65-320a-476b-a100-ef50c8da088d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (4286, 'ed0e9a8c-e075-40db-b145-d300b7f9cea7'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (5828, 'b4bcc9d6-0657-4f0a-bdfd-c16904dd4eb1'), (23749, 'ebbc939a-6402-4c02-9891-1531598d8076'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (22225, '33434414-721b-423e-9aef-6fa82d028a76'), (19669, 'aee04d72-b4ef-45e8-8857-bc8147c6672b'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (7901, '17026dcd-41fe-493c-8a87-b0ddc5ea48cb'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (10998, '633599a3-8284-4a4a-987b-3f170a7e4efb'), (20223, 'a0ff3a3b-b5b7-4fcf-9f9d-68877d1cc4ad'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (3332, '09beafc1-1176-4db4-8ac6-78b13db9cee4'), (7433, '8b4a851e-9dbc-46a4-a7e6-6b51652c06b6'), (28428, '510aaa5d-d80b-419e-b31d-712d39155312'), (4366, 'fa03c662-5522-4732-a0ba-6b673062d9af'), (30481, '54d986c8-ef1b-46f2-ab54-283f310f483f'), (1301, '5137aed7-d723-4ecf-8874-26a7c9ecb926'), (29465, '9283c3d4-1b53-4b60-baf2-ee5185c2fba6'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (27460, '42d93388-c034-4d39-b6e2-99a5343894ac'), (6996, 'f3e44d90-39fb-4443-9dcf-2d3531beeab4'), (27481, '670fac96-da41-4462-a2f5-621a35d75778'), (347, 'd614507c-0464-48b7-bad7-c25df4c56372'), (27483, '640acfcf-de9c-46e8-b825-bb59cbffb305'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (352, '756c346a-bd77-440b-b6a9-1fabbf0c457d'), (21343, '08be7e91-b4c4-40b2-8817-a2af8d306009'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (8563, '46031f7f-3f23-4de5-882a-d8559db6cccb'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (14712, '1861bc32-ca30-45b0-bdb8-516bd6dd3448'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (14725, 'f93b2fc4-f0b6-42bb-9b39-5f3d69f3047b'), (24456, '7c13e0a3-5c54-4f1c-aaeb-db3f2c401de8'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (7050, '09c789c8-d58e-472f-ac73-8ac7722c751d'), (15762, '4e4f666b-dc03-4e9c-8dfe-2ccbb5a5c56b'), (22418, '1a9db661-e116-4d00-ae23-3f4205ad8f32'), (403, 'b6c24f9c-2894-4834-96cd-4a13ea060bb0'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (408, 'e160735e-ba9e-4144-a8f4-5ccdb5009af7'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (411, '96253f57-9a1a-4672-829b-a8e8241df3cf'), (29599, '143208c5-54d7-4e6b-ba1d-40ad953a9c0d'), (6561, 'ee35c81c-c680-4dfc-852b-7c11aae2a074'), (24485, '4de8b02f-cf9b-40a4-9212-1d8e11cd76f5'), (14758, '1b2c9206-d61c-4339-9c05-bb827322695d'), (6056, 'aba34742-67ab-47e8-a239-b5b49e9d199d'), (9640, '8d7047fe-653b-4dc3-9771-979787c23e4c'), (23469, '0f1f930e-b230-42f4-ba5c-b6d73abc59f0'), (23470, 'dfeab081-8379-4027-83f7-a17a7573a82e'), (24512, 'd2884679-070d-46c2-901f-85d4e0fd09b0'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (21962, '4ed9bb78-d26f-490e-ba3a-60bd868eff98'), (27083, 'fd3ceb3f-d838-4aa0-b054-2fcdd5929735'), (19403, 'c0f08689-c366-48f2-a497-c82f608735e2'), (14795, '8769e57e-63f1-41f6-be7b-287da69210bc'), (21964, 'a593fe2e-a9b6-4481-b26e-cfbd6438142e'), (6096, 'b1db7fca-c845-4de6-a30d-f88d2ad8689d'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (21972, '85cbe3b6-6131-4a4a-9548-070d1c3b8c2d'), (15836, 'f6ad3be6-00de-45fb-8497-71c9ae81f26b'), (21980, '9654551a-62fd-46b7-91c6-1b2ce272aaec'), (24030, '5237b189-9b15-48b1-b842-3eaeb2062ead'), (13280, '2c3ad26d-8418-4d01-a881-e5dd6be54e3f'), (15841, '73c3ef6b-2bae-40e2-a3de-ab7157a99930'), (3560, '96502103-444b-4974-9066-b1a0835216ab'), (3049, '0b0dfb40-acc7-4fe9-a526-43495756e63c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (3050, '7c7b731c-b363-44e1-8e4d-ab75aa50b634'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (13303, 'e6783d90-3750-4204-9c37-e9973ef7bf05'), (13306, '16f349e6-14fc-4aae-a2c0-2b6d6e4ecd46'), (13311, '99c4f125-6d87-4171-83bf-344a9aedf37a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In this blog post, we will go over the most effective techniques at the time of writing this blog post to tackle these challenges for efficient LLM deployment:

1.  **Lower Precision**: Research has shown that operating at reduced numerical precision, namely 8-bit and 4-bit, can achieve computational advantages without a considerable decline in model performance.

2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.
1. [Easy Deployment](https://huggingface.co/docs/inference-endpoints/index): Deploy models as production-ready APIs with just a few clicks, eliminating the need to handle infrastructure or MLOps.
2. [Cost Efficiency](https://huggingface.co/docs/inference-endpoints/autoscaling): Benefit from automatic scale to zero capability, reducing costs by scaling down the infrastructure when the endpoint is not in use, while paying based on the uptime of the endpoint, ensuring cost-effectiveness.
3. [Enterprise Security](https://huggingface.co/docs/inference-endpoints/security): Deploy models in secure offline endpoints accessible only through direct VPC connections, backed by SOC2 Type 2 certification, and offering BAA and GDPR data processing agreements for enhanced data security and compliance.
1. [Easy Deployment](https://huggingface.co/docs/inference-endpoints/index): Deploy models as production-ready APIs with just a few clicks, eliminating the need to handle infrastructure or MLOps.
2. [Cost Efficiency](https://huggingface.co/docs/inference-endpoints/autoscaling): Benefit from automatic scale to zero capability, reducing costs by scaling down the infrastructure when the endpoint is not in use, while paying based on the uptime of the endpoint, ensuring cost-effectiveness.
3. [Enterprise Security](https://huggingface.co/docs/inference-endpoints/security): Deploy models in secure offline endpoints accessible only through direct VPC connections, backed by SOC2 Type 2 certification, and offering BAA and GDPR data processing agreements for enhanced data security and compliance.
As models get bigger and bigger, deploying them into production to run inference has become increasingly challenging. Both hardware and software have seen a lot of innovations to address these challenges, so let's dive in to see how to efficiently overcome them!


## BLOOMZ

[BLOOM](https://arxiv.org/abs/2211.05100) is a 176-billion-parameter autoregressive model that was trained to complete sequences of text. It can handle 46 different languages and 13 programming languages. Designed and trained as part of the [BigScience](https://bigscience.huggingface.co/) initiative, BLOOM is an open-science project that involved a large number of researchers and engineers all over the world. More recently, another model with the exact same architecture was released: [BLOOMZ](https://arxiv.org/abs/2211.01786), which is a fine-tuned version of BLOOM on several tasks leading to better generalization and zero-shot[^1] capabilities.
## Solving Domain Challenges and Constraints by Partnering with Hugging Face

There are a number of unique challenges we face within our domain, including entropy injected by merchants, processing/payment companies, institutional differences, and shifts in user behavior. Designing and building efficient model performance alerting along with realistic benchmarking datasets has proven to be an ongoing challenge. Another significant hurdle is determining the optimal number of classes for our system - each class represents a significant amount of effort to create and maintain. Therefore, we must consider the value it provides to users and our business.
 
Here is the question: What is the most efficient way to overcome deployment challenges for cross-framework models?","Based on the context, the most efficient way to overcome deployment challenges for cross-framework models is by using the following techniques:

1. **Lower Precision**: Operating at reduced numerical precision, namely 8-bit and 4-bit, to achieve computational advantages without a considerable decline in model performance.
2. **Flash Attention**: Using a variation of the attention algorithm that provides a more memory-efficient approach and realizes increased efficiency due to optimized GPU memory utilization.
3. **Easy Deployment**: Deploying models as production-ready"
What approach best simplifies Transformer evaluation across multimodal benchmarks?,"[(4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (5152, '2eb1dd24-f252-4573-b700-3d513da59540'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (26156, 'cb06116c-89a8-4cb8-ae81-f3734a853307'), (27229, '79b37341-3681-4528-9ab1-2ab8778b564d'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (8807, '170b9534-cae9-40e8-bbe4-468c9b1e40f0'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (4223, '386573d6-bd83-4bd5-824c-7fbdd3122bad'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (25784, '7cb641f4-1b19-4387-a006-38511a58f088'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (14024, 'bf6710a5-977c-4793-87fc-584e835dc19f'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (752, '3828e3bc-8304-4f32-a0c4-ebe5bd0b19fd'), (15605, '9f241ce4-3d04-4868-8576-832003581357'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (5373, '996b47fd-ae44-4706-9fa3-41559df71109'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (7943, 'dd28dc4a-920f-4fc9-b713-5719ff38f1a8'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (2843, '2e20db58-f224-4700-9774-c63622aa8a4d'), (18222, '5880432c-3d6d-4287-b3fe-745b7ed4b67f'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (5427, '0dc0ff65-4fe2-4b69-8624-ada20379a111'), (21816, '5d5883c9-44d9-476e-bda0-c334c5912e4a'), (4415, 'b9a9a85e-e4e0-4cb7-b45f-4ec40e2891b6'), (13639, 'af509e72-cf08-461e-813f-b0e762ebdbfa'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (338, '83c0ff2b-59c2-4ffc-bf85-93ed5abbf667'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (28552, '9703bdeb-a1c7-4a77-af5d-48eb7e427983'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (21927, '7ba35d9f-9182-4565-95a7-0ad47ed75750'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (27048, 'e3873222-b7e8-4f6f-9732-ae991af045de'), (30638, 'bb43e4ba-f6ab-478d-8c31-6535c42d1276'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (12725, '69c1bca3-0390-4f32-82e9-9812d1bc36a6'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (9161, 'cc82182d-8b0c-4897-9bfe-8cc5080325ee'), (11723, '6a66155a-e619-439b-96b5-4cef0a9ece1f'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (16335, '41fef57c-6a30-487e-9e0f-4d02276e4a7d'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (475, 'fc8680f8-2eee-4940-8f49-33e177da211a'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: promote models that generalize to different NLU tasks, the benchmark includes datasets from varying domains and
applications. Additionally, we release HerBERT, a Transformer-based model trained specifically for the Polish language,
which has the best average performance and obtains the best results for three out of nine tasks. Finally, we provide an
extensive evaluation, including several standard baselines and recently proposed, multilingual Transformer-based
models.*
LSH (to enable deployment on GPU architectures). We evaluate our algorithm on the GLUE benchmark with standard 512 sequence 
length where we see favorable performance relative to a standard pretrained Transformer. On the Long Range Arena (LRA) benchmark, 
for evaluating performance on long sequences, our method achieves results consistent with softmax self-attention but with sizable 
speed-ups and memory savings and often outperforms other efficient self-attention methods. Our code is available at this https URL*
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
_Note: that we did not optimize the model for the GPU environment, the models were evaluated in fp32._

When it comes to benchmarking Transformer models, there are two metrics that are most adopted:
* **Latency**: the time it takes for the model to perform a single prediction (pre-process, prediction, post-process).
* **Throughput**: the number of executions performed in a fixed amount of time for one benchmark configuration

We looked at latency across different setups and models to understand the benefits and tradeoffs of the new Inferentia2 instance. If you want to run the benchmark yourself, we created a [Github repository](https://github.com/philschmid/aws-neuron-samples/tree/main/benchmark) with all the information and scripts to do so. 

### Results
- Abstract: Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. So, we now need a well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. I'll talk about a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically
 
Here is the question: What approach best simplifies Transformer evaluation across multimodal benchmarks?","Based on the context, the approach that best simplifies Transformer evaluation across multimodal benchmarks is a systematic and unified benchmark, such as the Long Range Arena (LRA) benchmark, which evaluates model quality under long-context scenarios and consists of a suite of tasks with sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities."
How can multilingual Transformer models be fine-tuned for diverse tasks?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (8733, '138675fc-8785-4cf2-b567-24851fd927ad'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (3661, 'ea664e53-46c8-4bc7-bdb4-451f2ad8c76e'), (10829, '3c5a2c97-33b4-4ac1-a2d9-3f64d6772d3a'), (5202, 'ffdc2e4f-ce95-48dd-8a87-2c908bbecdcf'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (30822, '1357395c-e339-49a4-9470-5057472025be'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (14997, '63b9b6ec-6baa-4247-a808-3fa1f5673d11'), (19102, 'da415ff0-b51e-4aeb-9852-fbe5a6a4f9f4'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (4796, '8f601501-25a6-4972-aa7a-aa9e072baede'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (11494, '8db96499-6302-45aa-bca4-bce4bec72dee'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (277, '91e3b387-1104-48c6-98d8-5d391b18159f'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (1829, '98ed0df5-e51e-4766-9380-1458db6d3ec6'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (6989, '03087320-4dbe-4ba4-8b78-3427b35f3489'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (8017, 'f56d8d5f-9293-4c6b-8217-c1fb077fbe01'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (8020, '4658bd1d-e022-460e-9d6d-af439960931b'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (7000, '8d065847-3189-4122-97bf-d861398ddd06'), (7002, 'f6c1eb0a-744c-4cac-9ea7-db3de3ac11d6'), (7006, 'ca729c38-08a8-487f-85b8-173e988c8e43'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (22895, '3e1b26c9-fb73-4f76-98f6-9a263f7ab477'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (15740, '1ff3f9f4-82bb-4766-8c01-807ece2b0afb'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (31161, 'e6392f6d-29c2-4b5e-99d0-0af8c82a0670'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (18905, 'a3346d1d-f6c7-4b12-8e61-f1307d4f96d1'), (18906, 'b70bf7ea-06d3-4b07-93e6-b16b084feea9'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (18909, '51930168-debc-4692-b466-bbf665eaf089'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (482, 'ecfc6092-627b-4285-bb59-322895262902'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (21476, 'fc40dcb8-cfe2-4449-a2ff-c6b8962d8db5'), (29156, '63322eab-3574-4afc-99b5-c979b3dd7be0'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (18429, '4f938c80-c628-4d41-b604-430b07615100')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *We re-evaluate the standard practice of sharing weights between input and output embeddings in state-of-the-art
pre-trained language models. We show that decoupled embeddings provide increased modeling flexibility, allowing us to
significantly improve the efficiency of parameter allocation in the input embedding of multilingual models. By
reallocating the input embedding parameters in the Transformer layers, we achieve dramatically better performance on
standard natural language understanding tasks with the same number of parameters during fine-tuning. We also show that
allocating additional capacity to the output embedding provides benefits to the model that persist through the
fine-tuning stage even though the output embedding is discarded after pre-training. Our analysis shows that larger
output embeddings prevent the model's last layers from overspecializing to the pre-training task and encourage
As you can see from this table, the majority of Transformer models for summarization (and indeed most NLP tasks) are monolingual. This is great if your task is in a ""high-resource"" language like English or German, but less so for the thousands of other languages in use across the world. Fortunately, there is a class of multilingual Transformer models, like mT5 and mBART, that come to the rescue. These models are pretrained using language modeling, but with a twist: instead of training on a corpus of one language, they are trained jointly on texts in over 50 languages at once!

We'll focus on mT5, an interesting architecture based on T5 that was pretrained in a text-to-text framework. In T5, every NLP task is formulated in terms of a prompt prefix like `summarize:` which conditions the model to adapt the generated text to the prompt. As shown in the figure below, this makes T5 extremely versatile, as you can solve many tasks with a single model!
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
```

While this example showed how this can be done with one specific type of base model, any [Sentence Transformer](https://huggingface.co/models?library=sentence-transformers&sort=downloads) model could be switched in for different performance and tasks. For instance, using a multilingual Sentence Transformer body can extend few-shot classification to multilingual settings.

## Next steps
We've shown that SetFit is an effective method for few-shot classification tasks. In the coming months, we'll be exploring how well the method generalizes to tasks like natural language inference and token classification. In the meantime, we're excited to see how industry practitioners apply SetFit to their use cases - if you have any questions or feedback, open an issue on our [GitHub repo](https://github.com/huggingface/setfit) 🤗.

Happy few-shot learning!
{/if}

For many NLP applications involving Transformer models, you can simply take a pretrained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand. Provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning, transfer learning will usually produce good results. 

However, there are a few cases where you'll want to first fine-tune the language models on your data, before training a task-specific head. For example, if your dataset contains legal contracts or scientific articles, a vanilla Transformer model like BERT will typically treat the domain-specific words in your corpus as rare tokens, and the resulting performance may be less than satisfactory. By fine-tuning the language model on in-domain data you can boost the performance of many downstream tasks, which means you usually only have to do this step once!
 
Here is the question: How can multilingual Transformer models be fine-tuned for diverse tasks?","Multilingual Transformer models can be fine-tuned for diverse tasks by reallocating the input embedding parameters in the Transformer layers, which allows for increased modeling flexibility and improved performance on standard natural language understanding tasks. Additionally, allocating additional capacity to the output embedding can provide benefits to the model that persist through the fine-tuning stage. Furthermore, fine-tuning the language model on in-domain data can boost the performance of many downstream tasks."
What makes Hugging Face APIs critical for multimodal workflow deployment?,"[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (19977, 'da27be80-5381-43b5-9542-ef033eb07df9'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (5677, '5b721666-5858-40b2-82e0-f9bb1f89c6ab'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8269, '63e383a9-dbfa-4c9f-a286-0668a682d313'), (4176, 'aa889fe9-43b3-48ac-a21a-f96e230490b0'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4179, 'b6031fcd-da9e-4a41-ac79-36bf3eaa3bd0'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (1127, '61bce4e3-ba06-42fd-82da-5f4b1ae2562a'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (31354, '50289d67-f345-42d3-bb11-7b4a1e8c5530'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (4237, 'aa571318-cb21-42dc-96d5-9247c29a9ac5'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (3238, '558f6733-8507-471e-b4d2-520504677468'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (22247, '4c0c828d-cbfa-4f24-9d14-9eed657d9e71'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (18783, '44d0b22a-e8ef-4cba-bda9-fd404cb063c2'), (18788, 'c8b88dcc-6e3b-4f12-9ccc-23fa571e9a77'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (7026, 'a54a170b-414e-4326-bff0-925d0cf07fab'), (21373, '46f93944-1684-4015-b212-08fbd8125761'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (6091, 'ec43fc42-f705-45a0-8110-0fb4ac955090'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24535, '5f65c014-0a38-4090-88ab-510c3bc8926e'), (6106, '13c9c3c6-6d87-4cba-a886-332bfef5da3d'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (15837, '78ef6dea-b85c-4549-ad3c-0ef7afe5cd99'), (31716, '63f41724-0a0f-4e87-8baf-46b9e5e590f8'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (27114, 'bc032949-d8d4-4c0e-8898-fdcb416b7e1b'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In combination with the data issues widget, the Python API offers a great way to integrate the results of existing scripts (e.g. data quality checks or model monitoring) into a scalable data inspection workflow.

## Using Spotlight on the Hugging Face hub

You can use Spotlight directly on your local NLP, audio, CV or multimodal dataset. If you would like to showcase your dataset or model results on the Hugging Face hub, you can use Hugging Face spaces to launch a Spotlight visualization for it.

We have already prepared [example spaces](https://huggingface.co/renumics) for many popular NLP, audio and CV datasets on the hub. You can simply duplicate one of these spaces and specify your dataset in the `HF_DATASET` variable.

You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.
```

The current API does not enforce strict rate limitations. Instead, Hugging Face balances the loads evenly between all our available resources and favors steady flows of requests. If you need to embed several texts or images, the [Hugging Face Accelerated Inference API](https://huggingface.co/docs/api-inference/index) would speed the inference and let you choose between using a CPU or GPU.
```

## Pros and cons 

When to use this API and when to not use it? Let's discuss in this section the pros and cons 

Pros:
- The model gets modified in-place, meaning the model will preserve all its original attributes and methods
- Works for any torch module, and any modality (vision, text, multi-modal)

Cons:
- You need to manually writing Hugging Face `from_pretrained` and `save_pretrained` utility methods if you want to easily save / load adapters from the Hugging Face Hub.
- You cannot use any of the utility method provided by `PeftModel` such as disabling adapters, merging adapters, etc.
## API and client library interaction with the Hub

Interacting with the Hugging Face Hub via an [API](https://huggingface.co/docs/hub/api) or the [`huggingface_hub`](https://huggingface.co/docs/huggingface_hub/index) Python library is possible. This includes creating new repositories, uploading data programmatically and creating and modifying metadata for datasets. This can be powerful for research workflows where new data or annotations continue to be created. The client library also makes uploading large datasets much more accessible. 

## Community 

The Hugging Face Hub is already home to a large community of researchers, developers, artists, and others interested in using and contributing to an ecosystem of open-source machine learning. Making your datasets accessible to this community increases their visibility, opens them up to new types of users and places your datasets within the context of a larger ecosystem of models, datasets and libraries.
In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.

In the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.

## Compliance as a Core Principle

As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.
 
Here is the question: What makes Hugging Face APIs critical for multimodal workflow deployment?","Based on the context, Hugging Face APIs are critical for multimodal workflow deployment because they offer a scalable data inspection workflow, enable integration of existing scripts (e.g., data quality checks or model monitoring), and provide a way to showcase datasets or model results on the Hugging Face hub. Additionally, the APIs allow for interacting with the Hugging Face Hub via an API or the `huggingface_hub` Python library, making it possible to create new repositories, upload data programmatically, and create"
What is a key consideration when using pretrained Transformers for multimodal applications?,"[(5120, 'ddf54aa1-62ec-4203-a18e-bcc935c8a3ed'), (11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (18443, '00876f43-8b96-47cb-ba09-71690498db3d'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (8212, '151df07a-3ff2-413f-a05f-6d34e67c0e2f'), (5141, 'aa79a23d-5dce-4f0c-905f-502f514390e9'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (12897, '78a4117e-544a-4fa9-9c46-63ee897d9e66'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (17510, '4186dada-bac9-4760-b476-16d67e9b4f11'), (2663, 'efa7e63d-051b-4d0d-b130-483c5160dd59'), (23145, '093af302-133e-498d-a722-88a8b6199cee'), (25706, '07429df9-ec51-44b8-bbaa-43e5e886bf4d'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (20617, '476882ca-9720-4486-939b-4dfc903b0857'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (20636, '3a97d7e3-78ab-4911-9bbf-01b10ea9777a'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (20661, '3d03f793-112e-4835-89e0-d0c9ab6a236d'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (4812, 'b008816e-72ef-4b5b-9a8d-f2ade4e8823c'), (5324, '5eab4adf-5266-49ae-85d2-9bc2ee7519e3'), (17103, 'f3f12925-7e43-4057-bc7e-1a29829c0068'), (13526, '594c0819-9996-42bd-8329-7cec367352a5'), (13535, '818b2f5b-6b46-4f5f-99b7-cc2d76fc212e'), (18147, '08a3519f-4ec3-40a5-a501-a54a7a0613e1'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (4850, '3a950f12-3c2d-47ce-a6a8-1abdfb1befa0'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (13565, '118171eb-372a-4886-8180-64c88a38710f'), (4861, '79210cef-fada-44fb-a563-86b3d8b879d2'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (263, 'f2e4842b-dafb-4139-b722-2bb49405a14f'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (16674, 'a05d2b01-8fd3-48e1-8077-85f9ef33864d'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (298, 'a9b73b5b-eef0-437d-842d-a511870ed8c2'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (307, '6b64ed6e-acaf-4565-8edd-d7398bd867b4'), (19764, '3964a67e-698f-4796-91c3-29d446e12522'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (12105, 'e4f29e10-0755-4ad0-87ea-18507f153a17'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (6997, '35d0066b-d1f2-45f1-a295-d792e0dda90d'), (6999, '6eccfd6c-e1db-46ef-9c86-1860086df042'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (17761, '95299ba8-4ba2-4666-a1f4-2d7e527ebbfe'), (11114, '933fa587-6033-469c-b633-a24147cec80c'), (11115, 'a4f77954-5dd3-4d2c-a0a8-d2241981c438'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (9083, '40b57069-cb95-4690-8c8a-f74d274c5eee'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (22918, '201f6149-0a67-49ea-81bd-e07891bdb9c1'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (9119, 'cd5f2faa-9cfe-42e6-8006-4a8264ce76d0'), (4511, 'af6fa441-8f03-460f-ad80-c0b9214b289c'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (31650, 'b7d49d66-3894-4743-b041-a97c182c96a7'), (10145, 'ce96ae5b-5345-4742-bb66-c625e53c53e0'), (10149, '717ac025-7b4c-4c12-a316-fbdbf28787d5'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (9129, '4e3c49f8-4ca3-47af-a572-5135008633d3'), (8622, 'e1ca78ca-d2cd-47c7-9732-52d5a70c7e15'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (10170, '4052ca58-8126-4b6e-ab1d-9bc6799339a3'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (9150, 'de3eb793-c6de-4fd2-9f8c-1b0b686cf6d1'), (2509, '53bab419-4140-4137-aa56-67f5880cf609'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (7123, '5550e6f5-759d-4aef-9d97-83b53144cc4b'), (10194, 'e573dcfe-a4f3-435f-9afe-101265a292e5'), (5074, 'e34f5010-d00a-48d9-b2db-4b2925161439'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (18413, 'c087ad9c-11fd-4007-b215-f0a96fd9c1e4'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (5110, 'ff805522-0e2d-45ba-be4f-6e3fac713294'), (18423, 'bceac124-b7c2-4000-85e8-bc1990d7dce1'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (6652, '73ebb877-db9a-49cb-833a-ad9063ffeee5')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
Why should I use transformers?

1. Easy-to-use state-of-the-art models:
  - High performance on NLU and NLG tasks.
  - Low barrier to entry for educators and practitioners.
  - Few user-facing abstractions with just three classes to learn.
  - A unified API for using all our pretrained models.
  - Lower compute costs, smaller carbon footprint:

2. Researchers can share trained models instead of always retraining.
  - Practitioners can reduce compute time and production costs.
  - Dozens of architectures with over 10,000 pretrained models, some in more than 100 languages.

3. Choose the right framework for every part of a model's lifetime:
  - Train state-of-the-art models in 3 lines of code.
  - Move a single model between TF2.0/PyTorch frameworks at will.
  - Seamlessly pick the right framework for training, evaluation and production.
## Why should I use transformers?

1. Easy-to-use state-of-the-art models:
    - High performance on natural language understanding & generation, computer vision, and audio tasks.
    - Low barrier to entry for educators and practitioners.
    - Few user-facing abstractions with just three classes to learn.
    - A unified API for using all our pretrained models.

1. Lower compute costs, smaller carbon footprint:
    - Researchers can share trained models instead of always retraining.
    - Practitioners can reduce compute time and production costs.
    - Dozens of architectures with over 60,000 pretrained models across all modalities.

1. Choose the right framework for every part of a model's lifetime:
    - Train state-of-the-art models in 3 lines of code.
    - Move a single model between TF2.0/PyTorch/JAX frameworks at will.
    - Seamlessly pick the right framework for training, evaluation, and production.
-->

# Fine-tune a pretrained model

[[open-in-colab]]

There are significant benefits to using a pretrained model. It reduces computation costs, your carbon footprint, and allows you to use state-of-the-art models without having to train one from scratch. 🤗 Transformers provides access to thousands of pretrained models for a wide range of tasks. When you use a pretrained model, you train it on a dataset specific to your task. This is known as fine-tuning, an incredibly powerful training technique. In this tutorial, you will fine-tune a pretrained model with a deep learning framework of your choice:

* Fine-tune a pretrained model with 🤗 Transformers [`Trainer`].
* Fine-tune a pretrained model in TensorFlow with Keras.
* Fine-tune a pretrained model in native PyTorch.

<a id='data-processing'></a>

## Prepare a dataset

<Youtube id=""_BZearw7f0w""/>
The authors also use the Perceiver for multimodal autoencoding. The goal of multimodal autoencoding is to learn a model that can accurately reconstruct multimodal inputs in the presence of a bottleneck induced by an architecture. The authors train the model on the [Kinetics-700 dataset](https://deepmind.com/research/open-source/kinetics), in which each example consists of a sequence of images (i.e. frames), audio and a class label (one of 700 possible labels). This model is also implemented in HuggingFace Transformers, and available as `PerceiverForMultimodalAutoencoding`. For brevity, I will omit the code of defining this model, but important to note is that it uses `PerceiverMultimodalPreprocessor` to prepare the `inputs` for the model. This preprocessor will first use the respective preprocessor for each modality (image, audio, label) separately. Suppose one has a video of 16 frames of resolution 224x224 and 30,720 audio samples, then the modalities are preprocessed as follows:
 
Here is the question: What is a key consideration when using pretrained Transformers for multimodal applications?","A key consideration when using pretrained Transformers for multimodal applications is data processing, specifically preparing the dataset for each modality (e.g., image, audio, label) separately using a preprocessor for each modality, such as `PerceiverMultimodalPreprocessor`."
What challenges arise when adapting Transformer models for multimodal benchmarks?,"[(4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (5152, '2eb1dd24-f252-4573-b700-3d513da59540'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (4646, '13611a54-20bb-45b0-89e7-31b0bb22b402'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (27229, '79b37341-3681-4528-9ab1-2ab8778b564d'), (21089, '9e8976c4-72d9-49eb-9b17-5078f33abec4'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (14442, '63643091-647a-4c7f-a8e1-f89173754c81'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (15605, '9f241ce4-3d04-4868-8576-832003581357'), (15606, '71214a46-ce5e-4c6f-bf75-2b9806ccb2de'), (5375, 'bef61cba-2291-49ca-b2a4-47a9e695a375'), (3841, '474c0e89-5aa1-4d76-bea3-bd48b6e8909a'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (18222, '5880432c-3d6d-4287-b3fe-745b7ed4b67f'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (21816, '5d5883c9-44d9-476e-bda0-c334c5912e4a'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (13639, 'af509e72-cf08-461e-813f-b0e762ebdbfa'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (338, '83c0ff2b-59c2-4ffc-bf85-93ed5abbf667'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (16217, 'c034bc99-4e0b-4ab5-9729-b49db86d0700'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5534, '3e4707b0-83e3-4137-b771-207785927e14'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (27048, 'e3873222-b7e8-4f6f-9732-ae991af045de'), (30638, 'bb43e4ba-f6ab-478d-8c31-6535c42d1276'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26049, '062d386e-2fae-46d0-90c6-f27462a7ce1e'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (11721, '1aead5c5-7fcb-464e-b7ab-40f535514f4c'), (9161, 'cc82182d-8b0c-4897-9bfe-8cc5080325ee'), (11723, '6a66155a-e619-439b-96b5-4cef0a9ece1f'), (16335, '41fef57c-6a30-487e-9e0f-4d02276e4a7d'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (17364, '375858e6-fad8-49d7-9f08-70b4ff973b3e'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (475, 'fc8680f8-2eee-4940-8f49-33e177da211a'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Performance and Scalability

Training large transformer models and deploying them to production present various challenges.  
During training, the model may require more GPU memory than available or exhibit slow training speed. In the deployment 
phase, the model can struggle to handle the required throughput in a production environment.

This documentation aims to assist you in overcoming these challenges and finding the optimal setting for your use-case. 
The guides are divided into training and inference sections, as each comes with different challenges and solutions. 
Within each section you'll find separate guides for different hardware configurations, such as single GPU vs. multi-GPU 
for training or CPU vs. GPU for inference.
which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how practitioners solve Document AI use cases.
## Conclusion

Transformer models have emerged as the go-to solution for many machine learning tasks. However, deploying them in production has been challenging due to their large size and latency requirements. Thanks to AWS Inferentia2 and the collaboration between Hugging Face and AWS, developers and organizations can now leverage the benefits of state-of-the-art models without the prior need for extensive machine learning expertise. You can start testing for as low as 0.76$/h.

The initial benchmarking results are promising, and show that Inferentia2 delivers superior latency performance when compared to both Inferentia and NVIDIA A10G GPUs. This latest breakthrough promises high-quality machine learning models can be made available to a much broader audience delivering AI accessibility to everyone.
- Abstract: Transformers do not scale very well to long sequence lengths largely because of quadratic self-attention complexity. In the recent months, a wide spectrum of efficient, fast Transformers have been proposed to tackle this problem, more often than not claiming superior or comparable model quality to vanilla Transformer models. So, we now need a well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide spectrum of tasks and datasets makes it difficult to assess relative model quality amongst many models. I'll talk about a systematic and unified benchmark, LRA, specifically focused on evaluating model quality under long-context scenarios. LRA is a suite of tasks consisting of sequences ranging from 1K to 16K tokens, encompassing a wide range of data types and modalities such as text, natural, synthetic images, and mathematical expressions requiring similarity, structural, and visual-spatial reasoning. We systematically
However, for all their greatness, Transformers can be challenging to deploy in production. On top of the infrastructure plumbing typically associated with model deployment, which we largely solved with our [Inference Endpoints](https://huggingface.co/inference-endpoints) service, Transformers are large models which routinely exceed the multi-gigabyte mark. Large language models (LLMs) like [GPT-J-6B](https://huggingface.co/EleutherAI/gpt-j-6B), [Flan-T5](https://huggingface.co/google/flan-t5-xxl), or [Opt-30B](https://huggingface.co/facebook/opt-30b) are in the tens of gigabytes, not to mention behemoths like [BLOOM](https://huggingface.co/bigscience/bloom), our very own LLM, which clocks in at 350 gigabytes.
 
Here is the question: What challenges arise when adapting Transformer models for multimodal benchmarks?","The context does not explicitly mention the challenges of adapting Transformer models for multimodal benchmarks. However, it does mention that multimodal models, which use both text and visual information together using a multimodal Transformer, can achieve 95% accuracy. It also mentions the challenges of deploying large transformer models in production, such as requiring more GPU memory than available or exhibiting slow training speed, and struggling to handle the required throughput in a production environment. But it does not specifically discuss the challenges of adapting Transformer models for"
How do Hugging Face frameworks facilitate seamless integration of multimodal tasks?,"[(19976, 'ec852d21-1da0-46da-8293-267f04f04242'), (7178, '086ecf90-fb2d-4fb0-9e37-23a45c3703ea'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (16928, '1dd8da94-9861-4d4b-ae5d-0182de0a33d2'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (31267, '52b79a7a-4a8f-4cd9-ba52-8a8e1cab9155'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (29742, '41dd1415-0367-415c-b18f-14be16e6b316'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (7224, '49503e5a-ba18-43c6-9722-173bbeee8dd9'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (22086, '6f92aef8-fd03-45fb-8920-1fc2364e4e13'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (11386, 'b62622b1-cbec-4754-9710-20faddd1d382'), (27775, '56108da4-6463-4bb1-8393-42e02e4b2cc9'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (22783, '898e6630-6e42-41f4-bf5e-7ad200e3339e'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (19218, '29f57a47-3df1-4b88-a202-974f96482787'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (28440, 'f186228b-4ecd-4f33-a849-39b930f0800a'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15299, 'c141a388-b78f-49f1-aefc-636b7195e91c'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (24011, '4be4591b-14cb-4bfb-80c1-06501be969f7'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (10235, '6285ff51-c4b7-47f1-9c33-4cf592322ccc'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In combination with the data issues widget, the Python API offers a great way to integrate the results of existing scripts (e.g. data quality checks or model monitoring) into a scalable data inspection workflow.

## Using Spotlight on the Hugging Face hub

You can use Spotlight directly on your local NLP, audio, CV or multimodal dataset. If you would like to showcase your dataset or model results on the Hugging Face hub, you can use Hugging Face spaces to launch a Spotlight visualization for it.

We have already prepared [example spaces](https://huggingface.co/renumics) for many popular NLP, audio and CV datasets on the hub. You can simply duplicate one of these spaces and specify your dataset in the `HF_DATASET` variable.

You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.
## What can you find on the Hugging Face Hub?

### Models 

The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.

### Datasets
The Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.

### Spaces

Hugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications.
!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Integrate any ML framework with the Hub

The Hugging Face Hub makes hosting and sharing models with the community easy. It supports
[dozens of libraries](https://huggingface.co/docs/hub/models-libraries) in the Open Source ecosystem. We are always
working on expanding this support to push collaborative Machine Learning forward. The `huggingface_hub` library plays a
key role in this process, allowing any Python script to easily push and load files.
Integrate your library with the Hub

The Hugging Face Hub aims to facilitate sharing machine learning models, checkpoints, and artifacts. This endeavor includes integrating the Hub into many of the amazing third-party libraries in the community. Some of the ones already integrated include [spaCy](https://spacy.io/usage/projects#huggingface_hub), [AllenNLP](https://allennlp.org/), and [timm](https://rwightman.github.io/pytorch-image-models/), among many others. Integration means users can download and upload files to the Hub directly from your library. We hope you will integrate your library and join us in democratizing artificial intelligence for everyone.

Integrating the Hub with your library provides many benefits, including:
To use your own custom model hosted on Hugging Face, change the model endpoint in the API Wizard.

## Usage Tips

1. Keep in mind that the API makes calls asynchronously, and returns a response or error via callbacks.
2. Address slow response times or performance issues by changing model endpoints to lower resource models.

## Conclusion

The Hugging Face Unity API offers a simple way to integrate AI models into your Unity projects. We hope you found this tutorial helpful. If you have any questions or would like to get more involved in using Hugging Face for Games, join the [Hugging Face Discord](https://hf.co/join/discord)!
 
Here is the question: How do Hugging Face frameworks facilitate seamless integration of multimodal tasks?","The context does not provide a direct answer to how Hugging Face frameworks facilitate seamless integration of multimodal tasks. However, it can be inferred that Hugging Face frameworks support multimodal tasks through its datasets and models that cover various domains and modalities, including text, image, audio, and multi-modal datasets. Additionally, the Hugging Face Hub allows for the integration of various machine learning libraries, which can potentially facilitate the integration of multimodal tasks."
What is the primary benefit of integrating multilingual checkpoints in Transformer workflows?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (18436, '4fa9002f-45cb-457b-bd41-fea02a5a5ab9'), (16406, '57321316-4910-41b2-9525-559cf4584035'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (14905, '692ff229-0297-4d95-89d9-2dcae49a2c59'), (25146, '16da7a4a-2a62-4876-80b2-e6377efa1d21'), (24126, 'f6d0b6d4-13c2-4d59-a59d-6b341a5a1508'), (7237, '80165281-82eb-48a3-a4a3-f76ce44e2f4c'), (10331, '3e81c36c-c486-497f-8503-f7e56fb08165'), (10332, 'a0e48a36-723f-4add-b319-aff6ed8392a6'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (12897, '78a4117e-544a-4fa9-9c46-63ee897d9e66'), (29793, '5e074918-2d97-434c-917f-4f8530780023'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (7272, '90527d40-f42d-47ed-a5d3-255fa1425973'), (6761, '92f3ace2-8987-4983-8026-5a483a8c2ce5'), (7277, 'fcdd1c1a-bc8a-4b1f-aa30-7718209282e6'), (20601, 'a07e30af-9ee2-4d89-adb0-f3f1cd92ade2'), (9362, '6813e5b6-857c-48cc-be86-09d411ea6a2d'), (19604, '97b911ed-5d6d-4a6b-b2b1-60cd3abef208'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (19606, 'dc0a368e-71aa-4086-96ca-a5305aa5a216'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (23728, '411de616-5192-4306-a85f-cdbd526173fa'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (4796, '8f601501-25a6-4972-aa7a-aa9e072baede'), (3772, 'c0e7815c-c4aa-4bef-8e97-c02f74a920de'), (19646, '6395f7ed-ad70-4ff7-85ca-da08897c72d5'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (8900, '657b3f68-c86a-4fb7-ad21-b651f49dec92'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (24266, 'bb53534c-1c72-4bf1-a269-6eb7327b939f'), (15052, '66b79a6c-5bde-43e5-8338-9ab4304488b6'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (8411, 'c24bfa67-a32e-47b1-b8f8-1f42ce317c3f'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (4843, '135935ad-d46d-4c4a-bcff-d8a3f30f8f8b'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (4864, '6b9273ae-f64e-4551-a2ce-78c773e1eb40'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (27406, '954a0b75-0188-4266-84c0-4a2f946626bf'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (1815, '67147ae7-6db3-4161-bfd0-33826ce17ddf'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (25883, 'f01a6bd7-35e0-4d59-b1e1-8879b0662972'), (15132, '8dd8343f-285b-436f-84a7-620e9144edce'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (5930, '4af5f45c-7439-42aa-ab68-23e2bbfd45b1'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (26421, '2c89420e-e9cb-4967-9268-cef5f028e779'), (26426, '613fabb3-d6a2-48f0-b6f1-a63e4b3230af'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (13642, '092cdf28-d7d0-42cf-9aef-cd07972bc95c'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (3922, '6f3ca0aa-6e55-4248-8281-52387a0ce0a2'), (4438, 'c433e5f5-9651-489c-8a76-f04bf6884777'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (7009, '4fe8511b-00f0-404a-934b-6f00dad4ecae'), (1890, '4d7210c7-1467-4e1b-b056-da922b7ff796'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (8035, '85373154-23b0-4c8d-826b-c39c88e0e81b'), (13669, 'fd5f73cd-31c4-4ed9-a912-115b0d44f3b5'), (25449, '2156608f-8c49-4668-a8e8-672285ea14c7'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (15218, 'd658d9cf-b160-4a25-97f2-b466716c5d87'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (7546, '125265e4-f906-4180-bf64-53f1be1bd941'), (23935, 'f0a1c8e7-8bd7-4f7a-8186-f19388eabd4c'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (5012, '783a3119-6807-482d-9906-92f9b1dadb1a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (16311, 'ff1c1d88-bc88-43e0-9d16-50398857fbc0'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (7613, '2f0d0b7d-35df-48b0-a562-dfc0a78e5974'), (20937, 'bc8955bf-129c-4b11-91ac-b38eb73b23ac'), (9167, 'd5140d81-d406-4efe-bd96-206f95bd719b'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (18909, '51930168-debc-4692-b466-bbf665eaf089'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (21476, 'fc40dcb8-cfe2-4449-a2ff-c6b8962d8db5'), (21478, '1e3d6490-9e13-469c-ab39-9f3cfda389ef'), (26089, 'f7b97c83-1a20-41e4-ac6a-d926f2dd301a'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (17403, '38d35ffd-8bed-4e4d-863e-d2214e1e2d06'), (9726, 'ef486ead-c143-4c25-a64a-bf67a128b117')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: This model was contributed by [thomwolf](https://huggingface.co/thomwolf). The original code can be found [here](https://github.com/facebookresearch/XLM/).

## Usage tips

- XLM has many different checkpoints, which were trained using different objectives: CLM, MLM or TLM. Make sure to
  select the correct objective for your task (e.g. MLM checkpoints are not suitable for generation).
- XLM has multilingual checkpoints which leverage a specific `lang` parameter. Check out the [multi-lingual](../multilingual) page for more information.
- A transformer model trained on several languages. There are three different type of training for this model and the library provides checkpoints for all of them:
## The technical philosophy

In this section, we wanted to share our philosophy behind adding support for Computer Vision in 🤗 Transformers so that the community is aware of the design choices specific to this area.

Even though Transformers started with NLP, we support multiple modalities today, for example – vision, audio, vision-language, and Reinforcement Learning. For all of these modalities, all the corresponding models from Transformers enjoy some common benefits:

- Easy model download with a single line of code with `from_pretrained()`
- Easy model upload with `push_to_hub()`
- Support for loading huge checkpoints with efficient checkpoint sharding techniques
- Optimization support (with tools like [Optimum](https://huggingface.co/docs/optimum))
- Initialization from model configurations
- Support for both PyTorch and TensorFlow (non-exhaustive)
- and many more
This model can also be used with TTS checkpoints from [Massively Multilingual Speech (MMS)](https://arxiv.org/abs/2305.13516) 
as these checkpoints use the same architecture and a slightly modified tokenizer.

This model was contributed by [Matthijs](https://huggingface.co/Matthijs) and [sanchit-gandhi](https://huggingface.co/sanchit-gandhi). The original code can be found [here](https://github.com/jaywalnut310/vits).

## Usage examples

Both the VITS and MMS-TTS checkpoints can be used with the same API. Since the flow-based model is non-deterministic, it 
is good practice to set a seed to ensure reproducibility of the outputs. For languages with a Roman alphabet, 
such as English or French, the tokenizer can be used directly to pre-process the text inputs. The following code example 
runs a forward pass using the MMS-TTS English checkpoint:

```python
import torch
from transformers import VitsTokenizer, VitsModel, set_seed
The abstract from the paper is the following:

*Unsupervised pretraining of large neural models has recently revolutionized Natural Language Processing. By
warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple
benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language
Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We
developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT,
GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both
encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation,
Text Summarization, Sentence Splitting, and Sentence Fusion.*
-->

# Multilingual models for inference

[[open-in-colab]]

There are several multilingual models in 🤗 Transformers, and their inference usage differs from monolingual models. Not *all* multilingual model usage is different though. Some models, like [bert-base-multilingual-uncased](https://huggingface.co/bert-base-multilingual-uncased), can be used just like a monolingual model. This guide will show you how to use multilingual models whose usage differs for inference.

## XLM

XLM has ten different checkpoints, only one of which is monolingual. The nine remaining model checkpoints can be split into two categories: the checkpoints that use language embeddings and those that don't.

### XLM with language embeddings

The following XLM models use language embeddings to specify the language used at inference:
 
Here is the question: What is the primary benefit of integrating multilingual checkpoints in Transformer workflows?","The primary benefit of integrating multilingual checkpoints in Transformer workflows is not explicitly stated in the provided context. However, based on the information provided, it can be inferred that the primary benefit is the ability to leverage pre-trained models for multiple languages, allowing for more efficient and effective processing of multilingual data. This is hinted at in the section on multilingual models for inference, which suggests that some models can be used for multiple languages, and in the description of XLM, which is a transformer model trained"
What makes preprocessing tools essential for Transformer model deployment in multimodal tasks?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (5133, '45481eac-a1b8-418a-89cb-43db3787f5f0'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (24636, '34c928bf-700f-4de6-b158-b590ff679457'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (23149, 'e50ebf4c-b036-4492-a402-70e3243542d9'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (23229, '0db85d3b-56c0-4fd4-a3fa-065cd9c9d588'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (9415, '78661659-8f11-4005-800b-e1d23d747852'), (13516, '23d28c93-4a90-4b14-b846-aa433a8b0feb'), (13526, '594c0819-9996-42bd-8329-7cec367352a5'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (20185, '26583e89-c8f7-4107-a950-11734ddb8fbf'), (18140, '2b5de1ef-6517-4c54-b9b2-3d94550ca7a5'), (14567, '3330776a-c8c7-4367-b728-fba1c1251ddc'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (1267, '9a5d21ab-3229-443e-b277-ad00c75169ec'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (5876, '6c565e09-ac75-4392-891f-2a1bbddd3dd9'), (13558, '415ec308-3b10-4657-a9b2-e4f3aa21d028'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (256, '154c2da9-4985-48e3-86b8-642c818c758e'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (9006, '0cac4a26-422e-4ebf-9413-41b9cd0b9ccf'), (14639, 'a900f9b4-f019-4b6d-b17d-388b48503f24'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (14657, 'd5903c56-f92a-451c-b8e1-27c7ee957814'), (12105, 'e4f29e10-0755-4ad0-87ea-18507f153a17'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (17741, '28a0fb96-9ba3-48c6-a7bc-158aa6526cb7'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (17740, '278dd567-d102-4857-990f-de19046c1e3c'), (26961, 'ff74d859-f5af-47b1-af2a-fc6d709448f2'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (9109, '46c1eb91-a107-4d91-86e5-205812209154'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (20887, 'e4c88d19-b6b7-484a-b21a-646c0f2984c3'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (5533, '48b60d32-1e6a-4c2f-9e50-ea8a9f304531'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (9120, '6e6140c9-95c6-4b7e-8af0-9dfe53916a31'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (11682, '8a107311-7324-4c5a-b356-461949969836'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (9142, 'e002d670-ef16-4d43-a1ba-a4f082cd96cf'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (18403, '89fb1534-a456-4992-b162-b64ff48680bb'), (5100, '0e2cf3f9-d5c7-4a37-9665-321547a73704'), (18414, '4bfd7618-6567-411a-9d84-140a5555b25d'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (11764, '1059a45c-506a-456a-829b-6dadf14c9b51'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (18429, '4f938c80-c628-4d41-b604-430b07615100'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Conclusion
Thanks to the last updates applied on the TensorFlow models in transformers, one can now easily deploy its models in production using TensorFlow Serving. One of the next steps we are thinking about is to directly integrate the preprocessing part inside the SavedModel to make things even easier.
-->

# Preprocess

[[open-in-colab]]

Before you can train a model on a dataset, it needs to be preprocessed into the expected model input format. Whether your data is text, images, or audio, they need to be converted and assembled into batches of tensors. 🤗 Transformers provides a set of preprocessing classes to help prepare your data for the model. In this tutorial, you'll learn that for:

* Text, use a [Tokenizer](./main_classes/tokenizer) to convert text into a sequence of tokens, create a numerical representation of the tokens, and assemble them into tensors.
* Speech and audio, use a [Feature extractor](./main_classes/feature_extractor) to extract sequential features from audio waveforms and convert them into tensors.
* Image inputs use a [ImageProcessor](./main_classes/image) to convert images into tensors.
* Multimodal inputs, use a [Processor](./main_classes/processors) to combine a tokenizer and a feature extractor or image processor.

<Tip>
Let's quickly go over each of these.

## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]

Like other neural networks, Transformer models can't process raw text directly, so the first step of our pipeline is to convert the text inputs into numbers that the model can make sense of. To do this we use a *tokenizer*, which will be responsible for:

- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*
- Mapping each token to an integer
- Adding additional inputs that may be useful to the model
```

With two basic classes - configuration and model - and an additional preprocessing class (tokenizer, image processor, feature extractor, or processor), you can create any of the models supported by 🤗 Transformers. Each of these base classes are configurable, allowing you to use the specific attributes you want. You can easily setup a model for training or modify an existing pretrained model to fine-tune.
- **Preprocessing classes** convert the raw data into a format accepted by the model. A [tokenizer](main_classes/tokenizer) stores the vocabulary for each model and provide methods for encoding and decoding strings in a list of token embedding indices to be fed to a model. [Image processors](main_classes/image_processor) preprocess vision inputs, [feature extractors](main_classes/feature_extractor) preprocess audio inputs, and a [processor](main_classes/processors) handles multimodal inputs.
 
Here is the question: What makes preprocessing tools essential for Transformer model deployment in multimodal tasks?","According to the context, preprocessing tools are essential for Transformer model deployment in multimodal tasks because they convert raw data (text, images, audio) into a format accepted by the model. Specifically, they:

* Split input into tokens, subwords, or symbols
* Map each token to an integer
* Add additional inputs that may be useful to the model
* Extract sequential features from audio waveforms
* Convert images into tensors
* Combine multiple modalities (e.g., text, images"
What is the most effective approach for fine-tuning Stable Diffusion models on Intel CPUs?,"[(4608, '11eba3f7-462f-400d-a1c6-26fbdecddb11'), (4609, 'b27a1438-b788-4f0f-8269-2c66f61be2cc'), (4611, '9ba60db2-6853-40a1-88bd-22525fbdb8e9'), (7184, '8450f91c-e0e1-4e31-99c8-57f8d43d9cdb'), (7187, '7558cc18-3214-4474-9cb7-a75d7841660d'), (10259, '04153c79-f234-46ea-8580-29e4c41994bd'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (12313, 'f97f9df8-f45a-4596-97a2-488e661d5c02'), (15898, 'b6f9af12-8c93-4907-8584-6063e7adc079'), (12316, 'b5c70710-fff2-4908-b500-962f6636a8b7'), (22051, 'fce832e0-65d6-4e6d-9f0d-d59fa33910fd'), (29732, '23e38ff1-32ee-4ec1-be96-ae1f89a1758f'), (7206, 'b78e1a91-c2d0-411c-ac86-3b8ef5a071ab'), (17456, '3d0b462d-4f6f-4e29-bdf7-d84ce48e8560'), (17466, '31a76609-9b66-4aeb-8d09-68b1fa0b5c33'), (7244, '285ab510-3ccc-4789-a279-59038db4de4c'), (26701, '3c9d16cf-eb89-4c9e-9e76-5bcbda107d24'), (590, '6523fa0b-894c-49e0-bfb1-fb3213623bf3'), (591, '06236243-318b-48f5-8a7d-cfb9b4f1e0d5'), (589, '5f6c042b-5281-483f-b557-84661e4b6441'), (7246, 'f2f24efc-81fd-40de-9450-3d9bc633abf7'), (13396, 'e9acadfa-60e3-4c46-a14b-5b042e105d94'), (30805, '8d9b07a6-4ba5-40b1-a223-4f12f9892100'), (9816, '5a10bbff-24a7-4d06-b2b9-87ca115873d4'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (9824, 'dd81f094-0a63-41f4-9d2d-7fcedd15ebb6'), (9825, '06605af9-18ab-4286-8c12-621fa24b91c4'), (14434, '88961ae2-f1af-4f1b-a627-43082e2f0710'), (14436, 'b6c8ba09-18f8-4b76-9196-bdead257e416'), (31332, 'a8c29ace-6057-4878-83d9-acf7b71dfe84'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (14440, 'e0ab7c36-83a2-4564-8607-0dac39b536e7'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (14452, 'd602c8f8-3830-4f9e-8b34-1bea97e99242'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (14454, 'ce330ae8-8dec-4720-92e6-052305cb40dc'), (3195, '8ad2c051-95b7-4e6d-ba37-44db801a9218'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (25753, 'f0cc6c3f-c034-466c-b926-2db0b6e9a1ce'), (9887, 'd038f03f-34b1-42f4-baae-413ab5287074'), (7337, 'e9d756c4-fba8-4487-a964-02c57d1ae176'), (17597, '8e21cdc2-2daf-4ce2-b812-2ad19cdb6870'), (19135, 'd743a82b-46bc-41fd-98b0-40e850c520b4'), (4299, 'fa200320-e0ea-4961-b45c-b5ec32991353'), (31439, '44a37647-64bf-46ad-8a21-01bb8a1c4465'), (31440, '5e11482a-df4d-4bc0-a32d-7c84ead3fd59'), (9429, 'f9b10760-2b6c-4577-999d-9ce25869b8f0'), (10965, '2c19dbd9-994e-4405-9a0c-080ceb10642d'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (28373, 'a0034ae6-5318-4803-a687-4e805fb8b9a0'), (28374, '2f02209a-dbaf-4432-b002-393b2b850745'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (10989, '6bcf6d91-b596-48cd-903e-99802046e818'), (30455, '30b49335-6092-4e0b-876d-b396ad058840'), (6412, 'e491859b-0c3a-451c-8d54-e5536a56bf5f'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (23841, '5a99e4d4-7fec-4135-b408-ad9cae391ea4'), (26415, 'c36eb535-fc79-44b4-b747-4cf29cc693f2'), (26419, '0d675c36-bbb5-4d87-b0d5-ffe3d031d163'), (19251, '3f20e20d-4101-430d-b551-6b12b35c9220'), (10549, '86189e5e-3a32-427c-99ad-35cdc4bb8c32'), (19252, '85ba61bf-4c88-4d69-be0f-e94bd8341ed7'), (19256, '4307afee-c6a2-4b98-9213-6ad94155d798'), (19257, '58d4e9d1-772e-467c-bc2d-0eec151aeb17'), (26427, 'd5213d1c-651a-420e-9353-5b752301fe26'), (19267, 'eb0adcf3-cd17-4212-8ded-7cd8bae7bff4'), (19268, '05bebd21-659f-4279-85cb-8a5b154f6c47'), (19269, '2b788324-03f3-4218-95db-15f2077f16ff'), (26438, 'f329b776-81fd-470b-b8e7-ab77e1d208d0'), (3403, '9e1528a7-9061-4d1d-9764-ab5241585e99'), (30540, '8ec133e4-ff69-46a6-98b7-6047e81da377'), (22353, '466fdc29-f7dd-49cc-97b0-8676ca1a4540'), (10579, 'c5c569a7-6ff7-430d-adbd-b77fe1021f0d'), (5975, 'c2372c16-a0f7-4f31-b095-79df0d949f6e'), (5976, 'e8e3e65c-67a5-40a4-9e0e-3764eba99f00'), (26458, '2fac6939-29e7-473f-b941-a99b086bc5f3'), (5983, 'd398359b-b46c-4efd-978a-507676dbbea2'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (4462, '14e529ce-e387-47f4-9d38-a454c23e8bd7'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (30579, '9ca0751a-33ff-4f8b-a7ee-49343630ad20'), (6008, 'f1ddc133-b500-4631-9de5-add5ec3ca0f4'), (13178, '51262099-96cc-44dc-97e7-e541341cac30'), (26498, '8d241fd8-6c1c-4c2f-a561-694ae53eaa19'), (14212, '4b03c79b-3dd8-400a-a8d2-e793ad43770b'), (6024, '597afa6c-ee7a-4d5b-b5fb-087459f615a7'), (9621, 'b886005d-bf70-423c-ac80-118efe1f9377'), (3992, 'fb9ea9cb-6e33-4d1f-88ab-053278f81c3f'), (3997, '2662a32f-d93d-48c2-afe0-d243cab54e4c'), (3998, 'd69ff847-eb07-4bc3-8707-899d59ed3e9b'), (26528, '5f55bea4-0eee-4351-981a-0f1c903294d0'), (14753, '02332114-d3f0-4ffd-906c-68000226fe4f'), (4001, '180edc1d-3c2c-4683-a38c-b46c34f4374f'), (26536, 'ae57f721-abf1-4573-993d-835edf9caae5'), (26539, '39219aeb-aba4-483d-a93b-242ca1890612'), (26540, '313630db-d36c-4654-8dfa-112a43b9691f'), (4012, 'b1e161a0-d490-4161-880d-9be316427ee3'), (17344, 'dd623682-ea28-458a-8129-4731e7425a06'), (17346, '2c91cfdd-1757-4160-99b2-4ef422291d93'), (8650, '19f4e06e-36ef-41c6-bd20-a5e373e02f13'), (15315, '7e744e9a-2d02-41eb-8d2f-acb54e6336bf'), (8661, '99dfdb9b-a473-4c0c-b57a-abdd28b4582f'), (13281, '98202400-facb-40a9-b51c-38ac20101498'), (10212, '0d28df65-2cae-4253-8b4c-c834f7ec7e1f'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b'), (10222, 'a13b8c95-cbad-4812-ad41-a22be173a110'), (13296, '1deb15a7-660c-4670-836e-ea98398cf41b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: --
title: ""Fine-tuning Stable Diffusion models on Intel CPUs""
thumbnail: /blog/assets/stable-diffusion-finetuning-intel/01.png
authors:
- user: juliensimon
---

# Fine-tuning Stable Diffusion Models on Intel CPUs



Diffusion models helped popularize generative AI thanks to their uncanny ability to generate photorealistic images from text prompts. These models have now found their way into enterprise use cases like synthetic data generation or content creation. The Hugging Face hub includes over 5,000 pre-trained text-to-image [models](https://huggingface.co/models?pipeline_tag=text-to-image&sort=trending). Combining them with the [Diffusers library](https://huggingface.co/docs/diffusers/index), it's never been easier to start experimenting and building image generation workflows.
--
title: Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum
thumbnail: /blog/assets/train_optimize_sd_intel/thumbnail.png
authors:
- user: AlexKoff88
  guest: true
- user: MrOpenVINO
  guest: true
- user: helenai
  guest: true
- user: sayakpaul
- user: echarlaix
---

# Optimizing Stable Diffusion for Intel CPUs with NNCF and 🤗 Optimum


[**Latent Diffusion models**](https://arxiv.org/abs/2112.10752) are game changers when it comes to solving text-to-image generation problems. [**Stable Diffusion**](https://stability.ai/blog/stable-diffusion-public-release) is one of the most famous examples that got wide adoption in the community and industry. The idea behind the Stable Diffusion model is simple and compelling: you generate an image from a noise vector in multiple small steps refining the noise to a latent image representation. This approach works very well, but it can take a long time to generate an image if you do not have access to powerful GPUs.
If you enjoyed reading this post, you might also be interested in checking out [this post](https://huggingface.co/blog/stable-diffusion-inference-intel) that discusses other complementary approaches to optimize the performance of Stable Diffusion on 4th generation Intel Xeon CPUs.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
--
title: ""Accelerating Stable Diffusion Inference on Intel CPUs""
thumbnail: /blog/assets/136_stable_diffusion_inference_intel/01.png
authors:
- user: juliensimon
- user: echarlaix
---

# Accelerating Stable Diffusion Inference on Intel CPUs



Recently, we introduced the latest generation of [Intel Xeon](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html) CPUs (code name Sapphire Rapids), its new hardware features for deep learning acceleration, and how to use them to accelerate [distributed fine-tuning](https://huggingface.co/blog/intel-sapphire-rapids) and [inference](https://huggingface.co/blog/intel-sapphire-rapids-inference) for natural language processing Transformers.

In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.
 
Here is the question: What is the most effective approach for fine-tuning Stable Diffusion models on Intel CPUs?","Based on the context, the most effective approach for fine-tuning Stable Diffusion models on Intel CPUs is by utilizing the Intel Advanced Matrix Extensions (AMX) hardware accelerator for deep learning workloads, which is introduced in the fourth generation of Xeon CPUs, code-named Sapphire Rapids. This approach allows for fine-tuning Stable Diffusion models on CPU infrastructure, which was previously only possible on GPU infrastructure."
Which preprocessing strategy ensures robust ASR performance for real-world data?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (21560, '6de334f4-432b-4f38-86da-f421d030fb94'), (3128, 'cd116de5-9c26-431b-aacf-ee4e4b6e9331'), (8249, 'dfc301a7-42d9-4f57-a47c-176468d533ec'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (2647, 'a9b33316-4844-4d79-ace2-5d36fae71281'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (3679, '8e31b4d7-de14-44b3-be8a-7236eb150661'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (19044, '59dbc5e3-e3e8-4301-b9da-90a44c2cec46'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (4730, '1588743f-8472-4fdd-abe0-3ab3a94511ca'), (11901, '9b1f45ad-b8f1-42c4-9a6c-4e7607042fc4'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22144, '53697ae7-3817-4e34-ac41-c9d6703921e2'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22662, 'f32c3a87-abbc-4c09-a640-bf8926fe8342'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (16525, 'cf8c49ca-65af-4760-8f3c-6b497bee892c'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (22696, '47eb8a13-1c6a-4060-a807-7eb65a6df466'), (22699, '4f2bf23c-4a1f-42ce-8fcd-57d8b79aa246'), (11436, '4172866a-733a-4be4-b5ef-5247aa26dd70'), (11437, 'bcf6c108-66d8-4c82-8e0b-90a0f98d8602'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (9911, 'df3b7cf0-0e9c-474e-a7b8-0c3edb8eeb91'), (25785, 'cc8b2b20-c91a-4342-b3f7-0f6232628ce7'), (9915, 'ab8362c6-5cad-4fda-95ef-e059662aa711'), (9917, '04f2c05a-0905-4d27-b736-52553615786f'), (29374, '221cd942-7551-4df5-9a02-d0f73e055431'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (9920, 'e21cc79a-6c15-430d-8348-e9cf3e43374d'), (17087, '89258315-ff35-47ea-8f5d-62939612d015'), (21192, '33a1caa5-4ef6-4cf7-ad0c-79c8a5bca80d'), (9933, '361b3753-2f11-40ad-a8b0-91570e57b5d7'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (31473, '107a8b8a-6308-41aa-a924-46e82911e1bc'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (7946, 'dca97276-bfd9-4ea5-8570-0a1d528b911a'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (6421, 'b8aebff7-309e-4b44-a96e-026ae78e518f'), (26390, '7e9534d9-70b4-497b-9fae-b424e5d8e1a3'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (26930, '754b4dd4-7bf8-4d66-b302-6b736ac16f61'), (4916, 'eef334ce-c96c-41d6-8320-5bb2ac306f38'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (28472, '67899330-cf36-437d-850b-e81e37b4cfca'), (10552, '19f613db-4203-4d96-a744-9369443faac9'), (10555, 'bfe63e09-0fbc-492c-a580-7b3d9053592e'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28485, '90b822fb-e259-470d-9f26-b588fed88f91'), (12104, '91971395-1e61-4f8d-bc69-6e8b6191c233'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (19815, 'e594d514-fad1-4a8e-8e1c-6e918e2e11fc'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (18818, 'a938f2f4-0d19-4f1e-bd3e-858ce23a0d1d'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (3978, 'd33f16e0-da67-463b-9ea2-f6f1df110c41'), (13201, 'f0b65266-be39-49d4-9757-564e662bb018'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (13217, '1defdd33-80ca-4389-a1ab-e9ed401b4919'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14782, '1cc646cf-ac7b-4712-a786-9905b3365b13'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ---

---
**NOTE 2**

When training a model on large datasets it is recommended to run the data preprocessing 
in a first run in a **non-distributed** mode via `--preprocessing_only` so that 
when running the model in **distributed** mode in a second step the preprocessed data
can easily be loaded on each distributed device.

---

### Demo

In this demo run we pre-train a `""base-sized""` Wav2Vec2 model simply only on the validation
and test data of [librispeech_asr](https://huggingface.co/datasets/librispeech_asr).

The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available 
per device, consider reducing `--batch_size` and/or the `--max_duration_in_seconds`.
### Preprocessing
Graph transformer frameworks usually apply specific preprocessing to their datasets to generate added features and properties which help the underlying learning task (classification in our case).
Here, we use Graphormer's default preprocessing, which generates in/out degree information, the shortest path between node matrices, and other properties of interest for the model. 
 
```python
from transformers.models.graphormer.collating_graphormer import preprocess_item, GraphormerDataCollator

dataset_processed = dataset.map(preprocess_item, batched=False)
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
```

## Preprocess the data

To finetune a model, you must preprocess the data you plan to use to match precisely the approach used for the pre-trained model.
[`AutoImageProcessor`] takes care of processing image data to create `pixel_values`, `pixel_mask`, and
`labels` that a DETR model can train with. The image processor has some attributes that you won't have to worry about:

- `image_mean = [0.485, 0.456, 0.406 ]`
- `image_std = [0.229, 0.224, 0.225]`

These are the mean and standard deviation used to normalize images during the model pre-training. These values are crucial
to replicate when doing inference or finetuning a pre-trained image model.

Instantiate the image processor from the same checkpoint as the model you want to finetune.

```py
>>> from transformers import AutoImageProcessor

>>> checkpoint = ""facebook/detr-resnet-50""
>>> image_processor = AutoImageProcessor.from_pretrained(checkpoint)
 
Here is the question: Which preprocessing strategy ensures robust ASR performance for real-world data?","The context does not provide a direct answer to the question. It discusses various preprocessing strategies for different models (Wav2Vec2, Graphormer, and DETR) and datasets (librispeech_asr), but it does not explicitly state which preprocessing strategy ensures robust ASR (Automatic Speech Recognition) performance for real-world data."
How can multitask models improve speech recognition systems?,"[(18432, '7458df8c-e69b-43d8-a456-b53c44af4802'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (27145, '79d5186a-9f88-4b85-8e03-efaf80d1bc88'), (18441, 'cfa471d2-0b8d-475e-9d32-c7873eeb92f8'), (9744, 'ee479e38-312b-4563-958c-33ce1e0eedea'), (5139, 'ba7b9456-398d-4fbc-85d5-361dab3a6bdb'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (27180, 'd501116e-8c5c-40b0-b3af-1c259beca630'), (14925, '3646284b-3c56-4876-8332-253857a7160a'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (23128, 'faa6835d-5dca-4c7b-8505-f81cd74fb0b4'), (1624, '9c6c249c-815c-471a-9fbc-0b8c0c71e013'), (19034, 'a67ab03c-550b-4e68-a76c-aaff05b1523e'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (25193, '03a63ec7-afd1-45dd-acea-f1087aadab69'), (13931, '65851a09-218a-4a91-986b-2694bc577b97'), (19075, '131641af-b7be-4b3a-a31a-a76b816e3822'), (19076, 'c3f9c5df-c349-46ea-932a-eead03777122'), (22149, '9e6c02a3-618a-4a76-89f9-9b09c5630680'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (26255, '30372f00-e006-40e8-9ba2-91e9ccbec1b7'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (20644, '35d54144-39c9-4e31-b7cc-fa6416b43dcb'), (9901, '3fb89436-7a71-469f-963f-9170d1e95727'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (20660, '36d46a61-da08-4a79-b974-9b8f75c6d8d9'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (9928, '5c999e86-a8bc-40b7-aea5-7bf657e92285'), (21198, '9468cc16-d5f1-4d2f-b79a-15502e9e9fee'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (10450, '3d06676d-85ad-4fa3-965c-e9d2ff2a64d7'), (22229, '385e26d9-764d-40bb-b865-6ebcbef59dd0'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (4839, '8b5fcfe4-422d-41ad-ba2a-435d4859c7ef'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (6900, '0ae7974c-5457-40e4-a353-fde028bee870'), (26356, 'bd637992-bac8-476e-a6c7-09fa000bfe99'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (7933, '6347e581-ebbe-416a-b695-d3ee9376acb3'), (18174, 'f2cacc2f-d7e9-4f62-b292-98c6073da18d'), (30974, '67eb48a7-3c1c-4d73-8942-e82695fd2008'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (1811, 'f4e9b143-1453-475d-a8a1-593ae0511e73'), (13591, '418bd9e7-11b8-4f5b-85ea-39f2266eb907'), (18200, 'c90b132b-e7b4-4305-99ea-9c6f13711dd2'), (29468, '17fc845b-a10d-41de-978b-b91257cc0ada'), (18209, '3b3b2f22-3c8a-4356-b74e-c918809e7457'), (290, 'fb7ae342-9f29-473e-9ab5-9270648dd176'), (18216, 'de3c35a7-8d3e-4fcf-9068-350d1ac93944'), (28974, 'fe18f316-73e6-4400-8fc8-7b7c1e2d79ba'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (13617, '5d6f1844-a996-4b2b-9cce-54cc40faf578'), (23859, 'e47fd052-4097-479b-95f3-f65bc65a3c2c'), (23860, 'afa6eb69-b861-463e-94f7-a8cffe60467e'), (23865, '7bccc135-749d-4acc-a179-12c10e7b97b3'), (13626, '1256a71c-2ece-41bf-ba82-dc791b09e6c6'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (316, '3d68da03-3df0-4ea4-8d35-55ca5e6bf70e'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (13633, 'a2b91e02-6811-46c3-bf1a-994b48b51e25'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (325, '05618000-706f-40ed-8186-9da01584ee2a'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (332, 'be91d83d-d7eb-4f95-bba3-9a350b653a2d'), (13646, '3afe34a0-b03d-44b2-91e9-71e637ccc48c'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13661, '70fe1907-eadd-4d2b-b4b9-aec545b32395'), (22877, '79c5fcb1-22aa-4ce1-be25-33a02d2b8e03'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (17774, 'e13cc263-c072-4a3b-9f64-7b688f1c79bf'), (18802, '4c9381c5-5784-4e2c-acc4-8d6a5ab6aa52'), (18804, '313d5a47-6d9e-49a4-9bd5-4098e9462ef6'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (16249, '4d8818a6-4f0d-4f5a-a903-5049462b1fc8'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (8065, 'ad6fcc55-b711-476c-9874-0eb440cca8ba'), (18831, '73dccec5-c809-4615-b089-5d0e22f17cb5'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (18844, 'c97c2cdd-44fb-4531-87d5-afef8f19b517'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (18850, '1731352e-cdab-4fa7-8131-b488d0649c92'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (15268, 'c0f4cbd2-9369-47e9-9c6f-ed3053963c1a'), (15267, 'c893dfbc-198a-4578-9809-345571353a75'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (9148, '9a67e291-a5dd-4957-8d10-703a415fccb6'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (4041, '2284abda-6ee7-4523-bc73-621c8b8df07b'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (29659, '5a15e754-3b25-4dce-88a6-79573d35cf5d'), (13276, '4ced3c99-7d08-497e-bd27-b107bd85a0a6'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (15870, 'aa4dffab-9c16-4523-8b50-2bde7016c29f'), (14317, '8c6ac79e-1f72-496b-a40f-fc1c956bf3eb'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (21502, 'c2163ba2-004a-43ab-91b4-058e6123a72f'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The performance of speech recognition system can often significantly be improved by adding a 
language model for decoding. For more information on how to add a language model, please 
take a look at [How to combine n-gram language models with speech recognition models](#how-to-combine-n-gram-with-model).
*Expanding the language coverage of speech technology has the potential to improve access to information for many more people. 
However, current speech technology is restricted to about one hundred languages which is a small fraction of the over 7,000
languages spoken around the world. 
The Massively Multilingual Speech (MMS) project increases the number of supported languages by 10-40x, depending on the task. 
The main ingredients are a new dataset based on readings of publicly available religious texts and effectively leveraging
self-supervised learning. We built pre-trained wav2vec 2.0 models covering 1,406 languages, 
a single multilingual automatic speech recognition model for 1,107 languages, speech synthesis models 
for the same number of languages, as well as a language identification model for 4,017 languages. 
Experiments show that our multilingual speech recognition model more than halves the word error rate of
The abstract from the paper is the following:

*We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.*

This model was contributed by [Arthur Zucker](https://huggingface.co/ArthurZ). The Tensorflow version of this model was contributed by [amyeroberts](https://huggingface.co/amyeroberts).
The original code can be found [here](https://github.com/openai/whisper).

## Usage tips
Streaming mode can take your research to the next level: not only are the biggest datasets accessible to you, but you 
can easily evaluate systems over multiple datasets in one go without worrying about your disk space. Compared 
to evaluating on a single dataset, multi-dataset evaluation gives a better metric for the generalisation 
abilities of a speech recognition system (_c.f._ [End-to-end Speech Benchmark (ESB)](https://arxiv.org/abs/2210.13352)). 
The accompanying [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/audio_datasets_colab.ipynb) 
provides an example for evaluating the Whisper model on eight English speech recognition datasets in one script using 
streaming mode.
*In this paper, we propose a unified pre-training approach called UniSpeech to learn speech representations with both
unlabeled and labeled data, in which supervised phonetic CTC learning and phonetically-aware contrastive
self-supervised learning are conducted in a multi-task learning manner. The resultant representations can capture
information more correlated with phonetic structures and improve the generalization across languages and domains. We
evaluate the effectiveness of UniSpeech for cross-lingual representation learning on public CommonVoice corpus. The
results show that UniSpeech outperforms self-supervised pretraining and supervised transfer learning for speech
recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively (averaged over all
testing languages). The transferability of UniSpeech is also demonstrated on a domain-shift speech recognition task,
i.e., a relative word error rate reduction of 6% against the previous approach.*
 
Here is the question: How can multitask models improve speech recognition systems?","According to the context, multitask models can improve speech recognition systems by leveraging self-supervised learning and phonetically-aware contrastive learning, which can capture information more correlated with phonetic structures and improve generalization across languages and domains. This is demonstrated by the UniSpeech approach, which outperforms self-supervised pretraining and supervised transfer learning for speech recognition by a maximum of 13.4% and 17.8% relative phone error rate reductions respectively."
What key advantage does Intel Sapphire Rapids hardware provide for AI model fine-tuning?,"[(4608, '11eba3f7-462f-400d-a1c6-26fbdecddb11'), (10250, '3d60dd94-162e-41c3-b924-86cd1b72a0fc'), (1044, 'df1940f9-0e38-49a0-8661-82e045e7bc01'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (14873, '85d82d32-dcbc-4c5e-ac7b-70e15da6671e'), (1050, '3dd7da70-bcad-4e0f-8472-4305b11553cb'), (25113, 'ef062ca9-a818-427d-a215-12c8b19769be'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (28749, '1e60f874-1e0f-4209-8dc6-28b79fc8904c'), (28752, 'a8f3fc54-9b72-46a0-b4ac-29da5374d20e'), (28754, '2f370cf6-442c-48bd-a56c-8301e99110f6'), (2643, 'd3577235-2eda-4e17-a9e6-7cdf05401f5b'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (9824, 'dd81f094-0a63-41f4-9d2d-7fcedd15ebb6'), (2655, '19d73693-6464-4f45-b475-f486597c0c5d'), (5729, '8325f704-8f9e-470a-9d07-777ab1401244'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (5738, '1b3f6207-1eaf-4d30-b7f5-15ce5479c98a'), (9839, 'e9ad9e68-4227-4504-b534-090ee022062d'), (3190, '9ac364e4-812c-4816-b2b7-cd39501c3080'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (4216, '6e6bac64-ea1d-4856-b079-4679e05d1607'), (4217, '1a8e3437-ea12-4e7b-b9d9-6929a9a940d2'), (4225, '8a889917-10a2-425c-8ce5-9a4d7cf16589'), (4226, 'f995a19f-7e5a-4028-a50a-315c452620ee'), (4227, 'd9a4a5bd-4e54-4a56-8292-9967b567d5d8'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (22163, '8d167b0c-14d6-4205-a383-f37c9f717a64'), (21144, '733daaa7-2d91-4c46-b007-fe08a12ee437'), (21145, '8f8f780e-b116-4a49-a443-62c3ed772cb9'), (21146, '52b3b9bb-9ff0-4cbb-85ea-66902b53e6a6'), (21147, 'dd1291ef-03a7-4a6a-91f6-3ea791acefdb'), (9882, '3d63264c-0ee7-49b5-ae7c-5dc424d9d8c7'), (5806, '5054cac8-6958-4df8-9986-f0bb2157877f'), (22205, '2c941caf-5007-4276-8c86-d5eeff359547'), (6356, 'fd9611a0-cae2-4941-8609-f6440c2b0285'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (10973, '5d6db59d-1ca5-4c84-b8eb-9d478226c00c'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (26855, 'daaa5d9b-d90e-4dcb-b7be-640944232958'), (10989, '6bcf6d91-b596-48cd-903e-99802046e818'), (22775, '92b448c2-17a3-48db-9679-dfae336f0e2f'), (22779, '1b8ccb79-44ab-4d6d-b222-a9f1fd2244d4'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (18694, '14ec5ba0-bd18-4aa7-a57f-338402d25194'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (16660, '1dfbc513-b29b-4f26-ac16-6bccdb48a3d4'), (2836, 'dad2f3b4-337f-49e3-ae8a-2714f6bb79e0'), (25882, 'd29c1150-e982-47d6-af8d-81cbe1722e94'), (25883, 'f01a6bd7-35e0-4d59-b1e1-8879b0662972'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (9562, 'b75f1239-64b9-4c25-9106-2b9ff2961471'), (9563, '211904bc-1535-4038-9e7d-c897425b8bcc'), (23903, '923c4027-8d23-4edc-9d3e-daf70d4b077b'), (9567, '21f12568-4fc0-4680-b763-b7907ea0ea81'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (20322, '2ac8515a-2656-4500-b62a-859ab1fe9812'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (3941, 'a871cd71-4386-4492-ae52-7dda1294b854'), (3942, 'b94b5ec5-7ed0-4a6a-abb0-7f8bbc3cc4c3'), (3943, '3eb2b5e6-fc2b-4577-8b18-e8633255362d'), (3944, '4951a0e6-56b4-42c8-855e-2698bcf9658f'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (23928, 'fa312533-bb24-4e27-982f-befb97bafd49'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (23930, 'b54bef38-4510-49ab-87a1-dbc20ba90085'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (7062, 'c8c890e1-e7b8-4e8f-9799-111728d14f27'), (3993, '193bb31e-a1b5-4129-bd41-177f89121efd'), (7065, '0fb31b29-d4f9-4207-9934-c683259bbb1a'), (5535, 'ab3d50f8-003b-47e8-8bde-07e4ffe97423'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (6068, 'e7eed39e-8383-419a-ad3b-0f3947c404b9'), (12725, '69c1bca3-0390-4f32-82e9-9812d1bc36a6'), (19383, '44c8fa9d-7167-4105-9431-707875941cb8'), (19384, '75f513c0-3b52-4f83-aec6-7b436161c162'), (29112, 'f5df6a53-0df1-4420-9043-3d9ba923f41b'), (29114, '131eb762-91f6-4098-b8e1-c638fb32b6eb'), (19387, 'fd8aa818-6add-4bbf-a661-88e05a30e489'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (19390, 'c0f4fdad-c5e6-4511-aaf0-4cc870c059c1'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11711, 'f0aab09f-a3dc-496a-9cd8-08e8d39a1306'), (11713, '16213aad-a177-42a5-8be6-a4c1224c45d0'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19398, 'c99ed4d1-a0f8-47fc-9f4d-4867015d3211'), (19399, '7f318cf3-2841-4ac1-9fff-8943985050c9'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (11725, 'c643f493-d651-49b0-8fcb-81b52c34618a'), (11727, 'f0c4c308-23bc-4116-8f86-1ad541066b86'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (10734, 'ea64a246-2278-46a1-b73a-0035a319376d'), (31735, '31b3ae48-f7ec-45ee-adb9-0d7d5e3541ca'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (31740, '7f15437c-296d-44c3-89c4-94654b268fc9'), (31741, '3cb8440b-d4b0-4bd5-930d-1fc4596f3292')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In a follow-up post, we'll look at inference on Sapphire Rapids CPUs and the performance boost that they bring.

## Why You Should Consider Training On CPUs

Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach, especially when using techniques such as distributed training and fine-tuning on small and medium datasets. 

Xeon CPUs support advanced features such as Advanced Vector Extensions ([AVX-512](https://en.wikipedia.org/wiki/AVX-512)) and Hyper-Threading, which help improve the parallelism and efficiency of DL models. This enables faster training times as well as better utilization of hardware resources.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cluster. We will use [textual inversion](https://huggingface.co/docs/diffusers/training/text_inversion), a technique that only requires a small number of example images. We'll use only five!

Let's get started.

## Setting up the cluster

Our friends at [Intel](https://huggingface.co/intel) provided four servers hosted on the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html) (IDC), a service platform for developing and running workloads in Intel®-optimized deployment environments with the latest Intel processors and [performance-optimized software stacks](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html). 

Each server is powered by two Intel Sapphire Rapids CPUs with 56 physical cores and 112 threads. Here's the output of `lscpu`:
--
title: ""Accelerating Stable Diffusion Inference on Intel CPUs""
thumbnail: /blog/assets/136_stable_diffusion_inference_intel/01.png
authors:
- user: juliensimon
- user: echarlaix
---

# Accelerating Stable Diffusion Inference on Intel CPUs



Recently, we introduced the latest generation of [Intel Xeon](https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable.html) CPUs (code name Sapphire Rapids), its new hardware features for deep learning acceleration, and how to use them to accelerate [distributed fine-tuning](https://huggingface.co/blog/intel-sapphire-rapids) and [inference](https://huggingface.co/blog/intel-sapphire-rapids-inference) for natural language processing Transformers.

In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.
In this post, we're going to show you different techniques to accelerate Stable Diffusion models on Sapphire Rapids CPUs. A follow-up post will do the same for distributed fine-tuning.

At the time of writing, the simplest way to get your hands on a Sapphire Rapids server is to use the Amazon EC2 [R7iz](https://aws.amazon.com/ec2/instance-types/r7iz/) instance family. As it's still in preview, you have to [sign up](https://pages.awscloud.com/R7iz-Preview.html) to get access. Like in previous posts, I'm using an `r7iz.metal-16xl` instance (64 vCPU, 512GB RAM) with an Ubuntu 20.04 AMI (`ami-07cd3e6c4915b2d18`).

Let's get started! Code samples are available on [Gitlab](https://gitlab.com/juliensimon/huggingface-demos/-/tree/main/optimum/stable_diffusion_intel).

## The Diffusers library
 
Here is the question: What key advantage does Intel Sapphire Rapids hardware provide for AI model fine-tuning?","The key advantage that Intel Sapphire Rapids hardware provides for AI model fine-tuning is the introduction of the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads."
What is the primary benefit of combining n-gram models with acoustic models?,"[(21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (27140, 'e9b6a701-73ea-4220-95c6-b35ffe9aab01'), (9733, 'd8a9b090-d24d-4517-9a0b-03d326562952'), (21508, '191e2549-6667-4c43-839c-7668501e6123'), (9732, 'f2cb19d0-287c-46cb-b4f1-4824b76de53d'), (5129, 'fe8e6696-6fa6-46f6-a4a4-fe42849fe601'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (25106, 'd6b3482e-cc20-4f76-8c36-6add4edab0ef'), (9747, '362139c4-a349-4533-8add-38dc8c990f5d'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (9781, '7ac12bf7-06ea-480d-8a8c-dacfed7567c8'), (4166, 'd62d2dfe-88e9-4269-a3b8-a0c08a9ead60'), (19033, 'b32a6a6b-73de-4fa7-9615-58c2fbfe7015'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (16481, 'b0dc2bb0-6109-4207-b134-65c39511f54e'), (16483, '23532c2b-7b32-489c-8f93-99a40c321cfa'), (16488, '82e6921b-4f0a-43f7-af6b-e88247f9a075'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (19055, '03f15b1f-1ff6-4bc2-90aa-91b49ccf44ca'), (11901, '9b1f45ad-b8f1-42c4-9a6c-4e7607042fc4'), (19074, '2660675c-e9c3-4a2b-97a5-b40971554f7f'), (19075, '131641af-b7be-4b3a-a31a-a76b816e3822'), (19076, 'c3f9c5df-c349-46ea-932a-eead03777122'), (14981, '4b85fc82-f5ca-4436-82ce-5b7b3f15ac05'), (1671, 'b6fe0d87-4506-41f4-86d6-8a5340995e6e'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (16017, '0159b9c3-ea7e-41d0-928e-fce397c3adfe'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (15002, 'da4ee92f-fbca-42a2-9700-0788b253e4d5'), (3748, '332644a1-eafc-4d25-b177-40728dd8f84b'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (15028, '86c91e1c-d5df-4053-b7f8-f475c47edcf1'), (9911, 'df3b7cf0-0e9c-474e-a7b8-0c3edb8eeb91'), (15032, 'a5fd305b-6bf8-4f2d-b724-080da4c84f02'), (16055, '1c40cffb-366c-48a0-b2e8-3d3a31a0feda'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (15041, '043fa5bf-448c-493a-a908-29384cfeee7c'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (9928, '5c999e86-a8bc-40b7-aea5-7bf657e92285'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (17098, '806472d6-0d89-45f3-95ba-14638f4f5a85'), (21192, '33a1caa5-4ef6-4cf7-ad0c-79c8a5bca80d'), (21198, '9468cc16-d5f1-4d2f-b79a-15502e9e9fee'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (15056, '3aeddfc0-2bd8-44a0-8d5d-def75c612a76'), (720, 'aa7dece3-4d93-444e-a7e0-ff84a5130dd3'), (25307, 'a1dddf5c-6872-4923-9d66-941863c5e2e6'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26336, '33929add-1187-4dc3-af43-15f77a3c8c46'), (21217, '443fa618-da4f-4d8a-8e7c-9cb52e53395b'), (25321, 'b7befe0e-86b5-4603-8086-b358c0606f7c'), (31467, 'd5b8cb99-b5ce-4df9-8ab2-5c8ebaf7db26'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (17646, 'aedaad43-5e00-41ba-9c23-9672db5835b2'), (26356, 'bd637992-bac8-476e-a6c7-09fa000bfe99'), (6907, '1be7a8dc-407e-49e2-b978-d3551d21c091'), (26363, '7229a8f2-836b-4249-a128-0c3a3a567ef6'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (10510, '65932cde-592e-47aa-b229-26bebb986d68'), (13616, '3178abac-e5fc-46a5-85f4-80b7c2a42e37'), (26930, '754b4dd4-7bf8-4d66-b302-6b736ac16f61'), (5429, '4cc6156d-79ac-4b12-87d5-ef4b1cc6f55b'), (23864, 'a2c61a52-5e46-47ca-875a-6b4cc5921ba4'), (23865, '7bccc135-749d-4acc-a179-12c10e7b97b3'), (315, '4802f533-c061-485f-ab42-466a2409998b'), (28993, 'e94a1fa2-5915-4591-a20a-5ae790985a23'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (13648, '5af9b7d4-1b8a-436a-bf80-acbd7647fec7'), (13649, 'fe70249f-ab16-426a-9018-41aa8300b852'), (15186, '408a5ea1-a358-4b34-bce8-9381f63fae97'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13661, '70fe1907-eadd-4d2b-b4b9-aec545b32395'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (14180, '9e0c93a3-1a55-461d-9b74-e0ef9f22cae2'), (13669, 'fd5f73cd-31c4-4ed9-a912-115b0d44f3b5'), (13670, '2770a364-f587-45a8-89d3-153b4447c5a3'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (18804, '313d5a47-6d9e-49a4-9bd5-4098e9462ef6'), (10615, '7bf25661-c8bb-4b65-9e16-786afc1051f8'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (13190, '8b1d3ff6-345b-42e3-be4a-562d7b46bb0b'), (8072, '75702fbf-8f70-4667-ade7-37ec084bfc8b'), (13195, 'd9d25c33-f225-4c1f-80aa-eca578fcd3fb'), (18838, '76831eb6-5d71-4001-b374-7a7d4441f632'), (18845, '03dfc9be-b19e-496f-b177-54e25eeb2db3'), (13217, '1defdd33-80ca-4389-a1ab-e9ed401b4919'), (18850, '1731352e-cdab-4fa7-8131-b488d0649c92'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (15268, 'c0f4cbd2-9369-47e9-9c6f-ed3053963c1a'), (15267, 'c893dfbc-198a-4578-9809-345571353a75'), (9138, 'f8699643-8b4c-4ab0-b99d-d868f6a6f1bf'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (27072, '5557dbf2-8acf-4d5d-8381-ece6fc992f43'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (14801, 'dbcd48d3-a25b-4373-804a-62a5ab0e027e'), (14802, '148eaad9-71ab-478b-b6d7-1e16ddec31d1'), (29659, '5a15e754-3b25-4dce-88a6-79573d35cf5d'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (14317, '8c6ac79e-1f72-496b-a40f-fc1c956bf3eb'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ------------------------------------------------------------------------

\\({}^1 \\) Some research shows that a model such as
`facebook/wav2vec2-base-100h` - when sufficiently large and trained on
enough data - can learn language modeling dependencies between
intermediate audio representations similar to a language model.

Great, now that you have seen the advantages adding an *n-gram* language
model can bring, let's dive into how to create an *n-gram* and
`Wav2Vec2ProcessorWithLM` from scratch.

## **2. Getting data for your language model**

A language model that is useful for a speech recognition system should
support the acoustic model, *e.g.* Wav2Vec2, in predicting the next word
(or token, letter) and therefore model the following distribution:
Looking again at Table 9 of Appendix C of the [official Wav2Vec2 paper](https://arxiv.org/abs/2006.11477), it can be noticed that using a *Transformer*-based LM for decoding clearly yields better results than using an *n-gram* model, but the difference between *n-gram* and *Transformer*-based LM is much less significant than the difference between *n-gram* and no LM. 

*E.g.*, for the large Wav2Vec2 checkpoint that was fine-tuned on 10min only, an *n-gram* reduces the word error rate (WER) compared to no LM by *ca.* 80% while a *Transformer*-based LM *only* reduces the WER by another 23% compared to the *n-gram*. This relative WER reduction becomes less, the more data the acoustic model has been trained on. *E.g.*, for the large checkpoint a *Transformer*-based LM reduces the WER by merely 8% compared to an *n-gram* LM whereas the *n-gram* still yields a 21% WER reduction compared to no language model.
The performance of speech recognition system can often significantly be improved by adding a 
language model for decoding. For more information on how to add a language model, please 
take a look at [How to combine n-gram language models with speech recognition models](#how-to-combine-n-gram-with-model).
**Note**: If you're an experienced Docker user, feel free to create a custom docker image with all of the needed packages 
like the one in step 5. The Dockerfile for it is available here: 
[baaastijn/Dockerimages](https://github.com/baaastijn/Dockerimages/tree/main/Hugginface_challenge_speech).
Once you've built your image, push it to https://hub.docker.com/ and select it during the OVHcloud job creation.

For more quick tutorials about OVHcloud AI products, check out the showcase https://vimeo.com/showcase/8903300

## How to combine n-gram with acoustic model

Having trained a speech recognition model with CTC as shown in the section above, 
one can further improve the model's performance by adding an **n-gram language model**
to the decoding process of the model. By doing so, we are replacing the naive greedy decoding 
with **n-gram-boosted** beam search decoding.
## Table of Contents

- [TLDR;](#tldr)
- [Important dates](#important-dates)
- [How to install pytorch, transformers, datasets](#how-to-install-relevant-libraries)
- [Data and Preprocessing](#data-and-preprocessing)
- [How to fine-tune an acoustic model](#how-to-finetune-an-acoustic-model)
- [How to fine-tune with OVH could](#how-to-finetune-with-ovh-cloud)
- [How to combine n-gram language models with acoustic model](#how-to-combine-n-gram-with-acoustic-model)
- [Evaluation](#evaluation)
- [Prizes](#prizes)
- [Communication and Problems](#communication-and-problems)
- [Talks](#talks)
- [General Tips & Tricks](#general-tips-and-tricks)

## TLDR

Participants are encouraged to leverage pre-trained speech recognition checkpoints,
preferably [facebook/wav2vec2-large-xlsr-53](https://huggingface.co/facebook/wav2vec2-large-xlsr-53), 
to train a speech recognition system in a language of their choice.
 
Here is the question: What is the primary benefit of combining n-gram models with acoustic models?","The primary benefit of combining n-gram models with acoustic models is to significantly improve the performance of speech recognition systems, specifically by reducing the word error rate (WER). According to the context, an n-gram model can reduce the WER by around 80% compared to no language model, and even a Transformer-based LM can only reduce the WER by an additional 23% compared to an n-gram model."
How can multilingual ASR models be optimized for low-resource languages?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (22530, '1de22cdb-825f-48bc-b92b-c9a32575561a'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (7688, '157a8a18-f0d3-4b4c-9ca0-8abdff1aa12f'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (22542, '9f48bc8c-0874-46b9-a37c-65b01da9c5b7'), (3091, 'a48ade52-a88a-48fc-b5a3-a226b83924c0'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (6167, '15f088a9-d05c-4c8f-bd6b-bf5effe648dc'), (25114, 'a31eec45-03e1-4363-9d73-c0001a931f70'), (25130, '9406e5c9-54c7-4f43-beaf-e6aee3ee4a40'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (19036, 'c2c8e452-3a34-4af6-9073-5ae159778bd8'), (30822, '1357395c-e339-49a4-9470-5057472025be'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (30823, '6a1ba3fc-cac8-4ca8-bb2a-f30168b0c3ea'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (2731, '82686c50-994c-41d1-a986-d0c024bd9a91'), (2732, 'e9542adf-b364-469d-be29-3f2e6646c7b8'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (6903, '807eb0ed-deee-4627-8c1d-d65ce5f2f592'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (22794, '364ec0e7-da72-440c-aef3-1db9aed7f3dc'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (11021, '70dce8d6-04e9-4cc4-a376-5c986e05ef58'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (26916, '1fc9c7b2-afcd-4326-868e-7cba27da69b2'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (12092, 'c1a47309-d1e5-445c-a69d-57467c21e350'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (21338, 'f2a1c716-da2e-48e9-a235-2c5f9cf76008'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (24455, '9009a74c-8887-4bf8-bbd1-bce07b985d0f'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21495, '90b03904-a64e-420c-95a4-be77d44b1afa'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (22522, '4124762d-a0d2-4d53-9116-b07b817c6c09'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing
per-language performance; XLM-Ris very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We
will make XLM-R code, data, and models publicly available.*
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1,100 spoken languages can be identified, transcribed and generated with the various [language identification, speech recognition, and text-to-speech checkpoints released](https://huggingface.co/models?other=mms).

In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning. 

For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
 
Here is the question: How can multilingual ASR models be optimized for low-resource languages?","According to the context, multilingual ASR models can be optimized for low-resource languages through fine-tuning, specifically using Adapter training as in Massive Multilingual Speech (MMS) which achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning."
What strategy ensures seamless deployment of multitask ASR models?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (14875, '863f500b-8805-487f-a9d0-1a5268b9eb36'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (9775, '6190ef64-7999-4538-bc7e-cf311a507c44'), (21561, '68fa59f6-381e-4dee-ae59-873812588c22'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (7768, '224a5adb-2c8c-434b-b5b5-b6fab137f1b8'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (26256, '53133aac-8f9c-4d76-93ac-3ddce707e57b'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (24757, '5419b92d-c4d3-4ba7-9492-e245a4e5c9dd'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (17079, 'a22e8141-1b79-4327-968c-eb9f9d1bda26'), (6340, '659fd3d4-1c85-4ad8-a958-c4a96dac7ac4'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (20681, 'b77beac2-80af-4a70-b49f-7a58c98e1a86'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (22225, '33434414-721b-423e-9aef-6fa82d028a76'), (10450, '3d06676d-85ad-4fa3-965c-e9d2ff2a64d7'), (20690, 'cbcf4eb2-979a-495d-a8ad-950b3cb6692a'), (19180, 'd45d3ebb-1f28-4a4a-85e0-48a8bce67527'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (8437, 'b3c221ca-a7b5-46b4-aa8f-56aee28b0c55'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (29468, '17fc845b-a10d-41de-978b-b91257cc0ada'), (8477, '4c3c64ff-fb0a-4e08-9494-89bb67cdefff'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (29474, '84d6f505-7c0c-4b24-ab51-57fb76f18f57'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (29480, '33135ed5-7728-4e82-a872-6eccb9bfa1d5'), (5930, '4af5f45c-7439-42aa-ab68-23e2bbfd45b1'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (29487, '2321da63-59f7-4fbb-b56d-00810837d618'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (3920, '1471a463-1bdb-4579-a89b-08a0d16a92ec'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (27478, '3112c19e-b052-473f-9193-730ca5349439'), (27481, '670fac96-da41-4462-a2f5-621a35d75778'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (27484, 'fe3505b9-3176-4041-81f3-76181cde167a'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (19844, '6055a25b-6146-46a4-baaf-7e3a028a6e6f'), (18309, '77eb1ea6-c7df-40f4-a112-aec1b6d6ae26'), (14730, 'f15e632b-c114-4f17-8b86-bb2e9957cf34'), (14731, '2f1c2099-ebc2-431b-885f-6845a75a2d23'), (403, 'b6c24f9c-2894-4834-96cd-4a13ea060bb0'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (411, '96253f57-9a1a-4672-829b-a8e8241df3cf'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (11681, 'df07ea75-1571-4f3d-8817-e08b0eb8e6ae'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (6056, 'aba34742-67ab-47e8-a239-b5b49e9d199d'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (7603, '4ca0d46e-3015-4396-a64e-854cd42ae466'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (24515, '67f98c3e-185d-40a8-983f-4ac4f1c56a92'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (13272, '46ca73c5-5516-437f-af04-f7d8c971dfaa'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (27110, '29d1cd61-2e6b-4cd2-b5bd-de93aaa25f99'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699'), (15871, '23fe4629-d4f8-4f28-85a5-d238feb3371a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: SeamlessM4T-v2 is a collection of models designed to provide high quality translation, allowing people from different linguistic communities to communicate effortlessly through speech and text. It is an improvement on the [previous version](https://huggingface.co/docs/transformers/main/model_doc/seamless_m4t). For more details on the differences between v1 and v2, refer to section [Difference with SeamlessM4T-v1](#difference-with-seamlessm4t-v1).

SeamlessM4T-v2 enables multiple tasks without relying on separate models:

- Speech-to-speech translation (S2ST)
- Speech-to-text translation (S2TT)
- Text-to-speech translation (T2ST)
- Text-to-text translation (T2TT)
- Automatic speech recognition (ASR)

[`SeamlessM4Tv2Model`] can perform all the above tasks, but each task also has its own dedicated sub-model.

The abstract from the paper is the following:
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
8. LoHa: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098)
9. LoKr: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650) based on [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://arxiv.org/abs/2309.14859) implementation
10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
11. OFT: [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)
Finetuning Whisper-large-V2 on Colab using PEFT-Lora + BNB INT8 training

In this Colab, we present a step-by-step guide on how to fine-tune Whisper for any multilingual ASR dataset using Hugging Face 🤗 Transformers and 🤗 PEFT. Using 🤗 PEFT and `bitsandbytes`, you can train the `whisper-large-v2` seamlessly on a colab with T4 GPU (16 GB VRAM). In this notebook, with most parts from [fine_tune_whisper.ipynb](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb#scrollTo=BRdrdFIeU78w) is adapted to train using PEFT LoRA+BNB INT8.

For more details on model, datasets and metrics, refer blog [Fine-Tune Whisper For Multilingual ASR with 🤗 Transformers](https://huggingface.co/blog/fine-tune-whisper)



## Inital Setup


```python
!add-apt-repository -y ppa:jonathonf/ffmpeg-4
!apt update
!apt install -y ffmpeg
```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use 🤗 Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
 
Here is the question: What strategy ensures seamless deployment of multitask ASR models?","The strategy that ensures seamless deployment of multitask ASR models is MultiTask Prompt Tuning, which enables parameter-efficient transfer learning."
What preprocessing considerations are crucial for robust ASR evaluations?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22529, '1ff19103-8daa-4bb0-ae94-e63b88c169a2'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (29219, 'adf5fa87-721a-4bea-96f4-9491147ce70c'), (21548, 'e7019919-6237-4926-8823-50d944591b8d'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (30259, '39ca0aec-f1a0-42ed-bdac-a95454f5f95c'), (21560, '6de334f4-432b-4f38-86da-f421d030fb94'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (14917, '141af70d-3a35-4041-8e5f-9073db7d21e8'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (5715, 'a8ce43b6-c0b0-4563-ad30-c367f9a45b99'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (25709, 'd4436e38-6093-40ed-b39c-faee6f5bae46'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (4730, '1588743f-8472-4fdd-abe0-3ab3a94511ca'), (22144, '53697ae7-3817-4e34-ac41-c9d6703921e2'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (11905, '36ef56a0-66da-48a3-9fa8-8f6e268d8f94'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22662, 'f32c3a87-abbc-4c09-a640-bf8926fe8342'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (21130, 'a10bc757-86c3-4dea-86be-e23b41ea6ab0'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (28306, '3c0328c5-3673-42be-bc9a-3fe3d22eaa2d'), (22696, '47eb8a13-1c6a-4060-a807-7eb65a6df466'), (22699, '4f2bf23c-4a1f-42ce-8fcd-57d8b79aa246'), (24235, 'ea71c622-0f52-4f2f-a826-3643289d9388'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16051, '7ef3b4d6-2896-4ff8-8c3e-15ec9ed4f6ec'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (17088, '17e59b2c-7682-45e9-abb8-a88720517b2c'), (21192, '33a1caa5-4ef6-4cf7-ad0c-79c8a5bca80d'), (23241, 'c3465bca-0d65-4295-bcb3-ef82752ad478'), (9933, '361b3753-2f11-40ad-a8b0-91570e57b5d7'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (31456, '92c22a11-b5e0-4f93-81ca-c7802854d9b2'), (26338, 'c3ae9637-8b7a-42b5-af7f-76b6722ab0fc'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (31473, '107a8b8a-6308-41aa-a924-46e82911e1bc'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (26374, 'ab6b8f80-6158-4925-b405-c3ffdf036bd1'), (7946, 'dca97276-bfd9-4ea5-8570-0a1d528b911a'), (7952, 'bf82155c-99fa-4935-9c09-5ef3eba820c5'), (26390, '7e9534d9-70b4-497b-9fae-b424e5d8e1a3'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (10551, '23b0fa1f-8a46-4da7-aac0-1805e14a220b'), (10552, '19f613db-4203-4d96-a744-9369443faac9'), (26937, 'a68f8601-700e-405b-868e-cf20c77cb9db'), (28472, '67899330-cf36-437d-850b-e81e37b4cfca'), (10555, 'bfe63e09-0fbc-492c-a580-7b3d9053592e'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28485, '90b822fb-e259-470d-9f26-b588fed88f91'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (19815, 'e594d514-fad1-4a8e-8e1c-6e918e2e11fc'), (18814, 'f732d8e9-6555-4bda-ba15-6b227351dcfa'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (2432, '73c38322-cc61-4826-9870-4162d473eef8'), (18818, 'a938f2f4-0d19-4f1e-bd3e-858ce23a0d1d'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (13201, 'f0b65266-be39-49d4-9757-564e662bb018'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (13225, '57dc4972-6e1b-45f7-aafc-58752ba3618d'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14782, '1cc646cf-ac7b-4712-a786-9905b3365b13'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (2511, '9d4f9ab1-abb1-483e-84e9-4cd82481acea'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (17914, 'e3f62a9f-6c82-48c3-84c5-d5b2d2cdf56b'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (24062, '17a005d1-952b-439c-bf0d-09520cded7bf')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ---

---
**NOTE 2**

When training a model on large datasets it is recommended to run the data preprocessing 
in a first run in a **non-distributed** mode via `--preprocessing_only` so that 
when running the model in **distributed** mode in a second step the preprocessed data
can easily be loaded on each distributed device.

---

### Demo

In this demo run we pre-train a `""base-sized""` Wav2Vec2 model simply only on the validation
and test data of [librispeech_asr](https://huggingface.co/datasets/librispeech_asr).

The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available 
per device, consider reducing `--batch_size` and/or the `--max_duration_in_seconds`.
Since those choices are not always obvious when in doubt feel free to ask on Slack or even better post on the forum, as was 
done, *e.g.* [here](https://discuss.huggingface.co/t/spanish-asr-fine-tuning-wav2vec2/4586).

## Tips and tricks

This section summarizes a couple of tips and tricks across various topics. It will continously be updated during the week.

### How to combine multiple datasets into one

Check out [this](https://discuss.huggingface.co/t/how-to-combine-local-data-files-with-an-official-dataset/4685) post.

### How to effectively preprocess the data


### How to do efficiently load datasets with limited ram and hard drive space

Check out [this](https://discuss.huggingface.co/t/german-asr-fine-tuning-wav2vec2/4558/8?u=patrickvonplaten) post.


### How to do hyperparameter tuning


### How to preprocess and evaluate character based languages


## Further reading material
A pretrained checkpoint expects its input data to have been sampled more or less from the same distribution as the data it was trained on. The same speech signals sampled at two different rates have a very different distribution, *e.g.*, doubling the sampling rate results in twice as many data points. Thus,
before fine-tuning a pretrained checkpoint of an ASR model, it is crucial to verify that the sampling rate of the data that was used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model.

A `Wav2Vec2FeatureExtractor` object requires the following parameters to be instantiated:
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution. For example, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.
A pretrained checkpoint expects its input data to have been sampled more
or less from the same distribution as the data it was trained on. The
same speech signals sampled at two different rates have a very different
distribution, *e.g.*, doubling the sampling rate results in data points
being twice as long. Thus, before fine-tuning a pretrained checkpoint of
an ASR model, it is crucial to verify that the sampling rate of the data
that was used to pretrain the model matches the sampling rate of the
dataset used to fine-tune the model.
 
Here is the question: What preprocessing considerations are crucial for robust ASR evaluations?","According to the context, one crucial preprocessing consideration for robust ASR evaluations is to verify that the sampling rate of the data used to pretrain the model matches the sampling rate of the dataset used to fine-tune the model. This is because the same speech signals sampled at different rates have different distributions, and a pretrained checkpoint expects its input data to have been sampled from a similar distribution as the data it was trained on."
How can hardware optimizations improve Transformer model fine-tuning?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12819, '4f074103-3a3b-4754-b9c0-3072c395a614'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (2591, 'e61cc32e-e588-487d-a8be-8a9025bff659'), (27168, 'c58b0c00-3b4b-4ee1-8f01-287cfcf0f690'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (29777, '7d8e342a-bf02-40c1-938a-a973c17ac825'), (2643, 'd3577235-2eda-4e17-a9e6-7cdf05401f5b'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (18550, '01e230dd-8782-4f9f-82af-2058aa9bd4d9'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (5816, '22081f3b-7162-4300-aca0-83f0132e06fb'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (8903, 'dce0bc34-0d40-483c-9870-a80c6a620c0b'), (6355, '9cf0e1e3-ca85-4c73-afc6-a71c99755175'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (19176, 'b6695f80-cc94-40a0-b82e-7e93ea33b2d0'), (23785, 'd977df81-c972-4b3e-a756-008f08c72b27'), (6891, 'be9e7900-0749-480d-a021-5b9aaef63126'), (19187, '3903562f-59a1-4286-92d6-9666ba3e9bd1'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (17166, 'f8edb48c-3037-4068-b3cc-1bc5a12bd5d3'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (25883, 'f01a6bd7-35e0-4d59-b1e1-8879b0662972'), (25884, '585808ef-bcbd-4bda-9dc1-376342a66a99'), (17188, '088b5e4c-6891-4808-bd38-f2e2d894daab'), (17190, '03819edd-dcd1-4e8c-baa0-f847b307fe77'), (16678, '1e0fefb6-23b9-4fea-8cfe-628c3ca26e11'), (23847, '25175e5a-8fc3-4050-87fd-bf640d60de2f'), (16681, 'fc9a2a86-bdfe-4b9d-8e65-72359a9c1b04'), (15659, '54679008-9ef1-4235-9d99-b2a9666e3d13'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (21823, '2c7f618b-1d3b-4a96-8de1-482cef62d95f'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (22869, '6f556512-c1c8-4550-a311-ac340d684b4a'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (7000, '8d065847-3189-4122-97bf-d861398ddd06'), (7001, 'cff7419b-0e07-4098-b93b-b6faae2bc7d3'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (21345, '721fab0b-7c94-420c-b346-46fdaea2cb58'), (22885, '6f79d12f-3eff-4c84-a3bc-e653b1d7ab70'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (17769, 'd8775cf3-139e-4533-94e5-f3cee8da4051'), (23912, '269f3664-542a-4c1e-b029-6c9a8442db1b'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (23918, '648aab91-0479-46e6-a00a-7d416b53cdd0'), (23920, '918f2d3c-9fd1-41a7-b7bf-db620459fcb1'), (30576, '76618c07-7a1a-4ffb-bc13-6da43eb286d5'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (17804, '38ff8192-e7d1-4d5e-89e4-69ac324e6498'), (16269, '3219e462-3f68-4d60-9fa6-a62c0ee9a23c'), (5518, '11e9678e-5c8d-487d-84ce-dd6eb387ca4f'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (20886, '9a6992ff-702a-446c-89d4-1aef06d82e29'), (5534, '3e4707b0-83e3-4137-b771-207785927e14'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5537, '98a3798c-4c95-4db2-aad9-77037f52f460'), (2978, 'd83ec157-742d-421c-bcb3-e4a1c2453cc3'), (27044, '767657af-e75a-495b-b30d-7db8878df60e'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (23475, '68a40f24-cf09-4158-a626-b4357e796ceb'), (29108, '9e11c702-51f9-480f-93f7-58160aa21e3a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (23476, '388d466b-2a2e-407a-a311-325d163b5224'), (12725, '69c1bca3-0390-4f32-82e9-9812d1bc36a6'), (20409, '32a52105-bdf2-4140-986d-2852a87f084e'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (11713, '16213aad-a177-42a5-8be6-a4c1224c45d0'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (475, 'fc8680f8-2eee-4940-8f49-33e177da211a'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (31735, '31b3ae48-f7ec-45ee-adb9-0d7d5e3541ca'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (1526, '3d4ad8ab-39dc-4f72-9a8a-e4df56de9c0b'), (31741, '3cb8440b-d4b0-4bd5-930d-1fc4596f3292'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Also, hyperparameter tuning is another aspect of transformer fine tuning and can have [huge impacts on accuracy](https://medium.com/distributed-computing-with-ray/hyperparameter-optimization-for-transformers-a-guide-c4e32c6c989b). For scalable and easy hyperparameter tuning, check out the [Ray Tune](https://docs.ray.io/en/latest/tune/) library. By using [Ray Tune’s integration with PyTorch Lightning](https://medium.com/distributed-computing-with-ray/scaling-up-pytorch-lightning-hyperparameter-tuning-with-ray-tune-4bd9e1ff9929), or the [built-in integration with Huggingface transformers](https://huggingface.co/blog/ray-tune), you can run experiments to find the perfect hyperparameters for your RAG model.

And lastly, stay tuned for a potential Tensorflow implementation of [RAG](https://ai.facebook.com/blog/retrieval-augmented-generation-streamlining-the-creation-of-intelligent-natural-language-processing-models) on [Huggingface](https://huggingface.co/)!
Transformer models are increasingly large and complex, which can cause production challenges for latency-sensitive applications like search or chatbots. Unfortunately, latency optimization has long been a hard problem for Machine Learning (ML) practitioners. Even with deep knowledge of the underlying framework and hardware platform, it takes a lot of trial and error to figure out which knobs and features to leverage.

Intel provides a complete foundation for accelerated AI with the Intel Xeon Scalable CPU platform and a wide range of hardware-optimized AI software tools, frameworks, and libraries. Thus, it made perfect sense for Hugging Face and Intel to join forces and collaborate on building powerful model optimization tools that let users achieve the best performance, scale, and productivity on Intel platforms.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
![IGF performance](result_igf.png)

Figure 1: Comparing IGF to Standard Fine-tuning:
IGF with constant (p < 10−3 , t-test) and shifting(p < 10−6 , t-test) thresholding significantly outperform standard fine-tuning. The left-hand figure shows
test-set perplexity after each fine-tuning batch, averaged over 50 runs (error bars denote ± one standard error). The right-hand figure shows the perplexity of each
method after 60 batches. IGF with shifting thresholding (red) clearly improves over standard batched fine-tuning with Adam

## How to use this project?

To fine-tune a transformer model with IGF on a language modeling task, use the following script:
This process will also achieve better results than training from scratch (unless you have lots of data), which is why you should always try to leverage a pretrained model -- one as close as possible to the task you have at hand -- and fine-tune it.

## General architecture[[general-architecture]]

In this section, we'll go over the general architecture of the Transformer model. Don't worry if you don't understand some of the concepts; there are detailed sections later covering each of the components.

<Youtube id=""H39Z_720T5s"" />

## Introduction[[introduction]]

The model is primarily composed of two blocks:

* **Encoder (left)**: The encoder receives an input and builds a representation of it (its features). This means that the model is optimized to acquire understanding from the input.
* **Decoder (right)**: The decoder uses the encoder's representation (features) along with other inputs to generate a target sequence. This means that the model is optimized for generating outputs.
 
Here is the question: How can hardware optimizations improve Transformer model fine-tuning?","Hardware optimizations can improve Transformer model fine-tuning by leveraging specialized hardware accelerators, such as Intel's Advanced Matrix Extensions (AMX), which can accelerate deep learning workloads. This can lead to better performance, scale, and productivity on specific hardware platforms. For example, fine-tuning NLP Transformers and inference with NLP Transformers and Stable Diffusion models can be accelerated using AMX, resulting in improved efficiency and accuracy."
How can multitask prompt tuning enhance multilingual model performance?,"[(11279, '884c0122-efae-4da0-9f5a-a74765dcd2a2'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (16919, '942238c0-9ba8-45fc-bd5c-b13953074601'), (16920, 'cd0788e0-82b0-4b44-9dd4-8b8f83cf3ab9'), (16921, 'a31bb2a1-bb83-4766-8534-5664f075ae46'), (11289, '7f93911b-ca40-4dd9-bd7e-f9daca16ed17'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (25150, 'c125fa87-ac9a-4bb7-9926-8dc62c5fb983'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (25151, '15aa2c96-bd80-4f22-bfb0-0ec6a68df5d3'), (30784, '5abe754b-87a4-4fc2-9103-0a0a0aab8e09'), (30785, 'fe7b98b8-14ab-4e54-8af2-68f748159690'), (10830, '63fe604b-e2e2-49c7-bfbb-16c17a3cb8c7'), (7264, '78d13fda-39a6-498d-ac64-7f953e785549'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (30825, '8934d6ff-828c-4bde-8179-5d5a150bf5e1'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (6275, 'b7f843fa-b573-4778-a2b6-d3781b16a73b'), (9368, 'a2674ef7-b70d-4243-8f59-ea725770523f'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (9388, 'c5897364-fd39-4d7e-b9dc-2c0e6429edc9'), (30381, 'fb5013aa-10f1-4372-90a9-b341b7365744'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (5815, '7a89c479-9823-4c0f-a0de-e3ecf304e920'), (696, 'b996a5f4-7c16-44f1-aa17-bcf65b15101a'), (22721, '3c6eb39a-0e3f-4234-8cdb-5330689e73b3'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (17622, '18cebb0d-bc71-4646-91ee-08fdc8fb51a7'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (731, 'ad4324cd-9c6c-4555-88d6-faf72cb8a9da'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (31464, 'db90851b-ecc3-4a50-94bf-c72e7cff4956'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (4847, 'daa43fb0-dad9-451b-bddf-101880195771'), (7923, '3d12a194-ca8a-4ffe-9e77-8fb01bda0fd8'), (7924, '9e9cce0b-318b-4407-8527-b450af5fc69f'), (7925, '898386e9-41b2-43b2-bf00-f5fe304bcf79'), (7926, 'f97b01a4-b979-42d6-8789-bec7870b5e30'), (7927, 'be9f55e3-25af-4774-95e1-537c53e8e6e5'), (7928, '3af4d2b1-479b-4154-9dfd-f8a6036bc83c'), (7929, '698a3ccc-aa9e-4fff-af7d-792856b4a975'), (17657, '12121e46-a187-42c0-a9ad-22a96e02c8b9'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (20749, 'b2e0610f-aa0c-4852-b1ad-2bf187ecf97e'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (26421, '2c89420e-e9cb-4967-9268-cef5f028e779'), (30534, '95e21773-3553-4d8f-8b88-fff6315116ba'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (21324, 'c689a247-869d-4ea7-b9f0-170aa19a493b'), (8525, '931d0e8a-fa6c-4b84-9b4a-f7329eb19392'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (17774, 'e13cc263-c072-4a3b-9f64-7b688f1c79bf'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (3442, '4062307d-58b5-40f7-bed4-85431cd27d5a'), (3441, 'a50198ff-bdce-4045-af16-a7f3e4ed8e3a'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (20860, 'f6cd2e41-5cbe-4525-a0d2-89d48c843903'), (21887, '2fc3be04-7823-41a9-af0e-6c649e006652'), (18309, '77eb1ea6-c7df-40f4-a112-aec1b6d6ae26'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (28555, '3900d0c8-9171-4b7d-bf48-27c131dec37b'), (28556, '6de4578f-f7a2-4271-a425-65baf13f4391'), (10648, '66ffc54d-754b-40e5-b926-93bd59b981cb'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (28579, 'faeb2a24-8c7e-43f7-b3f4-3ae4017753be'), (22439, 'f6be4ebe-45cf-4e8d-b4ac-b4a7dffabdb7'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (28585, 'd3223831-24e7-4557-a7a0-28f800f26eb0'), (7601, '3ecbf4ed-0cd3-483b-88d7-128fe443adff'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (26043, 'a7b91905-1ecc-4b7e-a8c2-bd488f3d3b67'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (25033, '1414432c-aab4-4c4c-b04a-1e8d688255ed'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (1489, 'e1d38ea6-67f5-4afa-aee9-dcf5a5fe79ad'), (30677, 'a25bef54-3520-4f86-a7af-0451578e6208'), (29145, '24502636-e9e7-4bd4-b9ad-efe37d36eb09'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (25565, 'bfb5de19-ac27-4d58-ade9-2cf5e047b9eb'), (482, 'ecfc6092-627b-4285-bb59-322895262902'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (13288, '50cf0dc9-5985-4564-b357-edc6d717de24'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (15871, '23fe4629-d4f8-4f28-85a5-d238feb3371a'), (1522, '28731235-d234-44e1-9e19-7f8f6084cb19'), (16890, 'c8b38db6-3834-4660-9877-40161fc8646f'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (15869, '5e46d120-df1b-4d36-bd63-b7491a1e4eff'), (15870, 'aa4dffab-9c16-4523-8b50-2bde7016c29f'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: - the prompt tokens can be inserted anywhere in the input sequence, and it isn't restricted to only the beginning
- the prompt tokens are only added to the input instead of adding them to every layer of the model
- introducing *anchor* tokens can improve performance because they indicate characteristics of a component in the input sequence

The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables GPT-like models to compete with BERT-like models on NLU tasks.

Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification) for a step-by-step guide on how to train a model with P-tuning.

## Multitask prompt tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png""/>
</div>
<small><a href=""https://hf.co/papers/2103.10385"">Multitask prompt tuning enables parameter-efficient transfer learning</a>.</small>
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
8. LoHa: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098)
9. LoKr: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650) based on [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://arxiv.org/abs/2309.14859) implementation
10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
11. OFT: [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)
The abstract from the paper is:

*Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters*.
[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:

1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Multitask Prompt Tuning

[Multitask Prompt Tuning](https://huggingface.co/papers/2303.02861)  decomposes the soft prompts of each task into a single learned transferable prompt instead of a separate prompt for each task. The single learned prompt can be adapted for each task by multiplicative low rank updates.

The abstract from the paper is:
 
Here is the question: How can multitask prompt tuning enhance multilingual model performance?","The context does not provide a direct answer to how multitask prompt tuning can enhance multilingual model performance. The provided information explains what multitask prompt tuning is, how it works, and its benefits in adapting large language models to multiple downstream tasks, but it does not specifically mention multilingual models or their performance. Therefore, I cannot provide a concise and relevant answer to the question based on the given context."
What is the main challenge when training ASR models for cross-language tasks?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22530, '1de22cdb-825f-48bc-b92b-c9a32575561a'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (30211, 'b3191d20-4a96-4fec-acc1-45b0aa492a6e'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (7688, '157a8a18-f0d3-4b4c-9ca0-8abdff1aa12f'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (3597, '837a4f94-d203-4386-b6fe-7eca8f6530c4'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (5688, '8ea1b0c9-0786-4d95-b2d8-d71aa171b06b'), (21564, '0a7617e7-bf68-4cb7-8d7d-de8e7fb8ac10'), (30269, 'ce1b2ec6-16bc-48d4-8e48-ee802e65abe6'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (14430, 'df8628a2-0050-42f4-be97-ef5d21f207ed'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (30823, '6a1ba3fc-cac8-4ca8-bb2a-f30168b0c3ea'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (19098, 'b3de1848-ec7a-4f5e-9caa-d5d7b8bbe37f'), (9379, '124e807e-b7a7-4f35-89e3-cc8b593f5dc6'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (7341, 'b601a33f-7e1f-42fd-90b5-1a99b5987075'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (22233, 'e0249e25-a674-416b-92ef-5a9ae6c0f3d3'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26334, 'f86f1902-9868-454f-9ef6-f7c11ce29015'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (9452, '59aaeec8-b95f-4c82-89b3-f8905ba521d8'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (11009, 'eb1201bc-1aad-46e8-8ded-a206fbebb020'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (11012, '2c7ed7d6-155a-4473-b3f7-232806a7a924'), (22794, '364ec0e7-da72-440c-aef3-1db9aed7f3dc'), (22795, '52c61654-a4db-433c-b73f-a890c21cbc8a'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (25360, 'c6c8d7e1-fefc-474a-b353-fa7f01cb1e6f'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (23860, 'afa6eb69-b861-463e-94f7-a8cffe60467e'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (8019, '48aa920d-6f37-4719-a7a4-3bd78af534fe'), (25428, 'd391c525-320b-49a3-83dc-e6a90c266db7'), (8023, '1dec94c2-ffe1-4e8f-a0e8-d220743cc341'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (8034, '6748368f-01a0-48c5-9d0b-812ee69deeff'), (8035, '85373154-23b0-4c8d-826b-c39c88e0e81b'), (17274, '74261f18-6c75-412b-a605-5744fcbd65d1'), (17275, '28cd80ba-a3db-4846-85a3-d2c64ffcd5cd'), (8061, '4d530664-dbd5-4efe-8c30-754bc0a96a38'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (18835, 'a0c92c94-1008-4485-b005-fc90f449f3a6'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (18841, '3e67f2e3-24c6-4a81-9530-adc1eec87d54'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (18844, 'c97c2cdd-44fb-4531-87d5-afef8f19b517'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (10187, '6a08ba17-cbca-4708-9674-af5034a30cd7'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21495, '90b03904-a64e-420c-95a4-be77d44b1afa'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (9721, 'b6f73d08-a4da-44e3-86ef-0fbd6f8a93d0'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
*This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a
wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred
languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly
outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +13.8% average accuracy on
XNLI, +12.3% average F1 score on MLQA, and +2.1% average F1 score on NER. XLM-R performs particularly well on
low-resource languages, improving 11.8% in XNLI accuracy for Swahili and 9.2% for Urdu over the previous XLM model. We
also present a detailed empirical evaluation of the key factors that are required to achieve these gains, including the
trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource
Our best WER is 32.0% - not bad for 8h of training data! The big question is how this 
compares to other ASR systems. For that, we can view the [`hf-speech-bench`](https://huggingface.co/spaces/huggingface/hf-speech-bench), 
a leaderboard that categorises models by language and dataset, and subsequently ranks 
them according to their WER.

<figure>
<img src=""assets/111_fine_tune_whisper/hf_speech_bench.jpg"" alt=""Trulli"" style=""width:100%"">
</figure>

Our fine-tuned model significantly improves upon the zero-shot performance of the Whisper 
`small` checkpoint, highlighting the strong transfer learning capabilities of Whisper.

We can automatically submit our checkpoint to the leaderboard when we
push the training results to the Hub - we simply have to set the appropriate key-word 
arguments (kwargs). You can change these values to match your dataset, language and 
model name accordingly:
```

Play with an interactive demo for the [speech-to-text task](https://huggingface.co/spaces/Matthijs/speecht5-asr-demo).

## Conclusion

SpeechT5 is an interesting model because — unlike most other models — it allows you to perform multiple tasks with the same architecture. Only the pre-nets and post-nets change. By pre-training the model on these combined tasks, it becomes more capable at doing each of the individual tasks when fine-tuned.

We have only included checkpoints for the speech recognition (ASR), speech synthesis (TTS), and voice conversion tasks but the paper also mentions the model was successfully used for speech translation, speech enhancement, and speaker identification. It’s very versatile!
**Wav2Vec2** is a pretrained model for Automatic Speech Recognition (ASR) and was released in [September 2020](https://ai.facebook.com/blog/wav2vec-20-learning-the-structure-of-speech-from-raw-audio/) by *Alexei Baevski, Michael Auli, and Alex Conneau*. Soon after the strong performance of Wav2Vec2 was demonstrated on one of the most popular English datasets for ASR, called [LibriSpeech](https://huggingface.co/datasets/librispeech_asr), *Facebook AI* presented two multi-lingual versions of Wav2Vec2, called [XLSR](https://arxiv.org/abs/2006.13979) and [XLM-R](https://ai.facebook.com/blog/-xlm-r-state-of-the-art-cross-lingual-understanding-through-self-supervision/), capable of recognising speech in up to 128 languages. XLSR stands for *cross-lingual speech representations* and refers to the model's ability to learn speech representations that are useful across multiple languages.
 
Here is the question: What is the main challenge when training ASR models for cross-language tasks?",The main challenge when training ASR models for cross-language tasks is dealing with low-resource languages.
How can ASR pipelines be optimized for efficiency on Intel CPUs?,"[(4608, '11eba3f7-462f-400d-a1c6-26fbdecddb11'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (11298, 'e265f736-1fce-44dc-83cd-0e039d2d5f03'), (28707, '1579ae2b-a259-4e21-a4ab-6554888fc9db'), (22052, '1a82c910-0703-4313-a37c-d5ad255b37fb'), (11299, '1c0ce627-8ae5-472a-a2f2-43e1bdfd88df'), (11302, 'a3c2eeca-7cdc-4f91-b078-57ecfbe53731'), (10276, 'debee365-bf66-467b-9618-eea6f12bffa9'), (14378, 'bdbac296-8dd0-4a3f-b93d-26f2653df54e'), (14382, 'f0a3cc11-b2fc-4b21-a76e-c0d46a55834a'), (30770, '964e522f-8c2f-4031-a160-73dccc219846'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (3129, 'eb945bfb-2047-4a55-a268-2f7b9efc42f6'), (10302, '3b9c2808-fcbc-4138-a267-153f19e63291'), (10819, '9fb4e5bf-b2d1-4c28-ac14-c7b7fd6f77ae'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (9803, 'a0b49fbb-8568-4355-8bdb-8c64f1a568ad'), (28750, '909829c2-8f61-4347-b43a-4fbe10ee781f'), (9296, 'b982a595-b413-47c2-a522-d71819d17b20'), (28752, 'a8f3fc54-9b72-46a0-b4ac-29da5374d20e'), (28754, '2f370cf6-442c-48bd-a56c-8301e99110f6'), (9297, 'b1ddc06d-35a1-4ac0-b68f-697de22d55e0'), (15956, 'd03f6960-2286-4540-a440-e6bb93b141f6'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (97, 'c110d4b5-e404-4749-8851-600a26eec5e4'), (5730, '73559eea-5360-4f48-8b5e-01efa4ef8a4f'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (9832, 'bdc6bea1-7f20-4300-aae9-3f7ea03a6783'), (9834, '5c02c88b-44fc-4234-9888-577064cae93c'), (9835, '221255a0-2941-471c-9c13-9a9cbe7621a7'), (5738, '1b3f6207-1eaf-4d30-b7f5-15ce5479c98a'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (14454, 'ce330ae8-8dec-4720-92e6-052305cb40dc'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (24739, 'dcc5e35f-9208-4a2c-b2ef-771fe6332967'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (20690, 'cbcf4eb2-979a-495d-a8ad-950b3cb6692a'), (28372, '15a0b464-97c2-440b-bdc2-bd235ffd25b0'), (20694, '0a712ebe-d559-4eea-b2a2-7181b1e80f23'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (25816, '0aee80dc-2820-4f8b-a131-2042458d62e0'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (2795, 'f76ad827-6b8b-4dd5-a216-0ef9acdfe558'), (10989, '6bcf6d91-b596-48cd-903e-99802046e818'), (22768, '0aa32bd4-ae8d-4d98-b212-4087c9da1896'), (22769, '502d926b-7aad-40f9-a9de-26bea53039a1'), (25334, '33427231-4e65-4935-ba92-1cda98b9c5fa'), (22775, '92b448c2-17a3-48db-9679-dfae336f0e2f'), (30456, '7f75b6b0-ffd7-420b-9078-22030660ddb2'), (22779, '1b8ccb79-44ab-4d6d-b222-a9f1fd2244d4'), (18685, '52fdc5bf-09c1-40d7-85f6-9c586ebe5ef0'), (8957, 'd821fc5e-a896-4f11-b3ca-931a9c70ca93'), (4869, '991872b9-4c4c-4b9d-965f-980c21d4fbce'), (4874, 'e733caa3-5f74-4bf7-b370-8d9bca505b85'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (24343, '1aa794ee-f259-4d0c-89b9-9dba2932d569'), (15648, 'f58ba64c-38eb-47ea-9da1-86ef9f6ad5ed'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (17207, 'fdc3377d-63a0-4bb4-b993-7a30e8b7189f'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (1872, '4ab11bb3-d5e2-4c59-ac01-a9f3d8773989'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (9556, '4ebcb0ec-bfc7-44ca-92cd-96ee50f00f5f'), (10580, '5b2828bb-b3d9-4084-bbf1-9a510e142f1f'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (3941, 'a871cd71-4386-4492-ae52-7dda1294b854'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (14712, '1861bc32-ca30-45b0-bdb8-516bd6dd3448'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (19867, '88ae508e-cedb-469a-b846-e494beb34b6b'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (2474, 'f1912b84-7ebb-463f-b45e-8044c7679524'), (26540, '313630db-d36c-4654-8dfa-112a43b9691f'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (26541, 'bf53285c-42c7-4678-b22d-2dfb988d32b9'), (19383, '44c8fa9d-7167-4105-9431-707875941cb8'), (19384, '75f513c0-3b52-4f83-aec6-7b436161c162'), (29112, 'f5df6a53-0df1-4420-9043-3d9ba923f41b'), (29114, '131eb762-91f6-4098-b8e1-c638fb32b6eb'), (1466, 'c444b35a-68fb-4fae-8309-306d1b03ac87'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (29118, '553e8a5e-a197-4b4c-9e25-dce549489f39'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (11713, '16213aad-a177-42a5-8be6-a4c1224c45d0'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19398, 'c99ed4d1-a0f8-47fc-9f4d-4867015d3211'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (19407, '5f9399cd-785b-459e-bf05-4d7cc41174c0'), (17365, '5fff32a6-0c83-4121-b3e6-5237d83d0fde'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (12270, '002549e4-baef-4cb3-baf4-279f524b849b'), (8691, 'c4abf821-f5ee-419a-b75d-9cfb752e2e97'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (31740, '7f15437c-296d-44c3-89c4-94654b268fc9'), (31741, '3cb8440b-d4b0-4bd5-930d-1fc4596f3292')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: -->

# Efficient Training on CPU

This guide focuses on training large models efficiently on CPU.

## Mixed precision with IPEX

IPEX is optimized for CPUs with AVX-512 or above, and functionally works for CPUs with only AVX2. So, it is expected to bring performance benefit for Intel CPU generations with AVX-512 or above while CPUs with only AVX2 (e.g., AMD CPUs or older Intel CPUs) might result in a better performance under IPEX, but not guaranteed. IPEX provides performance optimizations for CPU training with both Float32 and BFloat16. The usage of BFloat16 is the main focus of the following sections.
## Introduction: Using Intel Software to Optimize AI Efficiency on CPU

As we detailed in our [previous blog post](https://huggingface.co/blog/bert-cpu-scaling-part-1), Intel Xeon CPUs provide a set of features especially designed for AI workloads such as AVX512 or VNNI (Vector Neural Network Instructions) 
for efficient inference using integer quantized neural network for inference along with additional system tools to ensure the work is being done in the most efficient way. 
In this blog post, we will focus on software optimizations and give you a sense of the performances of the new Ice Lake generation of Xeon CPUs from Intel. Our goal is to give you a full picture of what’s available on the software side to make the most out of your Intel hardware. 
As in the previous blog post, we show the performance with benchmark results and charts, along with new tools to make all these knobs and features easy to use.
[Optimum](https://github.com/huggingface/optimum) aims to make this work easy, providing performance optimization tools targeting efficient AI hardware,
built in collaboration with our Hardware Partners, and turn Machine Learning Engineers into ML Optimization wizards.

With the [Transformers](https://github.com/huggingface/transformers) library, we made it easy for researchers and engineers to use state-of-the-art models,
abstracting away the complexity of frameworks, architectures and pipelines.

With the [Optimum](https://github.com/huggingface/optimum) library, we are making it easy for engineers to leverage all the available hardware features at their disposal,
abstracting away the complexity of model acceleration on hardware platforms.

## 🤗 Optimum in practice: how to quantize a model for Intel Xeon CPU
### 🤔 Why quantization is important but tricky to get right
For the results shown in this blog, we used the default number of 50 inference steps. With fewer inference steps, inference speed will be faster, but this has an effect on the quality of the resulting image. How large this effect is depends on the model and the [scheduler](https://huggingface.co/docs/diffusers/using-diffusers/schedulers). We recommend experimenting with different number of steps and schedulers and find what works best for your use case.

Below we show how to perform inference with the final pipeline optimized to run on Intel CPUs:

```python
from optimum.intel import OVStableDiffusionPipeline

# Load and compile the pipeline for performance.
name = ""OpenVINO/stable-diffusion-pokemons-tome-quantized-aggressive""
pipe = OVStableDiffusionPipeline.from_pretrained(name, compile=False)
pipe.reshape(batch_size=1, height=512, width=512, num_images_per_prompt=1)
pipe.compile()
### 🔥 How to easily quantize Transformers for Intel Xeon CPUs with Optimum

![Automatic quantization code snippet](assets/25_hardware_partners_program/carbon_inc_quantizer.png)

## Follow 🤗 Optimum: a journey to democratize ML production performance
### ⚡️State of the Art Hardware
Optimum will focus on achieving optimal production performance on dedicated hardware, where software and hardware acceleration techniques can be applied for maximum efficiency.
We will work hand in hand with our Hardware Partners to enable, test and maintain acceleration, and deliver it in an easy and accessible way through Optimum, as we did with Intel and Neural Compressor.
We will soon announce new Hardware Partners who have joined us on our journey toward Machine Learning efficiency.

### 🔮 State-of-the-Art Models
 
Here is the question: How can ASR pipelines be optimized for efficiency on Intel CPUs?","The context does not provide a direct answer to the question of how ASR (Automatic Speech Recognition) pipelines can be optimized for efficiency on Intel CPUs. The context primarily focuses on optimizing large models for efficient training on CPU using IPEX and Optimum, with a specific emphasis on Intel Xeon CPUs. It does not mention ASR pipelines or provide guidance on optimizing them for efficiency on Intel CPUs."
What key advantage does multitask prompt tuning provide in ASR models?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (16919, '942238c0-9ba8-45fc-bd5c-b13953074601'), (16920, 'cd0788e0-82b0-4b44-9dd4-8b8f83cf3ab9'), (16921, 'a31bb2a1-bb83-4766-8534-5664f075ae46'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (16922, '6ffff410-16b9-4bc9-88db-d6d4c3ff10bd'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (10830, '63fe604b-e2e2-49c7-bfbb-16c17a3cb8c7'), (11859, 'eca711ae-a436-4004-a11f-ed98258cbd62'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (3170, '547ca77f-648b-46d1-b2aa-1e8780f7216e'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (16492, 'e9159689-7c45-42ef-90ba-2efb9da7ad91'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (30333, '463aafaa-1474-444d-8fe3-5bc8c8d95517'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (23684, '5278bf06-a69b-482f-a31f-a9e84563cc75'), (22662, 'f32c3a87-abbc-4c09-a640-bf8926fe8342'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (18065, '70066450-a7ad-4637-83d5-a5f132a8ac63'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9'), (9368, 'a2674ef7-b70d-4243-8f59-ea725770523f'), (9370, 'db1c6e3e-6f2c-4a80-961d-6c6bfe33415e'), (22170, '19f7f6b0-2cc2-4dd9-9c7a-ce79106eed74'), (25759, 'b2156363-4f81-4b08-a11e-6e35136835d6'), (17078, '039b5cc5-3a5b-4f86-8e06-5f8da65798df'), (13501, '2a2b7fc2-3c43-4d05-a703-7d2ed7939397'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (7923, '3d12a194-ca8a-4ffe-9e77-8fb01bda0fd8'), (7924, '9e9cce0b-318b-4407-8527-b450af5fc69f'), (7925, '898386e9-41b2-43b2-bf00-f5fe304bcf79'), (7926, 'f97b01a4-b979-42d6-8789-bec7870b5e30'), (7927, 'be9f55e3-25af-4774-95e1-537c53e8e6e5'), (7928, '3af4d2b1-479b-4154-9dfd-f8a6036bc83c'), (7929, '698a3ccc-aa9e-4fff-af7d-792856b4a975'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (21280, 'bbe915f8-674b-4422-a8c3-9fbc5e7e1858'), (31008, '3f401b96-9ff0-4f59-b937-0dead68da5d8'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (21324, 'c689a247-869d-4ea7-b9f0-170aa19a493b'), (8525, '931d0e8a-fa6c-4b84-9b4a-f7329eb19392'), (27473, '628787b7-dd50-49d4-bbfc-cd5771b01254'), (21331, 'c706e01b-6612-49e1-b624-510376de99e7'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (17774, 'e13cc263-c072-4a3b-9f64-7b688f1c79bf'), (30576, '76618c07-7a1a-4ffb-bc13-6da43eb286d5'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (1396, 'e7ea027d-6b31-4aba-a173-5f8353760042'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (21887, '2fc3be04-7823-41a9-af0e-6c649e006652'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (18309, '77eb1ea6-c7df-40f4-a112-aec1b6d6ae26'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (28556, '6de4578f-f7a2-4271-a425-65baf13f4391'), (5520, 'e2f816ac-dce2-4a36-9d80-a615b27cd149'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (17821, '0ecce6f7-f4da-4215-8bc0-6acfc0848cca'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (28578, 'b92ae2ad-120c-4111-97a4-b07d7c8f587d'), (28579, 'faeb2a24-8c7e-43f7-b3f4-3ae4017753be'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (28585, 'd3223831-24e7-4557-a7a0-28f800f26eb0'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14782, '1cc646cf-ac7b-4712-a786-9905b3365b13'), (29638, '090cbe12-3fd1-4622-b456-102ca3ae6d96'), (13768, 'c6a758eb-40e3-49f1-9d61-3634e6e7918f'), (26067, '61a8f2fe-8e9a-4fd1-994b-c13c84582f4a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (15868, 'defeffab-5c59-48a0-8058-9ff3d5f02504'), (15869, '5e46d120-df1b-4d36-bd63-b7491a1e4eff'), (14820, '89fe99d0-6248-4567-a75b-ec89163e1708'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (16885, 'cefd3130-e406-4b58-ab29-142eff3b2561'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (16889, '4e9f1ab4-cab8-40fe-870f-69a3b341a3d7'), (16890, 'c8b38db6-3834-4660-9877-40161fc8646f'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (15870, 'aa4dffab-9c16-4523-8b50-2bde7016c29f'), (15871, '23fe4629-d4f8-4f28-85a5-d238feb3371a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: 7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
8. LoHa: [FedPara: Low-Rank Hadamard Product for Communication-Efficient Federated Learning](https://arxiv.org/abs/2108.06098)
9. LoKr: [KronA: Parameter Efficient Tuning with Kronecker Adapter](https://arxiv.org/abs/2212.10650) based on [Navigating Text-To-Image Customization:From LyCORIS Fine-Tuning to Model Evaluation](https://arxiv.org/abs/2309.14859) implementation
10. LoftQ: [LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models](https://arxiv.org/abs/2310.08659)
11. OFT: [Controlling Text-to-Image Diffusion by Orthogonal Finetuning](https://arxiv.org/abs/2306.07280)
- the prompt tokens can be inserted anywhere in the input sequence, and it isn't restricted to only the beginning
- the prompt tokens are only added to the input instead of adding them to every layer of the model
- introducing *anchor* tokens can improve performance because they indicate characteristics of a component in the input sequence

The results suggest that P-tuning is more efficient than manually crafting prompts, and it enables GPT-like models to compete with BERT-like models on NLU tasks.

Take a look at [P-tuning for sequence classification](../task_guides/ptuning-seq-classification) for a step-by-step guide on how to train a model with P-tuning.

## Multitask prompt tuning

<div class=""flex justify-center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/peft/mpt.png""/>
</div>
<small><a href=""https://hf.co/papers/2103.10385"">Multitask prompt tuning enables parameter-efficient transfer learning</a>.</small>
The abstract from the paper is:

*Prompt tuning, in which a base pretrained model is adapted to each task via conditioning on learned prompt vectors, has emerged as a promising approach for efficiently adapting large language models to multiple downstream tasks. However, existing methods typically learn soft prompt vectors from scratch, and it has not been clear how to exploit the rich cross-task knowledge with prompt vectors in a multitask learning setting. We propose multitask prompt tuning (MPT), which first learns a single transferable prompt by distilling knowledge from multiple task-specific source prompts. We then learn multiplicative low rank updates to this shared prompt to efficiently adapt it to each downstream target task. Extensive experiments on 23 NLP datasets demonstrate that our proposed approach outperforms the state-of-the-art methods, including the full finetuning baseline in some cases, despite only tuning 0.035% as many task-specific parameters*.
[Multitask prompt tuning (MPT)](https://hf.co/papers/2103.10385) learns a single prompt from data for multiple task types that can be shared for different target tasks. Other existing approaches learn a separate soft prompt for each task that need to be retrieved or aggregated for adaptation to target tasks. MPT consists of two stages:

1. source training - for each task, its soft prompt is decomposed into task-specific vectors. The task-specific vectors are multiplied together to form another matrix W, and the Hadamard product is used between W and a shared prompt matrix P to generate a task-specific prompt matrix. The task-specific prompts are distilled into a single prompt matrix that is shared across all tasks. This prompt is trained with multitask training.
2. target adaptation - to adapt the single prompt for a target task, a target prompt is initialized and expressed as the Hadamard product of the shared prompt matrix and the task-specific low-rank prompt matrix.
1. LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)
2. Prefix Tuning: [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://aclanthology.org/2021.acl-long.353/), [P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)
3. P-Tuning: [GPT Understands, Too](https://arxiv.org/abs/2103.10385)
4. Prompt Tuning: [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691)
5. AdaLoRA: [Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)  
6. $(IA)^3$: [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
7. MultiTask Prompt Tuning: [Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2303.02861)
 
Here is the question: What key advantage does multitask prompt tuning provide in ASR models?","Based on the context, the key advantage of multitask prompt tuning is that it enables parameter-efficient transfer learning, allowing a single prompt to be shared across multiple tasks, which can be adapted to different target tasks with minimal additional parameters (only 0.035% of task-specific parameters need to be tuned). This makes it a more efficient approach compared to traditional fine-tuning methods."
What is the best approach for aligning multilingual datasets in ASR workflows?,"[(30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (21509, '2e90ee09-c728-44e9-aa15-984bb64553e5'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30220, '683831a6-5b21-4e34-8508-f7affdc57f5a'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (22542, '9f48bc8c-0874-46b9-a37c-65b01da9c5b7'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (23068, '17b19121-a9f3-446f-b249-3b1e64bae14f'), (23070, 'dbb25c0a-8cb6-4e62-9b53-c8a6dd06b4c4'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (18492, '16d82abe-ccda-4e30-8882-b21bbc9fc3b2'), (29256, 'e581785d-f30a-47db-99a4-eccdd13d7903'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (24659, '1ce77664-fd28-405b-b2df-b85564bdba01'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (7269, '2203fa03-15b5-4a03-87c9-1ad09aa9d60a'), (7276, '0d98213c-cf33-4909-bcf6-74ccaf3923d7'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (16031, 'aeac043d-c603-454d-af9a-5ddd34d267ff'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (9381, '1695844a-c3fc-40f7-8e86-4f2d6eea7efa'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (691, '49245ea8-a50c-409d-b767-70947845114d'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (17088, '17e59b2c-7682-45e9-abb8-a88720517b2c'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (17617, 'd0201a2e-3af0-47a5-98e7-e13defa7db4f'), (5842, '3b64b1f4-3187-44e6-a1b6-2814799742b2'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (9950, '3570da80-429e-4059-b1b0-db0dddb1ae97'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (3320, '3f22941d-5a99-4599-b75a-7aac2efeee6e'), (28414, '0e69ac2d-e27f-4301-b639-f7070dc7a084'), (13064, '589f6f88-a4f1-45d4-863a-1d5eee88fa12'), (22794, '364ec0e7-da72-440c-aef3-1db9aed7f3dc'), (22795, '52c61654-a4db-433c-b73f-a890c21cbc8a'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (23329, '9afe4111-7b48-4f69-87cf-d1e360f0331a'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (26421, '2c89420e-e9cb-4967-9268-cef5f028e779'), (10551, '23b0fa1f-8a46-4da7-aac0-1805e14a220b'), (15672, '3c536d00-c1fc-410b-a634-96a25e8778ee'), (23353, '209f08ab-4109-44e3-9822-fbb3609f60fd'), (6457, '416d0180-84a0-47ca-816b-e4151270b222'), (25914, 'd343a79e-38fb-4fce-9fcc-0d1e23c73b40'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (6461, 'b1f177f3-e93d-412b-b346-92ad1e300db7'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (6463, 'deb1abed-40b0-4c6f-ab05-30f0375d9840'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28484, '23f6ca4b-af45-4d12-bed5-4f7e47be8e0b'), (838, 'b88fee3d-341d-4685-8fc6-fd51e1adf587'), (6471, '7303414c-e3c5-4e1b-8155-b0a2132f27a4'), (840, '41ff972f-57da-4a02-9c34-558a02677b72'), (841, '3ea3ba81-cfdc-4309-824d-56a17beadb1d'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (4438, 'c433e5f5-9651-489c-8a76-f04bf6884777'), (13664, '0d35fe7f-3409-414c-9082-1c9c87a329cf'), (13666, '8b9a35b3-46c1-4f42-8832-2fff472ad908'), (13668, '86808aee-afa4-4657-9f26-d6a8a9ad0e15'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (24939, 'ff1da006-ddd2-47fd-a177-9f39ad77b0f3'), (16759, '1b62ceba-cef9-48f4-9d42-37f27b4c1c9c'), (20860, 'f6cd2e41-5cbe-4525-a0d2-89d48c843903'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (30608, 'b8bd449a-543d-4b0d-afdc-19e36c13014c'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (21951, '358a38e1-9865-45a7-bb11-f4d56f194e8e'), (7110, 'ace1a408-647b-4149-a9c1-b6a9a032d552'), (18886, 'a01aef5d-3c56-4f8d-8854-e68353f4d189'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (19962, '04858203-03cf-45bb-9b5a-7f506048c00c'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Limitations and bias

CER is useful for comparing different models for tasks such as automatic speech recognition (ASR) and optic character recognition (OCR), especially for multilingual datasets where WER is not suitable given the diversity of languages. However, CER provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.

Also, in some cases, instead of reporting the raw CER, a normalized CER is reported where the number of mistakes is divided by the sum of the number of edit operations (`I` + `S` + `D`) and `C` (the number of correct characters), which results in CER values that fall within the range of 0–100%.


## Citation
```

## Limitations and bias

CER is useful for comparing different models for tasks such as automatic speech recognition (ASR) and optic character recognition (OCR), especially for multilingual datasets where WER is not suitable given the diversity of languages. However, CER provides no details on the nature of translation errors and further work is therefore required to identify the main source(s) of error and to focus any research effort.

Also, in some cases, instead of reporting the raw CER, a normalized CER is reported where the number of mistakes is divided by the sum of the number of edit operations (`I` + `S` + `D`) and `C` (the number of correct characters), which results in CER values that fall within the range of 0–100%.


## Citation
```

Common Voice is but one multilingual ASR dataset that we can download from the Hub - 
there are plenty more available to us! To view the range of datasets available for speech recognition, 
follow the link: [ASR Datasets on the Hub](https://huggingface.co/datasets?task_categories=task_categories:automatic-speech-recognition&sort=downloads).

### Prepare Feature Extractor, Tokenizer and Data

The ASR pipeline can be de-composed into three components: 
1) A feature extractor which pre-processes the raw audio-inputs
2) The model which performs the sequence-to-sequence mapping 
3) A tokenizer which post-processes the model outputs to text format
</Tip>

This guide will show you how to train a [`openai/whisper-large-v2`](https://huggingface.co/openai/whisper-large-v2) model for multilingual automatic speech recognition (ASR) using a combination of `int8` quantization and LoRA. You'll train Whisper for multilingual ASR on Marathi from the [Common Voice 11.0](https://huggingface.co/datasets/mozilla-foundation/common_voice_11_0) dataset.

Before you start, make sure you have all the necessary libraries installed:

```bash
!pip install -q peft transformers datasets accelerate evaluate jiwer bitsandbytes
In this blog, we present a step-by-step guide on fine-tuning Whisper 
for any multilingual ASR dataset using Hugging Face 🤗 Transformers. This blog 
provides in-depth explanations of the Whisper model, the Common Voice dataset and 
the theory behind fine-tuning, with accompanying code cells to execute the data 
preparation and fine-tuning steps. For a more streamlined version of the notebook 
with fewer explanations but all the code, see the accompanying [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb).

## Table of Contents
 
Here is the question: What is the best approach for aligning multilingual datasets in ASR workflows?","The context does not provide a direct answer to the question of what is the best approach for aligning multilingual datasets in ASR workflows. The context discusses the limitations and bias of Character Error Rate (CER) in evaluating ASR models, and provides a guide on fine-tuning a Whisper model for multilingual ASR using a combination of quantization and LoRA. It does not provide information on aligning multilingual datasets in ASR workflows. Therefore, I cannot provide a concise and relevant"
How can hardware optimizations improve multitask ASR pipelines?,"[(9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (27167, '9d246d60-330f-44ca-83ab-b6504683772e'), (14382, 'f0a3cc11-b2fc-4b21-a76e-c0d46a55834a'), (30770, '964e522f-8c2f-4031-a160-73dccc219846'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (30776, 'a5497ed7-9df9-460a-9ab8-8fffde7862ad'), (3129, 'eb945bfb-2047-4a55-a268-2f7b9efc42f6'), (10819, '9fb4e5bf-b2d1-4c28-ac14-c7b7fd6f77ae'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (9286, 'c5343836-1184-4597-bf0e-0d37580c36fc'), (9803, 'a0b49fbb-8568-4355-8bdb-8c64f1a568ad'), (9296, 'b982a595-b413-47c2-a522-d71819d17b20'), (9297, 'b1ddc06d-35a1-4ac0-b68f-697de22d55e0'), (3154, 'cf9d8add-4e29-4a90-b3d6-2679327c71ec'), (15956, 'd03f6960-2286-4540-a440-e6bb93b141f6'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (21083, '181a386a-5f35-46d6-ad54-7431f4ed8555'), (97, 'c110d4b5-e404-4749-8851-600a26eec5e4'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (9838, '24e469f0-838c-4b10-8de3-82174bdc04da'), (14450, 'afa781a5-8958-4c40-b2da-1a49f6796c3e'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (4733, '9090becd-bf3b-4322-a272-ea7519a0322b'), (16510, '28558a81-343e-4be7-a632-bbc1180a6dda'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (6799, '7fdaa5a2-96ae-4629-98d8-4011d0f44647'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (21144, '733daaa7-2d91-4c46-b007-fe08a12ee437'), (25759, 'b2156363-4f81-4b08-a11e-6e35136835d6'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (6851, 'd6a44bed-daac-46f7-9a6d-c0b18d267b90'), (6340, '659fd3d4-1c85-4ad8-a958-c4a96dac7ac4'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (20681, 'b77beac2-80af-4a70-b49f-7a58c98e1a86'), (4300, '22ecff5e-c61a-4907-8ccf-fd6dc4c176a0'), (21196, 'ffd5fcb7-a390-425f-ad65-9e97cac08aec'), (21197, '3b2fbad2-3fdb-4725-9c19-a5b8f139a1a0'), (20690, 'cbcf4eb2-979a-495d-a8ad-950b3cb6692a'), (20693, '6ceda67f-d532-4fa1-99ca-1fe2e1e1fe8c'), (20694, '0a712ebe-d559-4eea-b2a2-7181b1e80f23'), (25816, '0aee80dc-2820-4f8b-a131-2042458d62e0'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (25334, '33427231-4e65-4935-ba92-1cda98b9c5fa'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (21252, '04d49b8c-b911-46ad-bc96-905ee2cf9197'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (16661, 'f6db86f2-9435-45f8-814c-5423a566c4e4'), (2838, 'bf581280-a9b8-476e-9359-bb349445e1a0'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (15648, 'f58ba64c-38eb-47ea-9da1-86ef9f6ad5ed'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (24383, '4b93009f-15f5-4e1c-bd98-faf19d8913fe'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (9556, '4ebcb0ec-bfc7-44ca-92cd-96ee50f00f5f'), (10580, '5b2828bb-b3d9-4084-bbf1-9a510e142f1f'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (31585, 'fb49c353-5521-403e-9fc1-683986fdcdd2'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (14723, '590af83f-86aa-4c0e-95d9-69cb943f9fa6'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (26009, '6a71569e-66a0-4f25-bcbb-cc66181dd233'), (18842, '39d0c44e-884f-41d2-8074-52e917e3db22'), (19867, '88ae508e-cedb-469a-b846-e494beb34b6b'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (2467, 'de7d1230-4e2b-4516-ad9f-cd73c8375904'), (2474, 'f1912b84-7ebb-463f-b45e-8044c7679524'), (26540, '313630db-d36c-4654-8dfa-112a43b9691f'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (26541, 'bf53285c-42c7-4678-b22d-2dfb988d32b9'), (29114, '131eb762-91f6-4098-b8e1-c638fb32b6eb'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (11708, '0f988067-40bf-4450-8ca2-187dd4258914'), (11710, 'a43885e2-972b-41b6-83cc-15f9d35ca97d'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (26057, '9ed838e2-7b57-4bf0-ac30-97ec4268ad86'), (4047, '1a54fabd-203c-4e44-9c9a-efc4a3b0004a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b'), (19944, '5fb1084c-cdb3-4c12-90d8-ef3744dfe8b5'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (12270, '002549e4-baef-4cb3-baf4-279f524b849b'), (10222, 'a13b8c95-cbad-4812-ad41-a22be173a110'), (8691, 'c4abf821-f5ee-419a-b75d-9cfb752e2e97'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

Diffusers supports different optimization techniques to improve the latency
and memory footprint of a pipeline. Since videos are often more memory-heavy than images,
we can enable CPU offloading and VAE slicing to keep the memory footprint at bay.

Let's generate a video of 8 seconds (64 frames) on the same GPU using CPU offloading and VAE slicing:

```python
import torch
from diffusers import DiffusionPipeline
from diffusers.utils import export_to_video

pipe = DiffusionPipeline.from_pretrained(""damo-vilab/text-to-video-ms-1.7b"", torch_dtype=torch.float16, variant=""fp16"")
pipe.enable_model_cpu_offload()

# memory optimization
pipe.enable_vae_slicing()

prompt = ""Darth Vader surfing a wave""
video_frames = pipe(prompt, num_frames=64).frames
video_path = export_to_video(video_frames)
video_path
ONNX Runtime Training achieves such throughput improvements via several memory and compute optimizations. The memory optimizations enable ONNX Runtime to maximize the batch size and utilize the available memory efficiently whereas the compute optimizations speed up the training time. These optimizations include, but are not limited to, efficient memory planning, kernel optimizations, multi tensor apply for Adam Optimizer (which batches the elementwise updates applied to all the model’s parameters into one or a few kernel launches), FP16 optimizer (which eliminates a lot of device to host memory copies), mixed precision training and graph optimizations like node fusions and node eliminations. ONNX Runtime Training supports both [NVIDIA](https://techcommunity.microsoft.com/t5/ai-machine-learning-blog/accelerate-pytorch-transformer-model-training-with-onnx-runtime/ba-p/2540471) and [AMD
These three types of improvements can be used in tandem, resulting in [high throughput solutions](https://github.com/huggingface/text-generation-inference). However, after applying hardware-specific optimizations, there are limited options to reduce latency – and the existing options are expensive. Let’s fix that!

## Language decoder forward pass, revisited
This model was contributed by [UmerHA](https://twitter.com/UmerHAdil). ❤️

<Tip warning={true}>

🧪 Many of the SDXL ControlNet checkpoints are experimental, and there is a lot of room for improvement. Feel free to open an [Issue](https://github.com/huggingface/diffusers/issues/new/choose) and leave us feedback on how we can improve!

</Tip>

<Tip>

Make sure to check out the Schedulers [guide](../../using-diffusers/schedulers) to learn how to explore the tradeoff between scheduler speed and quality, and see the [reuse components across pipelines](../../using-diffusers/loading#reuse-components-across-pipelines) section to learn how to efficiently load the same components into multiple pipelines.

</Tip>

## StableDiffusionXLControlNetXSPipeline
[[autodoc]] StableDiffusionXLControlNetXSPipeline
	- all
	- __call__

## StableDiffusionPipelineOutput
[[autodoc]] pipelines.stable_diffusion.StableDiffusionPipelineOutput
<Tip>

The majority of the optimizations described here also apply to multi-GPU setups!

</Tip>

## FlashAttention-2

<Tip>

FlashAttention-2 is experimental and may change considerably in future versions.

</Tip>

[FlashAttention-2](https://huggingface.co/papers/2205.14135) is a faster and more efficient implementation of the standard attention mechanism that can significantly speedup inference by:

1. additionally parallelizing the attention computation over sequence length
2. partitioning the work between GPU threads to reduce communication and shared memory reads/writes between them
 
Here is the question: How can hardware optimizations improve multitask ASR pipelines?","The context does not provide a direct answer to how hardware optimizations can improve multitask ASR (Automatic Speech Recognition) pipelines. The context discusses optimization techniques in Diffusers, such as CPU offloading and VAE slicing, to improve latency and memory footprint in video generation pipelines. It also mentions ONNX Runtime Training and its memory and compute optimizations, but does not specifically address multitask ASR pipelines. Therefore, I cannot provide a concise and relevant answer based on the provided context."
What preprocessing strategy ensures scalability in ASR workflows?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (17939, 'eb167bfc-4875-47c0-84b4-14d6a25744e6'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (12847, '68d0a4c5-5b64-45db-9974-b52b2400ed7e'), (8240, '8ebf663d-2227-4d15-b309-b3766d170931'), (10819, '9fb4e5bf-b2d1-4c28-ac14-c7b7fd6f77ae'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (5704, 'f5a76ce8-55f0-4aa3-9058-b471d25fed91'), (4681, '7bc70455-75da-43e5-b8f8-bc8b001be9be'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (15956, 'd03f6960-2286-4540-a440-e6bb93b141f6'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (7767, 'e95cb71e-c821-4e82-8438-8d31ed2b88e5'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (6747, 'ff44e6d7-41d2-4ecb-b1a7-9ecd129e495b'), (19551, '6d04c0ce-0c33-450d-afcc-cda4a5256d48'), (97, 'c110d4b5-e404-4749-8851-600a26eec5e4'), (28770, '55b2e710-53bf-486d-a6ee-4d3dbcb969f7'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (19560, '2052afa9-9548-48e4-b160-94d2cd8fa764'), (19563, '505b868d-35d2-4d6c-8268-8e729abd6f63'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (13945, 'a94b0abd-2e7a-44ce-ad86-49755f562e31'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (23180, '19fa40d9-195c-41bf-8036-87291228bb9a'), (23181, 'edd2bc72-58cb-49f2-93cb-c5ebc04d3bc5'), (24204, '24972dc6-4053-46d8-93be-df0faad47b39'), (6799, '7fdaa5a2-96ae-4629-98d8-4011d0f44647'), (23188, 'ff0a744b-a5eb-45a4-a8ac-be09b0554183'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (23199, '7a5edae0-1a08-4089-95df-2a0a35cdee26'), (24226, '04bdd033-f25e-4aba-9fcb-1d09de92f30e'), (24227, 'ee13513e-fd1c-449a-868c-dcb3e17794bd'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (688, 'dd7ba60b-ae7e-4773-acba-8c59fe920da9'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (17079, 'a22e8141-1b79-4327-968c-eb9f9d1bda26'), (697, 'a79771f8-9f63-4615-8faf-cca019cc8939'), (5309, '1063e5a8-266b-4339-bd57-f04a35f278f7'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (17087, '89258315-ff35-47ea-8f5d-62939612d015'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (21189, '871cb94b-d182-44a0-a677-1e5d4584c84d'), (21193, '9d2e65a3-d82a-46c0-8c81-74109482e8d9'), (21197, '3b2fbad2-3fdb-4725-9c19-a5b8f139a1a0'), (17614, 'a4127615-83f7-416f-8953-35d7a99ae728'), (28883, '7c053a1e-d815-49e8-b352-b637e16a3084'), (24278, '63ca5162-5533-4b4f-9360-0e30bba205d4'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (23274, '20ba3cf8-d2ff-4e5f-8b8b-323410a96fc3'), (747, '77a6bae7-44b2-42c4-b6b0-7376b41879bd'), (11502, '330fe3ba-ad53-40c9-bac9-59cda30571b5'), (11503, '63601f87-85f5-4f28-9e78-73ede8bc104b'), (19696, '43e599c7-2548-47c5-9039-cfa92c630506'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (31474, '3534bdd4-1cd9-41b8-b304-a906877e0f95'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (3332, '09beafc1-1176-4db4-8ac6-78b13db9cee4'), (17673, '92806ee2-b4a4-4e84-aae7-72b8d0ed86af'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (16142, 'c771e38b-cfa7-4d59-96c0-b3151c87a39d'), (20755, '4e54cbbb-0c60-4832-8b23-7b7fd3d4ead0'), (24343, '1aa794ee-f259-4d0c-89b9-9dba2932d569'), (4375, '65f6412b-c51e-4375-af24-295af347283d'), (13085, '4d9f7aaf-b3e1-461c-b421-8f235be30114'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (3381, 'dda6626e-40d7-4e0f-9c94-7611cd280f46'), (12088, '4190d432-3762-4793-841d-c92da826645c'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (24383, '4b93009f-15f5-4e1c-bd98-faf19d8913fe'), (17217, 'a0fd8b27-87f1-469b-b8d9-c451e80af7fb'), (11078, '32c306bd-f0ba-4028-8b2e-a210f0b4d55a'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (10580, '5b2828bb-b3d9-4084-bbf1-9a510e142f1f'), (9556, '4ebcb0ec-bfc7-44ca-92cd-96ee50f00f5f'), (27479, '14866e8c-155f-4e7c-b88c-f1244f815d5e'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (18784, '2192d6bc-685f-4f97-a350-5847ba1e9aa2'), (18785, '84a1f4c9-f868-4a4b-9957-7d7798bd4997'), (30584, '72eded47-82e5-4d94-aff8-ea0caf3f4dc7'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (26009, '6a71569e-66a0-4f25-bcbb-cc66181dd233'), (27546, '837105a9-01c9-4d5a-95d6-0812b7fb9c20'), (27545, 'c9fb2425-2ac4-4707-88ad-e11ddfbab842'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (4047, '1a54fabd-203c-4e44-9c9a-efc4a3b0004a'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (26080, '9b91b646-fa9f-4bb6-af8a-938897f96a99'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (10222, 'a13b8c95-cbad-4812-ad41-a22be173a110'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (25598, '8c2cf4ad-64ee-430a-8e85-335f69c1caf8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ---

---
**NOTE 2**

When training a model on large datasets it is recommended to run the data preprocessing 
in a first run in a **non-distributed** mode via `--preprocessing_only` so that 
when running the model in **distributed** mode in a second step the preprocessed data
can easily be loaded on each distributed device.

---

### Demo

In this demo run we pre-train a `""base-sized""` Wav2Vec2 model simply only on the validation
and test data of [librispeech_asr](https://huggingface.co/datasets/librispeech_asr).

The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available 
per device, consider reducing `--batch_size` and/or the `--max_duration_in_seconds`.
<Tip>

Take a look at the [`pipeline`] documentation for a complete list of supported tasks and available parameters.

</Tip>

## Pipeline usage

While each task has an associated [`pipeline`], it is simpler to use the general [`pipeline`] abstraction which contains 
all the task-specific pipelines. The [`pipeline`] automatically loads a default model and a preprocessing class capable 
of inference for your task. Let's take the example of using the [`pipeline`] for automatic speech recognition (ASR), or
speech-to-text.


1. Start by creating a [`pipeline`] and specify the inference task:

```py
>>> from transformers import pipeline

>>> transcriber = pipeline(task=""automatic-speech-recognition"")
```

The structure of this breakdown is to support relatively seamless support for CPU/GPU, while supporting doing
pre/postprocessing on the CPU on different threads

`preprocess` will take the originally defined inputs, and turn them into something feedable to the model. It might
contain more information and is usually a `Dict`.

`_forward` is the implementation detail and is not meant to be called directly. `forward` is the preferred
called method as it contains safeguards to make sure everything is working on the expected device. If anything is
linked to a real model it belongs in the `_forward` method, anything else is in the preprocess/postprocess.

`postprocess` methods will take the output of `_forward` and turn it into the final output that was decided
earlier.
```

To apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:

```py
>>> tokenized_billsum = billsum.map(preprocess_function, batched=True)
```

Now create a batch of examples using [`DataCollatorForSeq2Seq`]. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.

<frameworkcontent>
<pt>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```
</pt>
<tf>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=""tf"")
```

To apply the preprocessing function over the entire dataset, use 🤗 Datasets [`~datasets.map`] function. You can speed up `map` by 
setting `batched=True` to process multiple elements of the dataset at once. At this point, feel free to remove the columns you don't need.

```py
>>> processed_dataset = flat_dataset.map(preprocess_data, batched=True, remove_columns=['question','question_type',  'question_id', 'image_id', 'answer_type', 'label.ids', 'label.weights'])
>>> processed_dataset
Dataset({
    features: ['input_ids', 'token_type_ids', 'attention_mask', 'pixel_values', 'pixel_mask', 'labels'],
    num_rows: 200
})
```

As a final step, create a batch of examples using [`DefaultDataCollator`]:

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
 
Here is the question: What preprocessing strategy ensures scalability in ASR workflows?","The preprocessing strategy that ensures scalability in ASR (Automatic Speech Recognition) workflows is to run the data preprocessing in a non-distributed mode via `--preprocessing_only` in a first run, and then load the preprocessed data on each distributed device when running the model in distributed mode in a second step."
What is the primary benefit of Intel Sapphire Rapids for multitask ASR workflows?,"[(1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (1050, '3dd7da70-bcad-4e0f-8472-4305b11553cb'), (1051, 'df32721f-3e6f-4cee-9a43-f8fc965980b3'), (1052, '8485d62d-98bd-4d7b-ac26-8e4e589f2c90'), (1054, '0e5c47c0-b82f-425c-9f44-460bf81cd862'), (1056, '3bc16c57-9257-4f80-9f55-298c73652b36'), (6197, '36c5cec3-a6ae-4270-8e58-75e974db78d1'), (9286, 'c5343836-1184-4597-bf0e-0d37580c36fc'), (14409, 'c629ee91-7ca0-493d-90e2-e38fad5f7057'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19534, 'aa29101e-8223-47b0-8048-7918e9c1096d'), (28750, '909829c2-8f61-4347-b43a-4fbe10ee781f'), (28752, 'a8f3fc54-9b72-46a0-b4ac-29da5374d20e'), (28753, 'ce3e42fc-9f2b-4f7d-b77b-083e71dfe26a'), (28754, '2f370cf6-442c-48bd-a56c-8301e99110f6'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (19541, 'aef0b6eb-a3a4-4198-90b8-efd7ce09a5b4'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (28759, 'a16178be-a185-4015-93ef-67dbcd64a7eb'), (21080, '8a17b393-7880-420d-9b0d-415445f53d4a'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (7772, '41f15676-52c7-4b8c-8dd4-ec591cbf6295'), (19551, '6d04c0ce-0c33-450d-afcc-cda4a5256d48'), (9824, 'dd81f094-0a63-41f4-9d2d-7fcedd15ebb6'), (5729, '8325f704-8f9e-470a-9d07-777ab1401244'), (9828, '58c1416c-14cc-4c46-9c2e-85df9c86f7fc'), (5734, '4607726f-b051-415f-8f63-16cff07f61c5'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (5738, '1b3f6207-1eaf-4d30-b7f5-15ce5479c98a'), (16490, '762bf2a0-d570-4375-a9a7-243e14f50950'), (9837, 'd3636047-3bb2-436b-9685-4ec68f4523ad'), (3190, '9ac364e4-812c-4816-b2b7-cd39501c3080'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (4217, '1a8e3437-ea12-4e7b-b9d9-6929a9a940d2'), (14457, '24a91ac0-d333-49c9-a9a6-d6908a59a520'), (16508, 'f2d35370-8125-44f7-b499-2cc25d605af0'), (4733, '9090becd-bf3b-4322-a272-ea7519a0322b'), (4734, '45277eff-6f73-4fa7-ae08-09ab8f76bc8a'), (4225, '8a889917-10a2-425c-8ce5-9a4d7cf16589'), (4226, 'f995a19f-7e5a-4028-a50a-315c452620ee'), (4227, 'd9a4a5bd-4e54-4a56-8292-9967b567d5d8'), (10371, '5c19f562-1700-401e-a24f-c453e1909526'), (24718, '7a05e97f-6e48-4789-9119-b3405b2ec2f3'), (21144, '733daaa7-2d91-4c46-b007-fe08a12ee437'), (21145, '8f8f780e-b116-4a49-a443-62c3ed772cb9'), (21146, '52b3b9bb-9ff0-4cbb-85ea-66902b53e6a6'), (9882, '3d63264c-0ee7-49b5-ae7c-5dc424d9d8c7'), (21147, 'dd1291ef-03a7-4a6a-91f6-3ea791acefdb'), (17069, '4496b2e2-2c6b-48b8-bf4c-cc6ff08631d2'), (21179, 'b072bbe7-e1ae-424d-a1c7-6f7e80ecdb15'), (20693, '6ceda67f-d532-4fa1-99ca-1fe2e1e1fe8c'), (20694, '0a712ebe-d559-4eea-b2a2-7181b1e80f23'), (28375, '5d53bcfe-f7fa-4c29-ba2c-3401ac767c61'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (10973, '5d6db59d-1ca5-4c84-b8eb-9d478226c00c'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (5352, 'ed4e850a-88d1-45d9-b903-b070f0567d42'), (10989, '6bcf6d91-b596-48cd-903e-99802046e818'), (22775, '92b448c2-17a3-48db-9679-dfae336f0e2f'), (4869, '991872b9-4c4c-4b9d-965f-980c21d4fbce'), (4874, 'e733caa3-5f74-4bf7-b370-8d9bca505b85'), (24843, '21f7a134-1947-4f97-b2f3-504901357cbf'), (2833, 'f44aed77-24dc-4eb3-8b3d-a3c389b6e776'), (19220, 'c488928a-6d0c-4f53-8d0d-ccd5f6672069'), (2838, 'bf581280-a9b8-476e-9359-bb349445e1a0'), (2839, '1b8a013f-1039-402c-a79c-d53a4aa7068b'), (2846, 'ea46eb6f-007a-4605-996a-282ea8ae979d'), (16687, '2014f851-9280-4c04-9b47-ab629485a428'), (31034, 'a034f9fa-fd22-41d8-a2e8-629d5c7d0e2d'), (24383, '4b93009f-15f5-4e1c-bd98-faf19d8913fe'), (10051, '87f08473-69e9-406a-ba84-7aeea534a8b8'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (20322, '2ac8515a-2656-4500-b62a-859ab1fe9812'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (3940, 'e44eadc7-7fb7-4037-8a82-f44b8b79d45b'), (3941, 'a871cd71-4386-4492-ae52-7dda1294b854'), (3942, 'b94b5ec5-7ed0-4a6a-abb0-7f8bbc3cc4c3'), (3943, '3eb2b5e6-fc2b-4577-8b18-e8633255362d'), (3944, '4951a0e6-56b4-42c8-855e-2698bcf9658f'), (3945, '302238b3-6ba3-43d8-9806-25f6653471f9'), (17768, 'be4f212c-20bf-4311-8691-d86af603c103'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (374, 'eff4c155-2a49-4fc0-8c6e-118d6d9e3ee5'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (7065, '0fb31b29-d4f9-4207-9934-c683259bbb1a'), (5535, 'ab3d50f8-003b-47e8-8bde-07e4ffe97423'), (1451, 'f095b72f-5e6e-4d36-afc8-3aaab8db07ba'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (29112, 'f5df6a53-0df1-4420-9043-3d9ba923f41b'), (19384, '75f513c0-3b52-4f83-aec6-7b436161c162'), (29114, '131eb762-91f6-4098-b8e1-c638fb32b6eb'), (19387, 'fd8aa818-6add-4bbf-a661-88e05a30e489'), (19388, '141d4b90-0e49-4309-98b8-fb6890009430'), (19389, '332f6bf7-dbe6-4d54-abc2-cd9993918ba1'), (7612, '08676a44-b2b1-4d6f-bd21-fcce410c2dc0'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (11711, 'f0aab09f-a3dc-496a-9cd8-08e8d39a1306'), (19393, 'f4852e0f-e784-4fff-a019-f1accc480d2f'), (19397, '3a157ae9-2fa2-4ca9-8799-5aaeff723419'), (19398, 'c99ed4d1-a0f8-47fc-9f4d-4867015d3211'), (19400, '9f1742e8-339b-47ab-8ae4-16a900eb7538'), (26059, '056cb9da-4cf5-4de9-91bb-cd02195d02aa'), (26060, 'c163368d-4735-4bb6-8bf9-5b38c03c8927'), (19406, 'c711aaf2-7953-4aa9-97d3-0d5a01e92591'), (19407, '5f9399cd-785b-459e-bf05-4d7cc41174c0'), (19412, 'a5976ed5-05ef-4c3b-9355-6f6b1d46ba32'), (26073, '6d2e0fde-9092-4d31-a603-e5e841f1bc51'), (26074, '912c4a12-4d8d-40ed-86a4-1ff4e9b43b50'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (19428, 'de136064-8fa4-4750-89b5-91d689b7d535'), (26086, 'aa2bf598-35b3-47f0-b2c1-0a74dcd44c2d'), (19436, '22174e2d-567f-4064-87f2-30e6a36f3a3c'), (20463, '4cd629c7-7e89-48ab-aadc-dcbd3e778504'), (4089, 'ff79a3dc-ed28-4afe-b40a-fe017b03a4b4'), (4091, 'c0061148-3d18-405a-a8ee-9f1ef568fc2a'), (4093, 'a20ebed6-09b8-4472-884c-777dc540eb38')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In a follow-up post, we'll look at inference on Sapphire Rapids CPUs and the performance boost that they bring.

## Why You Should Consider Training On CPUs

Training a deep learning (DL) model on Intel Xeon CPUs can be a cost-effective and scalable approach, especially when using techniques such as distributed training and fine-tuning on small and medium datasets. 

Xeon CPUs support advanced features such as Advanced Vector Extensions ([AVX-512](https://en.wikipedia.org/wiki/AVX-512)) and Hyper-Threading, which help improve the parallelism and efficiency of DL models. This enables faster training times as well as better utilization of hardware resources.
Like Transformer models, you can fine-tune Diffusion models to help them generate content that matches your business needs. Initially, fine-tuning was only possible on GPU infrastructure, but things are changing! A few months ago, Intel [launched](https://www.intel.com/content/www/us/en/newsroom/news/4th-gen-xeon-scalable-processors-max-series-cpus-gpus.html#gs.2d6cd7) the fourth generation of Xeon CPUs, code-named Sapphire Rapids. Sapphire Rapids introduces the Intel Advanced Matrix Extensions (AMX), a new hardware accelerator for deep learning workloads. We've already demonstrated the benefits of AMX in several blog posts: [fine-tuning NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids), [inference with NLP Transformers](https://huggingface.co/blog/intel-sapphire-rapids-inference), and [inference with Stable Diffusion models](https://huggingface.co/blog/stable-diffusion-inference-intel).
Now, let's look at the new instructions in the Sapphire Rapids architecture.


## Advanced Matrix Extensions: New Instructions for Deep Learning

The Sapphire Rapids architecture introduces the Intel Advanced Matrix Extensions ([AMX](https://en.wikipedia.org/wiki/Advanced_Matrix_Extensions)) to accelerate DL workloads. Using them is as easy as installing the latest version of IPEX. There is no need to change anything in your Hugging Face code.
The AMX instructions accelerate matrix multiplication, an operation central to training DL models on data batches. They support both Brain Floating Point ([BF16](https://en.wikipedia.org/wiki/Bfloat16_floating-point_format)) and 8-bit integer (INT8) values, enabling acceleration for different training scenarios.
 
AMX introduces new 2-dimensional CPU registers, called tile registers. As these registers need to be saved and restored during context switches, they require kernel support: On Linux, you'll need [v5.16](https://discourse.ubuntu.com/t/kinetic-kudu-release-notes/27976) or newer.
 
Now, let's see how we can build a cluster of Sapphire Rapids CPUs for distributed training.

## Building a Cluster of Sapphire Rapids CPUs
This post will show you how to fine-tune a Stable Diffusion model on an Intel Sapphire Rapids CPU cluster. We will use [textual inversion](https://huggingface.co/docs/diffusers/training/text_inversion), a technique that only requires a small number of example images. We'll use only five!

Let's get started.

## Setting up the cluster

Our friends at [Intel](https://huggingface.co/intel) provided four servers hosted on the [Intel Developer Cloud](https://www.intel.com/content/www/us/en/developer/tools/devcloud/services.html) (IDC), a service platform for developing and running workloads in Intel®-optimized deployment environments with the latest Intel processors and [performance-optimized software stacks](https://www.intel.com/content/www/us/en/developer/topic-technology/artificial-intelligence/overview.html). 

Each server is powered by two Intel Sapphire Rapids CPUs with 56 physical cores and 112 threads. Here's the output of `lscpu`:
 
Here is the question: What is the primary benefit of Intel Sapphire Rapids for multitask ASR workflows?","The context does not explicitly mention multitask ASR workflows. Therefore, it is not possible to deduce the primary benefit of Intel Sapphire Rapids for multitask ASR workflows from the provided context."
What role does prompt tuning play in optimizing ASR workflows?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (4097, 'eb8c8ac8-c056-4fdb-b5a2-29a6a8a1c8f6'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (4098, '49bc81ae-8fd9-41bf-8201-2f4b4d7e565f'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9233, '90102358-5d1e-46af-a779-f2ce76242c11'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (16919, '942238c0-9ba8-45fc-bd5c-b13953074601'), (16920, 'cd0788e0-82b0-4b44-9dd4-8b8f83cf3ab9'), (16921, 'a31bb2a1-bb83-4766-8534-5664f075ae46'), (16922, '6ffff410-16b9-4bc9-88db-d6d4c3ff10bd'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (7203, '7a61dc03-12e8-49bb-b90b-975f57444e58'), (25138, 'd6090477-61e3-417d-83f6-edddc6ada8a2'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19537, '40665c5e-8a79-4732-9109-68975f6b56b0'), (19538, 'd960abba-de12-4057-8449-e4846415e745'), (11859, 'eca711ae-a436-4004-a11f-ed98258cbd62'), (15956, 'd03f6960-2286-4540-a440-e6bb93b141f6'), (19542, '8cb7e452-fdc5-4e18-ba20-570d1479be21'), (18521, '8becca4a-1c71-4ddd-84e1-63669674b2a9'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (19551, '6d04c0ce-0c33-450d-afcc-cda4a5256d48'), (3170, '547ca77f-648b-46d1-b2aa-1e8780f7216e'), (9836, 'f5501d12-d6b5-45a5-99f0-6d9407007a0a'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (16503, 'aee13d2f-6b06-4660-82a1-35d131edcf7e'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (30333, '463aafaa-1474-444d-8fe3-5bc8c8d95517'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (9368, 'a2674ef7-b70d-4243-8f59-ea725770523f'), (9370, 'db1c6e3e-6f2c-4a80-961d-6c6bfe33415e'), (25759, 'b2156363-4f81-4b08-a11e-6e35136835d6'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (15533, 'f1d6cadb-36c3-44ed-82c9-b7353e446fe4'), (17079, 'a22e8141-1b79-4327-968c-eb9f9d1bda26'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (22720, 'f2a11ea2-e26a-4d3e-9fa3-6c795d014219'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (29407, '670a8c58-a589-47b5-9578-452e87cb814b'), (29408, '3d78d5a6-4d2c-4f2b-9ede-60b9d32bbc33'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (7924, '9e9cce0b-318b-4407-8527-b450af5fc69f'), (7925, '898386e9-41b2-43b2-bf00-f5fe304bcf79'), (7926, 'f97b01a4-b979-42d6-8789-bec7870b5e30'), (7927, 'be9f55e3-25af-4774-95e1-537c53e8e6e5'), (18680, 'a2637c1d-1de7-4c47-aebb-03d0212e7052'), (7928, '3af4d2b1-479b-4154-9dfd-f8a6036bc83c'), (7929, '698a3ccc-aa9e-4fff-af7d-792856b4a975'), (7930, '60ff3e2c-3ad9-47ed-b921-c3cf85d5f93d'), (15102, '6dd80458-c2e1-4ca7-b85c-9030b176c7ab'), (6912, '39979d4d-c4d5-4967-a455-61a0a41fd8ba'), (15105, '613fa96f-a79e-4cb6-997f-817a7bd5d8dd'), (28935, '3726b37b-aa49-4e3b-8c46-b4ebd3388cbd'), (17163, '91671e10-cb42-42c8-80d7-529032a2f2a4'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (3864, '9e9d8aba-251e-4c73-a4df-662f1661fdf9'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (21280, 'bbe915f8-674b-4422-a8c3-9fbc5e7e1858'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (15654, 'a087c142-5d7a-4b96-b71b-6def7546f9b5'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (3901, 'b421effc-0dbb-491b-870c-a6041692844c'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (21324, 'c689a247-869d-4ea7-b9f0-170aa19a493b'), (8525, '931d0e8a-fa6c-4b84-9b4a-f7329eb19392'), (21331, 'c706e01b-6612-49e1-b624-510376de99e7'), (18780, '0ad209e7-8ada-48a6-9a9b-51482c2a6091'), (22371, '21051c79-d40f-41ae-ad3c-c20ce6cd2470'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (30576, '76618c07-7a1a-4ffb-bc13-6da43eb286d5'), (30577, 'e2e4e048-3402-4ac6-816a-a1c41eb5544b'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (21886, 'f51c2b7b-2bc9-43be-af59-961c40595ea3'), (21887, '2fc3be04-7823-41a9-af0e-6c649e006652'), (28554, '0ce55fbe-5081-4d09-a52f-35d88e7881bc'), (28555, '3900d0c8-9171-4b7d-bf48-27c131dec37b'), (28556, '6de4578f-f7a2-4271-a425-65baf13f4391'), (19853, '40bb732f-f865-45e3-bf29-9f8451bcb193'), (18832, 'e5e190bd-f7a3-4233-933f-552f425c3279'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (17821, '0ecce6f7-f4da-4215-8bc0-6acfc0848cca'), (17822, '20439f0d-de85-4a4f-91e9-58d6c81c72a7'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (28579, 'faeb2a24-8c7e-43f7-b3f4-3ae4017753be'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (28584, '4a2a9e1b-e158-4745-99c2-60ce2d30f36c'), (28585, 'd3223831-24e7-4557-a7a0-28f800f26eb0'), (16296, 'da19cd55-f253-4823-b44b-76b00a448890'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (29635, '1703fceb-c153-43cb-9e28-9f71223b471c'), (25034, '6df4e515-fab2-4473-8dc2-849bad23124e'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (26067, '61a8f2fe-8e9a-4fd1-994b-c13c84582f4a'), (19427, '6e19551e-2e96-47f6-9d74-921138792c97'), (19429, 'f722b877-d4a3-4934-976a-2a40c10274a9'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (16889, '4e9f1ab4-cab8-40fe-870f-69a3b341a3d7'), (16890, 'c8b38db6-3834-4660-9877-40161fc8646f'), (15868, 'defeffab-5c59-48a0-8058-9ff3d5f02504'), (15869, '5e46d120-df1b-4d36-bd63-b7491a1e4eff')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: [Prompt tuning](https://hf.co/papers/2104.08691) was developed for text classification tasks on T5 models, and all downstream tasks are cast as a text generation task. For example, sequence classification usually assigns a single class label to a sequence of text. By casting it as a text generation task, the tokens that make up the class label are *generated*. Prompts are added to the input as a series of tokens. Typically, the model parameters are fixed which means the prompt tokens are also fixed by the model parameters.

The key idea behind prompt tuning is that prompt tokens have their own parameters that are updated independently. This means you can keep the pretrained model's parameters frozen, and only update the gradients of the prompt token embeddings. The results are comparable to the traditional method of training the entire model, and prompt tuning performance scales as model size increases.
The main difference is that the prefix parameters are inserted in **all** of the model layers, whereas prompt tuning only adds the prompt parameters to the model input embeddings. The prefix parameters are also optimized by a separate feed-forward network (FFN) instead of training directly on the soft prompts because it causes instability and hurts performance. The FFN is discarded after updating the soft prompts.

As a result, the authors found that prefix tuning demonstrates comparable performance to fully finetuning a model, despite having 1000x fewer parameters, and it performs even better in low-data settings.

Take a look at [Prefix tuning for conditional generation](../task_guides/seq2seq-prefix-tuning) for a step-by-step guide on how to train a model with prefix tuning.

## P-tuning
*In this work, we explore ""prompt tuning"", a simple yet effective mechanism for learning ""soft prompts"" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's ""few-shot"" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method ""closes the gap"" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed ""prefix tuning"" of Li and
```

## Train

P-tuning uses a prompt encoder to optimize the prompt parameters, so you'll need to initialize the [`PromptEncoderConfig`] with several arguments:

- `task_type`: the type of task you're training on, in this case it is sequence classification or `SEQ_CLS`
- `num_virtual_tokens`: the number of virtual tokens to use, or in other words, the prompt
- `encoder_hidden_size`: the hidden size of the encoder used to optimize the prompt parameters

```py
peft_config = PromptEncoderConfig(task_type=""SEQ_CLS"", num_virtual_tokens=20, encoder_hidden_size=128)
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# P-tuning

[P-tuning](https://hf.co/papers/2103.10385) adds trainable prompt embeddings to the input that is optimized by a prompt encoder to find a better prompt, eliminating the need to manually design prompts. The prompt tokens can be added anywhere in the input sequence, and p-tuning also introduces anchor tokens for improving performance.

The abstract from the paper is:
 
Here is the question: What role does prompt tuning play in optimizing ASR workflows?","Based on the provided context, prompt tuning does not play a direct role in optimizing ASR (Automatic Speech Recognition) workflows. The context discusses prompt tuning in the context of text classification tasks on T5 models, and its application to sequence classification and conditional generation tasks. There is no mention of ASR workflows or speech recognition tasks."
What preprocessing considerations are critical for low-resource ASR evaluations?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (22528, 'cc18a340-ca81-4f1e-b314-02938b0d8511'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (21507, 'ecf3ffd4-7fde-49d9-b95d-327f07beb9f0'), (22529, '1ff19103-8daa-4bb0-ae94-e63b88c169a2'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30217, '659c043f-7c58-4769-b3e4-ae6ec788f25b'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (24597, '9e9272c4-0cf2-46ac-a0ec-8efd5bfe80aa'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (13360, 'ab3f6bf1-546f-4c7f-ade8-bfaa59be0cb6'), (9788, 'dc042abc-3f90-4ddb-9a04-4cd31ff8fba3'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14919, '35af750a-ff36-4c10-8d88-f116e2a480b8'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (19037, '26f42c79-e676-4f14-9252-568f13811fed'), (8807, '170b9534-cae9-40e8-bbe4-468c9b1e40f0'), (22120, '0f42d735-d8f3-47e8-9753-823a12cf2f75'), (25708, '0a161716-5f3c-45c5-aac2-4eb719f697f4'), (25709, 'd4436e38-6093-40ed-b39c-faee6f5bae46'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (22144, '53697ae7-3817-4e34-ac41-c9d6703921e2'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (23680, 'fca3e73c-0299-4597-b590-2aa537ced34f'), (11905, '36ef56a0-66da-48a3-9fa8-8f6e268d8f94'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (23682, 'bb01a0d1-67f1-439b-b0f2-5956be74a31f'), (22662, 'f32c3a87-abbc-4c09-a640-bf8926fe8342'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (9862, '33a9f610-ef4f-48c2-8df9-388fa0fd2a8f'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (28306, '3c0328c5-3673-42be-bc9a-3fe3d22eaa2d'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (22696, '47eb8a13-1c6a-4060-a807-7eb65a6df466'), (22699, '4f2bf23c-4a1f-42ce-8fcd-57d8b79aa246'), (24235, 'ea71c622-0f52-4f2f-a826-3643289d9388'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (16051, '7ef3b4d6-2896-4ff8-8c3e-15ec9ed4f6ec'), (9911, 'df3b7cf0-0e9c-474e-a7b8-0c3edb8eeb91'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (21192, '33a1caa5-4ef6-4cf7-ad0c-79c8a5bca80d'), (23241, 'c3465bca-0d65-4295-bcb3-ef82752ad478'), (9933, '361b3753-2f11-40ad-a8b0-91570e57b5d7'), (7897, 'b49e9eed-7307-41a6-ace9-39f2f03a63c0'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26338, 'c3ae9637-8b7a-42b5-af7f-76b6722ab0fc'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (15590, 'b5dec433-aff0-4c04-afd6-9f2beb8f529f'), (7400, '1d07b405-1719-43b8-8e99-83a98564ff4e'), (9958, '20edf0a1-57c5-4952-9cd7-83ba28261ddb'), (31473, '107a8b8a-6308-41aa-a924-46e82911e1bc'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (8436, 'cd5d3142-6714-4b42-a6e4-e1a1a7ec18f9'), (11001, '81fbae64-ae18-4503-9016-bebc33d649e2'), (26374, 'ab6b8f80-6158-4925-b405-c3ffdf036bd1'), (26390, '7e9534d9-70b4-497b-9fae-b424e5d8e1a3'), (4375, '65f6412b-c51e-4375-af24-295af347283d'), (8476, '570a83e4-cd08-4018-ada2-b19429c538d5'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (26923, '8d532e23-3301-4e90-9237-494eda2ae6e1'), (10550, '7c6765a2-95b6-4927-811b-cc057c9b726d'), (10552, '19f613db-4203-4d96-a744-9369443faac9'), (26937, 'a68f8601-700e-405b-868e-cf20c77cb9db'), (10555, 'bfe63e09-0fbc-492c-a580-7b3d9053592e'), (17211, '7ce7337b-fcc4-41d3-bbc0-478753c50a6a'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26946, 'd51e55e0-4a83-4946-a69c-5343acec6844'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (28485, '90b822fb-e259-470d-9f26-b588fed88f91'), (2889, '8961a3cb-971c-48cf-bf38-04f4f7a1d81d'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (29515, '18fb13c6-8761-4aea-ac01-1b771967088f'), (2896, '99c9721e-3a33-4487-a902-1af8000f24b2'), (16253, 'b4ce154c-ca23-4636-a910-14eea4792a6c'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (2432, '73c38322-cc61-4826-9870-4162d473eef8'), (18814, 'f732d8e9-6555-4bda-ba15-6b227351dcfa'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (3459, 'bc1a44b5-f64b-4598-85e8-e3280271e021'), (13201, 'f0b65266-be39-49d4-9757-564e662bb018'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (12180, '820aed82-3574-4ab3-96a2-991b0a57cbe1'), (13205, 'f30ad6ed-50d6-4c66-bf19-342d77107325'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (4510, '875f32ec-aa14-4e9f-892e-1ab5e108e275'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (4513, '59d5f033-bdc4-43a4-af13-03a83ae784db'), (4516, '9c4578f8-8a86-4670-9650-ea66bfff5aad'), (13225, '57dc4972-6e1b-45f7-aafc-58752ba3618d'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (14782, '1cc646cf-ac7b-4712-a786-9905b3365b13'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (25034, '6df4e515-fab2-4473-8dc2-849bad23124e'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ---

---
**NOTE 2**

When training a model on large datasets it is recommended to run the data preprocessing 
in a first run in a **non-distributed** mode via `--preprocessing_only` so that 
when running the model in **distributed** mode in a second step the preprocessed data
can easily be loaded on each distributed device.

---

### Demo

In this demo run we pre-train a `""base-sized""` Wav2Vec2 model simply only on the validation
and test data of [librispeech_asr](https://huggingface.co/datasets/librispeech_asr).

The demo is run on two Titan RTX (24 GB RAM each). In case you have less RAM available 
per device, consider reducing `--batch_size` and/or the `--max_duration_in_seconds`.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
```

### Automatic speech recognition

Automatic speech recognition (ASR) transcribes speech into text. It is one of the most common audio tasks due partly to speech being such a natural form of human communication. Today, ASR systems are embedded in ""smart"" technology products like speakers, phones, and cars. We can ask our virtual assistants to play music, set reminders, and tell us the weather. 

But one of the key challenges Transformer architectures have helped with is in low-resource languages. By pretraining on large amounts of speech data, finetuning the model on only one hour of labeled speech data in a low-resource language can still produce high-quality results compared to previous ASR systems trained on 100x more labeled data.

```py
>>> from transformers import pipeline
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
--
title: ""Fine-Tune MMS Adapter Models for low-resource ASR""
thumbnail: /blog/assets/151_mms/mms_map.png
authors:
- user: patrickvonplaten
---

# **Fine-tuning MMS Adapter Models for Multi-Lingual ASR**


<a target=""_blank"" href=""https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_Tune_MMS_on_Common_Voice.ipynb"">
    <img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/>
</a>

***New (06/2023)***: *This blog post is strongly inspired by [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2)* and can be seen as an improved version of it.
 
Here is the question: What preprocessing considerations are critical for low-resource ASR evaluations?","Based on the context, the preprocessing consideration that is critical for low-resource ASR evaluations is to run the data preprocessing in a non-distributed mode via `--preprocessing_only` before running the model in distributed mode. This allows the preprocessed data to be easily loaded on each distributed device."
How can multilingual fine-tuning improve ASR performance for underrepresented languages?,"[(21504, '2cab8554-08cc-4408-82b0-a29b1bca7783'), (30210, '04bf0147-a6c3-4e00-95a6-dcbb31e7033c'), (22531, 'ffeede51-2b21-449d-9995-464b61b90640'), (14852, 'd7d13b4b-ceec-438e-b1e4-c311204e6387'), (30214, '6ea50a37-4d40-43e6-bcb3-f78c85e07ce3'), (21511, '105c64de-f2b1-4fa7-9989-38ef0f828ff4'), (30220, '683831a6-5b21-4e34-8508-f7affdc57f5a'), (30222, '0d0bec8e-441f-42ef-8641-7361daf2972a'), (24595, 'fa056baf-de53-415b-a96e-98ef9e7209ff'), (9748, 'c91ae9ea-b42d-4bcf-af85-84126e3fd417'), (9747, '362139c4-a349-4533-8add-38dc8c990f5d'), (21530, 'c16fadf7-7dc9-4e2c-8db9-a8482ba6d61f'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (30269, 'ce1b2ec6-16bc-48d4-8e48-ee802e65abe6'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (25151, '15aa2c96-bd80-4f22-bfb0-0ec6a68df5d3'), (21569, '8a3f25b7-6b65-467e-bbf3-0948fa6daf40'), (14918, 'cae5945e-e32b-46c7-9523-f8582697ce25'), (14921, '4dcd179c-809a-4469-99a7-bc0e84c6ee2e'), (14924, '344dc950-fcf8-4b61-a878-e322c0e63038'), (8285, '544e9b1d-4837-4d80-957f-cbd37a213773'), (19047, '047fec8b-577a-401d-908a-4040a4a36e43'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (25707, 'a070ad83-13d6-4206-8c62-67a5128ccd1a'), (19056, 'f86121d0-9231-4005-81e3-49555d99cd27'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (22145, 'd6b77f30-bc64-4e31-99ed-26942ef40530'), (22148, '23248b2f-8cf3-41bd-9f95-52e4c5026196'), (22663, '7c9ca9a0-3008-4b77-b32b-406cecfc500a'), (21499, '285a3f3e-c209-4f3c-a103-9d92c504a34e'), (16016, 'd62003bd-6dc9-4f3a-be65-5f89b1ff17af'), (16017, '0159b9c3-ea7e-41d0-928e-fce397c3adfe'), (16019, '93f2e055-c2fe-4fc1-a64e-fd8f0315893c'), (16025, '441e2de3-ddea-4ada-a0db-2230bca2c66b'), (16027, '41f24992-4f5e-433c-aba4-8ed79519f16a'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (22699, '4f2bf23c-4a1f-42ce-8fcd-57d8b79aa246'), (22702, '1832154a-11c9-48e4-846a-e2b0f0cfb6ac'), (696, 'b996a5f4-7c16-44f1-aa17-bcf65b15101a'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (17097, '99a5ba3d-0682-444e-a216-6bec9c5496d4'), (719, 'fb7642ad-50bf-4c6a-9238-356b54ef0a53'), (5840, '3f3f00fa-5009-4d05-af30-99114a39e072'), (17622, '18cebb0d-bc71-4646-91ee-08fdc8fb51a7'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (26335, '24c46ed4-59a7-459d-b99d-97640cecf4e9'), (26342, 'd66d13fa-6ef4-435f-bd33-7df86411fb9f'), (17645, 'f702f2c7-5e5b-457d-bfe4-83adfb433795'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (24827, 'e7273685-0ec1-47f5-b09a-213ff45b3ea1'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (26909, '7d24af93-77f0-44e4-9431-e7bbd169b6f7'), (26910, 'bae59c82-ad86-496d-8586-f56f11670ed0'), (26911, '58ee795f-486f-4c5d-b21f-bddf874022de'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (23353, '209f08ab-4109-44e3-9822-fbb3609f60fd'), (10555, 'bfe63e09-0fbc-492c-a580-7b3d9053592e'), (10558, 'd0242be1-75d4-479c-9ba8-c23684e95a71'), (26948, '55791d56-ca14-4075-abe8-eff439a32353'), (30534, '95e21773-3553-4d8f-8b88-fff6315116ba'), (30535, '8522cdf6-9dc5-4ed1-9a89-16450bb0eac7'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (7000, '8d065847-3189-4122-97bf-d861398ddd06'), (13662, '54defe53-8805-4848-86c0-de658eca9688'), (8037, 'acff8297-9bab-439f-a6bd-861a8902de62'), (24423, '1e078b3b-3dff-4ed0-ab85-ba089bee8cb1'), (4460, 'aacb8d4f-efdb-42c0-922d-177bc7320339'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (13183, '6761a128-2aaf-475e-896b-51aeea2b197f'), (13185, '40273aff-d0d5-4294-b17a-61b5ffeb0e6b'), (13187, '8b722b77-0d33-414a-8708-90e44f17e8a1'), (30595, '827cae8b-22d5-4c39-8f8b-c5765d3c606f'), (13194, 'bac69453-4075-4d86-bbf3-f6ba449aabd3'), (13202, '0571b56c-fede-4d1f-9c1e-e5c28103a364'), (13210, '46e713b6-addb-493b-9174-75dfbc52e908'), (4509, 'a366d654-2430-4ef3-92bc-eaeb6bf4321a'), (9640, '8d7047fe-653b-4dc3-9771-979787c23e4c'), (7594, '74de14bd-6185-4ef4-bb4c-05b90db980d9'), (13233, 'eabe395c-7bc6-45aa-a2c4-3188bb695532'), (13236, '3f317f00-83aa-4cf7-924b-21158ea8cec5'), (23988, '53279c34-0659-489a-8752-7193605725d0'), (13238, 'fb6559e8-c5eb-4c79-82d9-d2d3c36c1463'), (4534, '3ff4dd5c-528a-4039-b0d8-9c5fd34866bb'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (14783, 'e56833b5-ce96-4139-8921-074b7184613a'), (29634, 'd756cc17-3381-4c73-99de-56f9d281f772'), (29635, '1703fceb-c153-43cb-9e28-9f71223b471c'), (25033, '1414432c-aab4-4c4c-b04a-1e8d688255ed'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (8659, '9cfba007-db65-4212-8b6c-43fafafc79df'), (7126, '8cfe90cf-ced0-4b81-be50-79a409bfc561'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (1522, '28731235-d234-44e1-9e19-7f8f6084cb19'), (21494, 'a57ff624-5e87-4c4b-8121-b27e168c439e'), (21496, 'f96184b1-16da-4757-a9cc-e15b83d07916'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (22523, '4001169b-9ad5-4b98-bacc-782f3efa48f1'), (21500, 'a661c762-ac97-47c9-bd3c-f56ee5ae17ca'), (22525, 'ce74dfc6-6e89-40e9-8673-198cf8c6d770'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

While the fine-tuned model yields satisfactory results on the Common 
Voice Hindi test data, it is by no means optimal. The purpose of this 
notebook is to demonstrate how the pre-trained Whisper checkpoints can 
be fine-tuned on any multilingual ASR dataset. The results could likely 
be improved by optimising the training hyperparameters, such as 
_learning rate_ and _dropout_, and using a larger pre-trained 
checkpoint (`medium` or `large`).

### Building a Demo
Now that we've fine-tuned our model, we can build a demo to show 
off its ASR capabilities! We'll use 🤗 Transformers 
`pipeline`, which will take care of the entire ASR pipeline, 
right from pre-processing the audio inputs to decoding the 
model predictions. We'll build our interactive demo with [Gradio](https://www.gradio.app). 
Gradio is arguably the most straightforward way of building 
machine learning demos; with Gradio, we can build a demo in 
just a matter of minutes!
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results. We'll show just how Whisper can be fine-tuned 
for low-resource languages in this Colab.
When scaled to 680,000 hours of labelled pre-training data, Whisper models 
demonstrate a strong ability to generalise to many datasets and domains.
The pre-trained checkpoints achieve competitive results to state-of-the-art 
ASR systems, with near 3% word error rate (WER) on the test-clean subset of 
LibriSpeech ASR and a new state-of-the-art on TED-LIUM with 4.7% WER (_c.f._ 
Table 8 of the [Whisper paper](https://cdn.openai.com/papers/whisper.pdf)).
The extensive multilingual ASR knowledge acquired by Whisper during pre-training 
can be leveraged for other low-resource languages; through fine-tuning, the 
pre-trained checkpoints can be adapted for specific datasets and languages 
to further improve upon these results.
```

## Closing Remarks

In this blog, we covered a step-by-step guide on fine-tuning Whisper for multilingual ASR 
using 🤗 Datasets, Transformers and the Hugging Face Hub. Refer to the [Google Colab](https://colab.research.google.com/github/sanchit-gandhi/notebooks/blob/main/fine_tune_whisper.ipynb) 
should you wish to try fine-tuning for yourself. If you're interested in fine-tuning other 
Transformers models, both for English and multilingual ASR, be sure to check out the 
examples scripts at [examples/pytorch/speech-recognition](https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition).
Meta AI's most recent release, [**Massive Multilingual Speech (MMS)**](https://ai.facebook.com/blog/multilingual-model-speech-recognition/) by *Vineel Pratap, Andros Tjandra, Bowen Shi, et al.* takes multi-lingual speech representations to a new level. Over 1,100 spoken languages can be identified, transcribed and generated with the various [language identification, speech recognition, and text-to-speech checkpoints released](https://huggingface.co/models?other=mms).

In this blog post, we show how MMS's Adapter training achieves astonishingly low word error rates after just 10-20 minutes of fine-tuning. 

For low-resource languages, we **strongly** recommend using MMS' Adapter training as opposed to fine-tuning the whole model as is done in [""Fine-tuning XLS-R on Multi-Lingual ASR""](https://huggingface.co/blog/fine-tune-xlsr-wav2vec2).
 
Here is the question: How can multilingual fine-tuning improve ASR performance for underrepresented languages?","According to the context, multilingual fine-tuning can improve ASR performance for underrepresented languages by leveraging the extensive multilingual ASR knowledge acquired by Whisper during pre-training. Through fine-tuning, the pre-trained checkpoints can be adapted for specific datasets and languages to further improve upon the results. Additionally, using MMS' Adapter training is strongly recommended for low-resource languages, which can achieve astonishingly low word error rates after just 10-20 minutes of fine-tuning."
What is the primary difference between ViLT and BLIP-2 models for Visual Question Answering?,"[(18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (31289, '9905fe51-ba24-492b-9d83-93aa4879a98b'), (31291, '728e3f48-a642-437c-b7b8-adf9580dc871'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (4667, 'ea371344-8b31-4041-8d3f-3a2d817bd4bb'), (4686, '59de41e9-0c89-49cd-9d19-e7b1fabd0bc4'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (4690, '73b0a8e6-ac3c-4071-b88d-beb7478681aa'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (20576, '40796d31-f95c-4e9d-a36b-2d6319601be0'), (20577, 'f3e33c55-54fa-4265-9f37-a4027f1529a8'), (613, 'af7aa22e-dddb-4910-bca7-0f5b3b336883'), (615, '6b9d1f09-198a-433d-88c8-f3161c71b310'), (20585, '33dcb7a1-776c-43ad-a7ce-be6345e2e6b9'), (3211, '66850aca-1339-44e1-b6ee-3612c185f4d3'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (4772, '2a92752e-7e2a-4b91-89ad-7510a946ebde'), (20651, '9c5a8380-0788-4e02-96cc-ae00c3f65050'), (4780, 'b2f80597-99f6-413f-9480-44d89c891e86'), (20654, '0dfc500c-a1cd-464b-917d-6d60a2832b61'), (11953, 'e5b47149-4a3e-4549-a9ef-c12eccfe493f'), (4786, '6e8dbd73-505a-4539-ad2a-55fb27d85c4f'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (1745, 'cd32a895-3ae9-4f82-8574-44ad1d9286e8'), (1746, '49e8dce4-afe5-4268-9ab9-6f9809115e1e'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (16090, 'de34769a-90f0-4c82-b82e-6d9a5a0e6f92'), (1759, 'bf7ed42d-f9db-4600-928b-8a5c2981b288'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (1769, '995d0156-b149-4fea-94ea-418d999909b4'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (4850, '3a950f12-3c2d-47ce-a6a8-1abdfb1befa0'), (3827, 'fe2a1935-1ae8-467c-85b8-dfb7bd441b10'), (3826, '256f85ac-16ed-48b7-8a96-c986ccb76483'), (4852, '44fd3946-6b3b-4d71-a95c-1553a282aa4b'), (4851, '7e0cad1c-05a0-4c88-b14a-c88e02040671'), (4854, '2e081be3-5c8c-4925-9a87-f6bb0a1a7c12'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (6909, '84556055-9fa1-4cff-bfe5-073bcd5b7f97'), (29437, 'a9873ed2-2cf8-44cc-9b63-9ba1c9d4a818'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (1795, '4930cd63-b8c3-4a27-b9b2-e2db184cbbfc'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (1823, '3d5fb357-1a86-4d95-8f1d-c77d575b1880'), (1824, '7b77c59e-36af-44d2-a1f5-b832deccf49b'), (25377, '11930567-432f-44a9-b953-a45bd742357d'), (1826, 'caefe288-d31b-42db-bba2-5e3087bd1325'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (4421, '95a1272e-1f19-494f-96d5-f36f9a919a0a'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (4965, 'f1c54284-48b0-429f-8306-c0a1ef88bda9'), (4967, '43778b66-3e71-4fd3-a9bf-f2cf850f713d'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (25963, 'fd0bea05-61c9-4e1b-8a8b-5171642f2f0d'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25965, '1cda8dc4-5b16-484d-b64c-99fc5b9e9d79'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (20851, 'b8c3710a-efdb-41e6-9f78-8cb777be59d6'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (4986, 'd1a0c49b-c4b7-4f64-b176-5c34266e3eaa'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16263, '548b0220-ed2d-461b-a176-886ee094b6d0'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (16267, '7ac5f372-d956-427b-832d-160d6a912937'), (5530, '89aa49e3-2bb4-41a3-8831-f5755770ebde'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (30130, '701c9d28-b4a0-4d27-8f27-eef55e68d62c'), (30133, '61f3f5a8-2c83-44a2-9a84-594c52ec5c89'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (2503, '2e1ff638-b3dd-42ac-9810-3e463ba23b97'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (10193, '26d1bfcb-e936-427b-92c3-e7dd97a94093'), (10194, 'e573dcfe-a4f3-435f-9afe-101265a292e5'), (13267, 'd14a13b5-d0f7-4354-9963-a77a57948d2c'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (11262, '5c2061dd-7364-4daa-bb21-a1404baa2b55')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
BLIP is a model that is able to perform various multi-modal tasks including:
- Visual Question Answering 
- Image-Text retrieval (Image-text matching)
- Image Captioning

The abstract from the paper is the following:
```

## Conclusion

BLIP-2 is a zero-shot visual-language model that can be used for multiple image-to-text tasks with image and image and 
text prompts. It is an effective and efficient approach that can be applied to image understanding in numerous scenarios, 
especially when examples are scarce. 


The model bridges the gap between vision and natural language modalities by adding a transformer between pre-trained models. 
The new pre-training paradigm allows this model to keep up with the advances in both individual modalities.


If you'd like to learn how to fine-tune BLIP-2 models for various vision-language tasks, check out [LAVIS library by Salesforce](https://github.com/salesforce/LAVIS) 
that offers comprehensive support for model training.


To see BLIP-2 in action, try its demo on [Hugging Face Spaces](https://huggingface.co/spaces/Salesforce/BLIP2).


## Acknowledgments
```

## Zero-shot VQA

The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach 
VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training 
paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). 
This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. 

Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a 
GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: 

```py
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
## What's under the hood in BLIP-2?

BLIP-2 bridges the modality gap between vision and language models by adding a lightweight Querying Transformer (Q-Former) 
between an off-the-shelf frozen pre-trained image encoder and a frozen large language model. Q-Former is the only 
trainable part of BLIP-2; both the image encoder and language model remain frozen. 

<p align=""center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/blip-2/q-former-1.png"" alt=""Overview of BLIP-2's framework"" width=500>
</p>

Q-Former is a transformer model that consists of two submodules that share the same self-attention layers: 
* an image transformer that interacts with the frozen image encoder for visual feature extraction 
* a text transformer that can function as both a text encoder and a text decoder
 
Here is the question: What is the primary difference between ViLT and BLIP-2 models for Visual Question Answering?","The primary difference between ViLT and BLIP-2 models for Visual Question Answering is that ViLT treats VQA as a classification problem, whereas BLIP-2 approaches VQA as a generative task."
How does Hugging Face's pipeline API simplify model deployment for multimodal tasks?,"[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4179, 'b6031fcd-da9e-4a41-ac79-36bf3eaa3bd0'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (4285, '267cef4b-a36b-4d83-89dd-8f9a385949a8'), (20673, '0ad1dd04-3ec3-4c8f-9d67-b44414433153'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (21188, 'e53bd9c2-9844-411f-810d-5e35fa824c01'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (16073, 'd4c6dc63-a9fc-4dae-9329-b1960ec19a2d'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (20223, 'a0ff3a3b-b5b7-4fcf-9f9d-68877d1cc4ad'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (31502, 'bb929da8-ea35-46c0-ad9a-f62f13fdb41c'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (14725, 'f93b2fc4-f0b6-42bb-9b39-5f3d69f3047b'), (20876, 'a784cd91-18de-4adf-a288-2724d2b961e1'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (404, '7c743e20-b03d-41db-9603-c4b8802e06a4'), (22421, 'bdae70d4-1482-4a7a-aa11-6e2aa1cd81c2'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (6041, 'e84e5c9b-3577-44c7-bff0-02e32c685dc4'), (6042, '5b1abefc-94b9-4cfd-99ca-23843fd92410'), (29593, '6a3d74c2-ed43-42ed-8939-0c89a33f7be8'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (6047, '162b5640-f06a-4c27-97b1-d3c317bbb669'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (6050, '3cb3a71c-89ac-4817-8a94-51f7aa7240cd'), (6053, 'b575f6f7-3134-49a8-8c9a-2a8136436019'), (6056, 'aba34742-67ab-47e8-a239-b5b49e9d199d'), (6060, '91bbfe9c-7eca-4bf4-baa9-b087c9640f38'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (31672, '94b34b38-3568-488b-94eb-579ac5ccf43e'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (21447, '8b7f3a13-78d2-4c1a-b32a-51b280baae78'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (24025, '95318624-c2ce-41c0-891f-a3b06efb2458'), (6106, '13c9c3c6-6d87-4cba-a886-332bfef5da3d'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (30202, '4e035267-8efe-4a23-b5fb-0f711e2929d4'), (13822, '964f0aa5-5fa7-4de6-a8e9-51e5d5428795'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Adding new tasks to the Hub

### Using Hugging Face transformers library

If your model is a `transformers`-based model, there is a 1:1 mapping between the Inference API task and a `pipeline` class. Here are some example PRs from the `transformers` library:
* [Adding ImageClassificationPipeline](https://github.com/huggingface/transformers/pull/11598)
* [Adding AudioClassificationPipeline](https://github.com/huggingface/transformers/pull/13342)

Once the pipeline is submitted and deployed, you should be able to use the Inference API for your model.

### Using Community Inference API with a supported library

The Hub also supports over 10 open-source libraries in the [Community Inference API](https://github.com/huggingface/api-inference-community).
We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for prototyping, comparing it against AWS Sagemaker and Hugging Face’s new model hosting Inference API. Given that we use GCP for data storage and Google Vertex Pipelines for model training, exporting models to AWS Sagemaker was clunky and bug prone. Thankfully, the set up for Hugging Face was quick and easy, and it was able to handle a small portion of traffic within a week. Hugging Face simply worked out of the gate, and this reduced friction led us to proceed down this path.
Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!

### Finding models

Over 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.


### Widgets
```



## **Deploy one of the 10,000+ Hugging Face Transformers to Amazon SageMaker for Inference**

To deploy a model directly from the Hugging Face Model Hub to Amazon SageMaker, we need to define two environment variables when creating the `HuggingFaceModel`. We need to define:

* HF_MODEL_ID: defines the model id, which will be automatically loaded from[ huggingface.co/models](http://huggingface.co/models) when creating or SageMaker Endpoint. The 🤗 Hub provides 10,000+ models all available through this environment variable.
* HF_TASK: defines the task for the used 🤗 Transformers pipeline. A full list of tasks can be found[ here](https://huggingface.co/transformers/main_classes/pipelines.html).

```python
from sagemaker.huggingface.model import HuggingFaceModel
 
Here is the question: How does Hugging Face's pipeline API simplify model deployment for multimodal tasks?","Hugging Face's pipeline API simplifies model deployment for multimodal tasks by providing a 1:1 mapping between the Inference API task and a `pipeline` class, allowing for efficient methods to reduce computation during each forward pass, and supporting over 10 open-source libraries in the Community Inference API."
What preprocessing steps are critical for preparing data for ViLT fine-tuning?,"[(11267, 'c3f0d944-1538-4736-bc5b-f1e7ab372a64'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (20000, '99cbdd98-6401-42b2-862f-4784d4f44e11'), (28717, '45cc6367-7b38-4f30-a36f-c96fb2293202'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (15420, '8ac3193e-579b-4ffe-9257-4fbbd1a77881'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4678, '2bd57fe6-d068-4693-a0a4-20061aca479f'), (4679, '1c426ec4-266d-40f4-b280-093732557e19'), (4680, 'f1b71842-c2a8-4de7-b6d6-b9115f88900c'), (4681, '7bc70455-75da-43e5-b8f8-bc8b001be9be'), (20041, '0521172d-93c3-47ab-8eb4-57b9a744de8f'), (4683, '7c1ca1c0-b096-4cdc-8e23-344f2a8241e1'), (4682, '8a06f5b7-f648-47b4-ad91-52a4cc50ac01'), (4686, '59de41e9-0c89-49cd-9d19-e7b1fabd0bc4'), (4687, '504ece2c-4a64-4dad-8929-ea172c5502f8'), (2647, 'a9b33316-4844-4d79-ace2-5d36fae71281'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (24666, '170a3fce-1f01-4e51-93bd-5117de3c3e22'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (26211, '50d45e9c-48aa-40ff-9c02-17e59100ffb4'), (26212, '8d5de3f2-1ac9-40c8-b7e9-607000e02cd5'), (10853, 'cc7466fe-c647-4c7e-a1ce-4288a078ba55'), (20071, '163a9ba3-cb9e-4a4f-bdd6-07ceaa145535'), (17519, '081958cd-a4a2-4ae6-8466-45cf710dfac1'), (7352, 'af9cca0a-f231-47a9-b943-dc040ea91c49'), (16022, '6c0867a3-4c0a-4968-8467-0a3a3cbb139b'), (16023, '58450f41-913e-43f0-83b5-33752282fcad'), (13980, 'a1b5a2e5-b7b6-4914-b2db-b07eb318ca48'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (688, 'dd7ba60b-ae7e-4773-acba-8c59fe920da9'), (695, '60a9d164-5477-4fb2-9800-d7082f6cb1ec'), (696, 'b996a5f4-7c16-44f1-aa17-bcf65b15101a'), (697, 'a79771f8-9f63-4615-8faf-cca019cc8939'), (23737, '11452b2c-8fcc-4643-a576-672d0c14b723'), (699, '8a2bbe2f-1190-41eb-b0dc-c82929da31d3'), (700, 'b300514d-f85d-4fdc-b535-980caa8a7eb6'), (701, 'b2d32815-445e-4190-8674-fec1ed50b7be'), (5309, '1063e5a8-266b-4339-bd57-f04a35f278f7'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (17087, '89258315-ff35-47ea-8f5d-62939612d015'), (5308, '0448ee11-7a7f-46fc-b497-22e65c8e98f4'), (709, '68b25497-c5e6-494d-aebf-9f31f7c98cc0'), (710, '01412160-d233-49a5-824c-d832796204fd'), (711, 'a4245782-6f60-4474-8005-f7b3118c404e'), (716, '3ac83e0f-ae99-4341-a5c8-b350a45098ad'), (17614, 'a4127615-83f7-416f-8953-35d7a99ae728'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (17621, '62b44da9-9424-43fe-b9b0-7350941336d3'), (17622, '18cebb0d-bc71-4646-91ee-08fdc8fb51a7'), (17623, '9ffafb20-3956-436d-ae73-eb7132a07981'), (17625, '50d73a1f-f490-4862-9f0f-c5665c0a57b8'), (17626, '7f59372b-cf68-4fbe-8aad-444ff614a6e7'), (731, 'ad4324cd-9c6c-4555-88d6-faf72cb8a9da'), (17627, '55fafd2b-3136-4290-b07b-fa6bf7eb5624'), (730, 'ea1ec52d-453d-4367-9ad2-cf17a9a3390e'), (732, 'd42b2bda-6ad0-4591-845a-4dd23901ca1a'), (735, 'c25f03c5-5326-4c07-ab56-3fb43f185d39'), (736, '4048342d-d66a-4523-baf0-d4c0cc020390'), (734, 'c5d74325-5484-4003-b26b-a16eac786dd6'), (31458, '772a8786-3e64-4959-84fa-931ce6254e05'), (17635, 'cb4c9589-721d-4162-839c-0a37be083184'), (17636, '43108549-f353-47b1-9c09-0cf0cd5e34c6'), (739, 'cb9b0c29-bf55-4e97-82d9-32783b8d6e54'), (15074, 'b21ad07b-c656-46c0-9be3-67f9761a4bfe'), (17637, 'd6cc36a7-f86a-4baa-9935-267d8eec3fcd'), (17642, '65fcc7ea-697c-444e-8b1c-9b81f01558fe'), (3829, 'a49bb9d9-ee4c-4e24-918b-fa5755dc6d28'), (11000, 'd8ad9374-4d7a-42cb-a6d2-a2dcc21fe7af'), (17656, '26a8bb3d-06d1-4e30-92b6-f78052157af5'), (17657, '12121e46-a187-42c0-a9ad-22a96e02c8b9'), (17658, '9d5fe55e-4bcc-4604-991c-a59dd47f9d33'), (17660, '58d387a6-27dd-442a-9d56-0e701018407e'), (17661, '64a5806b-66d6-44ce-ae4d-3cac3d217bde'), (17662, 'bae077a4-1a20-468a-9815-c0200cbf09b1'), (7416, '603d0497-9dca-491b-990e-1d3feded2210'), (17665, 'daf0b953-1682-4e7b-b581-3054986b069c'), (13064, '589f6f88-a4f1-45d4-863a-1d5eee88fa12'), (25356, '7fc6f9b3-10fd-48c7-95bd-fd7ba0cab29a'), (25357, '7931f736-b8bf-4e96-9f36-6d81f74d14c6'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5908, '144a7b30-6985-4f7d-adc3-de47096fc6ab'), (3864, '9e9d8aba-251e-4c73-a4df-662f1661fdf9'), (28451, '88706f8a-d83c-4cf1-be9f-45d38463d2b6'), (28966, 'e20c96fb-f68b-49eb-80b6-4dae3c0265dd'), (3879, 'c4e9db23-1143-48f2-aca9-0edfed4fafa9'), (1331, 'ebb87a3b-0175-4534-847a-4a99308db7bd'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (28473, 'e49da37f-50e8-4501-9439-6d0940c836bf'), (28474, 'a977933e-4805-4c69-ad24-6071d97d1c8e'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (7498, 'c69d9b19-9b79-4d2a-8817-7c4bedc3cc18'), (6998, 'cf13833c-718d-4919-974f-7f19e98a9b0d'), (7000, '8d065847-3189-4122-97bf-d861398ddd06'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (23919, '0c05c74a-e41d-471c-8a4e-48d652e115c3'), (30576, '76618c07-7a1a-4ffb-bc13-6da43eb286d5'), (23928, 'fa312533-bb24-4e27-982f-befb97bafd49'), (21882, 'df91fcb9-421c-4304-8cfa-7ab50bd9d3fa'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (3979, '052eb59c-fd2f-4556-8484-c015cc0f2c33'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (16296, 'da19cd55-f253-4823-b44b-76b00a448890'), (30133, '61f3f5a8-2c83-44a2-9a84-594c52ec5c89'), (4547, '9d7a8092-0c89-45c6-990c-7ac6a0743ada'), (25034, '6df4e515-fab2-4473-8dc2-849bad23124e'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (22485, '13852224-673a-4353-9524-8a0123f2dcc8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

Now that we have the mappings, we can replace the string answers with their ids, and flatten the dataset for a more convenient further preprocessing. 

```python
>>> def replace_ids(inputs):
...   inputs[""label""][""ids""] = [label2id[x] for x in inputs[""label""][""ids""]]
...   return inputs


>>> dataset = dataset.map(replace_ids)
>>> flat_dataset = dataset.flatten()
>>> flat_dataset.features
{'question': Value(dtype='string', id=None),
 'image_id': Value(dtype='string', id=None),
 'label.ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),
 'label.weights': Sequence(feature=Value(dtype='float64', id=None), length=-1, id=None)}
```

## Preprocessing data

The next step is to load a ViLT processor to prepare the image and text data for the model. 
[`ViltProcessor`] wraps a BERT tokenizer and ViLT image processor into a convenient single processor:

```py 
>>> from transformers import ViltProcessor

>>> processor = ViltProcessor.from_pretrained(model_checkpoint)
```

To preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. The processor will use 
the [`BertTokenizerFast`] to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids` for the text data. 
As for images, the processor will leverage [`ViltImageProcessor`] to resize and normalize the image, and create `pixel_values` and `pixel_mask`.

All these preprocessing steps are done under the hood, we only need to call the `processor`. However, we still need to 
prepare the target labels. In this representation, each element corresponds to a possible answer (label). For correct answers, the element holds 
their respective score (weight), while the remaining elements are set to zero.

The following function applies the `processor` to the images and questions and formats the labels as described above:

```py
>>> import torch
- The quickest way to get started with ViLT is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViLT)
  (which showcase both inference and fine-tuning on custom data).
- ViLT is a model that takes both `pixel_values` and `input_ids` as input. One can use [`ViltProcessor`] to prepare data for the model.
  This processor wraps a image processor (for the image modality) and a tokenizer (for the language modality) into one.
- ViLT is trained with images of various sizes: the authors resize the shorter edge of input images to 384 and limit the longer edge to
  under 640 while preserving the aspect ratio. To make batching of images possible, the authors use a `pixel_mask` that indicates
  which pixel values are real and which are padding. [`ViltProcessor`] automatically creates this for you.
- The design of ViLT is very similar to that of a standard Vision Transformer (ViT). The only difference is that the model includes
```

Even though not very confident, the model indeed has learned something. With more examples and longer training, you'll get far better results!

You can also manually replicate the results of the pipeline if you'd like:
1. Take an image and a question, prepare them for the model using the processor from your model.
2. Forward the result or preprocessing through the model.
3. From the logits, get the most likely answer's id, and find the actual answer in the `id2label`.

```py
>>> processor = ViltProcessor.from_pretrained(""MariaK/vilt_finetuned_200"")

>>> image = Image.open(example['image_id'])
>>> question = example['question']

>>> # prepare inputs
>>> inputs = processor(image, question, return_tensors=""pt"")

>>> model = ViltForQuestionAnswering.from_pretrained(""MariaK/vilt_finetuned_200"")

>>> # forward pass
>>> with torch.no_grad():
...     outputs = model(**inputs)
In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
 
Here is the question: What preprocessing steps are critical for preparing data for ViLT fine-tuning?","The preprocessing steps critical for preparing data for ViLT fine-tuning are:

1. Replacing string answers with their ids and flattening the dataset.
2. Encoding the images and questions using the `ViltProcessor`, which includes:
   - Tokenizing the text using `BertTokenizerFast` to create `input_ids`, `attention_mask`, and `token_type_ids`.
   - Resizing and normalizing the images using `ViltImageProcessor` to create `pixel_values` and `"
What is the role of `DefaultDataCollator` in Hugging Face training workflows?,"[(21506, '10e81bf1-e8b5-4532-acba-49c7f26175b9'), (10245, '022ae3cf-1084-40e3-b8c7-875a87e42cb4'), (28678, '8dfe9ddd-650b-425a-a742-d9a56022e06b'), (10248, 'ff896a10-9c03-49b8-9f92-67794ee39f38'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (1547, '90958293-5007-49f3-a95c-a714ec2ec897'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (1551, 'b2b7b102-cf63-4933-babf-b2616a5830c9'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (23073, 'c8719aa6-c675-4f8d-a578-bcb628162ea5'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (5155, '86625bb7-792a-426a-9ade-ea98317da2e0'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (23087, '78540d5c-200a-44a6-80b8-ac019e34baf9'), (18489, 'c196cc2b-dfd6-4797-a458-54ad07efda99'), (24640, '815d70f7-2c5b-4af2-8a4e-fa377374ffad'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (30788, 'e6d02e40-3fcd-462e-b943-0494c7d97dc3'), (26693, '74a1767c-cb28-44b5-9581-88def78019d3'), (1609, '335231ef-3cdd-4154-bea0-92054d45edc3'), (4682, '8a06f5b7-f648-47b4-ad91-52a4cc50ac01'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10840, '17512ae9-2273-4239-be81-2a79343a68e4'), (11874, '22a43f8f-a4b0-4a36-a63e-2c040cfcbfca'), (26212, '8d5de3f2-1ac9-40c8-b7e9-607000e02cd5'), (24684, '75f9561f-771f-480e-8772-129ea9d0bfe8'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (18548, 'd7720c9a-8876-4847-b0c5-0da58c90fa8c'), (19064, '95bc1665-922e-432b-81c8-6d235a40160b'), (21136, '6940c909-8fdc-4dfe-bd8c-d617a9c1cb8a'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (21153, '0e8dd5cf-1222-409e-8efa-1a63c0f03561'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (6838, '914b0e6c-1cb9-4a37-8d49-6ea8e3cbc15d'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (13501, '2a2b7fc2-3c43-4d05-a703-7d2ed7939397'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (13508, '91853fd6-7904-4ab6-949c-cd38b42bf46a'), (6854, '462959bd-83cb-4dab-9485-23b7022e034c'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (8912, 'dd14caa4-dcc9-4a93-8c96-8822b367576d'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (28408, '2f84453d-37fd-4c77-bc6d-02f1072ba804'), (28409, 'b394142a-f57a-4ead-81d8-6074058b8cfc'), (28410, '39e490e7-5fc3-4edd-8871-aa799375820d'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (23300, '2c8ef940-63f1-41ad-8fe0-9b4815570529'), (21765, '6a1e63b7-b878-4676-9087-269c0b32fba1'), (28426, 'e4bc4460-31a5-4610-950b-50921c69b0b0'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (6420, 'a0d6a281-060a-42e7-b028-400eafafb07f'), (28952, '79800c43-9871-4516-b042-0c6ed54ed114'), (26919, '5ebb5465-6f4c-4e1c-8326-5f71516e66a4'), (9010, '4283c577-f959-4e02-af03-eb6f5cb2316b'), (8504, '143ac2fe-2f79-4c07-bd2e-996611e74123'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (17725, '023e9079-979e-431a-bde6-e6c2d72c2ea4'), (16702, 'cf05cd6b-e33d-4bc7-a11e-5058f0d0676b'), (10561, 'ebc76b4a-3ab1-48ce-b521-3a8a47762e0c'), (16710, 'f734640a-edfc-4987-be06-0b998c1c4861'), (4937, '24ec3f32-f79f-4d91-a838-361fdd56703f'), (18773, 'f0fb53d3-2853-4eb6-8dd8-8cc780714000'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (27503, '26daa0bc-24ad-40e4-b28f-2e75235abb98'), (25456, 'ac2479c5-2682-4db2-a27e-fdf59164519f'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (25476, '3fcbb85d-17ea-4cf1-b253-7994b353a67e'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (30618, '31c10110-34ae-4496-933f-ace3e5324c3c'), (30619, 'aa17c983-fc1c-48fd-af6a-50bfa38933ae'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (30623, '87542353-b6e3-4257-a07f-19317964e33d'), (30629, '7cf3310b-91ee-4c5f-a7ea-66073b3a9197'), (7082, 'b972f2bc-c1a6-49e3-a883-bc6a5f268e63'), (13229, '3981f499-4b08-4449-ae2e-11d60161966f'), (5041, '812dcc27-ef00-4194-976d-6f9eb30583a9'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (6075, '1c43f542-2b0e-4720-8a0c-c2dfe6755b03'), (6076, 'ade21dd4-e5c9-4644-973c-7a7dfa4482a1'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (30143, '58fce874-a745-4ea3-ac33-f54c199f74bf'), (30145, '8737ac8e-3444-438c-af77-10675408cda1'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (30147, '704161fb-a34c-4702-9765-539a8216f58f'), (30146, 'a4a84e46-2b9c-439f-9071-9d614c4d6b16'), (30148, 'f0872b3e-b08e-4019-842f-1d052da7f05c'), (27592, '5627b8cf-a158-4716-84dd-15ac31472060'), (27593, '019dcf9b-fd02-4dad-b466-b9022ccfa9d2'), (24522, '84973e34-c9c8-4efe-97f2-d3f8efb83196'), (27595, '6d1182d3-71fe-43f7-9bb2-ce728541b305'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (31694, '596b45fa-fb66-4361-b28b-6e699a6738f1'), (22477, '06267b14-5048-460b-9547-63335518d2da'), (6094, '4cdfb0fb-1392-476a-90e8-10ab82ff69d8'), (16337, '2a82c5a8-c446-461e-87a4-57a53425634d'), (6091, 'ec43fc42-f705-45a0-8110-0fb4ac955090'), (6092, 'ec398005-1db2-4581-a851-a0ae9e2036c5'), (20441, '3fc2ef47-6bb7-49ee-b95e-300483ae096f'), (27098, '4dbebf8f-6846-4cf6-b5c4-39d614dd7a87'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (26091, 'eef85ff4-7a3d-44ce-99db-7e1ef9917d94'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: This section allows you to define a mapping from your IdP's user profile data from your IdP to the assigned role in Hugging Face.

- IdP Role Attribute Mapping

  A JSON path to an attribute in your user's IdP profile data.

- Role Mapping

  A mapping from the IdP attribute value to the assigned role in the Hugging Face organization.

You must map at least one admin role.

If there is no match, a user will be assigned the default role for your organization. The default role can be customized in the `Members` section of the organization's settings.

Role synchronization is performed on login.
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples 📄
```

## Standard Hugging Face workflow

### Handling the data

To fetch the IMDB dataset, we will use Hugging Face's `datasets` library. We then need to tokenize the data and create `PyTorch` dataloaders, this is all done in the `load_data` function:

```python
import random

import torch
from datasets import load_dataset
from torch.utils.data import DataLoader
from transformers import AutoTokenizer, DataCollatorWithPadding


DEVICE = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")
CHECKPOINT = ""distilbert-base-uncased""

def load_data():
    """"""Load IMDB data (training and eval)""""""
    raw_datasets = load_dataset(""imdb"")
    raw_datasets = raw_datasets.shuffle(seed=42)

    # remove unnecessary data split
    del raw_datasets[""unsupervised""]

    tokenizer = AutoTokenizer.from_pretrained(CHECKPOINT)

    def tokenize_function(examples):
        return tokenizer(examples[""text""], truncation=True)
```

## Create a HuggingFace Estimator and train our model

In order to create a SageMaker `Trainingjob` we can use a HuggingFace Estimator. The Estimator handles the end-to-end Amazon SageMaker training. In an Estimator, we define which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which hyperparameters are passed in. In addition to this, a number of advanced controls are available, such as customizing the output and checkpointing locations, specifying the local storage size or network configuration.

SageMaker takes care of starting and managing all the required Amazon EC2 instances for us with the Hugging Face DLC, it uploads the provided fine-tuning script, for example, our `train.py`, then downloads the data from the S3 bucket, `sess.default_bucket()`, into the container. Once the data is ready, the training job will start automatically by running.
_Q: Why should I use SageMaker Training to train Hugging Face models?_

A: SageMaker Training provides numerous benefits that will boost your productivity with Hugging Face : (1) first it is cost-effective: the training instances live only for the duration of your job and are paid per second. No risk anymore to leave GPU instances up all night: the training cluster stops right at the end of your job! It also supports EC2 Spot capacity, which enables up to 90% cost reduction. (2) SageMaker also comes with a lot of built-in automation that facilitates teamwork and MLOps: training metadata and logs are automatically persisted to a serverless managed metastore, and I/O with S3 (for datasets, checkpoints and model artifacts) is fully managed. Finally, SageMaker also allows to drastically scale up and out: you can launch multiple training jobs in parallel, but also launch large-scale distributed training jobs
 
Here is the question: What is the role of `DefaultDataCollator` in Hugging Face training workflows?","The context does not mention `DefaultDataCollator` at all. It talks about `DataCollatorWithPadding` in the code snippet, but not `DefaultDataCollator`. Therefore, it is not possible to answer this question based on the provided context."
What is the main advantage of using BLIP-2 over ViLT for zero-shot VQA?,"[(11264, 'f1b68b88-f347-4a90-9d24-0cae81ae704e'), (11283, '729f6a59-d667-445b-9197-bd8970806506'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (18979, '9336d4a9-e1a5-4fad-a990-f9ce022a25e3'), (28717, '45cc6367-7b38-4f30-a36f-c96fb2293202'), (26160, '524aa393-c139-4c52-acd8-92ca1729c0d0'), (10801, '7d50ab8d-be9f-4eec-80ae-b2dc0e4c988a'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (31293, 'bf6fa69e-7b65-4651-84a6-7287755969d5'), (10315, '5570dac5-1188-45c3-8401-666f1387bbf6'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (4690, '73b0a8e6-ac3c-4071-b88d-beb7478681aa'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (1663, '323fa14b-b505-4dea-ac38-9c7133039a48'), (1665, '0837dd42-4848-4039-9cd0-a3c8a59eec63'), (29838, '4085395a-76c9-44e4-bb5d-139a2ac8f1ef'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (24234, 'e9da4eca-e7b5-4a74-94f9-bd8ffcc53d29'), (25785, 'cc8b2b20-c91a-4342-b3f7-0f6232628ce7'), (1745, 'cd32a895-3ae9-4f82-8574-44ad1d9286e8'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (16090, 'de34769a-90f0-4c82-b82e-6d9a5a0e6f92'), (16094, '90e8a230-a59f-4644-b799-3e81823cb581'), (223, 'acfa4968-148f-4d01-9982-f4cd2bb8795b'), (27361, '334d3a20-8ea3-42cd-a731-147b016bd478'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (6893, '3e3a63e2-90f5-4d24-8729-3673b42d713d'), (23789, '1d90878f-1c99-451b-b6b9-c6a02aa3ee99'), (8432, '128c1a7c-3f4d-481a-9a0c-f20ba007042e'), (8433, 'a4916d14-dff1-4bea-9fe9-aad51bca0c7c'), (3826, '256f85ac-16ed-48b7-8a96-c986ccb76483'), (3827, 'fe2a1935-1ae8-467c-85b8-dfb7bd441b10'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (29437, 'a9873ed2-2cf8-44cc-9b63-9ba1c9d4a818'), (27393, 'dfa78164-2d38-4169-8e27-3903e460c6d8'), (8460, '59ac32c1-6a6a-4294-95a1-337d4ca90cb9'), (8461, 'd7a376d4-718d-43c6-953a-dc1bbfe0b36a'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (12045, '7efab0f1-50ed-4755-813f-824ba7849ab5'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (12049, '9ebd4171-a0ce-40d1-9883-c37ead9490d1'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5908, '144a7b30-6985-4f7d-adc3-de47096fc6ab'), (12048, '6d73bc8e-5e71-4307-8e26-99e5ab4a4784'), (1814, 'e4f58e5a-3444-47f9-aa56-481e3993c59c'), (8472, '1198c94a-3f34-4223-9da9-02f2c1303a9d'), (8473, 'fc6c139e-ef25-4ad8-b5ce-99d56c4b56fc'), (1824, '7b77c59e-36af-44d2-a1f5-b832deccf49b'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (18213, '076f6581-4145-4ca5-93e8-9e744b0574a7'), (5926, '535b4e72-3c6d-40bc-a545-1815b5819cba'), (16174, '7aeaf6a6-cf85-46b0-af0b-f0ab92bc6103'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (31029, '5b3fec47-d2ee-4bf7-bcba-7bd3e3a65030'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (23867, 'd49bd9cf-b6a6-4207-9330-94e086e9aae5'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (13630, '5f595309-fb4e-4261-92ad-938f4eb058f2'), (23873, '6259700a-095f-4a14-8048-95fb1df7f296'), (4420, 'acb220f7-87c0-4af4-883e-44df58ee00fb'), (4421, '95a1272e-1f19-494f-96d5-f36f9a919a0a'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (329, 'e5a52187-6e6d-47ba-bffa-cde854b2df46'), (17742, 'dd667567-f375-4a71-b695-60d6babe2ab0'), (7013, '7c2e994f-665b-4e39-a477-19fa189595ac'), (7015, '929caa94-db70-4c90-85ca-46a52fdf2f65'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (25963, 'fd0bea05-61c9-4e1b-8a8b-5171642f2f0d'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (20852, 'b516f3fa-d794-46a2-a94e-d391909f98d9'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (20851, 'b8c3710a-efdb-41e6-9f78-8cb777be59d6'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (8060, '9eef21f5-4170-4668-bc2d-bfb804087692'), (14206, '5bfbab3f-215b-442a-8bd0-1810e815b192'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16264, '5d166a05-2d80-4201-ae2f-b71993797a24'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (16267, '7ac5f372-d956-427b-832d-160d6a912937'), (10133, '7735b57a-5b30-4c4f-9ab3-c1e84672a8be'), (21400, '637da23f-83b4-4655-83e1-bd9d7d7048ee'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (11691, 'dd3ff1e4-0aa8-42d6-b141-406f1df3f143'), (30133, '61f3f5a8-2c83-44a2-9a84-594c52ec5c89'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (20415, '0ff6dba8-cfe6-4157-a337-93d21a8a4a9e'), (20416, '29126e8a-96b2-4d05-a07e-1c2689b8c2f3'), (10193, '26d1bfcb-e936-427b-92c3-e7dd97a94093'), (12270, '002549e4-baef-4cb3-baf4-279f524b849b'), (11262, '5c2061dd-7364-4daa-bb21-a1404baa2b55')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference. 

Before you begin, make sure you have all the necessary libraries installed. 

```bash
pip install -q transformers datasets
```

## Zero-shot VQA

The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach 
VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training 
paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). 
This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. 

Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a 
GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: 

```py
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# BLIP-2

## Overview

The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by
Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer
encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198), an 80 billion parameter model, by 8.7%
on zero-shot VQAv2 with 54x fewer trainable parameters. 

The abstract from the paper is the following:
However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*
 
Here is the question: What is the main advantage of using BLIP-2 over ViLT for zero-shot VQA?","The main advantage of using BLIP-2 over ViLT for zero-shot VQA is that BLIP-2 treats VQA as a generative task, which allows it to achieve state-of-the-art results on multiple visual-language tasks, including visual question answering, with 54x fewer trainable parameters compared to other models like Flamingo."
"How does Hugging Face ensure framework interoperability across JAX, PyTorch, and TensorFlow?","[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (3600, 'a5251b45-4980-4841-a1da-d832c0aa4e34'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (27672, '59487233-b752-42ea-aa5a-8cb853def35f'), (17439, '220bb0f1-e5a5-4c21-ba59-845839d83ad3'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (5154, '5281c353-e855-4d7c-987a-6ea222f9fad2'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (17440, 'b2f0a0e8-f794-4abe-b33a-7a304399ead6'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (13874, '3aa09c5f-d782-4417-bf6d-9a16dc742ef1'), (29236, 'c8256ce9-e984-4533-a82f-3f08a2d65e83'), (24634, '70652141-34ee-406f-816b-e497d503fd8d'), (22593, '1d7e4289-69b0-474f-a80c-712bb400aca9'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (20568, '71b15f8d-3a44-4e01-8044-0159f1e2e6b4'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (25702, 'c8a294a5-698c-4f59-a9f4-832868f1d619'), (1127, '61bce4e3-ba06-42fd-82da-5f4b1ae2562a'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (9840, 'b35e7b1d-c3d5-4e40-80c7-d9b3b8c7f3fe'), (31354, '50289d67-f345-42d3-bb11-7b4a1e8c5530'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (1181, '2e574621-b2e7-4ffb-ad69-7aa65d1e16fb'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (1191, 'ece7abdd-1376-495b-8daa-d664f2a27dae'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (30891, 'ce0c26e5-9ecb-4d1c-ac24-560c5974dda4'), (18098, '8ccdabe2-25b8-434c-8ed1-410a960610e9'), (25786, 'a3f4a2db-9125-4e13-89f2-07a0399eaa1d'), (20667, 'b0b180ee-2478-48b8-9446-bf6a9feabb3b'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (23744, 'a27e4e2d-9c3b-48f3-80d0-4422cdeebf37'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (23749, 'ebbc939a-6402-4c02-9891-1531598d8076'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (25302, 'bcf9a9f1-7a6b-4a62-87f7-68506603417d'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (7390, 'd82b7e48-5494-469a-b93b-42130344ea25'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (17123, 'b278f0df-995f-4b18-a7f0-c467054354da'), (10989, '6bcf6d91-b596-48cd-903e-99802046e818'), (7411, '6d36d940-f60e-4c46-aa45-25a07289bc33'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (21766, '22985197-fca5-4c83-aa74-fcd454c56d2a'), (8971, '5a35d107-5b71-4521-8856-416fb762b753'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (16147, '27008f92-4674-4cb1-b22a-6468630dc026'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (21785, '0f42515e-4921-41d6-a8f2-18c62768982e'), (6437, '84957027-ff15-4ac9-9366-7a35676d6fd0'), (23846, '9575a592-08d7-415e-806e-39bda889ef4a'), (26919, '5ebb5465-6f4c-4e1c-8326-5f71516e66a4'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (8492, '37b69803-2f18-4f43-850e-e3c81aa1cb27'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (350, '8edfb721-840b-4b1d-9036-65614d095d44'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (352, '756c346a-bd77-440b-b6a9-1fabbf0c457d'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (21365, '9556f219-576e-4d37-8eb8-c37f3fb358f5'), (21366, '3f2fa721-852c-4545-b287-9c8782de1be2'), (13176, 'a57502e1-52a5-41dc-aebb-b7cb7f00763d'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (22394, 'b66b7946-772b-43b6-8e7d-b5388eaf7cf6'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (13196, '5f4d0098-0f25-4cd7-9d59-4037a77c492b'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (6561, 'ee35c81c-c680-4dfc-852b-7c11aae2a074'), (30625, '8470c0ae-12ac-47e2-b0e3-e30990f063f1'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (17828, 'e0e24538-49e6-418a-8fb8-b10e6a8d00f1'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (22453, '4bad26aa-2ebe-4fb7-80b0-702de459d53c'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (6082, 'c6145d46-49ea-43dd-88bd-7d6c7daf3380'), (28614, '85bff174-da9e-4929-bdc0-904ec1a4bc99'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (9163, 'd7206e44-29b4-499b-92c4-0008be204752'), (6091, 'ec43fc42-f705-45a0-8110-0fb4ac955090'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (24532, '60a7b56c-8699-44c3-845d-6e93ee024964'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (31728, '5ccf1245-717b-44e0-837a-e164da6b012a'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: --
title: ""Hugging Face's TensorFlow Philosophy""
thumbnail: /blog/assets/96_tensorflow_philosophy/thumbnail.png
authors:
- user: rocketknight1
---

# Hugging Face's TensorFlow Philosophy



### Introduction


Despite increasing competition from PyTorch and JAX, TensorFlow remains [the most-used deep learning framework](https://twitter.com/fchollet/status/1478404084881190912?lang=en). It also differs from those other two libraries in some very important ways. In particular, it’s quite tightly integrated with its high-level API `Keras`, and its data loading library `tf.data`.
```

```python out
""""""
Question: Which frameworks can I use?
Answer: pytorch, tensorflow, and jax
""""""
```

Nice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won't always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the [Hugging Face forums](https://discuss.huggingface.co/) that can help you out! In the next section, we'll take a look at how you can craft good forum questions that are likely to get answered.
- Many models weren’t XLA-compatible
- Data collators didn’t use native TF operations

We think XLA is the future: It’s the core compiler for JAX, it has first-class support in TensorFlow, and you can even use it from [PyTorch](https://github.com/pytorch/xla). As such, we’ve made a [big push](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) to make our codebase XLA compatible and to remove any other roadblocks standing in the way of XLA and TPU compatibility. This means users should be able to train most of our TensorFlow models on TPUs without hassle.
Easily track and compare your experiments and training artifacts in SageMaker Studio's web-based integrated development environment (IDE).

**Built-in performance**

Hugging Face DLCs feature built-in performance optimizations for PyTorch and TensorFlow to train NLP models faster. The DLCs also give you the flexibility to choose a training infrastructure that best aligns with the price/performance ratio for your workload.

The Hugging Face Training DLCs are fully integrated with SageMaker distributed training libraries to train models faster than ever, using the latest generation of instances available on Amazon Elastic Compute Cloud.

Hugging Face Inference DLCs provide you with production-ready endpoints that scale quickly with your AWS environment, built-in monitoring, and a ton of enterprise features. 

---

## Resources, Documentation & Samples 📄
Because it loads so fast and is framework agnostic, we can even use the format
to load models from the same file in PyTorch or TensorFlow.


## The security audit

Since `safetensors` main asset is providing safety guarantees, we wanted to make sure
it actually delivered. That's why Hugging Face, EleutherAI, and Stability AI teamed up to get an external
security audit to confirm it.

Important findings:

- No critical security flaw leading to arbitrary code execution was found.
- Some imprecisions in the spec format were detected and fixed. 
- Some missing validation allowed [polyglot files](https://en.wikipedia.org/wiki/Polyglot_(computing)), which was fixed.
- Lots of improvements to the test suite were proposed and implemented.

In the name of openness and transparency, all companies agreed to make the report
fully public.

[Full report](https://huggingface.co/datasets/safetensors/trail_of_bits_audit_repot/resolve/main/SOW-TrailofBits-EleutherAI_HuggingFace-v1.2.pdf)
 
Here is the question: How does Hugging Face ensure framework interoperability across JAX, PyTorch, and TensorFlow?","Hugging Face ensures framework interoperability across JAX, PyTorch, and TensorFlow through its format, which loads models quickly and is framework-agnostic, allowing models to be loaded from the same file in PyTorch or TensorFlow."
What distinguishes generative models like BLIP-2 in their approach to VQA?,"[(5121, 'fcf9e2cf-e39a-48fd-a17a-15229f74c06e'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (14874, '3544b056-3986-41f6-be83-29f02a24a868'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (27181, '75d8f3c7-06fb-4203-ace8-42df0acee889'), (31289, '9905fe51-ba24-492b-9d83-93aa4879a98b'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (31293, 'bf6fa69e-7b65-4651-84a6-7287755969d5'), (31294, '7ea09c36-57a2-4e98-aa9e-74a154a4df96'), (19522, '9f3b6486-249b-493d-a542-2dfe80215a8d'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (4690, '73b0a8e6-ac3c-4071-b88d-beb7478681aa'), (4691, 'db667e7e-ab3f-4347-8a38-098edd3bdd0d'), (20576, '40796d31-f95c-4e9d-a36b-2d6319601be0'), (26231, 'b3e08d1a-4130-42bb-8c39-7e1831a663d6'), (3211, '66850aca-1339-44e1-b6ee-3612c185f4d3'), (14989, 'a296d3f3-7f20-42c9-b0ef-c3ff10c74e60'), (29838, '4085395a-76c9-44e4-bb5d-139a2ac8f1ef'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (22168, 'e6c58e6f-1ad9-42ec-beab-d54096522c9e'), (6136, '818cf127-78a6-43ae-aad7-568810131041'), (20637, 'cee55ddf-c508-4c56-bc26-2a42524b341a'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (4266, '68b45599-96c5-4b4c-9660-4d60ebedf678'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (11953, 'e5b47149-4a3e-4549-a9ef-c12eccfe493f'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (24767, '66d915f5-e086-4a00-9ff8-aeb39c0e4f11'), (23753, '6939167e-707a-4fbe-8d61-492be49ed360'), (1745, 'cd32a895-3ae9-4f82-8574-44ad1d9286e8'), (1746, '49e8dce4-afe5-4268-9ab9-6f9809115e1e'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (18139, '83b2dbad-3458-4f47-aadd-c109f2fb96d3'), (19164, 'fa1b353a-25b7-4b68-9b46-5f8b1a7591d9'), (4830, '148d40d4-4e33-44f5-b053-5902496fedc3'), (27361, '334d3a20-8ea3-42cd-a731-147b016bd478'), (23777, 'e8d6bb71-ce43-4ba7-bdcb-8d0d203337a7'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (1773, 'e07fbbd8-16f9-4b35-bbb3-b1b9950618b0'), (18161, '661008d6-f852-47ee-a976-37ed2665fd5e'), (13557, 'f70326e4-55ec-4169-9411-e479df639c4b'), (20730, 'b9f51b25-0a14-4b14-a1a6-92b1e0278fc3'), (27391, 'bf10fdf5-d841-4030-8d03-4cb6d0161538'), (20736, 'aa05e96c-11e4-4820-8359-22ca94a040b0'), (27393, 'dfa78164-2d38-4169-8e27-3903e460c6d8'), (255, '3a660243-78e6-48d5-9d2f-08abcc154636'), (1804, '4b1c4f8b-db8b-4055-bfdd-38d398f61539'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (18191, '1d155c0b-9666-479b-8395-ef2ea126d232'), (18192, '486c0912-86c9-4e52-886a-5fc65ac487ea'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (24339, 'bc00f956-dd91-4b6d-a59a-7b7567c85c23'), (277, '91e3b387-1104-48c6-98d8-5d391b18159f'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (13608, 'e2f6df65-9623-4dcd-b35e-179b1f087b0f'), (13609, '56b2dbe4-9220-4487-90a8-c5bc372fb9af'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (308, 'f9ad91dd-b5a3-4b59-9128-7e307a0833fe'), (27466, 'bfb56804-3d91-45c6-b17f-8d15d5d3a201'), (7002, 'f6c1eb0a-744c-4cac-9ea7-db3de3ac11d6'), (30557, '13b5cd75-8a53-45f3-8692-590cfed0510f'), (17248, '9811bc52-95c6-4672-b0a3-b9676891d640'), (8033, '6546b4e9-35ea-4967-9518-31b10ff71437'), (4965, 'f1c54284-48b0-429f-8306-c0a1ef88bda9'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (20851, 'b8c3710a-efdb-41e6-9f78-8cb777be59d6'), (9076, '9bf657ea-4f62-4fc8-aa59-a2969ecd9ac8'), (25972, '72d72947-f0cb-441d-aec1-2abff09ba3e3'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (20853, '5ae2ef01-4bbc-4567-bcf3-507491a42651'), (10613, '0835f4fb-6088-4ffa-9b78-b5b2e6fd58d4'), (22904, 'a678407d-0ca1-4128-a53e-8ab8dccdddf1'), (20852, 'b516f3fa-d794-46a2-a94e-d391909f98d9'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (16267, '7ac5f372-d956-427b-832d-160d6a912937'), (9097, '4aa5ab2b-ea90-4dbc-8bca-d4ab9ed22c7e'), (10137, 'ce88e225-95d2-48d7-9020-7d8644986a94'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (9128, 'f6ef6694-87ea-4687-8563-95e806c1ca37'), (9130, 'be721f3d-579c-439c-b4ae-c0d2ad2675df'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (18370, '9ced7d97-c05e-4f58-b223-d761b075a126'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (2502, '8e358fab-31fa-4f90-b90d-a95834e06f52'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (5067, '91e36725-33e2-46e7-b02d-8c389e2797de'), (18391, 'f9a3fbd7-5a63-4f5d-a344-2656a7553d4d'), (5088, 'f6c7fccc-2d9b-4fb8-abb5-8bacb17044c0'), (8176, '2cc6d0c0-3a2d-4ebc-8266-4d2ee6bf3300'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (18422, 'de5af8df-920d-4053-b25f-a6ad01eb6ce4'), (18424, '92ad1698-d5f0-4b2a-9009-6a889c288056'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699'), (5119, '9416e2a3-1acd-4094-8a5b-3c205a496d11')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Zero-shot VQA

The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach 
VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training 
paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). 
This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. 

Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a 
GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: 

```py
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference. 

Before you begin, make sure you have all the necessary libraries installed. 

```bash
pip install -q transformers datasets
In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# BLIP-2

## Overview

The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by
Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer
encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198), an 80 billion parameter model, by 8.7%
on zero-shot VQAv2 with 54x fewer trainable parameters. 

The abstract from the paper is the following:
```

The model takes image and text as input, so let's use the exact same image/question pair from the first example in the VQA dataset: 

```py 
>>> example = dataset[0]
>>> image = Image.open(example['image_id'])
>>> question = example['question']
```

To use BLIP-2 for visual question answering task, the textual prompt has to follow a specific format: `Question: {} Answer:`.

```py
>>> prompt = f""Question: {question} Answer:"" 
```

Now we need to preprocess the image/prompt with the model's processor, pass the processed input through the model, and decode the output:

```py
>>> inputs = processor(image, text=prompt, return_tensors=""pt"").to(device, torch.float16)

>>> generated_ids = model.generate(**inputs, max_new_tokens=10)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
""He is looking at the crowd""
 
Here is the question: What distinguishes generative models like BLIP-2 in their approach to VQA?","Generative models like BLIP-2 distinguish themselves in their approach to VQA by treating it as a generative task, rather than a classification task."
What dataset is used in the Hugging Face tutorial for fine-tuning ViLT?,"[(10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (3594, '4e1600b8-d386-41a9-bc22-67a03574e9d3'), (18443, '00876f43-8b96-47cb-ba09-71690498db3d'), (18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (18446, 'bcf63af1-2c2e-4f5d-9430-6b2678df17e0'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (3603, '868a7661-c119-493e-86ac-54156f0efe17'), (5141, 'aa79a23d-5dce-4f0c-905f-502f514390e9'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (5144, '8dd33c2d-9153-4b23-a629-85dc3d5ccbaa'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22565, '760daf74-bc6a-401f-9769-75fd06c85b7c'), (28716, '0e901ab3-56fc-46c6-a3c7-adee2baa6fca'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (30788, 'e6d02e40-3fcd-462e-b943-0494c7d97dc3'), (30792, '43ed9e13-950f-4202-8101-cfcff37a492b'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10841, '6e53fcd9-7666-4769-ba61-2d6fab8ac52c'), (14429, '49a2886b-403f-463e-8384-2fb84e5ab062'), (17512, 'ef84f7bb-d94c-4d1a-8fe8-3db8a9fd0d93'), (20585, '33dcb7a1-776c-43ad-a7ce-be6345e2e6b9'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (18548, 'd7720c9a-8876-4847-b0c5-0da58c90fa8c'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (23714, 'a2d69303-a006-44f0-8008-1accb10a3697'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (28844, '2ac701fb-f40a-465d-b95a-8ecf70258ad1'), (4780, 'b2f80597-99f6-413f-9480-44d89c891e86'), (20654, '0dfc500c-a1cd-464b-917d-6d60a2832b61'), (28847, '4eabf524-63bc-4222-8ad6-fab8a956e9cf'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (20657, '7effe90b-a405-4e14-866c-174351ec7edf'), (26284, '682808a8-2487-4bd4-bb4b-42af8997747c'), (28853, '58bff4dc-4b6c-429b-b0f7-11abaec8c635'), (23738, '37f3a8c8-36fe-4d79-a1bd-bcf96ff1cd69'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (18624, '355b21ab-d432-4cbc-80cf-045354644b16'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (11467, '99d6488b-6726-4a42-866c-9569f978641d'), (25298, '3927b484-b2d8-4332-9789-d423d25be837'), (25299, 'f6c0a00d-d54d-4ae5-a0b9-af2a5eba1630'), (1753, 'ade1268b-02e1-436c-9625-e1440c179387'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (1755, '3aab2664-c02c-4d5e-943d-b42f1d0e5b21'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (22752, '847c0672-9e44-445e-b14d-84817ad403ee'), (1777, '4ddc1fcc-541d-4553-b408-84494027daef'), (4850, '3a950f12-3c2d-47ce-a6a8-1abdfb1befa0'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (4853, '5364e758-db68-4033-81d4-35723ac64959'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (5887, 'b7b11479-9b35-4eaf-8fa0-44d3a179b02a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (1825, '48aee3a5-de24-49c5-b057-75a9d7ad9d07'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (3368, '17715a5e-5f2c-4df5-94da-06ac43fe341f'), (8504, '143ac2fe-2f79-4c07-bd2e-996611e74123'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (16702, 'cf05cd6b-e33d-4bc7-a11e-5058f0d0676b'), (4422, 'cae3f2e8-d9f5-411f-a4bf-e75aff751dec'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (15181, 'fb7c5366-41d1-41cb-9f21-6d3b57cf1e94'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (15217, '77662209-3d2c-443f-8e8c-3085e7ecc6bf'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (3979, '052eb59c-fd2f-4556-8484-c015cc0f2c33'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (9150, 'de3eb793-c6de-4fd2-9f8c-1b0b686cf6d1'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (9153, '18a80d0f-9886-469b-89d0-a4992c09ad54'), (30146, 'a4a84e46-2b9c-439f-9071-9d614c4d6b16'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (27593, '019dcf9b-fd02-4dad-b466-b9022ccfa9d2'), (12236, '0ed87905-d9a5-4e0a-bc72-5b70cfa711e1'), (6093, '5d63e976-ff26-4315-b418-58387364c07e'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (20441, '3fc2ef47-6bb7-49ee-b95e-300483ae096f'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (11235, '8558ce8a-5be5-4c58-91a4-e034fd8f061d'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (13801, 'aa3ed41c-4b41-428a-a670-fc2e12d17add'), (13802, '54b46bfe-ed8a-4198-9b58-bf91ffa494d0'), (13803, '55a48bc2-202f-4d62-9a6b-0945cb7df97e'), (15851, '0bde2ce9-28b0-4ae0-9459-a572cca7f941'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (15863, 'e17826a6-d5b3-4729-ad97-387ae765122d')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Resources

Demo notebooks regarding inference as well as fine-tuning ViT on custom data can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/VisionTransformer).
A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with ViT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.

`ViTForImageClassification` is supported by:
<PipelineTag pipeline=""image-classification""/>
# Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models


Generative AI has been taking the world by storm. As the data and AI company, we have been on this journey with the release of the open source large language model [Dolly](https://huggingface.co/databricks/dolly-v2-12b), as well as the internally crowdsourced dataset licensed for research and commercial use that we used to fine-tune it, the [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k). Both the model and dataset are available on Hugging Face. We’ve learned a lot throughout this process, and today we’re excited to announce our first of many official commits to the Hugging Face codebase that allows users to easily create a Hugging Face Dataset from an Apache Spark™ dataframe.
elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company. The next chapters require a good knowledge of Python and some basic knowledge of Machine Learning and Deep Learning. If you
### Using datasets from 🤗 datasets

Here we show how to pre-train a `ViT` from scratch for masked image modeling on the [cifar10](https://huggingface.co/datasets/cifar10) dataset.

Alternatively, one can decide to further pre-train an already pre-trained (or fine-tuned) checkpoint from the [hub](https://huggingface.co/). This can be done by setting the `model_name_or_path` argument to ""google/vit-base-patch16-224-in21k"" for example (and not specifying the `model_type` argument).
<Tip>

The tutorials only cover the basic skills you need to use 🤗 Datasets. There are many other useful functionalities and applications that aren't discussed here. If you're interested in learning more, take a look at [Chapter 5](https://huggingface.co/course/chapter5/1?fw=pt) of the Hugging Face course.

</Tip>

If you have any questions about 🤗 Datasets, feel free to join and ask the community on our [forum](https://discuss.huggingface.co/c/datasets/10).

Let's get started! 🏁
 
Here is the question: What dataset is used in the Hugging Face tutorial for fine-tuning ViLT?",The dataset used in the Hugging Face tutorial for fine-tuning ViT (not ViLT) is cifar10.
What is the purpose of label mappings in Visual Question Answering tasks?,"[(19466, '05a0980a-606f-4da9-ab09-c2aace9b4acd'), (19467, '8be7a151-f10b-4b06-9623-13ac201be523'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (14883, '941dfd94-da40-40b4-918c-65e533145721'), (556, '64d83ec5-06e6-45bf-9e20-f19282782542'), (4667, 'ea371344-8b31-4041-8d3f-3a2d817bd4bb'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4674, '6169a5f5-79ab-4d2b-b876-5be77a11f410'), (4675, '4f667ec7-c9d7-4f60-8ea8-b8b23d97c53d'), (4676, '9945c7ae-8d23-4933-bd20-bf82dfce4ab5'), (30789, '8b611d65-f397-46aa-bb7b-a9b95d2bd126'), (4678, '2bd57fe6-d068-4693-a0a4-20061aca479f'), (4680, 'f1b71842-c2a8-4de7-b6d6-b9115f88900c'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (10839, 'ec3d27e8-dc2c-424b-881d-53ec54223803'), (615, '6b9d1f09-198a-433d-88c8-f3161c71b310'), (617, '7364bcb9-c6d6-4e51-bc58-534c52f13e76'), (25197, '11048c76-26fa-4eca-b2eb-952cd5d06e3f'), (25198, '0e54b368-d707-4cbb-8c9b-6541bbcac541'), (8818, '0d90a54f-d75b-45a7-b5f0-9f04f2a4773c'), (9334, '33d669d5-da54-4f93-9eda-328428c8fdca'), (23168, 'd2dc356a-69f4-4cf2-bff3-df262a358222'), (24707, 'b83268da-8092-42eb-96fd-1c3b065d1862'), (10910, 'eeba803d-97cc-465d-926a-4c178c213900'), (21663, 'faf6a9a2-6557-41d1-922c-02ae59c135db'), (21665, '34c508be-eee5-4227-8805-09874dd2b89b'), (4272, 'f95f512b-761c-44b7-bf19-f0bcb5245e86'), (15543, '709e20d6-7763-4624-a403-ab88f87e09bb'), (29367, '8954b5d9-4dfb-4d14-868c-e726946b5055'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (3803, 'f0bab2c0-7345-4a53-bc07-5c845419f9d0'), (15582, '1db2a67f-8aa7-4c8a-92c2-6d76a2a0847f'), (15583, '0bd03221-d721-44d6-b845-89e89513d7b5'), (3808, '191dcb28-30e9-4713-9ef6-65929451ac28'), (15584, '159d9330-dea8-45de-ae6f-bfbaad3f6435'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (15585, '31f7ebf4-9f31-4897-9be5-0f1afccf4531'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (15592, '481a6c32-3e99-4c07-a468-dbe480fef1b4'), (15593, 'd41ecdb5-1abb-4bbd-a4dd-adf3baf6ee42'), (15594, '91d6a786-ffb2-47aa-a1e5-cc8d41892c41'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (3827, 'fe2a1935-1ae8-467c-85b8-dfb7bd441b10'), (29430, '8fd4a8e6-4e9e-4a74-82c1-8d537cfc6af8'), (3836, '624dbdb1-8d22-46bf-baa8-4d9305029369'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (29446, '56256f45-cc5f-4fe7-9631-d64b1173a4f6'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (28934, '1b96267b-c1c0-4cba-a5ba-7b64923cf15f'), (15116, 'f854258b-7d1a-42c4-b09d-e20f3bfe02c5'), (29455, 'e3a1b180-f8b4-43f1-ab4e-8b05a6cbdce3'), (29456, 'e7b05b8f-c244-4018-ac23-ea37f634fffd'), (25377, '11930567-432f-44a9-b953-a45bd742357d'), (25382, '7792917e-29da-4b6a-a948-18ea22bae887'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (25388, 'a38030d6-12df-43e0-990c-54e3f8fbf075'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (2866, '1190142f-1663-4e32-8d95-5f3b34034875'), (7476, '315f3f37-4f65-4c27-9ad9-e7efe1e8b8f6'), (9015, '6e28a1fc-953b-4f1b-8b44-61d598ff2f39'), (7482, 'b679da76-c666-4bb2-a02d-e88e599b9b6b'), (7486, '1159140f-aaf0-4cd5-ac1f-baf9adb8c63a'), (14655, 'c5adb19a-7ebd-4d40-a31d-55a6f8a1b978'), (24894, 'ab9c17d7-9f1e-4df3-917d-b90785ca064b'), (22337, 'dcd3d938-defe-4be3-b64f-3fd23ab4f8bf'), (14656, '812d63e0-cb77-4e29-9bbd-da3da9e4ffa5'), (13132, '77b0e917-d732-42f7-8d43-70b0b8eeeff0'), (19788, '7899cbd2-e4be-4403-a2ea-4ddf86fdf820'), (13133, 'fc226fe7-bb22-4a35-bf0e-5dd8422b9408'), (19790, '6de1ed59-1a63-4ee9-83bc-200bce20a458'), (23888, '5efd6f99-8edf-425d-bf42-fbca2966a6f1'), (3921, '6311b8eb-e903-45b5-bc7c-c24e69a96ce8'), (19789, 'c7935b66-ee13-4bc3-921b-9786b07a229c'), (3923, 'e87b7120-b4af-4c3f-9448-479a9d54a9d7'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (5480, 'e7ff7ba7-82d7-440d-bb24-3e84cb766756'), (25963, 'fd0bea05-61c9-4e1b-8a8b-5171642f2f0d'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (4973, 'b1432c51-fea8-4c96-acd5-2467f269cf18'), (4974, '9e5f0436-fc3e-4670-806a-f47aad3fdefb'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (24432, '65160c4e-e76b-478e-af89-46847fcd223f'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (24434, 'bb7afb46-7718-4546-95b2-cc62410b73cb'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (4986, 'd1a0c49b-c4b7-4f64-b176-5c34266e3eaa'), (4987, 'ce213b91-abda-41aa-a9e4-1bc721650b4e'), (4988, '69e0d926-dd7e-4083-a162-5b735e226b10'), (30587, 'cd50115c-100c-4ab4-afa0-085f9ccb859e'), (24960, '8e0e4635-d47a-4795-bdab-22ffe8fea5c8'), (3465, 'df743924-ed1e-4203-8840-9b440ee9aea1'), (399, '6471d080-c1de-442e-bbe9-aaf0cd61e347'), (5013, '474395e3-99c5-4cbe-b336-48aedca62e63'), (29591, 'd5215d35-b749-4c9b-9716-01632f84d6c6'), (28571, '255382e0-8ccf-4b19-85b2-5503370b8f36'), (31144, 'f9d7c8b7-134a-437f-b80f-69855baaf42e'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (2482, '596cae45-9565-42d2-9dab-675575bfea73'), (2500, '7b02e19d-828d-4f06-b75f-01aad79db47e'), (2503, '2e1ff638-b3dd-42ac-9810-3e463ba23b97'), (22481, 'dc81fdad-ba23-418c-90b4-2a4a24a952dc'), (9172, '7d703f2a-3f55-4071-8788-8ac016048cc2'), (15838, 'a7cc273a-40da-4c92-a3f3-afaa3e71b4c5'), (29159, '27f1f229-944d-4f99-bf23-7acab0f2fec7'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48'), (13293, 'a25e62b4-e680-4a30-ad15-f0e4580ba9ff'), (8176, '2cc6d0c0-3a2d-4ebc-8266-4d2ee6bf3300'), (6641, '74cd55d1-cecf-459b-9beb-2eddfb4c5714'), (25083, '0f936a22-efe4-4d4e-b788-5ae1ce1e9b75')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: he post-processing step in a question answering task. When doing question answering, the processing of the initial dataset implies splitting examples in several features, which may or may not contain the answer. Passing those features through the model will give us logits for the start and end positions, since our labels are the indices of the tokens that correspond to the start and end the answer. We must then somehow convert those logits into an answer, and then pick one of the various answers each feature gives to be THE answer for a given example. For the processing step, you should refer to the video linked below. It's not very different for validation, we just need to add a few lines to keep track of two things: instead of discarding the offset mapping, we keep them, and also include in them the information of where the context is by setting the offsets of the special tokens and the question to None. Then we also keep track of the example ID for each feature, to be able to map
he post-processing step in a question answering task. When doing question answering, the processing of the initial dataset implies splitting examples in several features, which may or may not contain the answer. Passing those features through the model will give us logits for the start and end positions, since our labels are the indices of the tokens that correspond to the start and end the answer. We must then somehow convert those logits into an answer, and then pick one of the various answers each feature gives to be THE answer for a given example. For the processing step, you should refer to the video linked below. It's not very different for validation, we just need to add a few lines to keep track of two things: instead of discarding the offset mapping, we keep them, and also include in them the information of where the context is by setting the offsets of the special tokens and the question to None. Then we also keep track of the example ID for each feature, to be able to map
that gets state-of-the-art results is to mask all the tokens of a given word instead of randomly chosen tokens. This will require us to use the word IDs we saw. When doing token classification, we'll need to convert the labels we have on words, to labels on each tokens. As for the offset mappings, it will be super useful when we need to convert token positions in a sentence into a span of text, which we will need to know when looking at question answering or when grouping the tokens corresponding to the same entity in token classification. To have a look at these tasks, check the videos linked below!
in them the information of where the context is by setting the offsets of the special tokens and the question to None. Then we also keep track of the example ID for each feature, to be able to map back feature to the examples that they originated from. If you don't want to compute the validation loss, you won't need to include all the special code that we used to create the labels. With this done, we can apply that preprocessing function using the map method. We take the SQUAD dataset like in the preprocessing for question-answering video. Once this is done, the next step is to create our model. We use the default model behind the question-answering pipeline here, but you should use any model you want to evaluate. We will run a manual evaluation loop, so we create a PyTorch DataLoader with our features. With it, we can compute and gather all the start and end logits like this, with a standard PyTorch evaluation loop. With this done, we can really dive into the post-processing. We will
```

As we can see, the first three examples (at indices 2, 3, and 4 in the training set) each gave four features and the last example (at index 5 in the training set) gave 7 features.

This information will be useful to map each feature we get to its corresponding label. As mentioned earlier, those labels are:

- `(0, 0)` if the answer is not in the corresponding span of the context
- `(start_position, end_position)` if the answer is in the corresponding span of the context, with `start_position` being the index of the token (in the input IDs) at the start of the answer and `end_position` being the index of the token (in the input IDs) where the answer ends
 
Here is the question: What is the purpose of label mappings in Visual Question Answering tasks?","The purpose of label mappings in Visual Question Answering tasks is to convert the labels from word-level to token-level, and to map the token positions in a sentence into a span of text, which is necessary for question answering and token classification tasks."
How does Hugging Face's pipeline API enhance the user experience for model testing?,"[(28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15889, 'aad2c393-9f11-40db-b577-03aba97e12a2'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (27672, '59487233-b752-42ea-aa5a-8cb853def35f'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (24634, '70652141-34ee-406f-816b-e497d503fd8d'), (13893, '821034d7-81d2-4f43-ab41-86255b1a41a6'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8269, '63e383a9-dbfa-4c9f-a286-0668a682d313'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (4192, 'fc859fbf-f289-4177-a3a6-21b11cb22ad3'), (1127, '61bce4e3-ba06-42fd-82da-5f4b1ae2562a'), (3183, '05adffa9-2646-489c-b3ad-2357cb108a8e'), (22129, '6e12827f-d0c2-45d5-93c5-c456a0f17de3'), (12913, '6b795525-cd89-4895-a8f6-d7b48571a2f3'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (1208, 'f92f90e4-fea0-4a20-8453-df5327bb6e2f'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (30911, '77b06868-0a69-441b-9b41-afd19f1bf77b'), (5822, '95a2e3de-66e6-4f99-8e23-04e96dc3f30f'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (7900, '7e98dfeb-7dfc-4433-aefb-7ff05809815d'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (1276, 'a3b6c81e-6463-482d-8a2d-7e4352b47f91'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (21761, 'd2006b50-d641-4db7-8cef-7f6c0b6f8eda'), (17160, '91bdea0b-d26a-4435-9b08-9f133f7cad52'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (10060, '7dcdfc7e-5594-47b8-b679-ed36876bb0c0'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (18788, 'c8b88dcc-6e3b-4f12-9ccc-23fa571e9a77'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1393, '0d273b86-428c-40bc-9c06-4b1cee5bf620'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (28539, '15ca206b-e242-4240-9c56-b957d70a1add'), (16252, 'ebc3b30d-233d-43e4-b012-9f5a958063fc'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (405, '66608065-a77a-4384-996d-4875f7d26596'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (5027, 'd4bbbeb5-0f59-45d8-b525-8c4180353a26'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (20901, '8e4eb670-266c-471e-aef1-087bd6dfa125'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (26044, '8f76898f-d973-4edd-85de-d497103fcbbd'), (26050, '9fa486e8-15c9-4518-b8bd-39facb02ccf8'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (29136, '7714cc41-6519-48fd-9faa-73ce00ad957d'), (24025, '95318624-c2ce-41c0-891f-a3b06efb2458'), (6106, '13c9c3c6-6d87-4cba-a886-332bfef5da3d'), (30684, '92463e79-9a4a-4e03-b283-8b4b367407f1'), (15837, '78ef6dea-b85c-4549-ad3c-0ef7afe5cd99'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (25077, '79b1b03d-1416-4d5e-8e14-65cb6a00c956'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In the beginning, we auditioned a hand-rolled, in-house model hosting solution we had been using for prototyping, comparing it against AWS Sagemaker and Hugging Face’s new model hosting Inference API. Given that we use GCP for data storage and Google Vertex Pipelines for model training, exporting models to AWS Sagemaker was clunky and bug prone. Thankfully, the set up for Hugging Face was quick and easy, and it was able to handle a small portion of traffic within a week. Hugging Face simply worked out of the gate, and this reduced friction led us to proceed down this path.
Hugging Face makes it really easy to share your spaCy pipelines with the community! With a single command, you can upload any pipeline package, with a pretty model card and all required metadata auto-generated for you. The inference API currently supports NER out-of-the-box, and you can try out your pipeline interactively in your browser. You'll also get a live URL for your package that you can `pip install` from anywhere for a smooth path from prototype all the way to production!

### Finding models

Over 60 canonical models can be found in the [spaCy](https://hf.co/spacy) org. These models are from the [latest 3.1 release](https://explosion.ai/blog/spacy-v3-1), so you can try the latest realesed models right now! On top of this, you can find all spaCy models from the community here https://huggingface.co/models?filter=spacy.


### Widgets
Hugging Face PRO users now have access to exclusive API endpoints for a curated list of powerful models that benefit from ultra-fast inference powered by [text-generation-inference](https://github.com/huggingface/text-generation-inference). This is a benefit on top of the free inference API, which is available to all Hugging Face users to facilitate testing and prototyping on 200,000+ models. PRO users enjoy higher rate limits on these models, as well as exclusive access to some of the best models available today.

## Contents
After an extensive three-month evaluation period, we chose Hugging Face to host our models. During this time, we gradually increased transaction volume to their hosted models and ran numerous simulated load tests based on our worst-case scenario volumes. This process allowed us to fine-tune our system and monitor performance, ultimately giving us confidence in the inference API's ability to handle our transaction enrichment loads.
## 2. New Optimum inference and pipeline features

With [release](https://github.com/huggingface/optimum/releases/tag/v1.2.0) of Optimum 1.2, we are adding support for [inference](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling_ort) and [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines). This allows Optimum users to leverage the same API they are used to from transformers with the power of accelerated runtimes, like [ONNX Runtime](https://onnxruntime.ai/).

**Switching from Transformers to Optimum Inference**
The [Optimum Inference models](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling_ort) are API compatible with Hugging Face Transformers models. This means you can just replace your `AutoModelForXxx` class with the corresponding `ORTModelForXxx` class in Optimum. For example, this is how you can use a question answering model in Optimum:
 
Here is the question: How does Hugging Face's pipeline API enhance the user experience for model testing?","Hugging Face's pipeline API enhances the user experience for model testing by allowing users to try out their pipeline interactively in their browser, and providing a live URL for their package that can be `pip install` from anywhere, making it easy to share models with the community and facilitating a smooth path from prototype to production."
What distinguishes ViLT's approach to VQA compared to traditional methods?,"[(5145, '558a691d-e873-4114-b59e-9c86d4765f27'), (26652, 'b573f67e-d2f1-4135-a4c5-ed62abb57566'), (22061, 'd5e3f551-208d-4a30-ae0f-2844bf5186ff'), (22062, 'b65a5638-467c-4923-b163-9a8467ee706d'), (22063, 'b6a4b9c4-c683-4973-b991-8c2cb24c7519'), (28717, '45cc6367-7b38-4f30-a36f-c96fb2293202'), (22065, '9fcae45a-a21a-4f0a-88e4-5deac5078287'), (22071, 'ad5c8e12-0663-4a6b-961c-c00ce8a8cd9e'), (4667, 'ea371344-8b31-4041-8d3f-3a2d817bd4bb'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (4672, 'd99e5f71-aff4-469d-9295-e833c20216b4'), (4683, '7c1ca1c0-b096-4cdc-8e23-344f2a8241e1'), (4686, '59de41e9-0c89-49cd-9d19-e7b1fabd0bc4'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (10853, 'cc7466fe-c647-4c7e-a1ce-4288a078ba55'), (13926, 'c6436b34-5943-4b25-b196-20d9695a4c1d'), (13927, '38fd6452-7b8a-4923-aeb8-b117f1211ee6'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (3208, '9e8569e0-0494-4658-93be-ca5a74e64344'), (3211, '66850aca-1339-44e1-b6ee-3612c185f4d3'), (6801, 'a8d073e8-4092-47e2-b7f0-c61300a489a7'), (6802, '816d52f3-ceec-436a-b89e-8e94e93f67cd'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (20651, '9c5a8380-0788-4e02-96cc-ae00c3f65050'), (25263, 'd7a4626e-6223-485c-95a7-4e2cd4cb33b1'), (29372, '9ce1019a-4a38-4555-b97e-0b17e7ee0dc4'), (20685, 'dd9690c6-ac92-4bc9-9801-cb047581cbb7'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (21210, 'a7d914db-0a82-4bef-889e-b11938bed886'), (27361, '334d3a20-8ea3-42cd-a731-147b016bd478'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (6893, '3e3a63e2-90f5-4d24-8729-3673b42d713d'), (23788, '56c682f7-2a5e-419a-b067-a91384f00487'), (23789, '1d90878f-1c99-451b-b6b9-c6a02aa3ee99'), (8432, '128c1a7c-3f4d-481a-9a0c-f20ba007042e'), (4852, '44fd3946-6b3b-4d71-a95c-1553a282aa4b'), (3828, 'cc724033-54f2-492d-98b4-a6313303b04c'), (8955, '11133dce-6d7c-45cf-b8e6-fa0180834552'), (23295, '9ce95fbe-1a90-472d-9c71-211f3df266e2'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (5908, '144a7b30-6985-4f7d-adc3-de47096fc6ab'), (12049, '9ebd4171-a0ce-40d1-9883-c37ead9490d1'), (29461, 'fd3cd966-a8f9-476f-9c8b-f7f6f835b035'), (12048, '6d73bc8e-5e71-4307-8e26-99e5ab4a4784'), (8472, '1198c94a-3f34-4223-9da9-02f2c1303a9d'), (15643, 'c3807abd-50f9-491d-8ccd-7a15e1f13bfa'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (1824, '7b77c59e-36af-44d2-a1f5-b832deccf49b'), (1826, 'caefe288-d31b-42db-bba2-5e3087bd1325'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (298, 'a9b73b5b-eef0-437d-842d-a511870ed8c2'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (23867, 'd49bd9cf-b6a6-4207-9330-94e086e9aae5'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (23873, '6259700a-095f-4a14-8048-95fb1df7f296'), (4422, 'cae3f2e8-d9f5-411f-a4bf-e75aff751dec'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (29515, '18fb13c6-8761-4aea-ac01-1b771967088f'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (1885, 'b6dd43af-f98a-4135-9a48-8d42803b72d5'), (7013, '7c2e994f-665b-4e39-a477-19fa189595ac'), (7015, '929caa94-db70-4c90-85ca-46a52fdf2f65'), (15210, '1889645d-b1b6-4aa5-8c2a-60d3f31f3c60'), (23409, '4b35edb5-5f84-4a04-ae2a-721a423b84df'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (13171, '155c1a67-c827-4ad3-86bf-5c20829ef13e'), (10613, '0835f4fb-6088-4ffa-9b78-b5b2e6fd58d4'), (22904, 'a678407d-0ca1-4128-a53e-8ab8dccdddf1'), (10616, '290af90a-eec1-4865-8e69-d84dbde36067'), (4986, 'd1a0c49b-c4b7-4f64-b176-5c34266e3eaa'), (22907, 'e073631d-92bc-47de-b5d9-ba8617019125'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (23948, '5b2e0ae2-187e-4e54-940b-7be5783c4a43'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (23955, '04cca42e-a12f-4c06-9ec3-34e4d4ac448a'), (919, '5f9769c1-e289-4680-bf6b-c06d7d29e9d9'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (3482, '1833cdcd-6873-41f4-accd-7516f4c12933'), (9119, 'cd5f2faa-9cfe-42e6-8006-4a8264ce76d0'), (7073, 'a58afddf-a5d9-4a17-b197-423b25fb7aac'), (30133, '61f3f5a8-2c83-44a2-9a84-594c52ec5c89'), (14773, 'eddfb9b5-65fd-4a2d-a578-118281f8c55e'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (22456, '55e20e3e-6f8f-413c-8a0f-663f7d1df98d'), (20412, 'f06728ac-eda3-4786-8dfd-87ab079fe244'), (20413, '5ced2137-1268-4467-9c14-20ab7b9e541d'), (20415, '0ff6dba8-cfe6-4157-a337-93d21a8a4a9e'), (20416, '29126e8a-96b2-4d05-a07e-1c2689b8c2f3'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (2502, '8e358fab-31fa-4f90-b90d-a95834e06f52'), (2503, '2e1ff638-b3dd-42ac-9810-3e463ba23b97'), (25034, '6df4e515-fab2-4473-8dc2-849bad23124e'), (10194, 'e573dcfe-a4f3-435f-9afe-101265a292e5'), (11236, 'ddb46a9b-1eac-418c-b631-5cfda6f4101b'), (18413, 'c087ad9c-11fd-4007-b215-f0a96fd9c1e4'), (5110, 'ff805522-0e2d-45ba-be4f-6e3fac713294'), (14846, 'cf703fd3-cc50-49a5-bfa6-2513ea2118cb')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
simplified to just the same convolution-free manner that we process textual inputs. We show that ViLT is up to tens of
times faster than previous VLP models, yet with competitive or better downstream task performance.*
- The design of ViLT is very similar to that of a standard Vision Transformer (ViT). The only difference is that the model includes
  additional embedding layers for the language modality.
- The PyTorch version of this model is only available in torch 1.10 and higher.
- The quickest way to get started with ViLT is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViLT)
  (which showcase both inference and fine-tuning on custom data).
- ViLT is a model that takes both `pixel_values` and `input_ids` as input. One can use [`ViltProcessor`] to prepare data for the model.
  This processor wraps a image processor (for the image modality) and a tokenizer (for the language modality) into one.
- ViLT is trained with images of various sizes: the authors resize the shorter edge of input images to 384 and limit the longer edge to
  under 640 while preserving the aspect ratio. To make batching of images possible, the authors use a `pixel_mask` that indicates
  which pixel values are real and which are padding. [`ViltProcessor`] automatically creates this for you.
- The design of ViLT is very similar to that of a standard Vision Transformer (ViT). The only difference is that the model includes
```

## Zero-shot VQA

The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach 
VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training 
paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). 
This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. 

Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a 
GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: 

```py
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
 
Here is the question: What distinguishes ViLT's approach to VQA compared to traditional methods?","According to the context, ViLT's approach to VQA distinguishes itself from traditional methods by treating Visual Question Answering (VQA) as a **classification problem**, whereas more recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a **generative task**."
How does BLIP-2 achieve superior performance in zero-shot VQA compared to ViLT?,"[(11264, 'f1b68b88-f347-4a90-9d24-0cae81ae704e'), (11283, '729f6a59-d667-445b-9197-bd8970806506'), (26143, 'abb163cb-3331-48d4-bb4d-66441f20f8d4'), (18979, '9336d4a9-e1a5-4fad-a990-f9ce022a25e3'), (26158, '630b0c8f-efd9-4513-9fec-588a683b117e'), (26160, '524aa393-c139-4c52-acd8-92ca1729c0d0'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (31293, 'bf6fa69e-7b65-4651-84a6-7287755969d5'), (19522, '9f3b6486-249b-493d-a542-2dfe80215a8d'), (10315, '5570dac5-1188-45c3-8401-666f1387bbf6'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (4690, '73b0a8e6-ac3c-4071-b88d-beb7478681aa'), (4691, 'db667e7e-ab3f-4347-8a38-098edd3bdd0d'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (20601, 'a07e30af-9ee2-4d89-adb0-f3f1cd92ade2'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (1663, '323fa14b-b505-4dea-ac38-9c7133039a48'), (1665, '0837dd42-4848-4039-9cd0-a3c8a59eec63'), (29838, '4085395a-76c9-44e4-bb5d-139a2ac8f1ef'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (30374, 'e8c323d8-135f-4457-a4fe-9e8d56c5ccd4'), (11952, '8b8c83fe-c48f-4de5-9a0a-46ff6e301bfa'), (25785, 'cc8b2b20-c91a-4342-b3f7-0f6232628ce7'), (1745, 'cd32a895-3ae9-4f82-8574-44ad1d9286e8'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (16090, 'de34769a-90f0-4c82-b82e-6d9a5a0e6f92'), (16094, '90e8a230-a59f-4644-b799-3e81823cb581'), (223, 'acfa4968-148f-4d01-9982-f4cd2bb8795b'), (27361, '334d3a20-8ea3-42cd-a731-147b016bd478'), (5351, '277d7208-e0c2-4707-86b5-a96557cac8a6'), (4842, '6a17f2b6-7cdd-4b6f-adf3-b132838489cc'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (6893, '3e3a63e2-90f5-4d24-8729-3673b42d713d'), (8432, '128c1a7c-3f4d-481a-9a0c-f20ba007042e'), (8433, 'a4916d14-dff1-4bea-9fe9-aad51bca0c7c'), (3826, '256f85ac-16ed-48b7-8a96-c986ccb76483'), (29437, 'a9873ed2-2cf8-44cc-9b63-9ba1c9d4a818'), (8450, 'e90f97af-6a8c-4ca1-8d96-7acf7aa7ca34'), (8460, '59ac32c1-6a6a-4294-95a1-337d4ca90cb9'), (8461, 'd7a376d4-718d-43c6-953a-dc1bbfe0b36a'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (12049, '9ebd4171-a0ce-40d1-9883-c37ead9490d1'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5908, '144a7b30-6985-4f7d-adc3-de47096fc6ab'), (1814, 'e4f58e5a-3444-47f9-aa56-481e3993c59c'), (24339, 'bc00f956-dd91-4b6d-a59a-7b7567c85c23'), (8472, '1198c94a-3f34-4223-9da9-02f2c1303a9d'), (8473, 'fc6c139e-ef25-4ad8-b5ce-99d56c4b56fc'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (30505, '9e92a10f-02d1-4093-aa4b-1504134b9dcc'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (16174, '7aeaf6a6-cf85-46b0-af0b-f0ab92bc6103'), (30515, '19196d30-23c6-4c8b-8787-154cacc058ac'), (31029, '5b3fec47-d2ee-4bf7-bcba-7bd3e3a65030'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (22328, '3eb494fc-8503-4a8f-8424-ee15b59d0d68'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (23873, '6259700a-095f-4a14-8048-95fb1df7f296'), (4420, 'acb220f7-87c0-4af4-883e-44df58ee00fb'), (21318, '1e44662b-5b7c-42ae-b550-2e1a4cf8d5e8'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15715, 'b6b83e1b-e049-44a5-bc43-dca8acd67993'), (7015, '929caa94-db70-4c90-85ca-46a52fdf2f65'), (4967, '43778b66-3e71-4fd3-a9bf-f2cf850f713d'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (20852, 'b516f3fa-d794-46a2-a94e-d391909f98d9'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (20851, 'b8c3710a-efdb-41e6-9f78-8cb777be59d6'), (15735, 'f473ff9d-2520-4b90-81e6-2bc07ffb2468'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (8060, '9eef21f5-4170-4668-bc2d-bfb804087692'), (14206, '5bfbab3f-215b-442a-8bd0-1810e815b192'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16264, '5d166a05-2d80-4201-ae2f-b71993797a24'), (8072, '75702fbf-8f70-4667-ade7-37ec084bfc8b'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (16267, '7ac5f372-d956-427b-832d-160d6a912937'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (8080, 'd6a21ffd-e82d-40f0-873f-40124dc6c6eb'), (10133, '7735b57a-5b30-4c4f-9ab3-c1e84672a8be'), (21400, '637da23f-83b4-4655-83e1-bd9d7d7048ee'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (11691, 'dd3ff1e4-0aa8-42d6-b141-406f1df3f143'), (4529, '795ebffd-b6ef-42a5-ba9b-9d869516b585'), (30133, '61f3f5a8-2c83-44a2-9a84-594c52ec5c89'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (20415, '0ff6dba8-cfe6-4157-a337-93d21a8a4a9e'), (11724, '8e395008-747c-4c0c-9751-3552e87121f7'), (12239, 'a37238df-c3f6-4c5a-98f2-2b75f0eb03da'), (20449, '00a22323-79c9-407d-87a0-c90048cc2a5d'), (12270, '002549e4-baef-4cb3-baf4-279f524b849b'), (11262, '5c2061dd-7364-4daa-bb21-a1404baa2b55')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7% in average recall@1), image captioning (+2.8% in CIDEr), and VQA (+1.6% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to videolanguage tasks in a zero-shot manner. Code, models, and datasets are released.*
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# BLIP-2

## Overview

The BLIP-2 model was proposed in [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597) by
Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi. BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer
encoder in between them, achieving state-of-the-art performance on various vision-language tasks. Most notably, BLIP-2 improves upon [Flamingo](https://arxiv.org/abs/2204.14198), an 80 billion parameter model, by 8.7%
on zero-shot VQAv2 with 54x fewer trainable parameters. 

The abstract from the paper is the following:
```

## Zero-shot VQA

The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach 
VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training 
paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). 
This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. 

Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a 
GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: 

```py
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference. 

Before you begin, make sure you have all the necessary libraries installed. 

```bash
pip install -q transformers datasets
 
Here is the question: How does BLIP-2 achieve superior performance in zero-shot VQA compared to ViLT?","BLIP-2 achieves superior performance in zero-shot VQA compared to ViLT because it treats VQA as a generative task, whereas ViLT treats it as a classification task. Additionally, BLIP-2 leverages frozen pre-trained image encoders and large language models (LLMs) by training a lightweight, 12-layer Transformer encoder in between them, which allows it to achieve state-of-the-art performance on various vision-language tasks with fewer trainable parameters."
What feature of Hugging Face's pipeline API supports zero-shot applications?,"[(10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (22037, '01e8996d-daff-47eb-b24c-ae98ba9c4de9'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (7725, '258a242e-844a-4498-bd2d-4c45f980f5f2'), (24127, '474b8446-9f4d-4ec3-9b0e-78695268902d'), (22083, 'dd15b594-110b-4773-b128-0658f80e911d'), (9284, 'b612daf8-a308-4ac1-a05d-d1af0da0a26b'), (20558, 'ee3657ca-549a-4f46-9322-da90f5505f63'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4180, 'fc3c916c-becf-4f0c-846f-a3ddb528a853'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (17509, '56ea90b0-f162-4bf5-8c05-654f3a183cd9'), (1127, '61bce4e3-ba06-42fd-82da-5f4b1ae2562a'), (10858, '33249571-1ade-46fd-ae89-e32ff806fdcd'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (2682, '49cee137-69ce-4503-b6cc-90982e9029ed'), (11386, 'b62622b1-cbec-4754-9710-20faddd1d382'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (150, '3bace117-5049-4192-b247-676e182cf109'), (149, '19d1f1a7-3ff1-4d99-b448-86fda7b1e1a1'), (153, 'a5e94b25-4bd0-4edc-be2a-4167144129cb'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (3238, '558f6733-8507-471e-b4d2-520504677468'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (5810, 'b507f388-1038-4bc8-8559-2d51ffaec850'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (20671, '817cbafe-ab8a-4498-88c1-de98146869d4'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (18113, '608d9ab0-33a4-42b5-ba2a-9fbe39a0bccb'), (21187, 'd9dcef43-8125-461e-9c4f-131925ad7523'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (712, 'a2d8b244-cc83-4286-af6e-08f1ef96ed7a'), (14535, 'b9835dda-ff2d-4d55-a497-5553a72948a6'), (20167, '2bb84049-c72c-4886-948b-e9b8893655fb'), (21203, '5f38e41b-22a4-4ec8-b993-4864b7079429'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (13531, 'e71c27f1-71e6-4781-9c94-fd5500c11c0f'), (10461, '791d16b9-ac09-45b6-a426-0eea3d3b4063'), (225, '3ad317d7-101b-4379-9a04-92d617ddfd14'), (17638, '2a3bbb6e-24c7-46d2-8b6a-c6fc79c55d6c'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (26883, '2933d4ed-2563-4956-8c7b-5b5afa75f91e'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (2832, '998a6650-0b25-4769-81b2-402f8fc9f186'), (2849, '00421fa8-9291-41d9-940f-88c42efc648e'), (31031, '306dd725-df38-43a2-a289-44917df0a6f1'), (5944, 'af31af69-da28-4239-b3f5-59c624ec8d83'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (5948, '68fab821-06c3-4a78-96e2-4dfd616f31c0'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (26441, '93d6b5ba-e469-4365-a6f7-e2178130a4e5'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (14165, 'f8e81797-b080-4a41-9f82-86eea16eabe8'), (9048, 'efc65a77-9ffc-44fc-9ad7-459f4b5bb373'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (31584, '08909b15-59c3-48af-b6fc-5206707e70f1'), (15215, '8b19f2fb-4cae-4528-8a1e-bbe343f82786'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (16760, 'd936a97e-a944-4271-bca8-ab614fbc7ca8'), (16762, '0a98d923-bf03-4726-a7c3-4f2c56457e83'), (16252, 'ebc3b30d-233d-43e4-b012-9f5a958063fc'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (19857, '894d909b-b9a8-4c18-8789-710e8a2fd282'), (17301, '9e130d4e-12df-4941-91c1-6e5fd07be093'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (6045, 'f8f84514-0fc4-4472-bc43-fe12c1366193'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (29598, '78d2e6f1-098e-4154-9814-d3529bde1b1a'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (11691, 'dd3ff1e4-0aa8-42d6-b141-406f1df3f143'), (18347, '0e59274a-81fe-4ef3-a419-b0f3d2a101d0'), (5037, 'bd256078-3488-4b79-aea0-4e8cb9e443d7'), (5041, '812dcc27-ef00-4194-976d-6f9eb30583a9'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (21447, '8b7f3a13-78d2-4c1a-b32a-51b280baae78'), (29652, '250e9a0d-0efe-494b-a5ce-ed0c9f5008c4'), (6105, '9efc65f9-7879-4b2c-8da5-721897d26d7d'), (6106, '13c9c3c6-6d87-4cba-a886-332bfef5da3d'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (12765, 'a9a75a73-70f1-4e42-a8a3-2f8413929d72'), (30685, 'a039eae8-64c2-468f-9dfd-126ca871a519'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (5625, 'c79981e0-e8ac-454a-a1ec-1e4b2c4b1ba2'), (30202, '4e035267-8efe-4a23-b5fb-0f711e2929d4'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Here is a list of things we’ll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#🤗-🤝-timm)
- [Diffusers](#🧨-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time 👁

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
```

## Zero-shot image classification pipeline

The simplest way to try out inference with a model supporting zero-shot image classification is to use the corresponding [`pipeline`].
Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads):

```python
>>> from transformers import pipeline

>>> checkpoint = ""openai/clip-vit-large-patch14""
>>> detector = pipeline(model=checkpoint, task=""zero-shot-image-classification"")
```

Next, choose an image you'd like to classify.

```py
>>> from PIL import Image
>>> import requests

>>> url = ""https://unsplash.com/photos/g8oS8-82DxI/download?ixid=MnwxMjA3fDB8MXx0b3BpY3x8SnBnNktpZGwtSGt8fHx8fDJ8fDE2NzgxMDYwODc&force=true&w=640""
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image
The Hugging Face Hub has [hundreds of object detection models](https://huggingface.co/models?pipeline_tag=object-detection) pre-trained in different datasets, able to identify and localize various object classes. 

One specific type of object detection models, called _zero-shot_, can receive additional text queries to search for target objects described in the text. These models can detect objects they haven't seen during training, instead of being constrained to the set of classes used during training.

The diversity of detectors goes beyond the range of output classes they can recognize. They vary in terms of underlying architectures, model sizes, processing speeds, and prediction accuracy.

A popular metric used to evaluate the accuracy of predictions made by an object detection model is the **Average Precision (AP)** and its variants, which will be explained later in this blog.
**Hugging Face + Google Sheets Demo**

With the [Inference API](https://huggingface.co/landing/inference-api/startups), you can easily use zero-shot classification right into your spreadsheets in Google Sheets. Just [add this script](https://gist.github.com/feconroses/302474ddd3f3c466dc069ecf16bb09d7) in Tools -> Script Editor:

<div class=""aspect-w-16 aspect-h-9"">
<iframe 
src=""https://www.youtube.com/embed/-A-X3aUYkDs"" 
frameborder=""0"" 
allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" 
allowfullscreen></iframe>
</div>



**Few-shot learning in practice**

We wrote a [blog post](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api) about what Few-Shot Learning is and explores how GPT-Neo and 🤗 Accelerated Inference API are used to generate your own predictions.

### **Expert Acceleration Program**
```

## Zero-shot image classification by hand

Now that you've seen how to use the zero-shot image classification pipeline, let's take a look how you can run zero-shot
image classification manually.

Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads).
Here we'll use the same checkpoint as before:

```py
>>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

>>> model = AutoModelForZeroShotImageClassification.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)
```

Let's take a different image to switch things up.

```py
>>> from PIL import Image
>>> import requests

>>> url = ""https://unsplash.com/photos/xBRQfR2bqNI/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjc4Mzg4ODEx&force=true&w=640""
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image
 
Here is the question: What feature of Hugging Face's pipeline API supports zero-shot applications?","The feature of Hugging Face's pipeline API that supports zero-shot applications is the ""zero-shot-image-classification"" task. This task allows for zero-shot image classification, which means that the model can classify images without being trained on the specific classes or tasks."
What distinguishes ViLT's architecture from BLIP-2 in vision-and-language tasks?,"[(5120, 'ddf54aa1-62ec-4203-a18e-bcc935c8a3ed'), (18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (28717, '45cc6367-7b38-4f30-a36f-c96fb2293202'), (22062, 'b65a5638-467c-4923-b163-9a8467ee706d'), (22063, 'b6a4b9c4-c683-4973-b991-8c2cb24c7519'), (31289, '9905fe51-ba24-492b-9d83-93aa4879a98b'), (31291, '728e3f48-a642-437c-b7b8-adf9580dc871'), (31293, 'bf6fa69e-7b65-4651-84a6-7287755969d5'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (20576, '40796d31-f95c-4e9d-a36b-2d6319601be0'), (20601, 'a07e30af-9ee2-4d89-adb0-f3f1cd92ade2'), (3211, '66850aca-1339-44e1-b6ee-3612c185f4d3'), (29838, '4085395a-76c9-44e4-bb5d-139a2ac8f1ef'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699'), (20634, 'b26ceca9-16f3-491c-a4e2-ce69a77a514c'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (4772, '2a92752e-7e2a-4b91-89ad-7510a946ebde'), (20651, '9c5a8380-0788-4e02-96cc-ae00c3f65050'), (4796, '8f601501-25a6-4972-aa7a-aa9e072baede'), (3790, '58a2a61f-05c0-4033-8294-f7bb9f645cb1'), (1745, 'cd32a895-3ae9-4f82-8574-44ad1d9286e8'), (1746, '49e8dce4-afe5-4268-9ab9-6f9809115e1e'), (4825, '6ab69da6-2bc1-4ebb-8ff1-014a9f3229cc'), (4829, '2327c019-21d7-4362-844c-4ec05792b517'), (1769, '995d0156-b149-4fea-94ea-418d999909b4'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (18157, '0fa37fed-c330-49ae-8d41-80e888e5cd81'), (6893, '3e3a63e2-90f5-4d24-8729-3673b42d713d'), (3823, '1b242c88-9fa6-4480-8d4a-167fc47e1885'), (8433, 'a4916d14-dff1-4bea-9fe9-aad51bca0c7c'), (4851, '7e0cad1c-05a0-4c88-b14a-c88e02040671'), (4852, '44fd3946-6b3b-4d71-a95c-1553a282aa4b'), (6909, '84556055-9fa1-4cff-bfe5-073bcd5b7f97'), (29437, 'a9873ed2-2cf8-44cc-9b63-9ba1c9d4a818'), (27391, 'bf10fdf5-d841-4030-8d03-4cb6d0161538'), (1795, '4930cd63-b8c3-4a27-b9b2-e2db184cbbfc'), (13575, '849335ec-ba91-41e8-9ac2-ab98d4ffd804'), (18183, '242b44b3-f2a1-4c2d-8f5f-d4e734e95af7'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (273, '0ec7334c-0cd3-4050-aa56-27d3f06ff6e0'), (8473, 'fc6c139e-ef25-4ad8-b5ce-99d56c4b56fc'), (1823, '3d5fb357-1a86-4d95-8f1d-c77d575b1880'), (1824, '7b77c59e-36af-44d2-a1f5-b832deccf49b'), (13600, '88b3b12c-4f41-4035-a2bc-f1935a009a0a'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (298, 'a9b73b5b-eef0-437d-842d-a511870ed8c2'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (307, '6b64ed6e-acaf-4565-8edd-d7398bd867b4'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (4421, '95a1272e-1f19-494f-96d5-f36f9a919a0a'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (7003, '4c94ce0b-dedb-4c1c-876c-9ddabb798517'), (7006, 'ca729c38-08a8-487f-85b8-173e988c8e43'), (16223, '53622eb2-87f4-4345-91be-31270bd4a8f3'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (25963, 'fd0bea05-61c9-4e1b-8a8b-5171642f2f0d'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25965, '1cda8dc4-5b16-484d-b64c-99fc5b9e9d79'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (20851, 'b8c3710a-efdb-41e6-9f78-8cb777be59d6'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (17776, '12a717db-67d1-4c7e-945c-aeb888ef89ed'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (9093, '1a36f6b7-6848-41d8-9192-5a45c7f3a94a'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16263, '548b0220-ed2d-461b-a176-886ee094b6d0'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (16267, '7ac5f372-d956-427b-832d-160d6a912937'), (29081, 'fb92a989-9e09-4948-b2bc-d0c04fb1372f'), (9119, 'cd5f2faa-9cfe-42e6-8006-4a8264ce76d0'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (9129, '4e3c49f8-4ca3-47af-a572-5135008633d3'), (30130, '701c9d28-b4a0-4d27-8f27-eef55e68d62c'), (30133, '61f3f5a8-2c83-44a2-9a84-594c52ec5c89'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (20412, 'f06728ac-eda3-4786-8dfd-87ab079fe244'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (10191, '1eec8235-d812-40cf-8080-206fff853f47'), (18387, '72dc24e5-573b-4a09-b0b9-bed0bea8a427'), (6613, '32a1a846-385b-4827-b68c-b525c3e4d41d'), (5084, '2e0ac4fe-4419-45c1-ab1d-4bb51b751518'), (18413, 'c087ad9c-11fd-4007-b215-f0a96fd9c1e4'), (5110, 'ff805522-0e2d-45ba-be4f-6e3fac713294'), (18423, 'bceac124-b7c2-4000-85e8-bc1990d7dce1'), (11262, '5c2061dd-7364-4daa-bb21-a1404baa2b55')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
- The design of ViLT is very similar to that of a standard Vision Transformer (ViT). The only difference is that the model includes
  additional embedding layers for the language modality.
- The PyTorch version of this model is only available in torch 1.10 and higher.
```

## Conclusion

BLIP-2 is a zero-shot visual-language model that can be used for multiple image-to-text tasks with image and image and 
text prompts. It is an effective and efficient approach that can be applied to image understanding in numerous scenarios, 
especially when examples are scarce. 


The model bridges the gap between vision and natural language modalities by adding a transformer between pre-trained models. 
The new pre-training paradigm allows this model to keep up with the advances in both individual modalities.


If you'd like to learn how to fine-tune BLIP-2 models for various vision-language tasks, check out [LAVIS library by Salesforce](https://github.com/salesforce/LAVIS) 
that offers comprehensive support for model training.


To see BLIP-2 in action, try its demo on [Hugging Face Spaces](https://huggingface.co/spaces/Salesforce/BLIP2).


## Acknowledgments
*The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text
Modern computer vision and natural language models have become more capable; however, they have also significantly 
grown in size compared to their predecessors. While pre-training a single-modality model is resource-consuming and expensive, 
the cost of end-to-end vision-and-language pre-training has become increasingly prohibitive. 
		 	 	 		
[BLIP-2](https://arxiv.org/pdf/2301.12597.pdf) tackles this challenge by introducing a new visual-language pre-training paradigm that can potentially leverage 
any combination of pre-trained vision encoder and LLM without having to pre-train the whole architecture end to end. 
This enables achieving state-of-the-art results on multiple visual-language tasks while significantly reducing the number 
of trainable parameters and pre-training costs. Moreover, this approach paves the way for a multimodal ChatGPT-like model.  

## What's under the hood in BLIP-2?
 
Here is the question: What distinguishes ViLT's architecture from BLIP-2 in vision-and-language tasks?","According to the context, ViLT's architecture is distinguished from BLIP-2's in vision-and-language tasks in that ViLT treats Visual Question Answering (VQA) as a **classification problem**, whereas BLIP-2 and other recent models treat VQA as a **generative task**."
How does the Graphcore/vqa dataset contribute to fine-tuning ViLT?,"[(10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (12826, 'c27aa5bf-57b7-4cb2-b181-d66524fd3593'), (2590, '2c85c633-2192-425e-abdf-6b1a1bd79a80'), (12832, 'c5fd30d3-bda8-4d0d-a745-75e3bc790cc7'), (13347, '47eaaf44-eb91-4e4d-973f-ce92e5f0eaa6'), (5176, '1137aae3-ef53-4023-a708-cdf71049a9df'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (4672, 'd99e5f71-aff4-469d-9295-e833c20216b4'), (4673, 'e0a6bdba-540b-4180-8f10-9d0e8e339908'), (5194, '56bcbd85-b877-4ac3-8e02-bc68d468ac06'), (4683, '7c1ca1c0-b096-4cdc-8e23-344f2a8241e1'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (5199, '586581ab-686e-424f-82c7-ac8a543730cf'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (4687, '504ece2c-4a64-4dad-8929-ea172c5502f8'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (22106, 'cfba120c-8c04-42a7-ae63-5d6e2d0cb3d7'), (10853, 'cc7466fe-c647-4c7e-a1ce-4288a078ba55'), (11896, '7350ccfc-c9c3-460d-a26a-c18c1855058c'), (1663, '323fa14b-b505-4dea-ac38-9c7133039a48'), (1674, '74a86d67-67a1-45ae-a1a2-fe0e3d0628ee'), (3211, '66850aca-1339-44e1-b6ee-3612c185f4d3'), (6802, '816d52f3-ceec-436a-b89e-8e94e93f67cd'), (3731, '8bd6343e-272e-4480-9171-ac5ac043ab85'), (1684, 'ca34db9e-f05a-47f7-94cc-563ce814ee3f'), (10909, '59ba860f-e676-4510-9d72-0adcaec2874f'), (15015, '1eff8140-5e27-4af4-b283-b0c9373c28c0'), (16042, '5586d49c-f128-434d-ad84-e232ca9cda2e'), (7351, '974ac7b4-c4c2-4761-8654-0816e4e2c955'), (23737, '11452b2c-8fcc-4643-a576-672d0c14b723'), (23738, '37f3a8c8-36fe-4d79-a1bd-bcf96ff1cd69'), (18623, '015910db-fd0a-4c1f-8420-c137cb6a5a86'), (23743, '1e8468a0-3b9a-43dd-8d98-034bd897f4b4'), (709, '68b25497-c5e6-494d-aebf-9f31f7c98cc0'), (716, '3ac83e0f-ae99-4341-a5c8-b350a45098ad'), (16589, 'cff8d8f8-ad97-404a-b367-e5bd7a469024'), (7389, 'b481fda8-b5de-4d04-8ec6-e36a9f1f4039'), (17635, 'cb4c9589-721d-4162-839c-0a37be083184'), (17642, '65fcc7ea-697c-444e-8b1c-9b81f01558fe'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (8432, '128c1a7c-3f4d-481a-9a0c-f20ba007042e'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (12049, '9ebd4171-a0ce-40d1-9883-c37ead9490d1'), (5908, '144a7b30-6985-4f7d-adc3-de47096fc6ab'), (8472, '1198c94a-3f34-4223-9da9-02f2c1303a9d'), (25882, 'd29c1150-e982-47d6-af8d-81cbe1722e94'), (3866, '45636a2d-f101-4a6e-83b0-d6ac592363f7'), (22304, '3897446b-d474-4a3d-9db5-5b91924cce5e'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (17715, '766b2e00-9291-4fdf-9f87-ce5ad820f681'), (4916, 'eef334ce-c96c-41d6-8320-5bb2ac306f38'), (4917, '7631ff00-5ae6-4fc2-86ed-2b476609479e'), (23867, 'd49bd9cf-b6a6-4207-9330-94e086e9aae5'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (4419, '54f34e71-e4b0-49b3-9756-3378458d26e3'), (4420, 'acb220f7-87c0-4af4-883e-44df58ee00fb'), (4422, 'cae3f2e8-d9f5-411f-a4bf-e75aff751dec'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (30536, 'ac4762f5-bb1f-490c-9b95-411657239357'), (4424, '58841961-84a6-4778-91f2-533dd14faacc'), (30538, 'e42324e4-1f5d-463e-90d7-348b739ff4ad'), (9563, '211904bc-1535-4038-9e7d-c897425b8bcc'), (9567, '21f12568-4fc0-4680-b763-b7907ea0ea81'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (4967, '43778b66-3e71-4fd3-a9bf-f2cf850f713d'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (30573, '5f37ae20-3ad7-45ba-9cc0-a1ad112ed65c'), (23919, '0c05c74a-e41d-471c-8a4e-48d652e115c3'), (17776, '12a717db-67d1-4c7e-945c-aeb888ef89ed'), (30575, 'cfbeccde-beb7-4f12-b6f9-5de18b092d57'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (5526, 'bc09ea40-d0a6-4fc4-9d02-df468ff6e9a8'), (5534, '3e4707b0-83e3-4137-b771-207785927e14'), (9641, '030d89d3-682d-4e99-9d75-9d5d7363d428'), (23466, '23f64c4f-e35a-45d8-adfc-44266623bd06'), (18858, '928cf327-06d1-420b-afd4-7b9a60d3d9bb'), (30133, '61f3f5a8-2c83-44a2-9a84-594c52ec5c89'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (1468, 'ef87b7f7-d9df-4073-95cf-1be9e04315eb'), (17853, '4fd84f89-35e8-4729-94e6-2866546990f1'), (20414, '82e69d7e-8b42-44c4-bbb6-a789b57fdb21'), (20415, '0ff6dba8-cfe6-4157-a337-93d21a8a4a9e'), (21951, '358a38e1-9865-45a7-bb11-f4d56f194e8e'), (20417, 'ae76dfd9-ef13-41cb-873e-e4742e5c42b9'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (29634, 'd756cc17-3381-4c73-99de-56f9d281f772'), (20421, 'eb24fc1a-2495-4441-8517-8e5e570b1f9a'), (2502, '8e358fab-31fa-4f90-b90d-a95834e06f52'), (25034, '6df4e515-fab2-4473-8dc2-849bad23124e'), (20427, '281c95fc-4190-4f75-9dca-de3adf69a009'), (25036, 'fda252b8-677d-44ec-b58c-1d82a5fd42c0'), (25035, '389af8a8-4399-4266-b669-8f3adca11512'), (20436, '1f921bb5-c487-466c-ad55-eb1c190f9df8'), (26072, '78de0a59-1d3e-48f3-a2ef-a0dd438d4d8d'), (29149, 'da3811b9-e3d4-4955-80f4-1f74e90996bf'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (20449, '00a22323-79c9-407d-87a0-c90048cc2a5d'), (20456, '2fea8919-613c-4a13-9948-1cf76918fb86'), (1514, 'd61b2641-9e09-4c4e-8e1e-72599ec17422'), (15855, '59fd244c-a8fa-4c7c-a422-3f51103f15c1')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
Models fine-tuned on the question-answering downstream task, such as [ViLT](https://arxiv.org/abs/2102.03334) and [GLIP](https://arxiv.org/abs/2112.03857), most commonly use the [VQA](https://visualqa.org/) (visual question-answering), [VQA v2](https://visualqa.org/), [NLVR2](https://lil.nlp.cornell.edu/nlvr/), [OKVQA](https://okvqa.allenai.org/), [TextVQA](https://huggingface.co/datasets/textvqa), [TextCaps](https://textvqa.org/textcaps/) and [VizWiz](https://vizwiz.org/) datasets. These datasets typically contain images paired with multiple open-ended questions and answers. Furthermore, datasets such as VizWiz and TextCaps can also be used for image segmentation and object localization downstream tasks. Some other interesting multi-modal downstream datasets are [Hateful Memes](https://huggingface.co/datasets/limjiayi/hateful_memes_expanded) for multi-modal classification, [SNLI-VE](https://github.com/necla-ml/SNLI-VE) for visual entailment prediction, and
<h2>ViT models – a perfect fit for IPU</h2>
<p>Graphcore IPUs are particularly well-suited to ViT models due to their ability to parallelise training using a combination of data pipelining and model parallelism. Accelerating this massively parallel process is made possible through IPU’s MIMD architecture and its scale-out solution centred on the IPU-Fabric.</p>
<p>By introducing pipeline parallelism, the batch size that can be processed per instance of data parallelism is increased, the access efficiency of the memory area handled by one IPU is improved, and the communication time of parameter aggregation for data parallel learning is reduced.</p>
<p>Thanks to the addition of a range of pre-optimized transformer models to the open-source Hugging Face Optimum Graphcore library, it’s incredibly easy to achieve a high degree of performance and efficiency when running and fine-tuning models such as ViT on IPUs.</p>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/6d2e26d5c1ad3df6ba966567086f8413.js""></script>
</div>
<p>We are going to import the ViT model from the checkpoint <code>google/vit-base-patch16-224-in21k</code>. The checkpoint is a standard model hosted by Hugging Face and is not managed by Graphcore.</p>
<div style=""font-size: 14px; line-height: 1.3;"">
<script src=""https://gist.github.com/nickmaxfield/1df44cf80f72e1132441e539e3c3df84.js""></script>
</div>
<p>To fine-tune a pre-trained model, the new dataset must have the same properties as the original dataset used for pre-training. In Hugging Face, the original dataset information is provided in a config file loaded using the <code>AutoImageProcessor</code>. For this model, the X-ray images are resized to the correct resolution (224x224), converted from grayscale to RGB, and normalized across the RGB channels with a mean (0.5, 0.5, 0.5) and a standard deviation (0.5, 0.5, 0.5).</p>
- The quickest way to get started with ViLT is by checking the [example notebooks](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ViLT)
  (which showcase both inference and fine-tuning on custom data).
- ViLT is a model that takes both `pixel_values` and `input_ids` as input. One can use [`ViltProcessor`] to prepare data for the model.
  This processor wraps a image processor (for the image modality) and a tokenizer (for the language modality) into one.
- ViLT is trained with images of various sizes: the authors resize the shorter edge of input images to 384 and limit the longer edge to
  under 640 while preserving the aspect ratio. To make batching of images possible, the authors use a `pixel_mask` that indicates
  which pixel values are real and which are padding. [`ViltProcessor`] automatically creates this for you.
- The design of ViLT is very similar to that of a standard Vision Transformer (ViT). The only difference is that the model includes
 
Here is the question: How does the Graphcore/vqa dataset contribute to fine-tuning ViLT?","The Graphcore/vqa dataset is used for fine-tuning the ViLT model. The dataset contains images paired with multiple open-ended questions and answers, which is suitable for the Visual Question Answering (VQA) task. By fine-tuning the ViLT model on this dataset, the model can learn to perform well on the VQA task, which is treated as a classification problem."
What preprocessing step is shared by both ViLT and BLIP-2 workflows?,"[(21005, 'cea02648-cefe-4c6b-ad57-d7800ee6227a'), (14365, 'eb13b721-66b5-420d-a9c8-b21fcc847e5f'), (30749, '1f54578e-bf90-4efe-b3d3-48ac13a8a2f9'), (14368, 'cac9221b-fad8-496d-9f30-a16d32005bbb'), (14374, '21bf672e-85a7-436e-9568-eeb113971822'), (28717, '45cc6367-7b38-4f30-a36f-c96fb2293202'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (14382, 'f0a3cc11-b2fc-4b21-a76e-c0d46a55834a'), (23597, '846f143a-3fb9-4502-9a7f-24a67f595a33'), (14385, '8d5ae057-0f6b-4488-8b86-a637dee0e0f9'), (22072, 'b23120bf-9578-40a1-9bc3-971a017e5c97'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (31293, 'bf6fa69e-7b65-4651-84a6-7287755969d5'), (1601, '06cbfead-420b-465d-a07c-566ed263f552'), (28739, '50948ec6-1039-4799-9d5c-f657b72f8d39'), (4679, '1c426ec4-266d-40f4-b280-093732557e19'), (4680, 'f1b71842-c2a8-4de7-b6d6-b9115f88900c'), (5704, 'f5a76ce8-55f0-4aa3-9058-b471d25fed91'), (4682, '8a06f5b7-f648-47b4-ad91-52a4cc50ac01'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (4686, '59de41e9-0c89-49cd-9d19-e7b1fabd0bc4'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (4689, 'b6be4a48-34b7-44fb-953d-92422d800636'), (4690, '73b0a8e6-ac3c-4071-b88d-beb7478681aa'), (17489, '4e5aff90-ad96-4464-b275-e780dde9605a'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (10853, 'cc7466fe-c647-4c7e-a1ce-4288a078ba55'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (1663, '323fa14b-b505-4dea-ac38-9c7133039a48'), (7815, 'ffedd8d3-edcc-43cc-9cec-a604d063dc3e'), (8331, '8128bb30-0280-49d8-bbb8-05845e8d1450'), (29841, '605e3205-172d-45de-8e04-c57e02e5d2b0'), (17047, 'f995dc4c-c26e-49ee-b3e9-8dc4d0563979'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (25260, '1259a78b-8351-4c46-9c62-21b1f1f5268f'), (688, 'dd7ba60b-ae7e-4773-acba-8c59fe920da9'), (690, 'b6bf8779-7ba4-44e2-956d-16eaea7da823'), (28341, '0b2405c2-12a4-40dd-acb6-f0bc3183d083'), (5308, '0448ee11-7a7f-46fc-b497-22e65c8e98f4'), (5309, '1063e5a8-266b-4339-bd57-f04a35f278f7'), (6338, 'f8556800-ba9c-4c54-b96e-56d695a0da64'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (29385, 'c938e65b-f31c-4815-b149-cd390796ee02'), (20681, 'b77beac2-80af-4a70-b49f-7a58c98e1a86'), (23753, '6939167e-707a-4fbe-8d61-492be49ed360'), (17614, 'a4127615-83f7-416f-8953-35d7a99ae728'), (20686, 'ab4b1ce0-19cb-495e-af96-5a71dbdbb1b4'), (17616, 'a59fd552-bc52-4d0c-9aba-aefd3b3839f5'), (28881, '8241c389-b959-4e5f-b465-412ae490c9f0'), (1745, 'cd32a895-3ae9-4f82-8574-44ad1d9286e8'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (28883, '7c053a1e-d815-49e8-b352-b637e16a3084'), (28890, 'b1ae8ca7-d69b-4326-a81e-655300398e52'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (8432, '128c1a7c-3f4d-481a-9a0c-f20ba007042e'), (3828, 'cc724033-54f2-492d-98b4-a6313303b04c'), (3829, 'a49bb9d9-ee4c-4e24-918b-fa5755dc6d28'), (29442, 'a2fb8394-94b3-447e-a512-9461a99a583e'), (25862, '79a6e398-efad-4052-aacc-dabd174c8793'), (29448, 'de680f2b-2364-4825-bd55-d130bdbe185f'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (5908, '144a7b30-6985-4f7d-adc3-de47096fc6ab'), (29457, '0a4a2d1d-c69f-448e-a4e8-9d1337382099'), (24341, 'fca4a714-785a-4515-adb7-310dc1177322'), (30992, '2403c4ec-eab0-4fa0-877a-b63257378b65'), (8472, '1198c94a-3f34-4223-9da9-02f2c1303a9d'), (4375, '65f6412b-c51e-4375-af24-295af347283d'), (17208, '2203f22a-0a57-4589-bd1b-a9d45aa91f1f'), (28473, 'e49da37f-50e8-4501-9439-6d0940c836bf'), (17209, '637b92d4-eb98-4446-b500-74fe9871a4dd'), (26426, '613fabb3-d6a2-48f0-b6f1-a63e4b3230af'), (17212, '3639fc6d-edf2-464b-b1f8-6a8147fa1270'), (22328, '3eb494fc-8503-4a8f-8424-ee15b59d0d68'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (11078, '32c306bd-f0ba-4028-8b2e-a210f0b4d55a'), (4425, '23d017d1-701c-4fe6-82c4-6939b57cb62e'), (7015, '929caa94-db70-4c90-85ca-46a52fdf2f65'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (31598, 'd3e6a577-47b5-4ed6-83a7-e22fb2d3c6c5'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (25972, '72d72947-f0cb-441d-aec1-2abff09ba3e3'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (20852, 'b516f3fa-d794-46a2-a94e-d391909f98d9'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (24965, 'b53de7cb-a469-4ab7-8a1a-6a360e08c13f'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (16267, '7ac5f372-d956-427b-832d-160d6a912937'), (15763, '403af043-7233-4cf7-8c33-047381da9a66'), (8093, 'a4fa8244-060e-44ef-a4fe-f7fe4b3c631d'), (15266, '616f3ab2-a1e6-46fc-90d9-4a9c2d47fcb8'), (24484, '988913b9-fdd8-4eac-81ab-b81d33deb3fc'), (26541, 'bf53285c-42c7-4678-b22d-2dfb988d32b9'), (26542, '55477efb-32ff-4ee8-9b02-dd83387411e5'), (26548, '91630b7e-ea5a-49c4-9f1d-3bc6a002ffc3'), (30134, 'ce26a20b-abea-49a7-a5fb-340df7e2f10d'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (17850, '655ea1af-498e-427a-898b-785921f04ee1'), (12250, '93403956-c604-496f-bf51-91d1d6680dd8'), (17379, '87a5fc61-b850-44b3-864f-1bb388fdbfd1'), (11236, 'ddb46a9b-1eac-418c-b631-5cfda6f4101b'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b'), (12777, '080cfea0-04ef-4ef8-9058-41551b00ab28')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

Next, we will download a random image of two cats and preprocess both the image and our  query question to transform them to the input format expected by the model. To do this, we can conveniently use the corresponding preprocessor class (`ViltProcessor`) and initialize it with the preprocessing configuration of the corresponding checkpoint. 

```py
import requests
from PIL import Image

processor = ViltProcessor.from_pretrained(""dandelin/vilt-b32-finetuned-vqa"")

# download an input image
url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
image = Image.open(requests.get(url, stream=True).raw)
text = ""How many cats are there?""

# prepare inputs
inputs = processor(image, text, return_tensors=""pt"")
```

To preprocess the data we need to encode the images and questions using the [`ViltProcessor`]. The processor will use 
the [`BertTokenizerFast`] to tokenize the text and create `input_ids`, `attention_mask` and `token_type_ids` for the text data. 
As for images, the processor will leverage [`ViltImageProcessor`] to resize and normalize the image, and create `pixel_values` and `pixel_mask`.

All these preprocessing steps are done under the hood, we only need to call the `processor`. However, we still need to 
prepare the target labels. In this representation, each element corresponds to a possible answer (label). For correct answers, the element holds 
their respective score (weight), while the remaining elements are set to zero.

The following function applies the `processor` to the images and questions and formats the labels as described above:

```py
>>> import torch
## What's under the hood in BLIP-2?

BLIP-2 bridges the modality gap between vision and language models by adding a lightweight Querying Transformer (Q-Former) 
between an off-the-shelf frozen pre-trained image encoder and a frozen large language model. Q-Former is the only 
trainable part of BLIP-2; both the image encoder and language model remain frozen. 

<p align=""center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/blip-2/q-former-1.png"" alt=""Overview of BLIP-2's framework"" width=500>
</p>

Q-Former is a transformer model that consists of two submodules that share the same self-attention layers: 
* an image transformer that interacts with the frozen image encoder for visual feature extraction 
* a text transformer that can function as both a text encoder and a text decoder
```

The model takes image and text as input, so let's use the exact same image/question pair from the first example in the VQA dataset: 

```py 
>>> example = dataset[0]
>>> image = Image.open(example['image_id'])
>>> question = example['question']
```

To use BLIP-2 for visual question answering task, the textual prompt has to follow a specific format: `Question: {} Answer:`.

```py
>>> prompt = f""Question: {question} Answer:"" 
```

Now we need to preprocess the image/prompt with the model's processor, pass the processed input through the model, and decode the output:

```py
>>> inputs = processor(image, text=prompt, return_tensors=""pt"").to(device, torch.float16)

>>> generated_ids = model.generate(**inputs, max_new_tokens=10)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
""He is looking at the crowd""
```

As we saw in [Chapter 1](/course/chapter1), this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline.svg"" alt=""The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg"" alt=""The full NLP pipeline: tokenization of text, conversion to IDs, and inference through the Transformer model and the model head.""/>
</div>

Let's quickly go over each of these.

## Preprocessing with a tokenizer[[preprocessing-with-a-tokenizer]]
 
Here is the question: What preprocessing step is shared by both ViLT and BLIP-2 workflows?","The preprocessing step shared by both ViLT and BLIP-2 workflows is the use of a tokenizer (specifically, `BertTokenizerFast`) to tokenize the text and create `input_ids`, `attention_mask`, and `token_type_ids` for the text data."
What is a shared advantage of using Hugging Face pipelines for both ViLT and BLIP-2?,"[(22023, '53b70cc1-249b-499d-9528-36572dcd69b3'), (18443, '00876f43-8b96-47cb-ba09-71690498db3d'), (5141, 'aa79a23d-5dce-4f0c-905f-502f514390e9'), (22037, '01e8996d-daff-47eb-b24c-ae98ba9c4de9'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (23573, '952579f3-7c93-46c5-ae22-ea5e5fd0b30a'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (28713, '0f3cbda1-fa85-468b-b581-01fd57df81ca'), (28716, '0e901ab3-56fc-46c6-a3c7-adee2baa6fca'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (20576, '40796d31-f95c-4e9d-a36b-2d6319601be0'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10853, 'cc7466fe-c647-4c7e-a1ce-4288a078ba55'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (2683, 'cc452464-9782-41f3-bb75-7b45461da969'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (3211, '66850aca-1339-44e1-b6ee-3612c185f4d3'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (9410, '9b7d8254-2d89-4787-a2de-21068114b4d5'), (20678, 'cd435f59-99e9-46f5-b2a4-b7a29bb29874'), (20167, '2bb84049-c72c-4886-948b-e9b8893655fb'), (20168, '183870fc-3f67-4f8e-82c9-97994e1b5ac7'), (17609, '4cdc2094-1a54-40b7-9a6e-05ea32af5ee7'), (1745, 'cd32a895-3ae9-4f82-8574-44ad1d9286e8'), (21203, '5f38e41b-22a4-4ec8-b993-4864b7079429'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (10461, '791d16b9-ac09-45b6-a426-0eea3d3b4063'), (225, '3ad317d7-101b-4379-9a04-92d617ddfd14'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (13575, '849335ec-ba91-41e8-9ac2-ab98d4ffd804'), (17160, '91bdea0b-d26a-4435-9b08-9f133f7cad52'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (18213, '076f6581-4145-4ca5-93e8-9e744b0574a7'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (5933, 'e8977096-c2bb-4f13-9f6e-6bfd1dbeb9bf'), (5942, '49c6ab8f-8f99-4a48-a3e8-a1c9132a3ecf'), (27451, 'aebb7e3e-e5ab-449c-915d-bb4210436786'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (4413, '99b25651-ea69-45df-a3a6-e1d6b63fccf8'), (13630, '5f595309-fb4e-4261-92ad-938f4eb058f2'), (4416, '1dcbdce0-3702-4cb7-87bd-33fa2fe556a7'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (329, 'e5a52187-6e6d-47ba-bffa-cde854b2df46'), (4424, '58841961-84a6-4778-91f2-533dd14faacc'), (16714, '0b7b6890-9874-4128-bffd-c02f8855c653'), (16716, 'd9f215fb-7aa2-4b50-a40d-d2e690dc5d03'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (9567, '21f12568-4fc0-4680-b763-b7907ea0ea81'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (25967, '3787a06f-6972-47b9-949a-cf92bd938b60'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (25970, 'e5880a95-5eb3-45bf-8beb-56e91bd452fc'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (25972, '72d72947-f0cb-441d-aec1-2abff09ba3e3'), (16760, 'd936a97e-a944-4271-bca8-ab614fbc7ca8'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (19857, '894d909b-b9a8-4c18-8789-710e8a2fd282'), (10133, '7735b57a-5b30-4c4f-9ab3-c1e84672a8be'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (6038, '4398309e-75d4-425a-b50a-40a436da5bd0'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (27043, '35b67fa4-5368-4414-b989-f368ae9e7668'), (17828, 'e0e24538-49e6-418a-8fb8-b10e6a8d00f1'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9150, 'de3eb793-c6de-4fd2-9f8c-1b0b686cf6d1'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (11711, 'f0aab09f-a3dc-496a-9cd8-08e8d39a1306'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (21447, '8b7f3a13-78d2-4c1a-b32a-51b280baae78'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (11235, '8558ce8a-5be5-4c58-91a4-e034fd8f061d'), (13801, 'aa3ed41c-4b41-428a-a670-fc2e12d17add'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (20990, '990fd7c0-e122-4862-9964-33bff049fafc'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In the second pre-training stage, the query embeddings now have the relevant visual information to the text as it has 
passed through an information bottleneck. These embeddings are now used as a visual prefix to the input to the LLM. This 
pre-training phase effectively involves an image-ground text generation task using the causal LM loss. 

As a visual encoder, BLIP-2 uses ViT, and for an LLM, the paper authors used OPT and Flan T5 models. You can find 
pre-trained checkpoints for both OPT and Flan T5 on [Hugging Face Hub](https://huggingface.co/models?other=blip-2). 
However, as mentioned before, the introduced pre-training approach allows combining any visual backbone with any LLM.

## Using BLIP-2 with Hugging Face Transformers

Using Hugging Face Transformers, you can easily download and run a pre-trained BLIP-2 model on your images. Make sure to use a GPU environment with high RAM if you'd like to follow along with the examples in this blog post.
## 2. New Optimum inference and pipeline features

With [release](https://github.com/huggingface/optimum/releases/tag/v1.2.0) of Optimum 1.2, we are adding support for [inference](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling_ort) and [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines). This allows Optimum users to leverage the same API they are used to from transformers with the power of accelerated runtimes, like [ONNX Runtime](https://onnxruntime.ai/).

**Switching from Transformers to Optimum Inference**
The [Optimum Inference models](https://huggingface.co/docs/optimum/main/en/onnxruntime/modeling_ort) are API compatible with Hugging Face Transformers models. This means you can just replace your `AutoModelForXxx` class with the corresponding `ORTModelForXxx` class in Optimum. For example, this is how you can use a question answering model in Optimum:
Let's get started! 🚀

## 1. What is Optimum? An ELI5

[Hugging Face Optimum](https://github.com/huggingface/optimum) is an open-source library and an extension of [Hugging Face Transformers](https://github.com/huggingface/transformers), that provides a unified API of performance optimization tools to achieve maximum efficiency to train and run models on accelerated hardware, including toolkits for optimized performance on [Graphcore IPU](https://github.com/huggingface/optimum-graphcore) and [Habana Gaudi](https://github.com/huggingface/optimum-habana). Optimum can be used for accelerated training, quantization, graph optimization, and now inference as well with support for [transformers pipelines](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#pipelines).

## 2. New Optimum inference and pipeline features
Hugging Face Infinity is a containerized solution for customers to deploy end-to-end optimized inference pipelines for State-of-the-Art Transformer models, on any infrastructure.

Hugging Face Infinity consists of 2 main services:
* The Infinity Container is a hardware-optimized inference solution delivered as a Docker container.
* Infinity Multiverse is a Model Optimization Service through which a Hugging Face Transformer model is optimized for the Target Hardware. Infinity Multiverse is compatible with Infinity Container.

The Infinity Container is built specifically to run on a Target Hardware architecture and exposes an HTTP /predict endpoint to run inference.

<br>
<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""Product overview"" src=""assets/46_infinity_cpu_performance/overview.png""></medium-zoom>
  <figcaption>Figure 1. Infinity Overview</figcaption>
</figure>
<br>
Hugging Face Infinity can deliver up to 800% higher throughput compared to vanilla transformers, and down to 1-4ms latency for sequence lengths up to 64 tokens.

The flexibility to optimize transformer models for throughput, latency, or both enables businesses to either reduce the amount of infrastructure cost for the same workload or to enable real-time use cases that were not possible before. 

If you are interested in trying out Hugging Face Infinity sign up for your trial at [hf.co/infinity-trial](https://hf.co/infinity-trial) 


## Resources
 
Here is the question: What is a shared advantage of using Hugging Face pipelines for both ViLT and BLIP-2?","The shared advantage of using Hugging Face pipelines for both ViLT and BLIP-2 is not explicitly mentioned in the context. However, based on the information provided, it can be inferred that using Hugging Face pipelines allows for easy integration and compatibility with various models, including ViLT and BLIP-2, and provides a unified API for inference and optimization."
What common feature enhances the interoperability of Hugging Face models?,"[(28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (3594, '4e1600b8-d386-41a9-bc22-67a03574e9d3'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (29235, '28f83b83-b564-4757-a1e4-3c0a127d21bd'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (4192, 'fc859fbf-f289-4177-a3a6-21b11cb22ad3'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (4237, 'aa571318-cb21-42dc-96d5-9247c29a9ac5'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (24804, '23cde3d9-30d2-4fd8-84f5-8504a15e3a42'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (7412, '895fce2b-48bf-4d3a-943b-5eb38a23099f'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (6039, '06aac366-32af-4336-abf4-6546946a9043'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (6079, '2234d568-f21c-4b71-b407-a24d00ca3c01'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

We're particularly fond of this upload feature because it encourages collaboration. People can annotate their own datasets independently of each other, but still benefit when they share the data with the wider community. 

## More to come

We hope that this direct integration with the Hugging Face ecosystem enables many users to experiment more. The Hugging Face Hub offers _many_ [models](https://huggingface.co/models) for a wide array of tasks as well as a wide array of languages. We really hope that this integration makes it easier to get data annotated, even if you've got a more domain specific and experimental use-case. 

More features for this library are on their way, and feel free to reach out on the [Prodigy forum](https://support.prodi.gy/) if you have more questions. 

We'd also like to thank the team over at Hugging Face for their feedback on this plugin, specifically @davanstrien, who suggested to add the upload feature. Thanks!
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft’s ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face’s [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
Using Hugging Face Integrations

Related spaces: https://huggingface.co/spaces/gradio/en2es
Tags: HUB, SPACES, EMBED

Contributed by <a href=""https://huggingface.co/osanseviero"">Omar Sanseviero</a> 🦙

## Introduction

The Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). 

Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API
_Q: What are you planning next through this partnership?_

A: Our common goal is to democratize state of the art Machine Learning. We will continue to innovate to make it easier for researchers, data scientists and ML practitioners to manage, train and run state of the art models. If you have feature requests for integration in AWS with Hugging Face, please [let us know in the Hugging Face community forum](https://discuss.huggingface.co/c/sagemaker/17).

_Q: I use Hugging Face with Azure Machine Learning or Google Cloud Platform, what does this partnership mean for me?_

A: A foundational goal for Hugging Face is to make the latest AI accessible to as many people as possible, whichever framework or development environment they work in. While we are focusing integration efforts with Amazon Web Services as our Preferred Cloud Provider, we will continue to work hard to serve all Hugging Face users and customers, no matter what compute environment they run on.
 
Here is the question: What common feature enhances the interoperability of Hugging Face models?","The common feature that enhances the interoperability of Hugging Face models is the ONNX Runtime, which allows for easier and faster training of models, and improves training times by 35% or more for many popular Hugging Face models."
How does BLIP-2 handle open-ended questions in VQA tasks differently from ViLT?,"[(10243, '3f0953d6-0ddf-455f-81a5-730727c169b6'), (19467, '8be7a151-f10b-4b06-9623-13ac201be523'), (19470, '47d1e32f-3c4a-4eda-825c-86db1fd61405'), (13840, '840724b4-9ff9-4305-a2e7-a560e78f15f0'), (25105, '421712a8-0f69-436b-b77b-73a56f5dce9f'), (14883, '941dfd94-da40-40b4-918c-65e533145721'), (14886, 'c48286c5-a1b4-43f0-ab50-f7520594909d'), (14887, '0e62ab72-eb46-45aa-8675-b7105bea8c54'), (24123, 'c45de4e2-767b-4e61-868c-98959a2a5f82'), (4668, 'eed05ef4-ac04-44b6-988c-170ffcb080c7'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (31293, 'bf6fa69e-7b65-4651-84a6-7287755969d5'), (4672, 'd99e5f71-aff4-469d-9295-e833c20216b4'), (4667, 'ea371344-8b31-4041-8d3f-3a2d817bd4bb'), (20030, '4689b0fe-4c93-4cf7-b0b9-1eaad09aec9e'), (28739, '50948ec6-1039-4799-9d5c-f657b72f8d39'), (4686, '59de41e9-0c89-49cd-9d19-e7b1fabd0bc4'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (4688, '2b532534-b87b-46ba-b0b7-f593336f8f3e'), (4690, '73b0a8e6-ac3c-4071-b88d-beb7478681aa'), (2642, '7b6b7476-f936-464b-a475-6d7d874b352a'), (21088, 'fb49fb03-082d-499d-ae0d-08b8fa2132f0'), (13927, '38fd6452-7b8a-4923-aeb8-b117f1211ee6'), (25197, '11048c76-26fa-4eca-b2eb-952cd5d06e3f'), (23161, 'ee70a11e-6b86-48eb-b666-f874e0af629e'), (23163, '5605f1e6-c255-4477-8a7a-45e22ba1e880'), (24707, 'b83268da-8092-42eb-96fd-1c3b065d1862'), (24710, 'd041d383-112f-4846-a373-a1be8cdb7919'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (8345, '5d771c1f-4f18-40c5-aeb8-f6bfd3b92680'), (667, '466a4c47-d6a2-4439-8124-9da2dea99f39'), (23715, 'bd16b50e-dd2e-4311-abb7-001c3b52c2ef'), (164, 'bc1dd7b2-fcc1-4dd8-b101-1b7d33afde31'), (25263, 'd7a4626e-6223-485c-95a7-4e2cd4cb33b1'), (4272, 'f95f512b-761c-44b7-bf19-f0bcb5245e86'), (21172, 'b567d6c1-3d9e-4ce1-9901-482134c733a6'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (21206, '47c42ec0-09a4-4e76-8135-fece0482ac7e'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (27361, '334d3a20-8ea3-42cd-a731-147b016bd478'), (15585, '31f7ebf4-9f31-4897-9be5-0f1afccf4531'), (8933, 'c28b4054-dd64-4db4-b8d6-22cbb55b111e'), (15592, '481a6c32-3e99-4c07-a468-dbe480fef1b4'), (15593, 'd41ecdb5-1abb-4bbd-a4dd-adf3baf6ee42'), (3820, 'c5a2c723-a4ef-43d4-a878-b9673dcc57f0'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (1278, 'bf52ec8c-62d2-41da-93c1-65044979f309'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (29446, '56256f45-cc5f-4fe7-9631-d64b1173a4f6'), (3335, '76db0f2f-35d9-4135-bace-d0df8958b946'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (29455, 'e3a1b180-f8b4-43f1-ab4e-8b05a6cbdce3'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (29456, 'e7b05b8f-c244-4018-ac23-ea37f634fffd'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (5908, '144a7b30-6985-4f7d-adc3-de47096fc6ab'), (3861, 'b29474d6-97e9-4a9a-87ad-fa2fb569cd9f'), (5919, 'bef2fd9d-5d6e-4d07-a8fe-1fc9829a8f50'), (20774, 'ad1a0664-122f-404f-99e9-817b2bd2d8f0'), (25383, 'def95823-c6f2-4c37-bff5-dc0f8fd2a2d4'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (4398, 'd0b5ead4-f8fd-4d2d-bded-79b027404dff'), (2866, '1190142f-1663-4e32-8d95-5f3b34034875'), (9015, '6e28a1fc-953b-4f1b-8b44-61d598ff2f39'), (15167, '1b9ae140-8223-4870-bb50-abbc78ae7a0d'), (13132, '77b0e917-d732-42f7-8d43-70b0b8eeeff0'), (17742, 'dd667567-f375-4a71-b695-60d6babe2ab0'), (8016, 'fdea28e4-713f-48df-af6c-164fe1b11e3e'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (11610, '7abc171e-bc54-496c-a7d6-c25500c846c1'), (20324, '2bb6a05c-4684-496c-bc1c-b3c4aac02cd3'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (25963, 'fd0bea05-61c9-4e1b-8a8b-5171642f2f0d'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25966, '15aa7397-77e8-434e-85a1-97a9194b01b1'), (24430, 'ee87514c-8adc-403a-aafa-38524ce4f7cd'), (24431, '849a4a2b-2007-4774-97c4-9702c4778a2e'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (20851, 'b8c3710a-efdb-41e6-9f78-8cb777be59d6'), (25971, '3edc1ae5-33a4-45d2-beb5-d5070fe39c09'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (24432, '65160c4e-e76b-478e-af89-46847fcd223f'), (24434, 'bb7afb46-7718-4546-95b2-cc62410b73cb'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (4986, 'd1a0c49b-c4b7-4f64-b176-5c34266e3eaa'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (25979, '8c83a906-7a5a-44b7-848c-6cf22c6ab93b'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (3455, '2df2d07e-3f99-4c2f-bac2-b2c66d9b658b'), (24960, '8e0e4635-d47a-4795-bdab-22ffe8fea5c8'), (23427, '44ebafae-3114-46b5-bbfa-3f2d81df8eb8'), (24965, 'b53de7cb-a469-4ab7-8a1a-6a360e08c13f'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (31113, 'c0a9b924-ebb3-4c34-b9e1-db430d7fd7d3'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (16267, '7ac5f372-d956-427b-832d-160d6a912937'), (12176, '0a4a4977-ff55-4caa-9ad8-d86e8642d685'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (5030, 'd65cd4d7-7315-45e1-a2d7-60c432346eb0'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (31145, '93d58b66-fb11-4396-ac56-a7b8a9f50d73'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (31149, 'debd5c00-01da-49e4-9fac-acfd4e6bcd78'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (16305, '5be483d8-14e3-4be2-bfcc-3de7b65d5883'), (16312, '4bf273cf-f946-4ef8-bd3d-a8ffcc0f5889'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (28604, '3f1941c1-775f-425b-9ca7-73abb8786647'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (2502, '8e358fab-31fa-4f90-b90d-a95834e06f52'), (2503, '2e1ff638-b3dd-42ac-9810-3e463ba23b97'), (13267, 'd14a13b5-d0f7-4354-9963-a77a57948d2c'), (7141, '4ae74680-77ed-491d-bd2f-8287b07c6919')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Models fine-tuned on the question-answering downstream task, such as [ViLT](https://arxiv.org/abs/2102.03334) and [GLIP](https://arxiv.org/abs/2112.03857), most commonly use the [VQA](https://visualqa.org/) (visual question-answering), [VQA v2](https://visualqa.org/), [NLVR2](https://lil.nlp.cornell.edu/nlvr/), [OKVQA](https://okvqa.allenai.org/), [TextVQA](https://huggingface.co/datasets/textvqa), [TextCaps](https://textvqa.org/textcaps/) and [VizWiz](https://vizwiz.org/) datasets. These datasets typically contain images paired with multiple open-ended questions and answers. Furthermore, datasets such as VizWiz and TextCaps can also be used for image segmentation and object localization downstream tasks. Some other interesting multi-modal downstream datasets are [Hateful Memes](https://huggingface.co/datasets/limjiayi/hateful_memes_expanded) for multi-modal classification, [SNLI-VE](https://github.com/necla-ml/SNLI-VE) for visual entailment prediction, and
```

## Zero-shot VQA

The previous model treated VQA as a classification task. Some recent models, such as BLIP, BLIP-2, and InstructBLIP approach 
VQA as a generative task. Let's take [BLIP-2](../model_doc/blip-2) as an example. It introduced a new visual-language pre-training 
paradigm in which any combination of pre-trained vision encoder and LLM can be used (learn more in the [BLIP-2 blog post](https://huggingface.co/blog/blip-2)). 
This enables achieving state-of-the-art results on multiple visual-language tasks including visual question answering. 

Let's illustrate how you can use this model for VQA. First, let's load the model. Here we'll explicitly send the model to a 
GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically: 

```py
>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration
>>> import torch
In this guide you'll learn how to:

- Fine-tune a classification VQA model, specifically [ViLT](../model_doc/vilt), on the [`Graphcore/vqa` dataset](https://huggingface.co/datasets/Graphcore/vqa).
- Use your fine-tuned ViLT for inference.
- Run zero-shot VQA inference with a generative model, like BLIP-2.

## Fine-tuning ViLT

ViLT model incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design for 
Vision-and-Language Pre-training (VLP). This model can be used for several downstream tasks. For the VQA task, a classifier 
head is placed on top (a linear layer on top of the final hidden state of the `[CLS]` token) and randomly initialized. 
Visual Question Answering is thus treated as a **classification problem**.

More recent models, such as BLIP, BLIP-2, and InstructBLIP, treat VQA as a generative task. Later in this guide we 
illustrate how to use them for zero-shot VQA inference.
```

The model takes image and text as input, so let's use the exact same image/question pair from the first example in the VQA dataset: 

```py 
>>> example = dataset[0]
>>> image = Image.open(example['image_id'])
>>> question = example['question']
```

To use BLIP-2 for visual question answering task, the textual prompt has to follow a specific format: `Question: {} Answer:`.

```py
>>> prompt = f""Question: {question} Answer:"" 
```

Now we need to preprocess the image/prompt with the model's processor, pass the processed input through the model, and decode the output:

```py
>>> inputs = processor(image, text=prompt, return_tensors=""pt"").to(device, torch.float16)

>>> generated_ids = model.generate(**inputs, max_new_tokens=10)
>>> generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()
>>> print(generated_text)
""He is looking at the crowd""
-->

# Visual Question Answering

[[open-in-colab]]

Visual Question Answering (VQA) is the task of answering open-ended questions based on an image. 
The input to models supporting this task is typically a combination of an image and a question, and the output is an 
answer expressed in natural language.

Some noteworthy use case examples for VQA include:
* Accessibility applications for visually impaired individuals.
* Education: posing questions about visual materials presented in lectures or textbooks. VQA can also be utilized in interactive museum exhibits or historical sites.
* Customer service and e-commerce: VQA can enhance user experience by letting users ask questions about products. 
* Image retrieval: VQA models can be used to retrieve images with specific characteristics. For example, the user can ask ""Is there a dog?"" to find all images with dogs from a set of images.

In this guide you'll learn how to:
 
Here is the question: How does BLIP-2 handle open-ended questions in VQA tasks differently from ViLT?","BLIP-2 handles open-ended questions in VQA tasks differently from ViLT in that it treats VQA as a generative task, whereas ViLT treats it as a classification task. In BLIP-2, the model generates an answer in natural language, whereas in ViLT, the model predicts an answer from a set of predefined classes."
What is the purpose of integrating text embeddings into Vision Transformers in ViLT?,"[(18444, '02cfbfe2-adf6-4e60-8ad2-490af1161135'), (5142, '72539295-6c18-4652-a932-e6e418672555'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (27688, 'a073f2c4-979d-44a2-9ead-1fd732d3029a'), (28714, '7664a905-9353-43af-a8e8-96b3a5fda730'), (28715, 'aaa48597-36d7-4f2f-8b37-6eafd713a6bf'), (28717, '45cc6367-7b38-4f30-a36f-c96fb2293202'), (22062, 'b65a5638-467c-4923-b163-9a8467ee706d'), (22063, 'b6a4b9c4-c683-4973-b991-8c2cb24c7519'), (22076, '37ef4059-7542-46a2-b62d-19a38915a327'), (4669, 'e9fb3734-49f3-459d-b0d7-16c0e9e4fa5a'), (10836, '55eaf7a1-2a0b-4c6c-bd63-01b4877236cb'), (88, '8bb2a9ef-a9ec-4471-b9ff-467c7e00b9ad'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (7810, '6464d47d-5a3f-4e13-b8a1-5d521cf34e11'), (15493, '781a0c11-3aee-4345-8769-4aa732ef87e3'), (15494, '440e0c1d-610b-4e25-b89b-6843bea20636'), (8330, 'b2bd4024-ac0b-42c0-9840-c85ddda39140'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (20651, '9c5a8380-0788-4e02-96cc-ae00c3f65050'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (20657, '7effe90b-a405-4e14-866c-174351ec7edf'), (3791, '57dded52-a5aa-4446-ab27-22e93dff8a17'), (1746, '49e8dce4-afe5-4268-9ab9-6f9809115e1e'), (3795, '177a5242-8be1-496a-8e8f-b2ade38bc78e'), (3796, '23e48029-5d36-4631-8a93-df271cc3072e'), (3798, 'a7c1c130-12a3-42f5-9adb-9fba72949e4e'), (3799, '1651e17d-3706-47fa-9cf4-a9f8a9ebb754'), (3802, '08d3554b-1b6c-4900-9e55-360d229eaf83'), (3809, 'df66e430-7b51-4327-885a-a5c4dba61637'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (24298, '1aa312da-1a6c-44c4-9f8e-72755f23c204'), (23787, '5bf0ef82-1e8c-479a-9986-c855338abeb1'), (6893, '3e3a63e2-90f5-4d24-8729-3673b42d713d'), (3822, 'b9bc39ef-056a-47c1-9e4b-02df6b6d4287'), (3824, 'ce8759e6-19bc-487f-a4be-f1afcd370909'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (3827, 'fe2a1935-1ae8-467c-85b8-dfb7bd441b10'), (4851, '7e0cad1c-05a0-4c88-b14a-c88e02040671'), (4852, '44fd3946-6b3b-4d71-a95c-1553a282aa4b'), (4853, '5364e758-db68-4033-81d4-35723ac64959'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (2811, '7a203781-c1a2-43a6-ad0b-ce893ad7bd5b'), (6909, '84556055-9fa1-4cff-bfe5-073bcd5b7f97'), (3837, '2b3f9754-3c83-4140-8261-a686b11fd43d'), (2815, '76b113fa-b31a-4fa1-932e-6496981da99a'), (27392, '929fcc41-fb2f-4210-bb6c-0f21445e2323'), (27394, '87641ab3-130f-4ecc-a519-c0a9818906ef'), (13575, '849335ec-ba91-41e8-9ac2-ab98d4ffd804'), (12045, '7efab0f1-50ed-4755-813f-824ba7849ab5'), (5902, 'dd4587f6-3961-4ab2-94c0-8eb6dd41a1b9'), (5903, '71370419-6899-4182-bf83-7edac1974281'), (5904, 'ba8e3ec7-e61a-4396-ba25-3150607d5122'), (5906, '37d24034-e80c-4629-b808-0af37ddcb4cc'), (5907, '2cb1e24f-18e4-4954-9917-49bcd8bda7c7'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (1823, '3d5fb357-1a86-4d95-8f1d-c77d575b1880'), (1824, '7b77c59e-36af-44d2-a1f5-b832deccf49b'), (1825, '48aee3a5-de24-49c5-b057-75a9d7ad9d07'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (18213, '076f6581-4145-4ca5-93e8-9e744b0574a7'), (17708, 'bc875152-3402-4283-9c21-9fd595947fbe'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (23868, 'c0393b68-9f5b-419f-bfb3-e9f2ff471502'), (4414, '0070b17d-3e37-4b4d-b0e4-6d764598d46e'), (4415, 'b9a9a85e-e4e0-4cb7-b45f-4ec40e2891b6'), (23869, '26cd5348-6284-4c3b-96a3-3840b630697b'), (13630, '5f595309-fb4e-4261-92ad-938f4eb058f2'), (4418, '9410b13d-3f78-4fa7-aeb6-8361c7619510'), (4419, '54f34e71-e4b0-49b3-9756-3378458d26e3'), (4421, '95a1272e-1f19-494f-96d5-f36f9a919a0a'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (329, 'e5a52187-6e6d-47ba-bffa-cde854b2df46'), (3922, '6f3ca0aa-6e55-4248-8281-52387a0ce0a2'), (25429, 'e4d79f98-e0bb-400b-b218-1b9fd7be51d3'), (25430, 'aa1837eb-99d6-499f-b595-e8a016d92f7d'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (25432, 'ef3e8e89-0c85-4148-bb8c-232bde5fccfb'), (25433, 'a6e27fea-5198-4c65-9a60-cf2e7b50ca41'), (5466, 'e6fb1185-8250-49a9-819b-c56bf802df85'), (7002, 'f6c1eb0a-744c-4cac-9ea7-db3de3ac11d6'), (16223, '53622eb2-87f4-4345-91be-31270bd4a8f3'), (15714, '552228e2-acc9-4ae3-a73e-0b4e9bac702e'), (7019, '783bea7f-52e6-4b1b-9d92-f4af9c12588e'), (23404, '6c4b5559-7888-45c6-be8b-4a28347c6577'), (25964, '5297072a-f709-4bdd-b4f4-6d65e5bc71d8'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (20851, 'b8c3710a-efdb-41e6-9f78-8cb777be59d6'), (14709, '9e554ac4-95fe-4e4e-8ecf-6dc2cc0bb63d'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (29081, 'fb92a989-9e09-4948-b2bc-d0c04fb1372f'), (5530, '89aa49e3-2bb4-41a3-8831-f5755770ebde'), (16803, '9a7b8acd-4249-4952-82ae-4e82a4698926'), (30130, '701c9d28-b4a0-4d27-8f27-eef55e68d62c'), (30131, '238a09bf-3422-4cda-b19b-b8b9886a02c8'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (20412, 'f06728ac-eda3-4786-8dfd-87ab079fe244'), (20413, '5ced2137-1268-4467-9c14-20ab7b9e541d'), (9151, '7b13071f-ae72-4eb8-8c41-b39d6a0c00cf'), (29124, '2a4c40e0-2e01-47f4-a1b0-a84901d0c22e'), (16334, '5c135098-636b-40f8-9a5b-2fe226f68f98'), (10192, '1cdc71ba-92b9-4368-bac0-dc3321d9d7dc'), (10194, 'e573dcfe-a4f3-435f-9afe-101265a292e5'), (13268, '51471f8c-77a6-43ab-9e2f-aa57a27ff726'), (11229, '713d1a76-24fb-4fef-adc0-c3088982f341'), (11262, '5c2061dd-7364-4daa-bb21-a1404baa2b55')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# ViLT

## Overview

The ViLT model was proposed in [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334)
by Wonjae Kim, Bokyung Son, Ildoo Kim. ViLT incorporates text embeddings into a Vision Transformer (ViT), allowing it to have a minimal design
for Vision-and-Language Pre-training (VLP).

The abstract from the paper is the following:
### Encoder[[cv-encoder]]

The [Vision Transformer (ViT)](model_doc/vit) opened the door to computer vision tasks without convolutions. ViT uses a standard Transformer encoder, but its main breakthrough was how it treated an image. It splits an image into fixed-size patches and uses them to create an embedding, just like how a sentence is split into tokens. ViT capitalized on the Transformers' efficient architecture to demonstrate competitive results with the CNNs at the time while requiring fewer resources to train. ViT was soon followed by other vision models that could also handle dense vision tasks like segmentation as well as detection.
In the second pre-training stage, the query embeddings now have the relevant visual information to the text as it has 
passed through an information bottleneck. These embeddings are now used as a visual prefix to the input to the LLM. This 
pre-training phase effectively involves an image-ground text generation task using the causal LM loss. 

As a visual encoder, BLIP-2 uses ViT, and for an LLM, the paper authors used OPT and Flan T5 models. You can find 
pre-trained checkpoints for both OPT and Flan T5 on [Hugging Face Hub](https://huggingface.co/models?other=blip-2). 
However, as mentioned before, the introduced pre-training approach allows combining any visual backbone with any LLM.

## Using BLIP-2 with Hugging Face Transformers

Using Hugging Face Transformers, you can easily download and run a pre-trained BLIP-2 model on your images. Make sure to use a GPU environment with high RAM if you'd like to follow along with the examples in this blog post.
Using Sentence Transformers at Hugging Face

`sentence-transformers` is a library that provides easy methods to compute embeddings (dense vector representations) for sentences, paragraphs and images. Texts are embedded in a vector space such that similar text is close, which enables applications such as semantic search, clustering, and retrieval. 

## Exploring sentence-transformers in the Hub
### Encoder[[mm-encoder]]

[VisualBERT](model_doc/visual_bert) is a multimodal model for vision-language tasks released shortly after BERT. It combines BERT and a pretrained object detection system to extract image features into visual embeddings, passed alongside text embeddings to BERT. VisualBERT predicts the masked text based on the unmasked text and the visual embeddings, and it also has to predict whether the text is aligned with the image. When ViT was released, [ViLT](model_doc/vilt) adopted ViT in its architecture because it was easier to get the image embeddings this way. The image embeddings are jointly processed with the text embeddings. From there, ViLT is pretrained by image text matching, masked language modeling, and whole word masking.
 
Here is the question: What is the purpose of integrating text embeddings into Vision Transformers in ViLT?",The purpose of integrating text embeddings into Vision Transformers in ViLT is to allow it to have a minimal design for Vision-and-Language Pre-training (VLP).
How do Hugging Face APIs simplify dataset alignment for ViLT and BLIP-2?,"[(10245, '022ae3cf-1084-40e3-b8c7-875a87e42cb4'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (9763, '8d9f45f2-2249-4f84-a433-0bfbe18b6c0d'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22057, 'd9401ad0-2004-414c-8b41-0c476329a2df'), (22059, '5b5a378e-3a1f-471b-a6d0-7127f2098c08'), (28716, '0e901ab3-56fc-46c6-a3c7-adee2baa6fca'), (22065, '9fcae45a-a21a-4f0a-88e4-5deac5078287'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (22071, 'ad5c8e12-0663-4a6b-961c-c00ce8a8cd9e'), (18489, 'c196cc2b-dfd6-4797-a458-54ad07efda99'), (22076, '37ef4059-7542-46a2-b62d-19a38915a327'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10843, '9b0e65eb-f487-4e2f-816f-c5ac6c1f034a'), (24156, 'cfb1685e-60cf-4fb0-a3ef-a2470dbb5881'), (24157, 'befedd3e-173a-4c86-9a3f-4d163de16f99'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (8818, '0d90a54f-d75b-45a7-b5f0-9f04f2a4773c'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (4741, '210088c4-7fef-45fb-b3bd-ba5759a63329'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26284, '682808a8-2487-4bd4-bb4b-42af8997747c'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (28844, '2ac701fb-f40a-465d-b95a-8ecf70258ad1'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (28365, 'cafd51db-2387-4509-9218-1d8d546c2dda'), (7383, '18e6f8b5-fb5e-4e5f-bd04-8da9ff10f029'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (7388, 'cbf2f2d9-458a-4eeb-ae18-dfd338ed988e'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (5905, 'd62eaa16-012d-4d8d-b58c-566b4c14dc6d'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (28963, '4453aa10-ab38-4236-bd25-06cc28951ea1'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (26405, '083107e3-5c6f-4e2d-ac5e-50717bdd56d6'), (28966, 'e20c96fb-f68b-49eb-80b6-4dae3c0265dd'), (11054, '397721c5-71da-4c64-ba5c-e20f0d7b0544'), (17716, '8f2ec951-3a42-4929-a6a4-ac35fd766887'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (19772, '7082dc50-05af-4137-974c-a017327474c9'), (18238, '49c747a4-ae32-4f7c-8cab-3f65e2a8f861'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (4937, '24ec3f32-f79f-4d91-a838-361fdd56703f'), (15181, 'fb7c5366-41d1-41cb-9f21-6d3b57cf1e94'), (14165, 'f8e81797-b080-4a41-9f82-86eea16eabe8'), (10581, '7f1db1d7-f699-4dd2-911e-40beb0d3e5f3'), (9559, 'a8284fc7-f6b2-4800-baa1-e5b290b17b91'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (25440, 'f344d73e-1edb-43a3-8bab-c235c533086c'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (25967, '3787a06f-6972-47b9-949a-cf92bd938b60'), (20848, '5f663ab4-c26d-47b2-b559-5b23998d7a91'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (25972, '72d72947-f0cb-441d-aec1-2abff09ba3e3'), (16759, '1b62ceba-cef9-48f4-9d42-37f27b4c1c9c'), (16760, 'd936a97e-a944-4271-bca8-ab614fbc7ca8'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (10109, '39a51748-3387-4a99-a4c9-fcc371c52067'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (399, '6471d080-c1de-442e-bbe9-aaf0cd61e347'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7060, '395b844f-bf7d-4cd0-a300-57c24b767beb'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (16324, '0cb7caef-995c-4aec-9c86-5e22705a71b6'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (15851, '0bde2ce9-28b0-4ae0-9459-a572cca7f941'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (15863, 'e17826a6-d5b3-4729-ad97-387ae765122d'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

<p align=""center"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/blip-2/cartoon.jpeg"" alt=""New Yorker Cartoon"" width=500>
</p>

We have an input image. Now we need a pre-trained BLIP-2 model and corresponding preprocessor to prepare the inputs. You 
can find the list of all available pre-trained checkpoints on [Hugging Face Hub](https://huggingface.co/models?other=blip-2). 
Here, we'll load a BLIP-2 checkpoint that leverages the pre-trained OPT model by Meta AI, which has 2.7 billion parameters.

```
from transformers import AutoProcessor, Blip2ForConditionalGeneration
import torch

processor = AutoProcessor.from_pretrained(""Salesforce/blip2-opt-2.7b"")
model = Blip2ForConditionalGeneration.from_pretrained(""Salesforce/blip2-opt-2.7b"", torch_dtype=torch.float16)
<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/blip2_architecture.jpg""
alt=""drawing"" width=""600""/> 

<small> BLIP-2 architecture. Taken from the <a href=""https://arxiv.org/abs/2301.12597"">original paper.</a> </small>

This model was contributed by [nielsr](https://huggingface.co/nielsr).
The original code can be found [here](https://github.com/salesforce/LAVIS/tree/5ee63d688ba4cebff63acee04adaef2dee9af207).

## Usage tips

- BLIP-2 can be used for conditional text generation given an image and an optional text prompt. At inference time, it's recommended to use the [`generate`] method.
- One can use [`Blip2Processor`] to prepare images for the model, and decode the predicted tokens ID's back to text.

## Resources

A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with BLIP-2.
Could we simplify that? Let's find out!

## Finding what we need

First we'll import the `HfApi`, which is a class that helps us interact with the backend hosting for Hugging Face. We can interact with the models, datasets, and more through it. Along with this, we'll import a few helper classes: the `ModelFilter` and `ModelSearchArguments`


```python
from huggingface_hub import HfApi, ModelFilter, ModelSearchArguments

api = HfApi()
```

We encourage you to log in to your Hugging Face account so you can upload and share your model with the community. When prompted, enter your token to log in:


```python
from huggingface_hub import notebook_login

notebook_login()
```

## Load the Pokémon BLIP captions dataset

Use the 🤗 Dataset library to load a dataset that consists of {image-caption} pairs. To create your own image captioning dataset
in PyTorch, you can follow [this notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/GIT/Fine_tune_GIT_on_an_image_captioning_dataset.ipynb). 


```python
from datasets import load_dataset

ds = load_dataset(""lambdalabs/pokemon-blip-captions"")
ds
```
```bash
DatasetDict({
    train: Dataset({
        features: ['image', 'text'],
        num_rows: 833
    })
})
In combination with the data issues widget, the Python API offers a great way to integrate the results of existing scripts (e.g. data quality checks or model monitoring) into a scalable data inspection workflow.

## Using Spotlight on the Hugging Face hub

You can use Spotlight directly on your local NLP, audio, CV or multimodal dataset. If you would like to showcase your dataset or model results on the Hugging Face hub, you can use Hugging Face spaces to launch a Spotlight visualization for it.

We have already prepared [example spaces](https://huggingface.co/renumics) for many popular NLP, audio and CV datasets on the hub. You can simply duplicate one of these spaces and specify your dataset in the `HF_DATASET` variable.

You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.
 
Here is the question: How do Hugging Face APIs simplify dataset alignment for ViLT and BLIP-2?","The context does not provide a direct answer to the question of how Hugging Face APIs simplify dataset alignment for ViLT and BLIP-2. The provided context discusses loading a pre-trained BLIP-2 model, using the Hugging Face Hub, and loading a dataset, but it does not specifically address dataset alignment for ViLT and BLIP-2. Therefore, I cannot provide a concise and relevant answer to the question based on the provided context."
What is the key difference between Unigram tokenization and BPE when applied in text preprocessing?,"[(29700, 'c1318e99-a283-4914-a7fa-45ad6c5e5852'), (19463, '72f624a9-b037-428d-9837-de3484ab1799'), (29704, 'eeb0832a-ecb4-44a7-9fac-ce47c316fd73'), (29703, 'c39eeb1d-4464-46d4-98a9-c31633860860'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (29709, 'e3ca7c6b-c286-4bf0-ae83-982df4ff7ff6'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (29711, 'd09f3d50-aa58-417b-b26c-c1f0457c6bdd'), (7703, 'bdbbf91e-5be0-4de8-b791-c7388d247942'), (7707, 'ed661609-30a8-4c88-a9c6-f1b5ad736827'), (20006, 'd78b8574-2f4c-4cfd-9142-1c1ed6cec36e'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (5702, 'e8e55799-cfe2-4401-a8ab-b255957b12cc'), (5703, 'deacec8b-e008-4941-922b-71bd1841aa71'), (10824, '6d96b69f-c6af-4766-b64b-337c86dd7faf'), (10825, '5ab44007-ce9e-4e86-a754-a03638ace665'), (24656, 'a088c20c-8d7d-4bfd-999d-984c49d82451'), (6225, '95dfb88e-25d4-40c5-8d6f-6d3af07e4e0e'), (6226, '496d95c0-9967-4764-94fe-bd01221c5dc4'), (6229, '829198af-0f31-4afe-aae8-358ab99a58c7'), (8794, '760b9795-eadb-4fbb-8458-6bcb928322ee'), (6239, '0624cf0f-179e-441a-bf41-454d44f35506'), (15466, 'bf2582ee-b984-4d41-952a-6497f66b25be'), (620, '607969d7-62ff-48ba-9157-ee5fc510a46a'), (6253, 'fa7b9b2e-c67f-4179-b634-54ad01e9f43b'), (7789, '206dd674-46b7-4f38-8150-9b15cfd06f87'), (7791, '80b0268e-e547-48fb-95d2-622713c6a9db'), (7792, '8d5c7246-3ed2-455b-a56e-538184bdb752'), (7790, 'a9e5791a-46e7-4308-8134-e24a084a312f'), (626, 'beb0b4ef-6aa8-4f19-90f6-06a202ff082e'), (7793, '40d93264-9180-45db-b051-ab44da705591'), (7796, '673ee2d2-28a6-419c-95fb-ca8cf2d75d9d'), (631, 'd8c42629-5fb7-4e48-8de4-57a1a4d10570'), (632, '1ae5445c-2cac-45f5-8d2c-a2d7986b517e'), (15481, '87ce1504-d1e1-49b1-9c72-7fb74be5a8e7'), (634, '06c1ab81-2669-4e9d-b9da-0defc2a88a4f'), (8314, 'b04a31e0-43f4-41a4-89cd-7b20c2fee1a8'), (636, 'df5ffc5e-744b-4965-ab3d-c9d781913d91'), (15485, '9b5dbea1-d5fa-4385-8c03-f268bd24aea8'), (15486, '3884e1e5-dfe0-40c4-88e6-43061473120e'), (15487, '211f581a-87ba-479c-bbb7-4bf419ff3c38'), (15480, 'e8dd4427-60bd-48a5-89aa-e3ea494636d4'), (18568, '3e6b192d-ac77-405e-b509-7c84053c11a8'), (10895, '7eb747de-916c-4162-ad00-34e215fde4ba'), (19624, 'eb28aace-3534-474d-a30f-6c9a92bc30b9'), (19626, 'f1c42c96-41d4-44fa-9192-5e01b58077d7'), (19628, 'b8983f6b-a6a9-441c-9182-505e83ffa1d4'), (690, 'b6bf8779-7ba4-44e2-956d-16eaea7da823'), (17616, 'a59fd552-bc52-4d0c-9aba-aefd3b3839f5'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (15581, '8a808209-2556-4ec5-bd15-34e03c26c682'), (4327, '906c0c93-5ef3-428b-ae43-ee8fb9df0863'), (4328, 'c0184070-a9a6-4733-997f-2604d2fbc82a'), (4330, '864c4298-108e-4916-b63a-d577039833cb'), (4331, '35d2346b-51ef-4c1e-b4d0-e1fbee5951f8'), (22281, '57d96939-0984-4e2a-b853-c04f922fe00d'), (22283, '7aa39503-89c5-48bf-b38d-819595efabe1'), (22288, '26e486c2-43b7-4f99-9d2d-024d2618471d'), (22289, '9e4402d9-ecfe-45c9-bb4e-7e0c7afbfa12'), (22290, 'effab2b0-1b81-4b47-9d10-d1f7245ceb5c'), (22300, '32418488-7979-4eb3-8192-83f21c3a051b'), (28453, '2acaecbf-0a07-4739-a5e6-f4a3a5a14829'), (24386, '6b682f9a-1ab3-4d5f-9f0b-cf01bdc048d0'), (11076, 'c68b6b40-c9ad-4e20-abf8-4fe2db7c0f3e'), (11077, '3eba11d4-e601-4b6a-b0f9-0fb7ecced3d3'), (16719, '7f7a0365-377c-4bfb-a3c8-97ce76ba9618'), (16720, '6f64e88f-35dc-46dc-9289-15572ee0760a'), (16722, '83a18f41-06dd-43eb-86c2-e1cb3a920950'), (16724, 'bcd7e1a3-4be5-4f08-9af9-feef218c2d7c'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (854, 'facc2f1f-14f0-407f-a646-e5a4afeca52e'), (16726, 'c0e0ead7-7fb9-4e1a-a8f4-41c13bb96ae1'), (16728, '9efc94ec-37a1-4b5b-a46c-432fb248c4c4'), (16729, '93966141-a692-437d-9210-42cd7827af64'), (16730, '4698f6a9-10ed-4b63-8a16-86a39a5de935'), (857, '4a58c58e-011b-4c6a-9024-25c07d7755eb'), (3420, '76ebd96a-be31-4cd0-a3e8-fef929c19287'), (16733, 'd41410d5-24e0-4886-8d53-23d71dc2fbf3'), (862, '58dce80f-7a13-41a9-8271-89fa056172d9'), (863, '1c31de44-0abf-41d6-96cd-508363daa9a7'), (864, '5920912b-08ae-4795-a60c-d67481685163'), (16737, '4a37b1c8-d2ed-48a2-a3e8-84f2a1e735c3'), (16738, '33cdddd8-07e1-4371-8c3e-b9f64f9afc3c'), (16736, '14094fc4-1dfe-4a5b-8595-d13531fb8d73'), (16740, '567a8c57-7015-46ae-87bb-d7aef479b1ca'), (865, 'bce264f6-c384-4db2-96e7-e1b9d03c8c83'), (24961, '3bcca0f3-29da-49e7-be84-4b41c54e57d4'), (901, 'f064c51b-00f5-4a7f-9992-c3711c8b265f'), (1415, '1411d969-fc20-4d02-9d2f-76d8d2732150'), (12174, '11b1816b-c38a-44f5-ae65-7589d37c8c3f'), (12175, '7d112eff-907a-4a4d-8a99-77bca14c5a4a'), (15247, '9e91e376-e5c3-42b5-822e-a77bfbed897b'), (15251, '12885ed4-98da-492a-a72b-67027a02f6a8'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (31149, 'debd5c00-01da-49e4-9fac-acfd4e6bcd78'), (31150, '12b8e584-1e0b-42bf-a6dc-7bf8638ebff5'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (31153, 'e4910d18-2062-476e-95cf-13e2844c2ada'), (31154, 'e49097a1-9b93-492c-9e9c-343a02a27e52'), (31155, '4ce7c97c-6ce1-4af5-976f-458bbfed6369'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (8175, '82c4968d-4fdd-43ea-8244-a62526fe507a'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ### 7. What is pre-tokenization for a subword tokenizer?

<Question
	choices={[
		{
			text: ""It's the step before the tokenization, where data augmentation (like random masking) is applied."",
			explain: ""No, that step is part of the preprocessing.""
		},
		{
			text: ""It's the step before the tokenization, where the desired cleanup operations are applied to the text."",
			explain: ""No, that's the normalization step.""
		},
		{
			text: ""It's the step before the tokenizer model is applied, to split the input into words."",
			explain: ""That's the correct answer!"",
			correct: true
		},
        {
			text: ""It's the step before the tokenizer model is applied, to split the input into tokens."",
			explain: ""No, splitting into tokens is the job of the tokenizer model.""
		}
	]}
/>

### 8. Select the sentences that apply to the BPE model of tokenization.
| WordPiece | This is a subword tokenization algorithm quite similar to BPE, used mainly by Google in models like BERT. It uses a greedy algorithm, that tries to build long words first, splitting in multiple tokens when entire words don’t exist in the vocabulary. This is different from BPE that starts from characters, building bigger tokens as possible. It uses the famous `##` prefix to identify tokens that are part of a word (ie not starting a word).  |
| Unigram | Unigram is also a subword tokenization algorithm, and works by trying to identify the best set of subword tokens to maximize the probability for a given sentence. This is different from BPE in the way that this is not deterministic based on a set of rules applied sequentially. Instead Unigram will be able to compute multiple ways of tokenizing, while choosing the most probable one. |
Normalization and pre-tokenization[[normalization-and-pre-tokenization]]

<CourseFloatingBanner chapter={6}
  classNames=""absolute z-10 right-0 top-0""
  notebooks={[
    {label: ""Google Colab"", value: ""https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb""},
    {label: ""Aws Studio"", value: ""https://studiolab.sagemaker.aws/import/github/huggingface/notebooks/blob/master/course/en/chapter6/section4.ipynb""},
]} />

Before we dive more deeply into the three most common subword tokenization algorithms used with Transformer models (Byte-Pair Encoding [BPE], WordPiece, and Unigram), we'll first take a look at the preprocessing that each tokenizer applies to text. Here's a high-level overview of the steps in the tokenization pipeline:
<a id='unigram'></a>

### Unigram

Unigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation
Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or
WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each
symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and
the most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in
conjunction with [SentencePiece](#sentencepiece).
},
		{
			text: ""BPE tokenizes words into subwords by splitting them into characters and then applying the merge rules."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""BPE tokenizes words into subwords by finding the longest subword starting from the beginning that is in the vocabulary, then repeating the process for the rest of the text."",
			explain: ""No, that's another tokenization algorithm's way of doing things.""
		},
	]}
/>
 
Here is the question: What is the key difference between Unigram tokenization and BPE when applied in text preprocessing?","The key difference between Unigram tokenization and BPE when applied in text preprocessing is that Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each symbol to obtain a smaller vocabulary, whereas BPE starts from characters, building bigger tokens as possible."
How does Gradio's gallery component simplify displaying image datasets?,"[(28162, '2fe3c2a3-a327-4912-ada2-190568c660fb'), (4103, '69742828-77bd-4856-a8aa-a0025c5d7383'), (29197, '596dee43-da92-457c-82fc-9bced93586f3'), (27662, '74026321-c9d0-413d-b398-a480fcc7cdd8'), (2064, '04c7accd-3d36-4c4c-af4d-e39364ae4144'), (10261, '889896bb-eb69-46fb-b043-075142527346'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (9254, '8f1698ef-6180-4ba4-bb5e-53e97d423ac3'), (28216, '5ef0c5f2-e9f3-49a4-9bda-40af0f54e3d0'), (28217, '17a071b0-182d-464b-89b7-d5c2cbb45575'), (28236, 'ff029be9-b5d9-47d9-8b0a-e79b53b6a8ea'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (2126, 'eaad6c83-eff0-4a22-b860-506e3447e15a'), (28243, '0bf9268d-f628-4160-b2fb-5d484fbd603f'), (28245, '58c7c630-c9dc-448b-b83d-b0f8d99cc157'), (28250, '31ec90bb-9213-4d5b-9a3c-7fd0daac0f7d'), (2143, '6e81dc66-340f-4578-890b-b72e0440f793'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (27752, 'ba8de9dc-6631-4a04-ae73-15640a799b35'), (27754, '4b78d6ef-9ea6-4e0e-9884-7acbb2ebb25b'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (28272, '8f0d48b9-997f-408d-a152-f00ba14dabd5'), (28276, 'f4a66bc9-205e-4eb3-8d50-50eb389d8352'), (8823, 'ea5522a2-014a-4aa8-83ed-569e174218bf'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (2177, 'e4f20765-678d-400f-acda-3fd9e844e57d'), (4740, 'a3172a98-921d-49f4-98ff-7a1f3290f6af'), (28293, '1c594543-8c42-4ed5-9db9-b8f7d53e4e1a'), (7823, '341c1953-c6b2-408b-9040-3ce33e764469'), (27793, 'e0cc82b3-f860-4440-b597-0466bf024466'), (2196, 'aa4313fe-e229-40bb-88bd-e16975e7fdf4'), (27811, '6351e475-b7a2-4ee3-88f7-ce204ff277fb'), (6820, '296a6e9a-0b6c-450c-8983-f33bcfc0a165'), (3756, '1a1f79f6-d46a-4043-97ae-61320836b7ce'), (27310, 'd9ba7f2d-8c4d-433b-883c-e6e6f53033b5'), (27320, '933721b6-356a-470d-b141-a3fe0e092606'), (30395, 'ab5263c1-7c85-43ee-bd27-89e9944fe67a'), (2244, '1189e75d-0b0d-4cb1-8aaa-50df2fbd21af'), (2245, '47834d58-902b-4e38-853e-427121d538ab'), (2246, 'b2714547-7d53-41a2-ade0-0dc1989a2476'), (2247, 'e9c8dd04-774e-481f-a3fe-757c0a391f09'), (2251, '49c92cce-066a-48de-9511-6f677fedd988'), (27852, '81090f0f-86b7-4c58-9fe0-a796307b1748'), (2253, '528c0950-87eb-4ce0-9db7-aeaf9089c226'), (2252, 'cf2e843d-3e19-4217-b439-3fcdb5926a08'), (2257, 'a342551c-d0e3-4d70-857a-23e1e82ecc87'), (2263, 'de23a944-32ae-4aca-9342-ed441d8275b4'), (27864, 'bb7de493-b7e0-4a2e-9e87-b00fbed9325c'), (27360, 'b3f91b27-9fab-492f-b8f6-e22b380d3729'), (2288, 'c0ef52c7-99ed-4694-93ae-6c412b9f86a1'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (23796, '851ba12c-6554-4982-8c9f-afe1d6deceff'), (23798, '0ab94912-64b7-489a-9d25-a3f911a1430a'), (21754, '87b73ba0-88c4-4302-9951-5e5c45977bc5'), (27913, '1a5af7ec-5880-4d95-9c6f-bfa4aa6121ae'), (4370, 'c87542a1-24d3-4ef2-955b-8b5e17ff37f8'), (4371, 'ee717cdd-8985-41e6-b386-5a6da655ddaf'), (30999, '53998aea-d5c1-4b05-87c7-09e519c96443'), (4376, '1df728ea-59da-4e86-bc43-d94d8cc3803b'), (27938, 'c92b35e4-1f06-466a-b0af-00dceb992607'), (2342, '865c895b-34a5-453b-b76f-f5c444c3b2df'), (2343, '967189ff-4b5c-4875-a56e-984caf1351ee'), (6954, 'dc0a4a82-5286-4d59-ac85-fe642c49151e'), (6955, '31ad389f-36a4-4c9d-a5c1-249c19c98870'), (2362, '1cfbdc0e-9682-4844-b541-ff932ae3ea3a'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (2369, '2d8765ce-950f-4812-be38-74a3ca44c156'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (2371, 'cbb96294-2e50-4502-a0f9-43fcc51106b5'), (2376, '7809bd4a-1a54-4abb-bb3f-068d12f9f4d2'), (2398, '674a4fbf-0ede-489e-a689-aedd65e6317a'), (28000, '14c388ba-8b8d-4268-8474-f3a833f333f1'), (2402, 'bf619499-284f-4b2f-9463-83cf285ec6ed'), (19823, '4a9e8566-d1fc-4875-a3a6-70e098a84848'), (28017, '29c87561-6f4c-4afc-adfd-b3054e4a4dad'), (2419, '379f8a89-bc78-4bfc-8f9e-48f29f13370e'), (31612, 'c6e810cc-92c8-4e66-9d4a-8ae615c991f9'), (1919, 'c64e4f18-2b7f-4516-8a23-1060fd924eba'), (1937, '9e067ec3-9c36-4419-9560-eb68ac22179f'), (28051, '2e9d8676-4090-4ac4-ac08-acae65f2d32d'), (2457, '28088bfe-a055-460e-bdc0-21af2dfe9963'), (16800, '2e36121d-c993-4b67-8fd9-84fa17182afa'), (3493, '8a1040ce-1b60-4e24-90e9-cb4e7127b718'), (28070, '69c3ce65-8bee-4b77-bb91-93ad624a7734'), (20393, '44c960bd-d282-4caa-8237-69095a151fd8'), (20394, '5ee7db72-d4a7-43e0-9cc0-f5da6aab5d5c'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (11185, '91d6bcb4-003e-4935-a022-88d5dd293dc8'), (1978, 'e6aa00b1-e118-4a20-9aea-377dc102cb43'), (7104, '6b525b1e-1bf3-4416-b7cb-00982ffdc82f'), (1990, '2e21c9d1-f37e-4ff5-b83e-a69e36441d2c'), (11729, '4691185d-26b0-403c-a3f3-6d2bc95ea285'), (15828, '3298be22-08aa-4a14-8e09-fcb23b3b9a59'), (11733, '27aacaae-aca4-411c-8fb2-30313de2897d'), (28118, 'cfd580df-4134-401a-aa1c-e5f6d5cb824a'), (28119, '1dcccb98-51b9-49bd-81ec-c5d5c62bea87'), (28120, '0e15845c-8278-4ae1-8b41-403e6698afe5'), (28121, 'f47bdf19-919c-4645-96d7-5bde219f7556'), (25559, '4fdfc851-a389-44bc-ade1-889798f2c676'), (11740, '005c72e8-ed81-43e5-93d7-fc3a05cec140'), (28125, '24144982-b2b1-4f85-a299-ae52b3b80ed5'), (28126, '82468f2e-776c-4d15-8134-4912158c36b3'), (28127, 'bd655a7b-d09b-4b55-8ba0-932251f68bcf'), (11745, '10784898-608a-4cde-8419-5358ef3928ae'), (11746, 'c1d1e06f-bcd1-4a21-95d5-aacec2df63fc'), (28131, '974a4165-1698-418f-90a1-f8e6e7cbb425'), (28137, '5116f0d1-8fcf-4fed-971e-06051bd8b4b1'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (3566, 'dc14161e-e267-41ea-a4c3-0189e9da1c9a'), (2039, 'a483702a-3d32-49dc-8a1b-cddefa16a85a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Similarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).

Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it.

![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)

In this image, the following preprocessing steps happen to send the data from the browser to your function:
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
```

Postprocessing is even simpler! Gradio automatically recognizes the format of the returned data (e.g. does the user's function return a `numpy` array or a `str` filepath for the `gr.Image` component?) and postprocesses it appropriately into a format that can be displayed by the browser.

So in the image above, the following postprocessing steps happen to send the data returned from a user's function to the browser:

* The `float` is displayed as a number and displayed directly to the user
* The list of string filepaths (`list[str]`) is interpreted as a list of image filepaths and displayed as a gallery in the browser

Take a look at the [Docs](https://gradio.app/docs) to see all the parameters for each Gradio component.

## Queuing

Every Gradio app comes with a built-in queuing system that can scale to thousands of concurrent users. You can configure the queue by using `queue()` method which is supported by the `gr.Interface`, `gr.Blocks`, and `gr.ChatInterface` classes.
How to Style the Gradio Dataframe

Tags: DATAFRAME, STYLE, COLOR

## Introduction

Data visualization is a crucial aspect of data analysis and machine learning. The Gradio `DataFrame` component is a popular way to display tabular data (particularly data in the form of a `pandas` `DataFrame` object) within a web application. 

This post will explore the recent enhancements in Gradio that allow users to integrate the styling options of pandas, e.g. adding colors to the DataFrame component, or setting the display precision of numbers. 

![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/df-highlight.png)

Let's dive in!

**Prerequisites**: We'll be using the `gradio.Blocks` class in our examples.
You can [read the Guide to Blocks first](https://gradio.app/quickstart/#blocks-more-flexibility-and-control) if you are not already familiar with it. Also please make sure you are using the **latest version** version of Gradio: `pip install --upgrade gradio`.
 
Here is the question: How does Gradio's gallery component simplify displaying image datasets?",Gradio's gallery component simplifies displaying image datasets by automatically handling the postprocessing needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).
What role does the Viterbi algorithm play in Unigram tokenization?,"[(19463, '72f624a9-b037-428d-9837-de3484ab1799'), (29703, 'c39eeb1d-4464-46d4-98a9-c31633860860'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (29708, '57d086cd-c094-4b43-998a-9bf67a10c377'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (29709, 'e3ca7c6b-c286-4bf0-ae83-982df4ff7ff6'), (6177, '9a6af5e6-9ab0-4924-a9e2-817c0c8f0901'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (5703, 'deacec8b-e008-4941-922b-71bd1841aa71'), (5704, 'f5a76ce8-55f0-4aa3-9058-b471d25fed91'), (10823, 'e21b6aee-30a8-41ca-a03b-c9cbbe3e780e'), (10826, '6897c0ab-5327-430c-9211-8dc59c47ede2'), (10824, '6d96b69f-c6af-4766-b64b-337c86dd7faf'), (24656, 'a088c20c-8d7d-4bfd-999d-984c49d82451'), (6225, '95dfb88e-25d4-40c5-8d6f-6d3af07e4e0e'), (6226, '496d95c0-9967-4764-94fe-bd01221c5dc4'), (6229, '829198af-0f31-4afe-aae8-358ab99a58c7'), (6232, 'd57b5e65-e59a-4e4b-8aa0-7139e4f39886'), (6235, 'da68fe2a-d338-45d1-a04c-022e49b5dcd4'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (6239, '0624cf0f-179e-441a-bf41-454d44f35506'), (6244, '69a8d278-e494-488d-a2f2-d94fc92f2bd2'), (15466, 'bf2582ee-b984-4d41-952a-6497f66b25be'), (7789, '206dd674-46b7-4f38-8150-9b15cfd06f87'), (7790, 'a9e5791a-46e7-4308-8134-e24a084a312f'), (7791, '80b0268e-e547-48fb-95d2-622713c6a9db'), (7792, '8d5c7246-3ed2-455b-a56e-538184bdb752'), (7793, '40d93264-9180-45db-b051-ab44da705591'), (6253, 'fa7b9b2e-c67f-4179-b634-54ad01e9f43b'), (7795, '54b5e9cb-abbe-4c4f-a09e-9691c4b11923'), (7796, '673ee2d2-28a6-419c-95fb-ca8cf2d75d9d'), (8314, 'b04a31e0-43f4-41a4-89cd-7b20c2fee1a8'), (15485, '9b5dbea1-d5fa-4385-8c03-f268bd24aea8'), (15487, '211f581a-87ba-479c-bbb7-4bf419ff3c38'), (18566, '5d408cd6-4635-4a46-9971-43575951f389'), (18567, '82b5f6ec-fc59-45e3-8d39-a7e17af46f95'), (18568, '3e6b192d-ac77-405e-b509-7c84053c11a8'), (18569, '09e8a18a-b009-4152-bc84-f97c593e35df'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (9377, '11462af2-125e-4c1a-91d0-9dba82209e93'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (19621, '2100eee7-6ff0-4550-a16b-6538c747e693'), (7342, '4e3f892e-b398-4de7-91ee-ce188a823d68'), (690, 'b6bf8779-7ba4-44e2-956d-16eaea7da823'), (17616, 'a59fd552-bc52-4d0c-9aba-aefd3b3839f5'), (10455, 'ac623491-7926-46e8-8a45-4c350813dacd'), (10456, 'ecdba708-8f55-48d6-86ee-2ec46b1e2778'), (10457, '34cff3a6-1baa-4a3d-a758-49c4d21bffe5'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (10460, 'ace98882-3633-40c7-b823-208bda3a4ca1'), (17118, 'c5ffb0d0-0167-4bd7-a752-a6b989f62797'), (4327, '906c0c93-5ef3-428b-ae43-ee8fb9df0863'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (11065, 'f276616a-e9b5-4dcd-ae38-4256ec518cf8'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (11076, 'c68b6b40-c9ad-4e20-abf8-4fe2db7c0f3e'), (11077, '3eba11d4-e601-4b6a-b0f9-0fb7ecced3d3'), (24900, '4889caeb-6d43-410d-b474-d87d0da7fe19'), (24902, '21707633-c150-463b-a123-71a45823b814'), (18254, 'ae1092dd-508c-4ec9-bd90-bb1e8733e822'), (16719, '7f7a0365-377c-4bfb-a3c8-97ce76ba9618'), (18260, '20b06811-c14a-4c0e-b429-99bce86da710'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (16724, 'bcd7e1a3-4be5-4f08-9af9-feef218c2d7c'), (16725, '167730fe-e8e3-492c-a3d4-99715b149522'), (16728, '9efc94ec-37a1-4b5b-a46c-432fb248c4c4'), (857, '4a58c58e-011b-4c6a-9024-25c07d7755eb'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (854, 'facc2f1f-14f0-407f-a646-e5a4afeca52e'), (861, '06ceed46-34a1-4772-b0c7-2cc21974805f'), (862, '58dce80f-7a13-41a9-8271-89fa056172d9'), (863, '1c31de44-0abf-41d6-96cd-508363daa9a7'), (864, '5920912b-08ae-4795-a60c-d67481685163'), (16737, '4a37b1c8-d2ed-48a2-a3e8-84f2a1e735c3'), (16738, '33cdddd8-07e1-4371-8c3e-b9f64f9afc3c'), (16739, '2155e313-e0b0-4dfa-b90d-3dda854127c2'), (16740, '567a8c57-7015-46ae-87bb-d7aef479b1ca'), (16741, '78ad9661-d100-44d8-818e-a6130c7ae5ee'), (16736, '14094fc4-1dfe-4a5b-8595-d13531fb8d73'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (18308, '3aa1ce96-60da-432f-b499-3b4d6c91a79d'), (1414, '9772c7cd-c19a-4ee9-abc0-bd27da5ee279'), (1415, '1411d969-fc20-4d02-9d2f-76d8d2732150'), (12174, '11b1816b-c38a-44f5-ae65-7589d37c8c3f'), (12175, '7d112eff-907a-4a4d-8a99-77bca14c5a4a'), (15248, 'ca77aef7-6039-4c5b-80b1-c76f83ab3f28'), (15247, '9e91e376-e5c3-42b5-822e-a77bfbed897b'), (1424, '7da8fa90-541d-4a40-831d-ce0c019039d4'), (15251, '12885ed4-98da-492a-a72b-67027a02f6a8'), (3479, '6d99cd47-980e-4cae-a7d5-415306bd443d'), (4505, '82ea4097-6dd6-4ae8-b0e9-416160ce9a4a'), (4506, '6381c1d4-8050-42bd-a4f9-137f3a69cf1b'), (4507, '7f03af48-608f-4463-ad30-e8d317782576'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (31149, 'debd5c00-01da-49e4-9fac-acfd4e6bcd78'), (29612, 'ae39f1ab-1b94-4882-b706-1e93f0941895'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (31153, 'e4910d18-2062-476e-95cf-13e2844c2ada'), (31154, 'e49097a1-9b93-492c-9e9c-343a02a27e52'), (31155, '4ce7c97c-6ce1-4af5-976f-458bbfed6369'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (29617, 'ff9c2445-4c03-4813-a426-7902846a33df'), (29616, '5f1337a0-af97-4b1c-8e08-b3cdfd401241'), (29615, '2275ba32-0f01-4d0c-990d-ceaf68cba2af'), (18914, 'a7c8f0c2-85bf-44c3-98d8-dcb026fa61d4'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: them. The token which we will remove is the token which impacts the least the loss: here the token ""bu"". We had mentioned at the beginning of the video that at each iteration we could remove p % of the tokens by iteration. The second token that could be removed at this iteration is the ""du"" token. And that's it, we just have to repeat these steps until we get the vocabulary of the desired size. One last thing, in practice, when we tokenize a word with a Unigram model we don't compute the set of probabilities of the possible splits of a word before comparing them to keep the best one but we use the Viterbi algorithm which is much more efficient. And that's it! I hope that this example has allowed you to better understand the Unigram tokenization algorithm.
```

Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named `best_segmentations`. We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated.

Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in `best_segmentations`.
To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end.

Let's take a look at an example using our vocabulary and the word `""unhug""`. For each position, the subwords with the best scores ending there are the following:
```

The special Unicode character `▁` and end-of-sequence token `</s>` indicate that we're dealing with the SentencePiece tokenizer, which is based on the Unigram segmentation algorithm discussed in [Chapter 6](/course/chapter6). Unigram is especially useful for multilingual corpora since it allows SentencePiece to be agnostic about accents, punctuation, and the fact that many languages, like Japanese, do not have whitespace characters.

To tokenize our corpus, we have to deal with a subtlety associated with summarization: because our labels are also text, it is possible that they exceed the model's maximum context size. This means we need to apply truncation to both the reviews and their titles to ensure we don't pass excessively long inputs to our model. The tokenizers in 🤗 Transformers provide a nifty `text_target` argument that allows you to tokenize the labels in parallel to the inputs. Here is an example of how the inputs and targets are processed for mT5:
explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules."",
			explain: ""No, that's how another tokenization algorithm works.""
		},
	]}
/>
 
Here is the question: What role does the Viterbi algorithm play in Unigram tokenization?","According to the context, the Viterbi algorithm plays a crucial role in Unigram tokenization by computing the best segmentation of each substring of the word, which allows for efficient tokenization of words into subwords."
What makes the Kandinsky 2.2 training scripts unique for text-to-image models?,"[(22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (22021, '326d1c69-b66f-453f-a521-b04ab091fdbf'), (22022, 'c0aa2596-89e8-40c6-93ec-2019c9e6250d'), (14861, '546cd8b3-eaff-4501-8d33-890502568333'), (14863, '1bf19b73-6586-4cf0-8ac5-0e8229805171'), (21008, '39722254-f2af-4404-8ad6-818011b35374'), (22033, 'ddd95851-27c2-466a-b3a5-0b0ed970f12b'), (7186, '9784ba9f-8eb4-4702-b93d-13533ea8276e'), (7189, '7e098dd8-73e7-4bd0-8572-847ad39a44b9'), (11288, '2451d2fc-6bb5-4e2f-ad80-c64efaba643f'), (11290, '48386c91-cc6d-4985-998d-420e6468fbca'), (22053, 'e4a8d350-c84c-4e46-b354-d2620d149413'), (3631, '99f7988c-01b0-4e51-97de-4c2bb7df90d9'), (3132, '5a088823-4d93-4854-a8c3-44647c3be7c4'), (20034, 'cec3a575-baee-47fe-bd32-87a49ed54644'), (31320, '34a40776-7620-4c6c-9874-7d7e8586a326'), (95, 'dc4aba7b-91c0-4051-ac61-98969e786309'), (24162, 'a4d7b6c5-1f0a-496c-85ca-6ead2ed48362'), (24167, '0dda5e1d-83b9-4d1e-ae93-055416ad1f76'), (24174, '4ac68f90-9ee8-492c-8038-ccbef1ac8b03'), (24175, '37ac8c56-bb68-4184-a1da-33ffb27f0c10'), (24181, '412aeb0f-9d7e-4170-999d-4920ce92f43b'), (26231, 'b3e08d1a-4130-42bb-8c39-7e1831a663d6'), (16002, '9dcc3b0e-a9e1-465a-bbc6-c7fd98131b04'), (18053, '0a511240-ce1a-4ec3-8b5e-81a1f02f9227'), (16009, 'cec20956-891d-4068-88cc-cca72c8e5a41'), (6287, '89e924a6-29d0-47d3-96a0-a0d16dfc1ce2'), (23196, '60eefbde-7a97-42e5-b4de-e49892e5b3db'), (4771, '0ac7dff4-d3c2-46b3-afeb-33cb4f28068c'), (7333, 'b10dbbc8-112d-480d-afdd-f87144539467'), (679, '09849b2c-dd7f-40c4-bf72-9e31223cd934'), (11953, 'e5b47149-4a3e-4549-a9ef-c12eccfe493f'), (29362, '978efe37-a64d-4457-a01a-cb9afaf32ae6'), (26305, 'd72a5370-1003-4496-9f34-0056f2a43174'), (30409, 'fd93f13f-0c96-42b7-b9ce-cb0f0de84a45'), (14544, '665ecec6-0feb-4755-ae77-975d2c5e49cf'), (10963, 'f47171e5-5fc8-4d52-9e0f-5d9033b1dc5f'), (25822, '219e62f2-d323-4433-aeee-026f4cbc4ee2'), (4319, 'd5e01603-e483-44c9-bd80-ca5e40f5b1eb'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (3815, '7fa3513f-2329-4efb-bfb6-74160ee1ef45'), (3819, '55858f62-94c9-431e-adea-9f0be7e80593'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (19192, '2767df7c-ddd7-446b-a752-0a2d87a1e317'), (18168, '3012d4d4-e60f-4363-ae7a-fa0b832c41f6'), (19194, 'f2337207-7b15-4acc-8d68-6059df62cd35'), (25347, 'b38e0f87-e853-46c1-9fcb-77d766c7bbc9'), (19203, 'b69a56d3-9bbd-4895-a87d-b944a26f8a93'), (19204, 'e5374f8a-a738-473d-9541-a851c339f045'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (25356, '7fc6f9b3-10fd-48c7-95bd-fd7ba0cab29a'), (25358, '3472f3bc-9dbe-46db-b3a7-e45d98d9acff'), (13585, 'dc97e6e4-5a6a-41b4-8851-91e3a0e3af3f'), (25362, 'd13794b5-b32e-4379-9e59-8de3bfd1d134'), (25365, '8c60b393-148a-4a10-8898-ebf74216dd62'), (284, '5e72c7ed-cf69-4a85-8460-5d28bf60ba77'), (17708, 'bc875152-3402-4283-9c21-9fd595947fbe'), (10030, '6fb9b07f-5a83-4be2-b3f5-96b0abee993f'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (3908, '91ab7337-4628-49b3-9525-25b72621e00d'), (22347, 'fb96b8eb-6fef-4edd-8b78-761583967bd7'), (10577, 'e3df7843-cd31-44f5-af24-817a30bec370'), (9555, 'c19b5f80-620b-4a82-82e1-81e843622c70'), (1883, '2ab39a22-5b35-4643-a74f-d3f146789c79'), (24411, 'e7d930b5-dc95-46f3-9354-2cfdd299090e'), (15196, '9fffeb50-1ef0-4656-9949-1b099c72b9b5'), (363, 'a89eaa05-3ff1-49d2-8124-aff7fd09fdaa'), (369, '6fff441c-9ab5-4609-aa29-ed7922127f66'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (15219, 'dca33206-1724-4483-9fa3-0be9c3f6e19e'), (15220, 'c3e6cdad-e35e-489c-9c5a-0f48a94e1c0f'), (15223, 'ad3f1bb1-faab-4407-8491-73f171da5c0b'), (15224, '86b33dec-137c-4878-923e-59c4417496e4'), (15225, 'e45e051b-508d-4edb-a799-603cc0764be9'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (15226, 'ef67749a-f8e0-4b7d-80df-4b93272004f3'), (15227, '783d1508-a579-481d-b6e6-850a89fbabd7'), (15229, 'd9128601-296a-4048-b339-073b76a365fc'), (15230, 'c4948062-4df0-4d3e-ac44-2b700dd82ff2'), (15231, '6a105467-a0bb-4e61-87a9-e15a45b1eaf0'), (15232, '02616d47-0652-4655-a052-f8e9da85ec5e'), (15234, 'cdf6355a-de88-4c51-aa4a-7cd9806d2292'), (13699, 'e6673957-64d4-4ab9-a6ab-290eebf4e65d'), (20356, '3ceeeab3-d21d-4cb4-b163-8b743cce48a9'), (15235, '28a7812d-6984-4612-8b4c-3914ad22414f'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (14215, '021a9b83-dbfa-4bef-9c05-19996def3b9c'), (16264, '5d166a05-2d80-4201-ae2f-b71993797a24'), (14217, '95672ec2-2c75-42c0-b494-b41a203e3710'), (15240, 'edb9141f-6fce-4432-9e81-b57d35ff490a'), (15243, '4189372b-3be4-4bca-b934-f8d9d37da251'), (14219, '3ca3c0e5-6511-46c5-9526-bcb2b027e970'), (15241, '108b26fe-34ae-404a-afc5-3f70969f1f81'), (15242, 'aeb5e8ca-b0a9-4bd3-b8b5-6db088e06216'), (9104, 'cf198758-ade7-47bb-82de-51e1e4e0bc2a'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (19353, '8e76e4a3-ad81-40dc-9473-3d1cd43ffb94'), (19354, '1418cc39-f208-4f98-8ce3-54510d44c778'), (16813, '84307ab5-f0bb-4bb8-be18-61dc9b9a2dc0'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (25027, 'f90dc755-9349-450c-8519-129aa7e8d712'), (18398, '881db9a6-a3a5-4163-a27d-d4319313f716'), (6111, '1ba05e99-81bc-4a88-bf70-0af8349dddf4'), (20965, '78c8fe8a-3494-4209-9ec8-7ac633e5b4b1'), (20966, '61420da0-bea9-4ed4-8b90-0716f6b0a31d'), (20967, '9c9e7c2c-88b4-4ef0-9fd3-fbfea2d90e56'), (20968, 'f80a61e6-a2e8-4a17-98b8-5b97fbbc4a22'), (20969, '448efe28-d593-4aab-8a7c-2e26e1a8be54'), (5095, '2c960b15-fe52-4cc0-8902-eb912dfbf0e0'), (23015, 'd9177564-c31b-4316-8d08-cb422482dd8b'), (22507, '4951a7c5-4a75-4989-97f5-053c7fb8e2e9'), (20976, '8728e796-7cea-417d-b266-4733e9efb117')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Training script

The training script is also similar to the [Text-to-image](text2image#training-script) training guide, but it's been modified to support training the prior and decoder models. This guide focuses on the code that is unique to the Kandinsky 2.2 training scripts.

<hfoptions id=""script"">
<hfoption id=""prior model"">

The [`main()`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L441) function contains the code for preparing the dataset and training the model.

One of the main differences you'll notice right away is that the training script also loads a [`~transformers.CLIPImageProcessor`] - in addition to a scheduler and tokenizer - for preprocessing images and a [`~transformers.CLIPVisionModelWithProjection`] model for encoding the images:
This script is experimental, and it's easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset.

</Tip>

Kandinsky 2.2 is a multilingual text-to-image model capable of producing more photorealistic images. The model includes an image prior model for creating image embeddings from text prompts, and a decoder model that generates images based on the prior model's embeddings. That's why you'll find two separate scripts in Diffusers for Kandinsky 2.2, one for training the prior model and one for training the decoder model. You can train both models separately, but to get the best results, you should train both the prior and decoder models.
Kandinsky2.2 text-to-image fine-tuning

Kandinsky 2.2 includes a prior pipeline that generates image embeddings from text prompts, and a decoder pipeline that generates the output image based on the image embeddings. We provide `train_text_to_image_prior.py` and `train_text_to_image_decoder.py` scripts to show you how to fine-tune the Kandinsky prior and decoder models separately based on your own dataset. To achieve the best results, you should fine-tune **_both_** your prior and decoder models.

___Note___:

___This script is experimental. The script fine-tunes the whole model and often times the model overfits and runs into issues like catastrophic forgetting. It's recommended to try different hyperparameters to get the best result on your dataset.___


## Running locally with PyTorch

Before running the scripts, make sure to install the library's training dependencies:

**Important**
The Kandinsky models are a series of multilingual text-to-image generation models. The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those results for the UNet.

[Kandinsky 2.1](../api/pipelines/kandinsky) changes the architecture to include an image prior model ([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)) to generate a mapping between text and image embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during training, leading to higher quality results. Finally, Kandinsky 2.1 uses a [Modulating Quantized Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.
```

</hfoption>
</hfoptions>

## Next steps

Congratulations on training a Kandinsky 2.2 model! To learn more about how to use your new model, the following guides may be helpful:

- Read the [Kandinsky](../using-diffusers/kandinsky) guide to learn how to use it for a variety of different tasks (text-to-image, image-to-image, inpainting, interpolation), and how it can be combined with a ControlNet.
- Check out the [DreamBooth](dreambooth) and [LoRA](lora) training guides to learn how to train a personalized Kandinsky model with just a few example images. These two training techniques can even be combined!
 
Here is the question: What makes the Kandinsky 2.2 training scripts unique for text-to-image models?","The Kandinsky 2.2 training scripts are unique for text-to-image models because they support training the prior and decoder models, which is not typically found in other text-to-image training scripts. The prior model creates image embeddings from text prompts, and the decoder model generates images based on these embeddings. This allows for more photorealistic images to be generated."
What preprocessing steps are essential for Proximal Policy Optimization (PPO) reinforcement learning?,"[(6144, '34a43938-8523-4217-9140-ca800a80ae2b'), (6145, 'a7fbd7de-a494-43e7-a736-ac47818c05d9'), (6146, 'd950b985-7fb5-46b2-9b05-0af6419fd745'), (10753, '599157ca-e54c-4c41-813e-6b9fb74ccb9f'), (6147, '5a0a9bce-c74d-4d84-9cbd-c2b32a3e4e57'), (6149, 'c49a7f6f-f774-4d7a-8ff4-176a9bbc89ed'), (6150, '3a9ebe50-23b4-42ff-897c-7a593d0b29c6'), (6152, '879840df-06da-44e5-844f-5819aeba72f9'), (6154, '6312ef17-99ba-4a6a-98eb-8e2daa40a7c3'), (6155, '8e716cf0-8f17-4943-812f-aa1511bf263a'), (6156, '72221968-c8f3-41dd-ac06-841acaa4bc06'), (6158, 'fb4aaf9f-149c-4c41-a3c4-40fdd495655c'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (9766, '2594a459-d00b-454c-a9d6-2f1060fb8162'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (20017, '7f1adcff-7661-482a-83a6-f88d6491fb7b'), (20018, 'd0dd9c49-d5d7-4e72-b765-717912a3ade0'), (12851, 'c31c286a-542f-4994-b97c-dc916101b6a9'), (12852, 'b241f9da-f9a4-4802-9ba7-8b3acc58a446'), (12850, 'c6e54b94-1bc1-4c66-8959-7c3a3451782c'), (12859, '490a120a-9a4a-4266-b754-fcd3a64022ed'), (6204, '718d0648-8e65-4227-8d3f-9b5184f97c80'), (6205, '7bfd5a75-e7c6-437c-9d88-35d4e021b99b'), (6206, '47e1ac12-235a-4b50-aba3-3689f03221c2'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30289, 'ee66192e-f98b-4622-bcb2-39745ac227a4'), (30297, '1ea23f14-b6ce-4790-86cb-ad9dc50aaa2f'), (21595, '5efa160a-36d2-4b83-aa17-269b32dddfc6'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30302, '4b7094d4-6aaf-42db-a006-4357c743ab41'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21600, '9af1cc4c-21f2-4e01-81b1-d37162920ffb'), (21086, '207c4ca6-7dd6-4784-a248-66cea4544db3'), (21598, 'e113a596-30cc-441f-ac19-90b0009cc7f4'), (30301, 'e9564e93-e1bd-4577-8ccc-3dd14eaf934b'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (21606, '136ec334-4b38-4957-a9a2-8b75cf91a5d1'), (27774, '978314d6-ab36-484a-8af5-e7e5eaf4e43b'), (17547, '91c00572-19ed-4c45-a397-6e45a63c5956'), (14487, '4f24c302-152f-4c11-b198-4be00041e6a0'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (22690, '9b2e765e-23fe-4aca-a149-55964e264042'), (14502, 'a6fde7e9-02c7-4939-96b0-3cc388688a2d'), (22695, '84ad0c76-d7d9-43b7-95a7-c3b740570efe'), (22694, 'd986f4e7-7083-4497-a780-384d8b0c76d8'), (14504, 'ae636879-64f6-4cce-be09-2b22b17df887'), (14505, '4e3531c6-8fe1-463f-b0b2-0e9e25382b06'), (14503, '38e55cbb-04e6-4490-806d-190ff6021fd4'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (174, '3afa87b4-1cf0-4362-96ad-7ef17bd6382d'), (25271, '00226601-abf1-4a43-9b40-66be353786ac'), (14520, '3504a659-cedb-49bd-b3fd-0aa751a63620'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (25278, '2af69953-2e84-47e5-9440-0826a158fbda'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21210, 'a7d914db-0a82-4bef-889e-b11938bed886'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (5866, '4108faf4-0e48-4871-8e5b-31983d6c0129'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11047, '939d2f75-31f8-4c36-92da-3b8dadf6c83a'), (1342, 'b4875f62-8418-4119-832f-2fa04ff8c912'), (10750, '820e2551-3410-4210-a73c-32820cc67dc8'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (11609, 'b190e428-9ac1-40c7-8b24-6f6b2f935086'), (19297, '610fc147-74d3-443a-8baa-c33ebc792921'), (13155, 'feb24b04-1aed-4866-9b73-320a1d8566b1'), (19299, '58e3889c-4edb-4806-9064-e951799e80cb'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (16745, 'bbcf4d12-db52-4214-bd20-fbb0a28f27e1'), (7017, '92a71275-5129-4975-b4b6-33fa814ca9c4'), (16746, '45080cea-724d-4d98-8278-7cd29fd71f9b'), (19316, 'c92acaab-42a6-4ecc-84ad-e8085e28944f'), (19317, 'cab6254e-890a-47c4-a68b-1b64258cc023'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (19325, 'fb7c67ab-594a-487c-ae87-f94d0dbe7c65'), (19326, '241f9811-1770-4b26-ad5c-e52a24b2279a'), (6022, '8da41f33-841b-4457-96e1-8f3a23faaf71'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (31631, 'ff89793b-da5c-481a-97bb-e3d16785ec5a'), (23951, 'b4716ef6-2e8a-4fc8-9ac0-c3fc45cca6bf'), (8600, '269c11e4-e332-4535-8735-0da74c456141'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (23456, 'd1baa005-a401-42d2-924c-aece54f9448a'), (15272, '557eb765-9567-4e6c-aed7-ccf7378079a7'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (13766, '324b6368-b377-4ac8-920b-ae7106ccadf1'), (13767, 'd460916f-0557-46c2-a9d3-da2d7989ac41'), (23498, '4c725dcf-74c2-45a9-8efb-757e4fd0c5b6'), (13772, '9460a058-4737-45bd-bd41-81f40b3f366c'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (21481, '9356a740-eb3f-4eec-975e-bd0b77cbd195'), (10731, 'df44e1e2-7486-4df3-a811-c05bb5e2cdc7'), (10732, '9d553fdc-e142-40ab-abff-8b9a2a7c81cb'), (21483, '5887ee51-0550-47b1-a9c9-bc00327706c9'), (21484, '780e97be-d635-41ff-a9ad-dc48ac21cb80'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd'), (6143, 'd3e6f6dd-dece-4c80-a8e8-de6ba6331451')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The intuition behind PPO [[the-intuition-behind-ppo]]


The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: **we want to avoid having too large of a policy update.**

For two reasons:
- We know empirically that smaller policy updates during training are **more likely to converge to an optimal solution.**
- A too-big step in a policy update can result in falling “off the cliff” (getting a bad policy) **and taking a long time or even having no possibility to recover.**
----

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png"" alt=""Stable Baselines3"">

To solve this problem, we're going to use SB3 **PPO**. [PPO (aka Proximal Policy Optimization) is one of the SOTA (state of the art) Deep Reinforcement Learning algorithms that you'll study during this course](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D).

PPO is a combination of:
- *Value-based reinforcement learning method*: learning an action-value function that will tell us the **most valuable action to take given a state and action**.
- *Policy-based reinforcement learning method*: learning a policy that will **give us a probability distribution over actions**.

Stable-Baselines3 is easy to set up:

1️⃣ You **create your environment** (in our case it was done above)

2️⃣ You define the **model you want to use and instantiate this model** `model = PPO(""MlpPolicy"")`
The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: **we want to avoid having too large policy updates.**

For two reasons:
- We know empirically that smaller policy updates during training are **more likely to converge to an optimal solution.**
- A too big step in a policy update can result in falling “off the cliff” (getting a bad policy) **and having a long time or even no possibility to recover.**

<figure class=""image table text-center m-0 w-full"">
  <img class=""center"" src=""assets/93_deep_rl_ppo/cliff.jpg"" alt=""Policy Update cliff""/>
  <figcaption>Taking smaller policy updates improve the training stability</figcaption>
  <figcaption>Modified version from RL — Proximal Policy Optimization (PPO) Explained by Jonathan Hui: https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12</figcaption>
</figure>
--
title: ""Proximal Policy Optimization (PPO)""
thumbnail: /blog/assets/93_deep_rl_ppo/thumbnail.png
authors:
- user: ThomasSimonini
---


# Proximal Policy Optimization (PPO)
<h2>Unit 8, of the <a href=""https://github.com/huggingface/deep-rl-class"">Deep Reinforcement Learning Class with Hugging Face 🤗</a></h2>




⚠️ A **new updated version of this article is available here** 👉 [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus [here.](https://huggingface.co/deep-rl-course/unit0/introduction)*
<img src=""assets/93_deep_rl_ppo/thumbnail.png"" alt=""Thumbnail""/>  

---

⚠️ A **new updated version of this article is available here** 👉 [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)
- *An Actor* that controls **how our agent behaves** (policy-based method).
- *A Critic* that measures **how good the action taken is** (value-based method).

Today we'll learn about Proximal Policy Optimization (PPO), an architecture that improves our agent's training stability by avoiding too large policy updates. To do that, we use a ratio that will indicates the difference between our current and old policy and clip this ratio from a specific range \\( [1 - \epsilon, 1 + \epsilon] \\) .

Doing this will ensure **that our policy update will not be too large and that the training is more stable.**
 
Here is the question: What preprocessing steps are essential for Proximal Policy Optimization (PPO) reinforcement learning?","The context does not explicitly mention preprocessing steps essential for Proximal Policy Optimization (PPO) reinforcement learning. It focuses on explaining the intuition behind PPO, its benefits, and how it works. Therefore, it is not possible to deduce the preprocessing steps essential for PPO from the provided context."
Why is `gradient_checkpointing` important in training Kandinsky 2.2?,"[(516, 'ce8c02d3-fb1b-4e21-b424-921eb9a2f16e'), (26118, '54e056e0-c70d-4b7f-a71b-a0cda0567d61'), (1545, '2d8b8a39-c24d-481c-bb1e-f25dc29bc460'), (1546, 'b8d3e1d7-0d8d-4330-9634-50dc3fe25b5f'), (21008, '39722254-f2af-4404-8ad6-818011b35374'), (7189, '7e098dd8-73e7-4bd0-8572-847ad39a44b9'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (15910, '82663015-b00f-462e-b5e9-265af9160dfd'), (15911, 'dea3e203-7c16-410e-9667-1ee9ffb39f00'), (1067, '5c533f5a-f138-40ef-aa01-572ed4cd6252'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (21556, '4dbfc5e1-05b5-4a57-88db-428197509bbe'), (30262, 'cb2921cb-3f58-4bd3-aa73-00e1462b10c4'), (26168, '930f8d18-7bc8-4f68-92f0-d815baeefbbe'), (1594, '9622f28a-d122-4157-9bd9-2dd99b146101'), (14397, 'c13b3dcf-abd9-47ba-af8f-be1c2fd14508'), (5694, 'dee73012-4ba6-4015-8099-1ff6348fc9b2'), (14398, 'b76f21ff-7ece-400d-b70c-3cde162ac19b'), (30794, '60649a7f-802b-4687-8b03-4c75a01eca78'), (12883, 'dfcf0173-7e8f-4f19-bb06-7c54d24fff66'), (12886, '18488b88-987f-476e-9eb1-02c4211c5a10'), (10326, 'bf75e85e-bc28-462e-a306-a52929809f83'), (10330, '6dfbf5a1-fc7e-4d69-9cc3-a54c7658ea1b'), (10331, '3e81c36c-c486-497f-8503-f7e56fb08165'), (10332, 'a0e48a36-723f-4add-b319-aff6ed8392a6'), (27230, '5059df05-3a89-417e-8e23-9e51a8a94606'), (95, 'dc4aba7b-91c0-4051-ac61-98969e786309'), (25698, '8985c103-fde3-4a20-b9db-329d63723708'), (24163, '6d254de1-aa08-4a87-9094-64313c3a0c4b'), (25188, 'bf31bc87-0988-43d1-bf36-d47a1e56fd34'), (4711, '07cf85f1-f8c8-45a0-b469-b6f11a42face'), (27240, '38e10db4-7730-4044-8f80-beab865eb4f1'), (21607, 'b8f10d9a-6a0f-4bc2-9750-2fbc7550343f'), (4204, 'eb9e94d4-02e1-461e-b271-6c8438d90b46'), (5750, '0d38d4b8-b137-4fc7-9983-bba8ce240e1e'), (23684, '5278bf06-a69b-482f-a31f-a9e84563cc75'), (23686, 'b6be9f72-5560-45bc-9531-11fb72d1dd58'), (21128, '1b0739a8-867b-4a25-bf9e-966b397ff990'), (28816, '9822c0bf-8b6f-4cd6-bb3c-e14660afe95f'), (659, '77f9993f-20a6-4964-b925-7de3be25efa8'), (15005, 'fd443fc4-2728-47dd-a5b5-dc1422eb46ce'), (1698, '584e76a5-4fde-4752-a5af-015210067dad'), (24235, 'ea71c622-0f52-4f2f-a826-3643289d9388'), (26287, '63f9c6c4-165a-4162-bed3-3270588915cb'), (16559, 'e324ef5c-e4c4-4abc-8017-c39eb895edb4'), (6844, '0e9c2f66-6441-471e-a0a1-ee5c92fb996f'), (6845, 'ea50d1e9-df84-49cc-b3c2-d7d2c6f312fe'), (1214, 'c6152586-1c48-41e4-975f-13183809eb4b'), (19646, '6395f7ed-ad70-4ff7-85ca-da08897c72d5'), (6337, '1c3d8092-7d15-4854-8c95-d6d89c63ebcc'), (30917, '963d0459-77e4-4c23-ae6f-2ef4da277f61'), (6343, 'cdbe6a68-4a3b-41d5-a3a1-c4bdf6f677db'), (6346, '97be871b-ee57-495c-bf58-605bb610abd9'), (6347, 'ff784d0d-58ae-482c-935d-fbf1bc3d8a8f'), (6348, 'c6923392-33ba-471c-bcf1-6fd85617b807'), (6358, 'c5b8b78f-d9a9-483d-8f87-70642fe0c191'), (14552, '93e86ad4-e93b-4540-9f9d-93fdea4dbf22'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (6363, '6b16d4ed-0738-4285-b6e3-9a6cb7ae6b79'), (6372, '43614a6b-9dff-4af3-a3b7-8c7c4f47b7fb'), (6373, 'de23d059-4b83-4c17-91f0-a7ef6e2da7dd'), (6374, '006f6db7-c046-47c2-9ed9-964dafff35c6'), (6375, '42a59b0c-15d9-4fab-b8b6-027fb2e2b470'), (23272, '6fccbec4-765f-4aea-9f16-e8840626988f'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (25347, 'b38e0f87-e853-46c1-9fcb-77d766c7bbc9'), (26377, 'b355f1b8-38d7-4007-bbb9-92606f0466d6'), (25355, 'f489b54c-c064-4641-adeb-65d6ce369963'), (25356, '7fc6f9b3-10fd-48c7-95bd-fd7ba0cab29a'), (25358, '3472f3bc-9dbe-46db-b3a7-e45d98d9acff'), (25361, '494455f3-7e50-481f-a8e6-4fa701d05ed0'), (25363, '615a618e-dd22-41bd-b32b-c4138f8c9b71'), (25365, '8c60b393-148a-4a10-8898-ebf74216dd62'), (25366, '11017aba-eb3b-4ab5-9ad5-19c82c9d881c'), (5931, '3f975e05-d4df-4eae-9878-2b89981c48a3'), (19756, '5850f137-5c4a-40b1-826a-95d20e9a906c'), (10543, '595b18a2-b752-4644-b8bf-9f5028af4359'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (20279, '17a9857c-7548-4c1d-a304-d96a32e94553'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (8505, 'fa1d4c6f-2910-4cba-b78a-191386e0bd13'), (29004, '0d480f85-b08e-4a8d-966b-ae0927a13963'), (9555, 'c19b5f80-620b-4a82-82e1-81e843622c70'), (25945, '6f168761-61f4-445d-a9de-fbd0f065cbda'), (16220, 'f492204c-2c6f-4c73-ae64-b3a3fb3b9fa1'), (25950, '9b5fe5dc-37a3-41d7-abf3-4535bc6b26a9'), (8035, '85373154-23b0-4c8d-826b-c39c88e0e81b'), (15718, '0d6a7a26-0a0d-4b56-8718-17e4d0d82549'), (7535, '112382bf-ba59-4a80-bce9-ee6ae2715df4'), (15219, 'dca33206-1724-4483-9fa3-0be9c3f6e19e'), (15220, 'c3e6cdad-e35e-489c-9c5a-0f48a94e1c0f'), (15221, 'cd73d3e1-f6f5-42b1-9fd6-1593cde041b7'), (11639, '06c18124-efc8-4182-bed4-17021f3967b6'), (15226, 'ef67749a-f8e0-4b7d-80df-4b93272004f3'), (8570, '6f60a637-48bf-416e-9912-e30bd7e541e6'), (18812, 'ea5d1385-ddab-4b2f-8e29-7cb24589a233'), (15229, 'd9128601-296a-4048-b339-073b76a365fc'), (15228, '511e53d3-26de-49ea-9af5-149d1dd1600a'), (15240, 'edb9141f-6fce-4432-9e81-b57d35ff490a'), (15241, '108b26fe-34ae-404a-afc5-3f70969f1f81'), (15242, 'aeb5e8ca-b0a9-4bd3-b8b5-6db088e06216'), (15243, '4189372b-3be4-4bca-b934-f8d9d37da251'), (6030, 'b80847b8-58e2-40de-9633-2c67a5dc619d'), (15767, 'b073d9f7-f678-42db-b6e4-9f73738cae55'), (5016, '2c7de3aa-8b01-440b-8b78-5d6d18c84a7d'), (15769, 'a1c468aa-10ab-49c8-b07b-418bdc8f5314'), (5018, '189b3cba-29f1-48ce-a236-436ddf0f8a62'), (17313, 'f9684d2d-c144-4e91-a2ec-f9a0fcdd92fe'), (25000, 'c4dd2159-6af5-43ea-80dd-2affde5c9e7a'), (26046, '2f06519a-dec8-4264-b29f-c2039c5ac40e'), (4549, 'f24ee5b9-be57-409d-b6f9-5aa645d8bb4e'), (4551, 'cdc3ab34-6fd4-483c-b74c-64484eda523a'), (8652, '9d7d1d5e-5e20-482e-be5a-9a048f3dbd8f'), (27605, 'cd1b3318-dc12-4df9-8cba-01f0361dd48c'), (21468, '4a929ab2-ace7-4a93-9676-273b57094c60'), (20447, '87310f99-c045-4f09-b15e-b8efc2843938'), (22499, '186e7c08-bc8c-42e3-92f8-c0884796b602'), (20969, '448efe28-d593-4aab-8a7c-2e26e1a8be54'), (22008, '448290dd-b8c1-4adb-b63d-f19aa5f1e21f'), (14329, '5c25ced2-d397-48f9-b1f2-cc6a3c514d16'), (22011, 'dde393e4-b99c-47e7-82ff-4724c17236f8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Depending on your GPU, you may need to enable `gradient_checkpointing` (⚠️ not supported for the prior model!), `mixed_precision`, and `gradient_accumulation_steps` to help fit the model into memory and to speedup training. You can reduce your memory-usage even more by enabling memory-efficient attention with [xFormers](../optimization/xformers) (version [v0.0.16](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212) fails for training on some GPUs so you may need to install a development version instead).

This guide explores the [train_text_to_image_prior.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py) and the [train_text_to_image_decoder.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py) scripts to help you become more familiar with it, and how you can adapt it for your own use-case.
**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations 
throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For 
an in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).

To enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArguments`]:

```py
training_args = TrainingArguments(
    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
)
```

### Addressing Challenge 3
Flash Attention and enabling gradient checkpointing are required for faster training and reducing VRAM usage to enable fine-tuning and save compute costs. The codebase currently uses monkey patching and the implementation is at [chat_assistant/training/llama_flash_attn_monkey_patch.py](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/llama_flash_attn_monkey_patch.py).

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf) introduces a way to compute exact attention while being faster and memory-efficient by leveraging the knowledge of the memory hierarchy of the underlying hardware/GPUs - The higher the bandwidth/speed of the memory, the smaller its capacity as it becomes more expensive.
```

Alternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-accelerate). 

<Tip>

While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.

</Tip>

## Mixed precision training

**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by 
utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point 
precision (fp32 or float32) to represent and process variables. However, not all variables require this high precision 
level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit 
floating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed 
in half-precision, while some are still in full precision, the approach is called mixed precision training.
These notes were written primarily for the training mode, but they should mostly apply for inference as well. For example, during inference Gradient Checkpointing is a no-op since it is only useful during training. Additionally, we found out that if you are doing a multi-GPU inference and not using [DeepSpeed-Inference](https://www.deepspeed.ai/tutorials/inference-tutorial/), [Accelerate](https://huggingface.co/blog/bloom-inference-pytorch-scripts) should provide a superior performance.


Other quick related performance notes:
- if you are training something from scratch always try to have tensors with shapes that are divisible by 16 (e.g. hidden size). For batch size try divisible by 2 at least. There are [wave and tile quanitization](https://developer.nvidia.com/blog/optimizing-gpu-performance-tensor-cores/) divisibility that is hardware-specific if you want to squeeze even higher performance from your GPUs.


### Activation Checkpointing or Gradient Checkpointing
 
Here is the question: Why is `gradient_checkpointing` important in training Kandinsky 2.2?","`Gradient_checkpointing` is important in training Kandinsky 2.2 because it helps fit the model into memory and speeds up training by saving strategically selected activations throughout the computational graph, so only a fraction of the activations need to be re-computed for the gradients. This reduces memory usage and enables fine-tuning, which helps save compute costs."
What is a core advantage of Hugging Face's CleanRL PPO implementation for reinforcement learning?,"[(6144, '34a43938-8523-4217-9140-ca800a80ae2b'), (10753, '599157ca-e54c-4c41-813e-6b9fb74ccb9f'), (6146, 'd950b985-7fb5-46b2-9b05-0af6419fd745'), (6145, 'a7fbd7de-a494-43e7-a736-ac47818c05d9'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (8211, '4abed52a-3bfe-4342-8128-9fea9c7b4168'), (4627, '7d9b610a-9fa2-470c-b979-a49c56531285'), (23573, '952579f3-7c93-46c5-ae22-ea5e5fd0b30a'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (13868, '7774b25f-7e55-479f-bd68-041a933d6cbf'), (20017, '7f1adcff-7661-482a-83a6-f88d6491fb7b'), (20018, 'd0dd9c49-d5d7-4e72-b765-717912a3ade0'), (8763, 'ebb2b81a-91f6-4bcf-99ee-4a2123350508'), (15938, '868867e0-63f6-4e1c-bf6c-ff6006a5be80'), (15940, '700dacdb-b5b3-49d9-93f9-479d38c003a9'), (6213, '68074c6b-690c-4204-8ebe-35226bbaa495'), (15942, 'cf4f339c-783e-4e9e-8128-267550effc11'), (15941, 'b55be617-b94e-461c-9aef-0ae569761bf9'), (30287, '18b349eb-8e12-454e-90f3-f676addc768d'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (30291, 'a243f586-f6c1-4087-830f-30f59b90fe84'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (21595, '5efa160a-36d2-4b83-aa17-269b32dddfc6'), (30300, 'b7007aea-b66b-47e5-a3c0-e6882e39bc2e'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30301, 'e9564e93-e1bd-4577-8ccc-3dd14eaf934b'), (21609, '96d39b02-625e-4e50-90e5-443032940147'), (17525, '7500c161-c38b-4986-ae2a-ce026b6c10f7'), (23162, 'adfa1aae-7eea-4d74-b807-13110e10c22f'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (27774, '978314d6-ab36-484a-8af5-e7e5eaf4e43b'), (27775, '56108da4-6463-4bb1-8393-42e02e4b2cc9'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (17534, 'e8a226a9-ce5d-412c-b8bb-1a951a1857fb'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (17034, '3900ad5e-267d-4c2f-bc71-54582b7591c6'), (14485, '1b77b214-47a2-46ac-bc11-5f067da8d731'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (21146, '52b3b9bb-9ff0-4cbb-85ea-66902b53e6a6'), (14502, 'a6fde7e9-02c7-4939-96b0-3cc388688a2d'), (22694, 'd986f4e7-7083-4497-a780-384d8b0c76d8'), (22695, '84ad0c76-d7d9-43b7-95a7-c3b740570efe'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (168, '9e9c95af-a155-459e-8f76-20ffd9b487ed'), (8875, '1365f3c9-8b11-4b5d-b6dd-9635cc1c3bdc'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (25775, '69e46490-4327-461a-80ba-48c7dd6a254a'), (25777, '222d6fa5-c0e6-4075-bb20-83f0dfe99f5c'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (25298, '3927b484-b2d8-4332-9789-d423d25be837'), (25299, 'f6c0a00d-d54d-4ae5-a0b9-af2a5eba1630'), (22744, 'c76afd22-789a-4575-b145-c3e7636022ba'), (15577, 'c280daec-1ce3-4351-8f75-58e3899812d5'), (15578, '4d5e8ece-7925-4dcd-8ef9-c0411120c161'), (22753, 'aa3e6a38-c6be-4a8e-a165-49b941acf423'), (10471, '67ff584a-cee5-4967-b1fd-0f425e72ef1c'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (5866, '4108faf4-0e48-4871-8e5b-31983d6c0129'), (23289, '03fb87ea-73f6-47d9-b730-c446a1a5d7a9'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (22802, 'f6d5e291-4153-4f59-997b-41775a8747c5'), (18715, '32f87712-b130-48be-96ba-624ffd4f4957'), (25372, 'fd50adcd-2129-477d-9810-19f85dbeb0bf'), (25373, 'c65cee3a-6cf0-4be0-bc39-e7ac8096a9c1'), (3368, '17715a5e-5f2c-4df5-94da-06ac43fe341f'), (1342, 'b4875f62-8418-4119-832f-2fa04ff8c912'), (10750, '820e2551-3410-4210-a73c-32820cc67dc8'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (19292, 'afb673f6-14ff-4231-907c-7eeeab58a39e'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (19295, 'dfcfe95c-eaad-4cd0-aec1-bbe10747c4e4'), (19296, '5c8e2bcc-b240-4c65-81c9-8a94e6b00fef'), (13155, 'feb24b04-1aed-4866-9b73-320a1d8566b1'), (13156, '36612860-a540-40dd-bc59-f955f3503a60'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (16745, 'bbcf4d12-db52-4214-bd20-fbb0a28f27e1'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (23944, 'fd1a1aa9-c058-47ff-ae6d-f4e2de03a110'), (23945, '7f1ad482-c614-4abc-afff-42b83470deb0'), (8599, 'b9ceb8e8-cbcd-4cd4-b341-73c69d823263'), (3480, 'b1759584-3b23-4843-b034-b5ffaf3d95ff'), (920, 'c038294c-305a-490b-a97a-0c66fd78a464'), (8600, '269c11e4-e332-4535-8735-0da74c456141'), (922, '662531ce-509f-4eca-8e29-21becb7b09ea'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (6066, '711d539a-1706-4a4a-ac5f-d4656fa0711e'), (13758, '99a9e0d3-b693-4445-af36-b42b304b70a2'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (13767, 'd460916f-0557-46c2-a9d3-da2d7989ac41'), (24522, '84973e34-c9c8-4efe-97f2-d3f8efb83196'), (23498, '4c725dcf-74c2-45a9-8efb-757e4fd0c5b6'), (13772, '9460a058-4737-45bd-bd41-81f40b3f366c'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (11747, '9a21d885-48d7-415b-8bd5-694df00cc849'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (21481, '9356a740-eb3f-4eec-975e-bd0b77cbd195'), (10731, 'df44e1e2-7486-4df3-a811-c05bb5e2cdc7'), (10732, '9d553fdc-e142-40ab-abff-8b9a2a7c81cb'), (21483, '5887ee51-0550-47b1-a9c9-bc00327706c9'), (21484, '780e97be-d635-41ff-a9ad-dc48ac21cb80'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6140, '7bac51c6-154a-4a54-a8de-936d68a7b2fe'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd'), (6143, 'd3e6f6dd-dece-4c80-a8e8-de6ba6331451')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Demos and GUIs built with Gradio give the power of machine learning to more and more people because they allow non-technical users to access, use, and give feedback on models. And our acquisition by Hugging Face is the next step in this ongoing journey of accessibility. Hugging Face has already radically democratized machine learning so that any software engineer can use state-of-the-art models with a few lines of code. By working together with Hugging Face, we're taking this even further so that machine learning is accessible to literally anyone with an internet connection and a browser. With Hugging Face, we are going to keep growing Gradio and make it the best way to share your machine learning model with anyone, anywhere 🚀
To reach that bar, as Machine Learning Engineers at Hugging Face we certainly have an unfair advantage sitting in the same (virtual) offices as the 🤗 Transformers and 🤗 Tokenizers maintainers 😬.  We are also extremely lucky for the rich partnerships we have developed through open source collaborations with hardware and cloud vendors like Intel, NVIDIA, Qualcomm, Amazon and Microsoft that enable us to tune our models x infrastructure with the latest hardware optimizations techniques.

If you want to feel the speed on our infrastructure, start a [free trial](https://huggingface.co/pricing) and we’ll get in touch.
If you want to benefit from our experience optimizing inference on your own infrastructure participate in our [🤗 Expert Acceleration Program](https://huggingface.co/support).
--
title: ""Proximal Policy Optimization (PPO)""
thumbnail: /blog/assets/93_deep_rl_ppo/thumbnail.png
authors:
- user: ThomasSimonini
---


# Proximal Policy Optimization (PPO)
<h2>Unit 8, of the <a href=""https://github.com/huggingface/deep-rl-class"">Deep Reinforcement Learning Class with Hugging Face 🤗</a></h2>




⚠️ A **new updated version of this article is available here** 👉 [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus [here.](https://huggingface.co/deep-rl-course/unit0/introduction)*
<img src=""assets/93_deep_rl_ppo/thumbnail.png"" alt=""Thumbnail""/>  

---

⚠️ A **new updated version of this article is available here** 👉 [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)
--
title: ""Introducing Decision Transformers on Hugging Face 🤗""
thumbnail: /blog/assets/58_decision-transformers/thumbnail.jpg
authors:
- user: edbeeching
- user: ThomasSimonini
---

# Introducing Decision Transformers on Hugging Face 🤗


At Hugging Face, we are contributing to the ecosystem for Deep Reinforcement Learning researchers and enthusiasts. Recently, we have integrated Deep RL frameworks such as [Stable-Baselines3](https://github.com/DLR-RM/stable-baselines3). 

And today we are happy to announce that we integrated the [Decision Transformer](https://arxiv.org/abs/2106.01345), an Offline Reinforcement Learning method, into the 🤗 transformers library and the Hugging Face Hub. We have some exciting plans for improving accessibility in the field of Deep RL and we are looking forward to sharing them with you over the coming weeks and months.
This is a complete shift in the Reinforcement Learning paradigm since we use generative trajectory modeling (modeling the joint distribution of the sequence of states, actions, and rewards) to replace conventional RL algorithms. This means that in Decision Transformers, we don’t maximize the return but rather generate a series of future actions that achieve the desired return.

The 🤗 Transformers team integrated the Decision Transformer, an Offline Reinforcement Learning method, into the library as well as the Hugging Face Hub.

## Learn about Decision Transformers

To learn more about Decision Transformers, you should read the blogpost we wrote about it [Introducing Decision Transformers on Hugging Face](https://huggingface.co/blog/decision-transformers)

## Train your first Decision Transformers
 
Here is the question: What is a core advantage of Hugging Face's CleanRL PPO implementation for reinforcement learning?","The context does not mention Hugging Face's CleanRL PPO implementation, so it is not possible to determine the core advantage of it. The context discusses Gradio, machine learning, and Hugging Face's contributions to the ecosystem for Deep Reinforcement Learning researchers and enthusiasts, but it does not provide information about CleanRL PPO implementation."
How does the Unigram tokenization algorithm ensure efficient vocabulary pruning?,"[(19463, '72f624a9-b037-428d-9837-de3484ab1799'), (29704, 'eeb0832a-ecb4-44a7-9fac-ce47c316fd73'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (29706, '238a7175-dfdb-4f6b-be1c-c7d39043352c'), (29707, '100df54f-edea-4f8f-a5a1-01f351b1efb0'), (29708, '57d086cd-c094-4b43-998a-9bf67a10c377'), (29709, 'e3ca7c6b-c286-4bf0-ae83-982df4ff7ff6'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (29703, 'c39eeb1d-4464-46d4-98a9-c31633860860'), (29720, 'bb7495ad-557a-4d33-a926-3938b3e952b2'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (5701, '78378a5e-622b-4073-9502-6b125c315d11'), (5703, 'deacec8b-e008-4941-922b-71bd1841aa71'), (24656, 'a088c20c-8d7d-4bfd-999d-984c49d82451'), (6225, '95dfb88e-25d4-40c5-8d6f-6d3af07e4e0e'), (6226, '496d95c0-9967-4764-94fe-bd01221c5dc4'), (6227, '87c854b5-8830-4132-9d3b-3c9ad5e7c89a'), (6229, '829198af-0f31-4afe-aae8-358ab99a58c7'), (6232, 'd57b5e65-e59a-4e4b-8aa0-7139e4f39886'), (6236, '19dbd2e0-ebb8-4c12-bbe1-aa0a3af3755d'), (6239, '0624cf0f-179e-441a-bf41-454d44f35506'), (6244, '69a8d278-e494-488d-a2f2-d94fc92f2bd2'), (15466, 'bf2582ee-b984-4d41-952a-6497f66b25be'), (15467, '5cc08676-551f-4df5-99d7-18d06b695bd3'), (7789, '206dd674-46b7-4f38-8150-9b15cfd06f87'), (7790, 'a9e5791a-46e7-4308-8134-e24a084a312f'), (7791, '80b0268e-e547-48fb-95d2-622713c6a9db'), (7792, '8d5c7246-3ed2-455b-a56e-538184bdb752'), (7793, '40d93264-9180-45db-b051-ab44da705591'), (6253, 'fa7b9b2e-c67f-4179-b634-54ad01e9f43b'), (7794, '47fc6ab7-bccf-4c40-8b79-a0ebca47c561'), (7796, '673ee2d2-28a6-419c-95fb-ca8cf2d75d9d'), (7795, '54b5e9cb-abbe-4c4f-a09e-9691c4b11923'), (631, 'd8c42629-5fb7-4e48-8de4-57a1a4d10570'), (15485, '9b5dbea1-d5fa-4385-8c03-f268bd24aea8'), (15487, '211f581a-87ba-479c-bbb7-4bf419ff3c38'), (18566, '5d408cd6-4635-4a46-9971-43575951f389'), (18567, '82b5f6ec-fc59-45e3-8d39-a7e17af46f95'), (18568, '3e6b192d-ac77-405e-b509-7c84053c11a8'), (18569, '09e8a18a-b009-4152-bc84-f97c593e35df'), (6800, 'ee670cef-b763-4a5c-9337-23b201f5aa8c'), (18585, '8632ca1c-da27-4f58-a21f-32139490a267'), (18587, '84bdfa2d-cdcf-45f8-9f0a-8ff54b2af410'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (9377, '11462af2-125e-4c1a-91d0-9dba82209e93'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (19621, '2100eee7-6ff0-4550-a16b-6538c747e693'), (7341, 'b601a33f-7e1f-42fd-90b5-1a99b5987075'), (7342, '4e3f892e-b398-4de7-91ee-ce188a823d68'), (24764, '645657c8-1e5b-415a-9175-a7b48fbed112'), (10455, 'ac623491-7926-46e8-8a45-4c350813dacd'), (10456, 'ecdba708-8f55-48d6-86ee-2ec46b1e2778'), (4327, '906c0c93-5ef3-428b-ae43-ee8fb9df0863'), (4329, 'ac5e90cc-b01b-40c1-badc-1a3d5707cf22'), (4330, '864c4298-108e-4916-b63a-d577039833cb'), (22282, 'b997169b-2f07-43ee-a8cb-781f57c0adc1'), (22286, '8d1ee09a-caeb-4449-b412-ad1bd7a94182'), (22288, '26e486c2-43b7-4f99-9d2d-024d2618471d'), (22289, '9e4402d9-ecfe-45c9-bb4e-7e0c7afbfa12'), (22290, 'effab2b0-1b81-4b47-9d10-d1f7245ceb5c'), (22300, '32418488-7979-4eb3-8192-83f21c3a051b'), (22302, 'dad51bd3-e9f8-4ed2-82b1-59534adcc14f'), (11076, 'c68b6b40-c9ad-4e20-abf8-4fe2db7c0f3e'), (11077, '3eba11d4-e601-4b6a-b0f9-0fb7ecced3d3'), (24902, '21707633-c150-463b-a123-71a45823b814'), (16723, 'd4f337a3-994a-44ba-9a0e-70bf73500ee2'), (18260, '20b06811-c14a-4c0e-b429-99bce86da710'), (16725, '167730fe-e8e3-492c-a3d4-99715b149522'), (16726, 'c0e0ead7-7fb9-4e1a-a8f4-41c13bb96ae1'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (16724, 'bcd7e1a3-4be5-4f08-9af9-feef218c2d7c'), (857, '4a58c58e-011b-4c6a-9024-25c07d7755eb'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (16730, '4698f6a9-10ed-4b63-8a16-86a39a5de935'), (862, '58dce80f-7a13-41a9-8271-89fa056172d9'), (863, '1c31de44-0abf-41d6-96cd-508363daa9a7'), (864, '5920912b-08ae-4795-a60c-d67481685163'), (16737, '4a37b1c8-d2ed-48a2-a3e8-84f2a1e735c3'), (16738, '33cdddd8-07e1-4371-8c3e-b9f64f9afc3c'), (865, 'bce264f6-c384-4db2-96e7-e1b9d03c8c83'), (16740, '567a8c57-7015-46ae-87bb-d7aef479b1ca'), (16736, '14094fc4-1dfe-4a5b-8595-d13531fb8d73'), (16739, '2155e313-e0b0-4dfa-b90d-3dda854127c2'), (1414, '9772c7cd-c19a-4ee9-abc0-bd27da5ee279'), (23430, '85213444-0ee5-42eb-947a-16b49dbe2d8c'), (1418, 'dcc80b3f-66e8-4b4b-9018-7cd3c8bbea59'), (15248, 'ca77aef7-6039-4c5b-80b1-c76f83ab3f28'), (15251, '12885ed4-98da-492a-a72b-67027a02f6a8'), (27545, 'c9fb2425-2ac4-4707-88ad-e11ddfbab842'), (19871, '0a993e9c-5a70-49a8-8b8a-ea79fff01d49'), (29610, 'ac1535f9-21ba-44dd-a66f-e962faf6b40f'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (29612, 'ae39f1ab-1b94-4882-b706-1e93f0941895'), (31149, 'debd5c00-01da-49e4-9fac-acfd4e6bcd78'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (29615, '2275ba32-0f01-4d0c-990d-ceaf68cba2af'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31153, 'e4910d18-2062-476e-95cf-13e2844c2ada'), (31154, 'e49097a1-9b93-492c-9e9c-343a02a27e52'), (31155, '4ce7c97c-6ce1-4af5-976f-458bbfed6369'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (29616, '5f1337a0-af97-4b1c-8e08-b3cdfd401241'), (29617, 'ff9c2445-4c03-4813-a426-7902846a33df'), (431, '063b8bbb-e761-4492-b59d-d3f01a457327'), (17362, 'd7daabea-fea0-430b-a3ee-3038eed2cbb8'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c'), (29611, '3c70029e-ab2c-422a-81de-13783746fec2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: n this video, we will study together ""the Unigram Language Model subword tokenization algorithm"".

The overall training strategy of a Unigram LM tokenizer is to start with a very large vocabulary and then to remove tokens at each iteration until we reach the desired size. At each iteration, we will calculate a loss on our training corpus thanks to the Unigram model. As the loss calculation depends on the available vocabulary, we can use it to choose how to reduce the vocabulary. So we look at the evolution of the loss by removing in turn each token from the vocabulary. We will choose to remove the p percents which increase the loss the less.
At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training
data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm
computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then
removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, *i.e.* those
symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has
reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.

Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of
tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:
have our initial vocabulary. The training of the Unigram tokenizer is based on the Expectation-Maximization method: at each iteration. We estimate the probabilities of the tokens of the vocabulary. Then we remove the p percent of tokens that minimize the loss on the corpus and which do not belong to the basic characters as we want to keep in our final vocabulary the basic characters to be able to tokenize any word. Let's go for it! The probability of a token is simply estimated by the number of appearance of this token in our training corpus divided by the total number of appearance of all the tokens. We could use this vocabulary to tokenize our words according to the unigram model. We will do it together to understand two things: how we tokenize a word with a Unigram model and how the loss is calculated on our corpus. The Unigram LM tokenization of our text ""Hug"" will be the one with the highest probability of occurrence according to our Unigram model. To find it, the simplest way to
<Question
	choices={[
		{
			text: ""Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules."",
			explain: ""No, that's the approach taken by a different tokenization algorithm.""
		},
		{
			text: ""Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram adapts its vocabulary by keeping the most frequent subwords."",
			explain: ""No, this incorrect.""
		},
		{
			text: ""Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules."",
</Tip>

## Training algorithm[[training-algorithm]]

Compared to BPE and WordPiece, Unigram works in the other direction: it starts from a big vocabulary and removes tokens from it until it reaches the desired vocabulary size. There are several options to use to build that base vocabulary: we can take the most common substrings in pre-tokenized words, for instance, or apply BPE on the initial corpus with a large vocabulary size.

At each step of the training, the Unigram algorithm computes a loss over the corpus given the current vocabulary. Then, for each symbol in the vocabulary, the algorithm computes how much the overall loss would increase if the symbol was removed, and looks for the symbols that would increase it the least. Those symbols have a lower effect on the overall loss over the corpus, so in a sense they are ""less needed"" and are the best candidates for removal.
 
Here is the question: How does the Unigram tokenization algorithm ensure efficient vocabulary pruning?","The Unigram tokenization algorithm ensures efficient vocabulary pruning by calculating a loss on the training corpus at each iteration, and then removing p percent of the tokens that increase the loss the least, i.e., those tokens that least affect the overall loss over the training data."
Why is the CLIP tokenizer critical in Kandinsky 2.2 training workflows?,"[(19463, '72f624a9-b037-428d-9837-de3484ab1799'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (23051, 'd0b37d24-40ce-4441-9f5c-56c750734c66'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (7703, 'bdbbf91e-5be0-4de8-b791-c7388d247942'), (6177, '9a6af5e6-9ab0-4924-a9e2-817c0c8f0901'), (20006, 'd78b8574-2f4c-4cfd-9142-1c1ed6cec36e'), (18998, '989e75dd-dc43-4eb5-91cb-215c4e4adc17'), (18999, '3a0a5a40-c022-4462-8532-707c6ae35820'), (19004, '112963e4-e074-4f17-aa4e-d0a1114feec3'), (19005, '08ceddf6-2b38-43dc-9c7d-c85e39600727'), (24641, '4e34563d-b7fc-47a6-818a-27c23fb158d9'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (19016, '02c05bd4-cf3d-4ffc-a737-bb651f95a396'), (19017, '63065524-2b10-49ad-b174-3dfd6a6e17e4'), (10825, '5ab44007-ce9e-4e86-a754-a03638ace665'), (24656, 'a088c20c-8d7d-4bfd-999d-984c49d82451'), (5718, 'e5c1a399-0835-4635-841b-8d00c339e0ac'), (8794, '760b9795-eadb-4fbb-8458-6bcb928322ee'), (620, '607969d7-62ff-48ba-9157-ee5fc510a46a'), (621, 'c4e9c934-01ff-495e-aa40-d550ec55813d'), (15472, '4d338f4f-4c7c-42d2-b3ed-a908af01e56d'), (626, 'beb0b4ef-6aa8-4f19-90f6-06a202ff082e'), (8820, '9b2d46af-dfec-4ae6-979f-15ebab2b9537'), (631, 'd8c42629-5fb7-4e48-8de4-57a1a4d10570'), (8314, 'b04a31e0-43f4-41a4-89cd-7b20c2fee1a8'), (635, '69881c4f-750b-499f-8624-576717e6c74f'), (634, '06c1ab81-2669-4e9d-b9da-0defc2a88a4f'), (8317, 'd76ee402-2ca8-411e-a02b-82eaa8267bb6'), (7806, '6d7638e5-caf5-4427-a7e6-a138c90c7e9c'), (639, '326d2897-a47b-4ab1-8c8b-a8afb1e4c1d9'), (7809, '173bc460-5338-4f7c-9b69-916e9e03ecfb'), (7814, '59fc1afc-32d4-4401-b7a7-f31ff5bf1424'), (7815, 'ffedd8d3-edcc-43cc-9cec-a604d063dc3e'), (10895, '7eb747de-916c-4162-ad00-34e215fde4ba'), (9364, 'b5cbad99-e88b-416c-8ca0-8c0539a759f9'), (7341, 'b601a33f-7e1f-42fd-90b5-1a99b5987075'), (11953, 'e5b47149-4a3e-4549-a9ef-c12eccfe493f'), (690, 'b6bf8779-7ba4-44e2-956d-16eaea7da823'), (693, '665e9332-4430-489e-943a-5bf43382f5ae'), (17592, '48ede2f2-438c-4cd3-b958-b20d71e0b9aa'), (24764, '645657c8-1e5b-415a-9175-a7b48fbed112'), (24765, '76edad45-cc47-4e23-9f0a-2e08d27696e9'), (26310, '42880a29-1a9a-4f47-b818-4c8ec4ebebc7'), (17616, 'a59fd552-bc52-4d0c-9aba-aefd3b3839f5'), (17619, '3dea2748-b71a-47bb-bdc9-115fef327df9'), (10455, 'ac623491-7926-46e8-8a45-4c350813dacd'), (728, '6ffb6313-3413-4a18-b2a0-023eb2aaa3cf'), (10457, '34cff3a6-1baa-4a3d-a758-49c4d21bffe5'), (16090, 'de34769a-90f0-4c82-b82e-6d9a5a0e6f92'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (10460, 'ace98882-3633-40c7-b823-208bda3a4ca1'), (30952, '8a52d3e7-dc7b-4e41-8b5f-65783b57c944'), (17654, 'd7e02c73-7e96-4589-8012-581207a7076a'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (25347, 'b38e0f87-e853-46c1-9fcb-77d766c7bbc9'), (29447, '4a05a595-b890-4362-8128-3af638520b31'), (11016, 'e4b1b7b7-839c-4405-86ab-e31e02c670b7'), (25356, '7fc6f9b3-10fd-48c7-95bd-fd7ba0cab29a'), (27407, 'cc6b271a-4d91-452a-b62b-5123e6bd61d7'), (28953, '6e1ad46f-6cdb-4457-8fc1-a17e2442ec7d'), (22300, '32418488-7979-4eb3-8192-83f21c3a051b'), (18225, '2e3f4c95-3c13-4fd1-a241-b6bea2e9cf65'), (7474, 'c46756cf-62a1-4fd4-913c-051f6e89f2d5'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (24378, '134c9776-8b50-4f81-930b-31fd36ec89f9'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (24385, '4c762cd0-b53c-4696-a0c2-a4b7801462b6'), (24386, '6b682f9a-1ab3-4d5f-9f0b-cf01bdc048d0'), (24387, 'e78b85c6-e8ae-4879-88f0-026ddeb290f7'), (24901, '20ed6577-9065-4b17-a364-b1463389e837'), (24902, '21707633-c150-463b-a123-71a45823b814'), (24903, 'a4cedab8-3717-4d08-b41b-f6159b58e741'), (24904, '0d263af5-23ad-4f96-a3b0-81bee9b6bd23'), (29518, '4cc923b4-5b7a-4f75-aa02-6c9a08dfdd03'), (16720, '6f64e88f-35dc-46dc-9289-15572ee0760a'), (9555, 'c19b5f80-620b-4a82-82e1-81e843622c70'), (3414, 'e29f0290-b0ea-452f-ba73-2e02a14207d8'), (857, '4a58c58e-011b-4c6a-9024-25c07d7755eb'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (3420, '76ebd96a-be31-4cd0-a3e8-fef929c19287'), (24937, '5ab51995-d45a-44ae-8339-d077a7afb102'), (24938, '20f0ba7e-1fcc-4eff-9442-3378b6a2b8c7'), (16235, '9568dd44-a9d7-44da-ad65-b915ae6fb4e1'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (15219, 'dca33206-1724-4483-9fa3-0be9c3f6e19e'), (15220, 'c3e6cdad-e35e-489c-9c5a-0f48a94e1c0f'), (24948, 'd300d5a7-8cbe-4743-9d77-61b75bb27316'), (24950, '2c98f384-3771-4860-8a77-6909d68ce78c'), (15227, '783d1508-a579-481d-b6e6-850a89fbabd7'), (15228, '511e53d3-26de-49ea-9af5-149d1dd1600a'), (15229, 'd9128601-296a-4048-b339-073b76a365fc'), (15230, 'c4948062-4df0-4d3e-ac44-2b700dd82ff2'), (901, 'f064c51b-00f5-4a7f-9992-c3711c8b265f'), (1414, '9772c7cd-c19a-4ee9-abc0-bd27da5ee279'), (23430, '85213444-0ee5-42eb-947a-16b49dbe2d8c'), (9609, 'd28b28d9-b489-458d-b6f2-5714b26e92a4'), (1417, '28953c68-f717-4112-87e7-af7f64ba7dc2'), (1423, 'bf41377e-0913-4528-8a14-d06bc0f4340e'), (15248, 'ca77aef7-6039-4c5b-80b1-c76f83ab3f28'), (22928, 'a8321404-e3d8-420e-b234-a78cf2bdc969'), (1424, '7da8fa90-541d-4a40-831d-ce0c019039d4'), (401, '18f06e81-3edb-4349-a6e5-ec6b1001817a'), (15251, '12885ed4-98da-492a-a72b-67027a02f6a8'), (27545, 'c9fb2425-2ac4-4707-88ad-e11ddfbab842'), (27546, '837105a9-01c9-4d5a-95d6-0812b7fb9c20'), (11676, 'a9cfe46a-9e1d-4ad1-9d16-634aeac45374'), (11677, 'b65b3085-bff1-4fe2-9cbe-69e92c3410b8'), (19871, '0a993e9c-5a70-49a8-8b8a-ea79fff01d49'), (31137, 'e5409b25-c834-46d6-96a1-5bcf6d1e6167'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (31142, '082ab158-87a0-4e61-a8d0-1d84aea36fce'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (5044, '3347ade6-a88b-4653-97f6-3b4beff0fa0c'), (20966, '61420da0-bea9-4ed4-8b90-0716f6b0a31d'), (20967, '9c9e7c2c-88b4-4ef0-9fd3-fbfea2d90e56'), (20969, '448efe28-d593-4aab-8a7c-2e26e1a8be54'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c'), (16375, '8c772737-ffa5-455e-b995-a3218ac3f244'), (16376, '80a55471-5b23-44df-944e-f7eb8aa355f3'), (14329, '5c25ced2-d397-48f9-b1f2-cc6a3c514d16'), (16379, '21189481-2c00-47b3-b535-eb24cf4443c0')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Training script

The training script is also similar to the [Text-to-image](text2image#training-script) training guide, but it's been modified to support training the prior and decoder models. This guide focuses on the code that is unique to the Kandinsky 2.2 training scripts.

<hfoptions id=""script"">
<hfoption id=""prior model"">

The [`main()`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L441) function contains the code for preparing the dataset and training the model.

One of the main differences you'll notice right away is that the training script also loads a [`~transformers.CLIPImageProcessor`] - in addition to a scheduler and tokenizer - for preprocessing images and a [`~transformers.CLIPVisionModelWithProjection`] model for encoding the images:
The description from it's GitHub page is:

*Kandinsky 2.2 brings substantial improvements upon its predecessor, Kandinsky 2.1, by introducing a new, more powerful image encoder - CLIP-ViT-G and the ControlNet support. The switch to CLIP-ViT-G as the image encoder significantly increases the model's capability to generate more aesthetic pictures and better understand text, thus enhancing the model's overall performance. The addition of the ControlNet mechanism allows the model to effectively control the process of generating images. This leads to more accurate and visually appealing outputs and opens new possibilities for text-guided image manipulation.*

The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).

<Tip>

Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.

</Tip>

<Tip>
The Kandinsky models are a series of multilingual text-to-image generation models. The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those results for the UNet.

[Kandinsky 2.1](../api/pipelines/kandinsky) changes the architecture to include an image prior model ([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)) to generate a mapping between text and image embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during training, leading to higher quality results. Finally, Kandinsky 2.1 uses a [Modulating Quantized Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.
The description from it's GitHub page is:

*Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas. As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.*

The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).

<Tip>

Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.

</Tip>

<Tip>
This script is experimental, and it's easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset.

</Tip>

Kandinsky 2.2 is a multilingual text-to-image model capable of producing more photorealistic images. The model includes an image prior model for creating image embeddings from text prompts, and a decoder model that generates images based on the prior model's embeddings. That's why you'll find two separate scripts in Diffusers for Kandinsky 2.2, one for training the prior model and one for training the decoder model. You can train both models separately, but to get the best results, you should train both the prior and decoder models.
 
Here is the question: Why is the CLIP tokenizer critical in Kandinsky 2.2 training workflows?","The context does not explicitly state why the CLIP tokenizer is critical in Kandinsky 2.2 training workflows. In fact, the CLIP tokenizer is not even mentioned in the provided context. The context only mentions the CLIPImageProcessor and CLIPVisionModelWithProjection, but not the tokenizer. Therefore, it is not possible to provide an answer to this question based on the provided context."
What is the purpose of the `Gallery` component in Gradio applications?,"[(28161, '6a39374d-d9fd-4229-8372-ce6d65514142'), (28162, '2fe3c2a3-a327-4912-ada2-190568c660fb'), (4103, '69742828-77bd-4856-a8aa-a0025c5d7383'), (10261, '889896bb-eb69-46fb-b043-075142527346'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (28200, 'bbcfc1e5-a100-4dd8-8b71-33a035bf6391'), (16426, '5b4e1e59-8d4c-41c6-86bd-80663aaba033'), (3119, '6a16877a-d058-484e-adf4-1e3130b324ae'), (16433, '3a6c1f63-b6aa-4990-b403-244dc7863130'), (11829, '38bd3f56-e5b3-4e10-8cb9-2c02e7602038'), (28216, '5ef0c5f2-e9f3-49a4-9bda-40af0f54e3d0'), (28217, '17a071b0-182d-464b-89b7-d5c2cbb45575'), (28236, 'ff029be9-b5d9-47d9-8b0a-e79b53b6a8ea'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (2126, 'eaad6c83-eff0-4a22-b860-506e3447e15a'), (28250, '31ec90bb-9213-4d5b-9a3c-7fd0daac0f7d'), (2140, '815963a3-04df-4294-814c-267a55fd68f5'), (2143, '6e81dc66-340f-4578-890b-b72e0440f793'), (28255, 'bcb424b2-729f-4991-87ac-45ef5ce77853'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (2156, '6157cc0e-af2d-44bc-8eda-72c429d67eb3'), (2157, '5a6cd955-3f01-4adf-b73e-3426fa8ab135'), (29293, 'da6b9a74-0f26-4025-adbf-4503df1b77fc'), (28272, '8f0d48b9-997f-408d-a152-f00ba14dabd5'), (2160, 'aab3ec8f-f715-464b-9c91-039450b42ea1'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (2177, 'e4f20765-678d-400f-acda-3fd9e844e57d'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (2190, '94082365-4697-4dc3-b51d-a9849ccaa824'), (7823, '341c1953-c6b2-408b-9040-3ce33e764469'), (27793, 'e0cc82b3-f860-4440-b597-0466bf024466'), (27811, '6351e475-b7a2-4ee3-88f7-ce204ff277fb'), (6820, '296a6e9a-0b6c-450c-8983-f33bcfc0a165'), (3756, '1a1f79f6-d46a-4043-97ae-61320836b7ce'), (3760, '4f7d33f2-8ea6-4d46-826d-95c7530db102'), (30395, 'ab5263c1-7c85-43ee-bd27-89e9944fe67a'), (2245, '47834d58-902b-4e38-853e-427121d538ab'), (2246, 'b2714547-7d53-41a2-ade0-0dc1989a2476'), (2247, 'e9c8dd04-774e-481f-a3fe-757c0a391f09'), (27849, '32372b5e-e02d-4105-9c4b-edb2719e80e0'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (2251, '49c92cce-066a-48de-9511-6f677fedd988'), (27852, '81090f0f-86b7-4c58-9fe0-a796307b1748'), (2257, 'a342551c-d0e3-4d70-857a-23e1e82ecc87'), (2263, 'de23a944-32ae-4aca-9342-ed441d8275b4'), (27864, 'bb7de493-b7e0-4a2e-9e87-b00fbed9325c'), (27360, 'b3f91b27-9fab-492f-b8f6-e22b380d3729'), (2287, '6b30a70b-fcb5-42bf-a607-1f8aaa309ffb'), (2288, 'c0ef52c7-99ed-4694-93ae-6c412b9f86a1'), (21743, '093f3b5e-006e-467e-b1c9-5120230b2150'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (23796, '851ba12c-6554-4982-8c9f-afe1d6deceff'), (21754, '87b73ba0-88c4-4302-9951-5e5c45977bc5'), (27913, '1a5af7ec-5880-4d95-9c6f-bfa4aa6121ae'), (26889, '739be90a-f89d-445a-be17-3ab3af2a79d6'), (4370, 'c87542a1-24d3-4ef2-955b-8b5e17ff37f8'), (4371, 'ee717cdd-8985-41e6-b386-5a6da655ddaf'), (2326, '752544ae-9d35-40cb-8f9b-057e92b1d7fe'), (30999, '53998aea-d5c1-4b05-87c7-09e519c96443'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (2342, '865c895b-34a5-453b-b76f-f5c444c3b2df'), (2343, '967189ff-4b5c-4875-a56e-984caf1351ee'), (6954, 'dc0a4a82-5286-4d59-ac85-fe642c49151e'), (6955, '31ad389f-36a4-4c9d-a5c1-249c19c98870'), (2362, '1cfbdc0e-9682-4844-b541-ff932ae3ea3a'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (2376, '7809bd4a-1a54-4abb-bb3f-068d12f9f4d2'), (2381, '25022ddc-db5e-48bf-a2bf-918a00646b76'), (2398, '674a4fbf-0ede-489e-a689-aedd65e6317a'), (28000, '14c388ba-8b8d-4268-8474-f3a833f333f1'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (28014, '3af9249f-4f93-4dbf-b269-27fa4aa9e315'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (28017, '29c87561-6f4c-4afc-adfd-b3054e4a4dad'), (19823, '4a9e8566-d1fc-4875-a3a6-70e098a84848'), (31611, 'c7bf921e-2d4f-44a9-bee9-1e5953f19748'), (31612, 'c6e810cc-92c8-4e66-9d4a-8ae615c991f9'), (28030, '96755a43-0fc9-461f-a1ae-8a79bed496cc'), (1919, 'c64e4f18-2b7f-4516-8a23-1060fd924eba'), (28031, '60a84144-14dc-4540-8855-3d816f1171e8'), (1937, '9e067ec3-9c36-4419-9560-eb68ac22179f'), (28051, '2e9d8676-4090-4ac4-ac08-acae65f2d32d'), (28064, 'a61145c1-f74b-4c7e-8efd-610db699307a'), (16800, '2e36121d-c993-4b67-8fd9-84fa17182afa'), (3493, '8a1040ce-1b60-4e24-90e9-cb4e7127b718'), (20394, '5ee7db72-d4a7-43e0-9cc0-f5da6aab5d5c'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (1975, 'a3373798-7376-4321-b2f0-d9eacd62e631'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (1978, 'e6aa00b1-e118-4a20-9aea-377dc102cb43'), (23484, 'a9b50d64-9255-4dee-a7c6-eb65f2c88b82'), (1990, '2e21c9d1-f37e-4ff5-b83e-a69e36441d2c'), (11729, '4691185d-26b0-403c-a3f3-6d2bc95ea285'), (11733, '27aacaae-aca4-411c-8fb2-30313de2897d'), (28119, '1dcccb98-51b9-49bd-81ec-c5d5c62bea87'), (28120, '0e15845c-8278-4ae1-8b41-403e6698afe5'), (28121, 'f47bdf19-919c-4645-96d7-5bde219f7556'), (11739, '0502e8df-e803-41dc-a2c1-535de5a0ac5e'), (11740, '005c72e8-ed81-43e5-93d7-fc3a05cec140'), (28125, '24144982-b2b1-4f85-a299-ae52b3b80ed5'), (11745, '10784898-608a-4cde-8419-5358ef3928ae'), (11746, 'c1d1e06f-bcd1-4a21-95d5-aacec2df63fc'), (28131, '974a4165-1698-418f-90a1-f8e6e7cbb425'), (28137, '5116f0d1-8fcf-4fed-971e-06051bd8b4b1'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (3566, 'dc14161e-e267-41ea-a4c3-0189e9da1c9a'), (2039, 'a483702a-3d32-49dc-8a1b-cddefa16a85a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The Gradio library provides several components for building Computer Vision applications on  Spaces such as [Video](https://gradio.app/docs/#video), [Gallery](https://gradio.app/docs/#gallery), and [Model3D](https://gradio.app/docs/#model3d). The community has been hard at work building some amazing Computer Vision applications that are powered by Spaces:
Similarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).

Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it.

![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)

In this image, the following preprocessing steps happen to send the data from the browser to your function:
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
- Speeds up Gallery component by using temporary files instead of base64 representation in the front-end by [@proxyphi](https://github.com/proxyphi), [@pngwn](https://github.com/pngwn), and [@abidlabs](https://github.com/abidlabs) in [PR 2265](https://github.com/gradio-app/gradio/pull/2265)
- Fixed some embedded demos in the guides by not loading the gradio web component in some guides by [@freddyaboulton](https://github.com/freddyaboulton) in [PR 2403](https://github.com/gradio-app/gradio/pull/2403)
- When an `Image` component is set to `source=""upload""`, it is now possible to drag and drop and image to replace a previously uploaded image by [@pngwn](https://github.com/pngwn) in [PR 2400](https://github.com/gradio-app/gradio/pull/2410)
- Improve documentation of the `Blocks.load()` event by [@abidlabs](https://github.com/abidlabs) in [PR 2413](https://github.com/gradio-app/gradio/pull/2413)
## Components

Gradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.

Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[""webcam"", ""upload""]`).

**Static and Interactive Components**
 
Here is the question: What is the purpose of the `Gallery` component in Gradio applications?",The purpose of the `Gallery` component in Gradio applications is to display a gallery of images.
What is the primary purpose of `gradient_checkpointing` in Kandinsky 2.2 training?,"[(516, 'ce8c02d3-fb1b-4e21-b424-921eb9a2f16e'), (26118, '54e056e0-c70d-4b7f-a71b-a0cda0567d61'), (1546, 'b8d3e1d7-0d8d-4330-9634-50dc3fe25b5f'), (21008, '39722254-f2af-4404-8ad6-818011b35374'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (15910, '82663015-b00f-462e-b5e9-265af9160dfd'), (15911, 'dea3e203-7c16-410e-9667-1ee9ffb39f00'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (26673, 'acb0c731-b7d9-4884-8760-09bd1882d5a6'), (21556, '4dbfc5e1-05b5-4a57-88db-428197509bbe'), (30262, 'cb2921cb-3f58-4bd3-aa73-00e1462b10c4'), (26168, '930f8d18-7bc8-4f68-92f0-d815baeefbbe'), (23613, 'db5c23f3-3d63-47e5-aeb1-d0040ea605d6'), (5694, 'dee73012-4ba6-4015-8099-1ff6348fc9b2'), (14398, 'b76f21ff-7ece-400d-b70c-3cde162ac19b'), (23615, '0beada75-63f1-42b8-9ef7-1a73dbe17485'), (14397, 'c13b3dcf-abd9-47ba-af8f-be1c2fd14508'), (30794, '60649a7f-802b-4687-8b03-4c75a01eca78'), (12883, 'dfcf0173-7e8f-4f19-bb06-7c54d24fff66'), (4693, 'a69e9047-4018-4d6e-9ab6-7734049c49fb'), (10326, 'bf75e85e-bc28-462e-a306-a52929809f83'), (12886, '18488b88-987f-476e-9eb1-02c4211c5a10'), (10330, '6dfbf5a1-fc7e-4d69-9cc3-a54c7658ea1b'), (10331, '3e81c36c-c486-497f-8503-f7e56fb08165'), (10332, 'a0e48a36-723f-4add-b319-aff6ed8392a6'), (25698, '8985c103-fde3-4a20-b9db-329d63723708'), (24163, '6d254de1-aa08-4a87-9094-64313c3a0c4b'), (25188, 'bf31bc87-0988-43d1-bf36-d47a1e56fd34'), (27238, '39b2c1e0-dff0-4879-9bef-52ae8488417e'), (4711, '07cf85f1-f8c8-45a0-b469-b6f11a42face'), (21607, 'b8f10d9a-6a0f-4bc2-9750-2fbc7550343f'), (24169, 'c022deee-9096-4e1a-9834-d3e8c60ba81c'), (4204, 'eb9e94d4-02e1-461e-b271-6c8438d90b46'), (5750, '0d38d4b8-b137-4fc7-9983-bba8ce240e1e'), (23684, '5278bf06-a69b-482f-a31f-a9e84563cc75'), (23686, 'b6be9f72-5560-45bc-9531-11fb72d1dd58'), (21128, '1b0739a8-867b-4a25-bf9e-966b397ff990'), (17034, '3900ad5e-267d-4c2f-bc71-54582b7591c6'), (28816, '9822c0bf-8b6f-4cd6-bb3c-e14660afe95f'), (659, '77f9993f-20a6-4964-b925-7de3be25efa8'), (15005, 'fd443fc4-2728-47dd-a5b5-dc1422eb46ce'), (1698, '584e76a5-4fde-4752-a5af-015210067dad'), (24235, 'ea71c622-0f52-4f2f-a826-3643289d9388'), (13483, 'be850dca-b3e8-427f-aaa7-cb450c7130c7'), (26287, '63f9c6c4-165a-4162-bed3-3270588915cb'), (16559, 'e324ef5c-e4c4-4abc-8017-c39eb895edb4'), (6844, '0e9c2f66-6441-471e-a0a1-ee5c92fb996f'), (6845, 'ea50d1e9-df84-49cc-b3c2-d7d2c6f312fe'), (1214, 'c6152586-1c48-41e4-975f-13183809eb4b'), (6337, '1c3d8092-7d15-4854-8c95-d6d89c63ebcc'), (30917, '963d0459-77e4-4c23-ae6f-2ef4da277f61'), (6343, 'cdbe6a68-4a3b-41d5-a3a1-c4bdf6f677db'), (6346, '97be871b-ee57-495c-bf58-605bb610abd9'), (6347, 'ff784d0d-58ae-482c-935d-fbf1bc3d8a8f'), (6348, 'c6923392-33ba-471c-bcf1-6fd85617b807'), (14552, '93e86ad4-e93b-4540-9f9d-93fdea4dbf22'), (21212, '7a3079ef-6251-48dd-9a6a-15850c026bda'), (6372, '43614a6b-9dff-4af3-a3b7-8c7c4f47b7fb'), (6373, 'de23d059-4b83-4c17-91f0-a7ef6e2da7dd'), (6374, '006f6db7-c046-47c2-9ed9-964dafff35c6'), (6375, '42a59b0c-15d9-4fab-b8b6-027fb2e2b470'), (23272, '6fccbec4-765f-4aea-9f16-e8840626988f'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (25347, 'b38e0f87-e853-46c1-9fcb-77d766c7bbc9'), (26377, 'b355f1b8-38d7-4007-bbb9-92606f0466d6'), (25355, 'f489b54c-c064-4641-adeb-65d6ce369963'), (25356, '7fc6f9b3-10fd-48c7-95bd-fd7ba0cab29a'), (25358, '3472f3bc-9dbe-46db-b3a7-e45d98d9acff'), (25361, '494455f3-7e50-481f-a8e6-4fa701d05ed0'), (25365, '8c60b393-148a-4a10-8898-ebf74216dd62'), (25366, '11017aba-eb3b-4ab5-9ad5-19c82c9d881c'), (30501, '71f1ef82-8aa3-4f97-b069-0fc3ab301bbf'), (5931, '3f975e05-d4df-4eae-9878-2b89981c48a3'), (19756, '5850f137-5c4a-40b1-826a-95d20e9a906c'), (10543, '595b18a2-b752-4644-b8bf-9f5028af4359'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (20279, '17a9857c-7548-4c1d-a304-d96a32e94553'), (8505, 'fa1d4c6f-2910-4cba-b78a-191386e0bd13'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (29004, '0d480f85-b08e-4a8d-966b-ae0927a13963'), (9555, 'c19b5f80-620b-4a82-82e1-81e843622c70'), (25945, '6f168761-61f4-445d-a9de-fbd0f065cbda'), (25950, '9b5fe5dc-37a3-41d7-abf3-4535bc6b26a9'), (15718, '0d6a7a26-0a0d-4b56-8718-17e4d0d82549'), (7535, '112382bf-ba59-4a80-bce9-ee6ae2715df4'), (20337, '1ac41f0f-b53f-41f2-adc7-29016e775aba'), (15219, 'dca33206-1724-4483-9fa3-0be9c3f6e19e'), (15220, 'c3e6cdad-e35e-489c-9c5a-0f48a94e1c0f'), (15221, 'cd73d3e1-f6f5-42b1-9fd6-1593cde041b7'), (11639, '06c18124-efc8-4182-bed4-17021f3967b6'), (15226, 'ef67749a-f8e0-4b7d-80df-4b93272004f3'), (8570, '6f60a637-48bf-416e-9912-e30bd7e541e6'), (18812, 'ea5d1385-ddab-4b2f-8e29-7cb24589a233'), (15229, 'd9128601-296a-4048-b339-073b76a365fc'), (15241, '108b26fe-34ae-404a-afc5-3f70969f1f81'), (15242, 'aeb5e8ca-b0a9-4bd3-b8b5-6db088e06216'), (6030, 'b80847b8-58e2-40de-9633-2c67a5dc619d'), (15767, 'b073d9f7-f678-42db-b6e4-9f73738cae55'), (11672, '5cb801f9-b79f-4264-b02e-8146fcbc6268'), (5016, '2c7de3aa-8b01-440b-8b78-5d6d18c84a7d'), (15769, 'a1c468aa-10ab-49c8-b07b-418bdc8f5314'), (5018, '189b3cba-29f1-48ce-a236-436ddf0f8a62'), (17313, 'f9684d2d-c144-4e91-a2ec-f9a0fcdd92fe'), (25000, 'c4dd2159-6af5-43ea-80dd-2affde5c9e7a'), (4547, '9d7a8092-0c89-45c6-990c-7ac6a0743ada'), (4549, 'f24ee5b9-be57-409d-b6f9-5aa645d8bb4e'), (4551, 'cdc3ab34-6fd4-483c-b74c-64484eda523a'), (8652, '9d7d1d5e-5e20-482e-be5a-9a048f3dbd8f'), (8656, '8cafa261-f4a7-4d84-aa8d-d9b4e7e6c024'), (27605, 'cd1b3318-dc12-4df9-8cba-01f0361dd48c'), (20950, '251402d9-a92c-4800-ba90-095892100773'), (21468, '4a929ab2-ace7-4a93-9676-273b57094c60'), (20447, '87310f99-c045-4f09-b15e-b8efc2843938'), (22499, '186e7c08-bc8c-42e3-92f8-c0884796b602'), (22008, '448290dd-b8c1-4adb-b63d-f19aa5f1e21f'), (22011, 'dde393e4-b99c-47e7-82ff-4724c17236f8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Depending on your GPU, you may need to enable `gradient_checkpointing` (⚠️ not supported for the prior model!), `mixed_precision`, and `gradient_accumulation_steps` to help fit the model into memory and to speedup training. You can reduce your memory-usage even more by enabling memory-efficient attention with [xFormers](../optimization/xformers) (version [v0.0.16](https://github.com/huggingface/diffusers/issues/2234#issuecomment-1416931212) fails for training on some GPUs so you may need to install a development version instead).

This guide explores the [train_text_to_image_prior.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py) and the [train_text_to_image_decoder.py](https://github.com/huggingface/diffusers/blob/main/examples/kandinsky2_2/text_to_image/train_text_to_image_decoder.py) scripts to help you become more familiar with it, and how you can adapt it for your own use-case.
**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations 
throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For 
an in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).

To enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArguments`]:

```py
training_args = TrainingArguments(
    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
)
```

### Addressing Challenge 3
Flash Attention and enabling gradient checkpointing are required for faster training and reducing VRAM usage to enable fine-tuning and save compute costs. The codebase currently uses monkey patching and the implementation is at [chat_assistant/training/llama_flash_attn_monkey_patch.py](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/llama_flash_attn_monkey_patch.py).

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf) introduces a way to compute exact attention while being faster and memory-efficient by leveraging the knowledge of the memory hierarchy of the underlying hardware/GPUs - The higher the bandwidth/speed of the memory, the smaller its capacity as it becomes more expensive.
```

Alternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-accelerate). 

<Tip>

While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.

</Tip>

## Mixed precision training

**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by 
utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point 
precision (fp32 or float32) to represent and process variables. However, not all variables require this high precision 
level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit 
floating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed 
in half-precision, while some are still in full precision, the approach is called mixed precision training.
### Activation Checkpointing or Gradient Checkpointing

Activation checkpointing and gradient checkpointing are two distinct terms that refer to the same methodology. It's very confusing but this is how it is.

Gradient checkpointing allows one to trade speed for GPU memory, which either allows one to overcome a GPU OOM, or increase their batch size, which often leads to a better performance.

HF Transformers models don't know anything about DeepSpeed's activation checkpointing, so if you try to enable that feature in the DeepSpeed config file, nothing will happen.

Therefore you have two ways to take advantage of this very beneficial feature:
 
Here is the question: What is the primary purpose of `gradient_checkpointing` in Kandinsky 2.2 training?","The primary purpose of `gradient_checkpointing` in Kandinsky 2.2 training is to save strategically selected activations throughout the computational graph, allowing only a fraction of the activations to be re-computed for the gradients, thereby reducing memory usage and enabling the model to fit into memory."
What distinguishes the Viterbi algorithm in Unigram tokenization?,"[(19463, '72f624a9-b037-428d-9837-de3484ab1799'), (29703, 'c39eeb1d-4464-46d4-98a9-c31633860860'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (29708, '57d086cd-c094-4b43-998a-9bf67a10c377'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (29709, 'e3ca7c6b-c286-4bf0-ae83-982df4ff7ff6'), (6177, '9a6af5e6-9ab0-4924-a9e2-817c0c8f0901'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (5703, 'deacec8b-e008-4941-922b-71bd1841aa71'), (5704, 'f5a76ce8-55f0-4aa3-9058-b471d25fed91'), (10824, '6d96b69f-c6af-4766-b64b-337c86dd7faf'), (10826, '6897c0ab-5327-430c-9211-8dc59c47ede2'), (10823, 'e21b6aee-30a8-41ca-a03b-c9cbbe3e780e'), (24656, 'a088c20c-8d7d-4bfd-999d-984c49d82451'), (6225, '95dfb88e-25d4-40c5-8d6f-6d3af07e4e0e'), (6226, '496d95c0-9967-4764-94fe-bd01221c5dc4'), (6229, '829198af-0f31-4afe-aae8-358ab99a58c7'), (6232, 'd57b5e65-e59a-4e4b-8aa0-7139e4f39886'), (6235, 'da68fe2a-d338-45d1-a04c-022e49b5dcd4'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (6239, '0624cf0f-179e-441a-bf41-454d44f35506'), (6244, '69a8d278-e494-488d-a2f2-d94fc92f2bd2'), (15466, 'bf2582ee-b984-4d41-952a-6497f66b25be'), (7789, '206dd674-46b7-4f38-8150-9b15cfd06f87'), (7790, 'a9e5791a-46e7-4308-8134-e24a084a312f'), (7791, '80b0268e-e547-48fb-95d2-622713c6a9db'), (7792, '8d5c7246-3ed2-455b-a56e-538184bdb752'), (7793, '40d93264-9180-45db-b051-ab44da705591'), (6253, 'fa7b9b2e-c67f-4179-b634-54ad01e9f43b'), (7795, '54b5e9cb-abbe-4c4f-a09e-9691c4b11923'), (7796, '673ee2d2-28a6-419c-95fb-ca8cf2d75d9d'), (8314, 'b04a31e0-43f4-41a4-89cd-7b20c2fee1a8'), (15485, '9b5dbea1-d5fa-4385-8c03-f268bd24aea8'), (15487, '211f581a-87ba-479c-bbb7-4bf419ff3c38'), (18566, '5d408cd6-4635-4a46-9971-43575951f389'), (18567, '82b5f6ec-fc59-45e3-8d39-a7e17af46f95'), (18568, '3e6b192d-ac77-405e-b509-7c84053c11a8'), (18569, '09e8a18a-b009-4152-bc84-f97c593e35df'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (9377, '11462af2-125e-4c1a-91d0-9dba82209e93'), (7342, '4e3f892e-b398-4de7-91ee-ce188a823d68'), (690, 'b6bf8779-7ba4-44e2-956d-16eaea7da823'), (24764, '645657c8-1e5b-415a-9175-a7b48fbed112'), (17616, 'a59fd552-bc52-4d0c-9aba-aefd3b3839f5'), (10455, 'ac623491-7926-46e8-8a45-4c350813dacd'), (10456, 'ecdba708-8f55-48d6-86ee-2ec46b1e2778'), (10457, '34cff3a6-1baa-4a3d-a758-49c4d21bffe5'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (10460, 'ace98882-3633-40c7-b823-208bda3a4ca1'), (4327, '906c0c93-5ef3-428b-ae43-ee8fb9df0863'), (4331, '35d2346b-51ef-4c1e-b4d0-e1fbee5951f8'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (22289, '9e4402d9-ecfe-45c9-bb4e-7e0c7afbfa12'), (11059, '75f5055b-6992-499e-8f1a-d6305b338422'), (11065, 'f276616a-e9b5-4dcd-ae38-4256ec518cf8'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (11076, 'c68b6b40-c9ad-4e20-abf8-4fe2db7c0f3e'), (11077, '3eba11d4-e601-4b6a-b0f9-0fb7ecced3d3'), (24900, '4889caeb-6d43-410d-b474-d87d0da7fe19'), (24902, '21707633-c150-463b-a123-71a45823b814'), (18254, 'ae1092dd-508c-4ec9-bd90-bb1e8733e822'), (16719, '7f7a0365-377c-4bfb-a3c8-97ce76ba9618'), (18260, '20b06811-c14a-4c0e-b429-99bce86da710'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (16725, '167730fe-e8e3-492c-a3d4-99715b149522'), (16724, 'bcd7e1a3-4be5-4f08-9af9-feef218c2d7c'), (16728, '9efc94ec-37a1-4b5b-a46c-432fb248c4c4'), (857, '4a58c58e-011b-4c6a-9024-25c07d7755eb'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (854, 'facc2f1f-14f0-407f-a646-e5a4afeca52e'), (861, '06ceed46-34a1-4772-b0c7-2cc21974805f'), (862, '58dce80f-7a13-41a9-8271-89fa056172d9'), (863, '1c31de44-0abf-41d6-96cd-508363daa9a7'), (864, '5920912b-08ae-4795-a60c-d67481685163'), (16737, '4a37b1c8-d2ed-48a2-a3e8-84f2a1e735c3'), (16738, '33cdddd8-07e1-4371-8c3e-b9f64f9afc3c'), (16739, '2155e313-e0b0-4dfa-b90d-3dda854127c2'), (16740, '567a8c57-7015-46ae-87bb-d7aef479b1ca'), (16736, '14094fc4-1dfe-4a5b-8595-d13531fb8d73'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (1414, '9772c7cd-c19a-4ee9-abc0-bd27da5ee279'), (1415, '1411d969-fc20-4d02-9d2f-76d8d2732150'), (12174, '11b1816b-c38a-44f5-ae65-7589d37c8c3f'), (12175, '7d112eff-907a-4a4d-8a99-77bca14c5a4a'), (15248, 'ca77aef7-6039-4c5b-80b1-c76f83ab3f28'), (15247, '9e91e376-e5c3-42b5-822e-a77bfbed897b'), (15249, '9085c8c6-0711-4a36-9552-7984531b417c'), (15251, '12885ed4-98da-492a-a72b-67027a02f6a8'), (3479, '6d99cd47-980e-4cae-a7d5-415306bd443d'), (4505, '82ea4097-6dd6-4ae8-b0e9-416160ce9a4a'), (4507, '7f03af48-608f-4463-ad30-e8d317782576'), (19871, '0a993e9c-5a70-49a8-8b8a-ea79fff01d49'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (31149, 'debd5c00-01da-49e4-9fac-acfd4e6bcd78'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (31153, 'e4910d18-2062-476e-95cf-13e2844c2ada'), (31154, 'e49097a1-9b93-492c-9e9c-343a02a27e52'), (31155, '4ce7c97c-6ce1-4af5-976f-458bbfed6369'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (29616, '5f1337a0-af97-4b1c-8e08-b3cdfd401241'), (29615, '2275ba32-0f01-4d0c-990d-ceaf68cba2af'), (29617, 'ff9c2445-4c03-4813-a426-7902846a33df'), (28606, '42f68123-a165-4f0c-892d-7b48c0879760'), (18914, 'a7c8f0c2-85bf-44c3-98d8-dcb026fa61d4'), (8175, '82c4968d-4fdd-43ea-8244-a62526fe507a'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: them. The token which we will remove is the token which impacts the least the loss: here the token ""bu"". We had mentioned at the beginning of the video that at each iteration we could remove p % of the tokens by iteration. The second token that could be removed at this iteration is the ""du"" token. And that's it, we just have to repeat these steps until we get the vocabulary of the desired size. One last thing, in practice, when we tokenize a word with a Unigram model we don't compute the set of probabilities of the possible splits of a word before comparing them to keep the best one but we use the Viterbi algorithm which is much more efficient. And that's it! I hope that this example has allowed you to better understand the Unigram tokenization algorithm.
To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end.

Let's take a look at an example using our vocabulary and the word `""unhug""`. For each position, the subwords with the best scores ending there are the following:
```

Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named `best_segmentations`. We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated.

Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in `best_segmentations`.
<a id='unigram'></a>

### Unigram

Unigram is a subword tokenization algorithm introduced in [Subword Regularization: Improving Neural Network Translation
Models with Multiple Subword Candidates (Kudo, 2018)](https://arxiv.org/pdf/1804.10959.pdf). In contrast to BPE or
WordPiece, Unigram initializes its base vocabulary to a large number of symbols and progressively trims down each
symbol to obtain a smaller vocabulary. The base vocabulary could for instance correspond to all pre-tokenized words and
the most common substrings. Unigram is not used directly for any of the models in the transformers, but it's used in
conjunction with [SentencePiece](#sentencepiece).
<Question
	choices={[
		{
			text: ""Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules."",
			explain: ""No, that's the approach taken by a different tokenization algorithm.""
		},
		{
			text: ""Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram adapts its vocabulary by keeping the most frequent subwords."",
			explain: ""No, this incorrect.""
		},
		{
			text: ""Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules."",
 
Here is the question: What distinguishes the Viterbi algorithm in Unigram tokenization?","The Viterbi algorithm in Unigram tokenization determines, for each position in the word, the segmentation with the best score that ends at that position. It does this by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. This allows it to find the path in the graph that has the best score."
How does the CLIP tokenizer support text-to-image models like Kandinsky 2.2?,"[(22022, 'c0aa2596-89e8-40c6-93ec-2019c9e6250d'), (23051, 'd0b37d24-40ce-4441-9f5c-56c750734c66'), (22033, 'ddd95851-27c2-466a-b3a5-0b0ed970f12b'), (7186, '9784ba9f-8eb4-4702-b93d-13533ea8276e'), (7189, '7e098dd8-73e7-4bd0-8572-847ad39a44b9'), (7703, 'bdbbf91e-5be0-4de8-b791-c7388d247942'), (6177, '9a6af5e6-9ab0-4924-a9e2-817c0c8f0901'), (15918, 'c7ba6127-5921-4286-880d-489225907f4f'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (23604, 'c054e5cf-cd13-4e07-976e-1f353b0e2eb3'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (10824, '6d96b69f-c6af-4766-b64b-337c86dd7faf'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (24655, 'da13a572-a576-4bd6-96a8-e0b76a56d6cd'), (95, 'dc4aba7b-91c0-4051-ac61-98969e786309'), (96, '3c9d0cd4-40b2-4ac1-94bb-bfbd26478cd0'), (620, '607969d7-62ff-48ba-9157-ee5fc510a46a'), (621, 'c4e9c934-01ff-495e-aa40-d550ec55813d'), (8313, 'afdff51a-3fc9-4cb0-be1f-fb19ca845cec'), (634, '06c1ab81-2669-4e9d-b9da-0defc2a88a4f'), (8314, 'b04a31e0-43f4-41a4-89cd-7b20c2fee1a8'), (635, '69881c4f-750b-499f-8624-576717e6c74f'), (7806, '6d7638e5-caf5-4427-a7e6-a138c90c7e9c'), (639, '326d2897-a47b-4ab1-8c8b-a8afb1e4c1d9'), (7809, '173bc460-5338-4f7c-9b69-916e9e03ecfb'), (7810, '6464d47d-5a3f-4e13-b8a1-5d521cf34e11'), (7814, '59fc1afc-32d4-4401-b7a7-f31ff5bf1424'), (7815, 'ffedd8d3-edcc-43cc-9cec-a604d063dc3e'), (6286, '17043260-ccb0-41be-a8d7-cd50fdf8f0c8'), (6287, '89e924a6-29d0-47d3-96a0-a0d16dfc1ce2'), (10895, '7eb747de-916c-4162-ad00-34e215fde4ba'), (682, 'a36d28c1-b770-43c9-b4da-cff067e67abf'), (11953, 'e5b47149-4a3e-4549-a9ef-c12eccfe493f'), (29376, '8c217225-9082-4990-a340-02f60c2f084b'), (26310, '42880a29-1a9a-4f47-b818-4c8ec4ebebc7'), (3799, '1651e17d-3706-47fa-9cf4-a9f8a9ebb754'), (16090, 'de34769a-90f0-4c82-b82e-6d9a5a0e6f92'), (16091, 'f4e121de-0b41-4c9e-8c14-13372d63deef'), (3805, 'c1fffa18-95ea-49ae-aba6-c62e34c28f4b'), (1249, '087ac398-f52c-42bc-96c6-a651dd21b098'), (30952, '8a52d3e7-dc7b-4e41-8b5f-65783b57c944'), (30953, '34879883-adb5-44d8-9f2d-56787f7ca167'), (3823, '1b242c88-9fa6-4480-8d4a-167fc47e1885'), (3826, '256f85ac-16ed-48b7-8a96-c986ccb76483'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (19194, 'f2337207-7b15-4acc-8d68-6059df62cd35'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (6910, '49d3e17d-b9ae-4481-9c7a-4c59ba0a76f5'), (25347, 'b38e0f87-e853-46c1-9fcb-77d766c7bbc9'), (5892, 'eb93f468-6296-41f1-9944-540dd69e7584'), (27395, '0ba8902b-8c14-48f2-a0d0-423c4fd30ae1'), (11015, '19b3068a-50d5-409f-8ce0-ca90b82c03e9'), (29448, 'de680f2b-2364-4825-bd55-d130bdbe185f'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (28431, '0bc21447-bf7d-467a-bc02-d7ca7d6a4f2d'), (25362, 'd13794b5-b32e-4379-9e59-8de3bfd1d134'), (17708, 'bc875152-3402-4283-9c21-9fd595947fbe'), (825, '06266606-7f41-4726-b90a-fd8ffceb74d4'), (26429, '5bae9ae8-99ba-41c9-9595-ae675ca50f04'), (24900, '4889caeb-6d43-410d-b474-d87d0da7fe19'), (24901, '20ed6577-9065-4b17-a364-b1463389e837'), (24902, '21707633-c150-463b-a123-71a45823b814'), (3908, '91ab7337-4628-49b3-9525-25b72621e00d'), (24903, 'a4cedab8-3717-4d08-b41b-f6159b58e741'), (24904, '0d263af5-23ad-4f96-a3b0-81bee9b6bd23'), (29518, '4cc923b4-5b7a-4f75-aa02-6c9a08dfdd03'), (16720, '6f64e88f-35dc-46dc-9289-15572ee0760a'), (9555, 'c19b5f80-620b-4a82-82e1-81e843622c70'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (5466, 'e6fb1185-8250-49a9-819b-c56bf802df85'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (6000, 'a2d674ff-4195-4bc5-97fa-d9595fb53b21'), (15220, 'c3e6cdad-e35e-489c-9c5a-0f48a94e1c0f'), (25973, '69e86d4c-ade5-467a-9587-afb008de297e'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (15227, '783d1508-a579-481d-b6e6-850a89fbabd7'), (15228, '511e53d3-26de-49ea-9af5-149d1dd1600a'), (13181, 'a3c9f584-04fc-4227-91ed-65b952d61a13'), (15230, 'c4948062-4df0-4d3e-ac44-2b700dd82ff2'), (31616, 'f6da62fc-c26d-4422-99b3-031dc14e92df'), (15233, '597eefb7-c394-452c-9119-7ec071576182'), (31619, '15b09247-3696-4abe-b6ab-82b08cf31da4'), (31621, '833b507c-b23d-45b8-936f-a85896b6367d'), (1414, '9772c7cd-c19a-4ee9-abc0-bd27da5ee279'), (901, 'f064c51b-00f5-4a7f-9992-c3711c8b265f'), (16265, '96da4566-8b20-4bc5-9099-a00692700384'), (15243, '4189372b-3be4-4bca-b934-f8d9d37da251'), (1423, 'bf41377e-0913-4528-8a14-d06bc0f4340e'), (22928, 'a8321404-e3d8-420e-b234-a78cf2bdc969'), (1424, '7da8fa90-541d-4a40-831d-ce0c019039d4'), (22930, '25b5e287-3b54-40ee-9fc0-06927f1cd7cd'), (15248, 'ca77aef7-6039-4c5b-80b1-c76f83ab3f28'), (15247, '9e91e376-e5c3-42b5-822e-a77bfbed897b'), (3479, '6d99cd47-980e-4cae-a7d5-415306bd443d'), (21401, 'd384f041-453e-4c80-98fa-77d1c1e207c7'), (11674, '6af5e499-b7b7-4209-934c-fa9f2072b05b'), (11676, 'a9cfe46a-9e1d-4ad1-9d16-634aeac45374'), (11677, 'b65b3085-bff1-4fe2-9cbe-69e92c3410b8'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (29124, '2a4c40e0-2e01-47f4-a1b0-a84901d0c22e'), (29125, '9846454a-817c-4b33-bfb6-750ebf2a3a5d'), (29127, '82149023-2091-41fa-98e9-b702609ed535'), (20965, '78c8fe8a-3494-4209-9ec8-7ac633e5b4b1'), (23014, 'd29e7466-2ce3-4018-8252-e65865458a74'), (20966, '61420da0-bea9-4ed4-8b90-0716f6b0a31d'), (20967, '9c9e7c2c-88b4-4ef0-9fd3-fbfea2d90e56'), (20968, 'f80a61e6-a2e8-4a17-98b8-5b97fbbc4a22'), (20969, '448efe28-d593-4aab-8a7c-2e26e1a8be54'), (23015, 'd9177564-c31b-4316-8d08-cb422482dd8b'), (13286, '7ee1ad39-4258-4a5d-81b7-e8d7746cca3a'), (23018, 'c153f31e-4b06-4783-8925-ddf8e7c1cc3d'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The description from it's GitHub page is:

*Kandinsky 2.2 brings substantial improvements upon its predecessor, Kandinsky 2.1, by introducing a new, more powerful image encoder - CLIP-ViT-G and the ControlNet support. The switch to CLIP-ViT-G as the image encoder significantly increases the model's capability to generate more aesthetic pictures and better understand text, thus enhancing the model's overall performance. The addition of the ControlNet mechanism allows the model to effectively control the process of generating images. This leads to more accurate and visually appealing outputs and opens new possibilities for text-guided image manipulation.*

The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).

<Tip>

Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.

</Tip>

<Tip>
```

## Training script

The training script is also similar to the [Text-to-image](text2image#training-script) training guide, but it's been modified to support training the prior and decoder models. This guide focuses on the code that is unique to the Kandinsky 2.2 training scripts.

<hfoptions id=""script"">
<hfoption id=""prior model"">

The [`main()`](https://github.com/huggingface/diffusers/blob/6e68c71503682c8693cb5b06a4da4911dfd655ee/examples/kandinsky2_2/text_to_image/train_text_to_image_prior.py#L441) function contains the code for preparing the dataset and training the model.

One of the main differences you'll notice right away is that the training script also loads a [`~transformers.CLIPImageProcessor`] - in addition to a scheduler and tokenizer - for preprocessing images and a [`~transformers.CLIPVisionModelWithProjection`] model for encoding the images:
The description from it's GitHub page is:

*Kandinsky 2.1 inherits best practicies from Dall-E 2 and Latent diffusion, while introducing some new ideas. As text and image encoder it uses CLIP model and diffusion image prior (mapping) between latent spaces of CLIP modalities. This approach increases the visual performance of the model and unveils new horizons in blending images and text-guided image manipulation.*

The original codebase can be found at [ai-forever/Kandinsky-2](https://github.com/ai-forever/Kandinsky-2).

<Tip>

Check out the [Kandinsky Community](https://huggingface.co/kandinsky-community) organization on the Hub for the official model checkpoints for tasks like text-to-image, image-to-image, and inpainting.

</Tip>

<Tip>
The Kandinsky models are a series of multilingual text-to-image generation models. The Kandinsky 2.0 model uses two multilingual text encoders and concatenates those results for the UNet.

[Kandinsky 2.1](../api/pipelines/kandinsky) changes the architecture to include an image prior model ([`CLIP`](https://huggingface.co/docs/transformers/model_doc/clip)) to generate a mapping between text and image embeddings. The mapping provides better text-image alignment and it is used with the text embeddings during training, leading to higher quality results. Finally, Kandinsky 2.1 uses a [Modulating Quantized Vectors (MoVQ)](https://huggingface.co/papers/2209.09002) decoder - which adds a spatial conditional normalization layer to increase photorealism - to decode the latents into images.
[Kandinsky 2.2](../api/pipelines/kandinsky_v22) improves on the previous model by replacing the image encoder of the image prior model with a larger CLIP-ViT-G model to improve quality. The image prior model was also retrained on images with different resolutions and aspect ratios to generate higher-resolution images and different image sizes.

[Kandinsky 3](../api/pipelines/kandinsky3) simplifies the architecture and shifts away from the two-stage generation process involving the prior model and diffusion model. Instead, Kandinsky 3 uses [Flan-UL2](https://huggingface.co/google/flan-ul2) to encode text, a UNet with [BigGan-deep](https://hf.co/papers/1809.11096) blocks, and [Sber-MoVQGAN](https://github.com/ai-forever/MoVQGAN) to decode the latents into images. Text understanding and generated image quality are primarily achieved by using a larger text encoder and UNet.
 
Here is the question: How does the CLIP tokenizer support text-to-image models like Kandinsky 2.2?","The CLIP tokenizer does not directly support text-to-image models like Kandinsky 2.2. Instead, the CLIP model is used as a text and image encoder in Kandinsky 2.1 and 2.2. The CLIP model encodes the text and image into embeddings, which are then used to generate images. The CLIPImageProcessor is used for preprocessing images, and the CLIPVisionModelWithProjection is used for encoding the images. The tokenizer is used"
What is the core function of Gradio’s `Gallery` component?,"[(28161, '6a39374d-d9fd-4229-8372-ce6d65514142'), (28162, '2fe3c2a3-a327-4912-ada2-190568c660fb'), (4103, '69742828-77bd-4856-a8aa-a0025c5d7383'), (10261, '889896bb-eb69-46fb-b043-075142527346'), (4120, '7a6adc1d-257f-4eb2-bcde-8613db08883f'), (28200, 'bbcfc1e5-a100-4dd8-8b71-33a035bf6391'), (16426, '5b4e1e59-8d4c-41c6-86bd-80663aaba033'), (3119, '6a16877a-d058-484e-adf4-1e3130b324ae'), (16433, '3a6c1f63-b6aa-4990-b403-244dc7863130'), (11829, '38bd3f56-e5b3-4e10-8cb9-2c02e7602038'), (28216, '5ef0c5f2-e9f3-49a4-9bda-40af0f54e3d0'), (28217, '17a071b0-182d-464b-89b7-d5c2cbb45575'), (28232, 'edcdb710-1075-4024-937a-e1e9b421cd61'), (28236, 'ff029be9-b5d9-47d9-8b0a-e79b53b6a8ea'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (2126, 'eaad6c83-eff0-4a22-b860-506e3447e15a'), (28250, '31ec90bb-9213-4d5b-9a3c-7fd0daac0f7d'), (2140, '815963a3-04df-4294-814c-267a55fd68f5'), (2143, '6e81dc66-340f-4578-890b-b72e0440f793'), (28255, 'bcb424b2-729f-4991-87ac-45ef5ce77853'), (10849, '5e73d3a0-f44a-46d9-8e1c-f24bdfb746d0'), (6763, '59f0ab5c-e151-4e99-b995-c9ea8996171c'), (2156, '6157cc0e-af2d-44bc-8eda-72c429d67eb3'), (28272, '8f0d48b9-997f-408d-a152-f00ba14dabd5'), (28790, 'f409e584-4377-467f-b70c-f57b8196be37'), (7799, '90107451-0822-4205-96bc-e19ca4488723'), (7804, '2293993d-e307-4e9a-af13-c42568ee3dec'), (27776, 'a9d089a8-2002-4ad1-a6dd-2ce70cb14d68'), (2177, 'e4f20765-678d-400f-acda-3fd9e844e57d'), (28290, '80e2b87c-3cf1-42e5-ad70-82425a7cb2ef'), (4740, 'a3172a98-921d-49f4-98ff-7a1f3290f6af'), (28293, '1c594543-8c42-4ed5-9db9-b8f7d53e4e1a'), (7823, '341c1953-c6b2-408b-9040-3ce33e764469'), (27793, 'e0cc82b3-f860-4440-b597-0466bf024466'), (27811, '6351e475-b7a2-4ee3-88f7-ce204ff277fb'), (6820, '296a6e9a-0b6c-450c-8983-f33bcfc0a165'), (3756, '1a1f79f6-d46a-4043-97ae-61320836b7ce'), (29868, 'a87fd676-df82-4ca4-b1c7-c7cce13dd5ce'), (3760, '4f7d33f2-8ea6-4d46-826d-95c7530db102'), (30395, 'ab5263c1-7c85-43ee-bd27-89e9944fe67a'), (27844, 'fb6d0216-35d2-4c4e-adff-45ae67110b4c'), (2245, '47834d58-902b-4e38-853e-427121d538ab'), (2246, 'b2714547-7d53-41a2-ade0-0dc1989a2476'), (2247, 'e9c8dd04-774e-481f-a3fe-757c0a391f09'), (27849, '32372b5e-e02d-4105-9c4b-edb2719e80e0'), (27850, 'a6f8cd0f-5c98-4144-acbf-8bb2e5f594c4'), (2251, '49c92cce-066a-48de-9511-6f677fedd988'), (27852, '81090f0f-86b7-4c58-9fe0-a796307b1748'), (2257, 'a342551c-d0e3-4d70-857a-23e1e82ecc87'), (2263, 'de23a944-32ae-4aca-9342-ed441d8275b4'), (27864, 'bb7de493-b7e0-4a2e-9e87-b00fbed9325c'), (27358, '6650005f-25f9-4491-945a-a6e21a1f32a7'), (27360, 'b3f91b27-9fab-492f-b8f6-e22b380d3729'), (2287, '6b30a70b-fcb5-42bf-a607-1f8aaa309ffb'), (2288, 'c0ef52c7-99ed-4694-93ae-6c412b9f86a1'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (23795, '2107261d-1ab6-45c8-a1fa-c4229f1927bf'), (23796, '851ba12c-6554-4982-8c9f-afe1d6deceff'), (23802, '6eca1503-7294-4693-919d-ca12a4bf5efa'), (21754, '87b73ba0-88c4-4302-9951-5e5c45977bc5'), (16640, '6c318d11-7364-4e1d-a15c-bc96d7fa98c0'), (27913, '1a5af7ec-5880-4d95-9c6f-bfa4aa6121ae'), (26889, '739be90a-f89d-445a-be17-3ab3af2a79d6'), (4371, 'ee717cdd-8985-41e6-b386-5a6da655ddaf'), (2326, '752544ae-9d35-40cb-8f9b-057e92b1d7fe'), (30999, '53998aea-d5c1-4b05-87c7-09e519c96443'), (26904, '9d40fa3c-8d42-4870-9ba4-b192870efa01'), (2342, '865c895b-34a5-453b-b76f-f5c444c3b2df'), (2343, '967189ff-4b5c-4875-a56e-984caf1351ee'), (2358, '61c00040-18d9-47df-b54f-732d42c2c5f9'), (2362, '1cfbdc0e-9682-4844-b541-ff932ae3ea3a'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (2376, '7809bd4a-1a54-4abb-bb3f-068d12f9f4d2'), (2381, '25022ddc-db5e-48bf-a2bf-918a00646b76'), (2398, '674a4fbf-0ede-489e-a689-aedd65e6317a'), (3934, '24000b88-1568-44be-a5d6-7fe94664b817'), (28000, '14c388ba-8b8d-4268-8474-f3a833f333f1'), (1902, 'c48d5037-147b-403c-8ac3-0efcd9d18b8e'), (28014, '3af9249f-4f93-4dbf-b269-27fa4aa9e315'), (2416, 'a52b2a54-de9a-48b4-9365-0626dbcd43c8'), (28017, '29c87561-6f4c-4afc-adfd-b3054e4a4dad'), (19823, '4a9e8566-d1fc-4875-a3a6-70e098a84848'), (2419, '379f8a89-bc78-4bfc-8f9e-48f29f13370e'), (31611, 'c7bf921e-2d4f-44a9-bee9-1e5953f19748'), (31612, 'c6e810cc-92c8-4e66-9d4a-8ae615c991f9'), (28030, '96755a43-0fc9-461f-a1ae-8a79bed496cc'), (1919, 'c64e4f18-2b7f-4516-8a23-1060fd924eba'), (1937, '9e067ec3-9c36-4419-9560-eb68ac22179f'), (28051, '2e9d8676-4090-4ac4-ac08-acae65f2d32d'), (16800, '2e36121d-c993-4b67-8fd9-84fa17182afa'), (3493, '8a1040ce-1b60-4e24-90e9-cb4e7127b718'), (20394, '5ee7db72-d4a7-43e0-9cc0-f5da6aab5d5c'), (14255, 'ad8a2d2a-108e-435d-a786-efbe9be03724'), (1970, 'af7a3664-be46-4edc-ab17-ca264d4a0b29'), (1975, 'a3373798-7376-4321-b2f0-d9eacd62e631'), (1976, 'dd500826-6c53-4f60-ab5e-1ca88cf811ed'), (1978, 'e6aa00b1-e118-4a20-9aea-377dc102cb43'), (23484, 'a9b50d64-9255-4dee-a7c6-eb65f2c88b82'), (1990, '2e21c9d1-f37e-4ff5-b83e-a69e36441d2c'), (11729, '4691185d-26b0-403c-a3f3-6d2bc95ea285'), (11733, '27aacaae-aca4-411c-8fb2-30313de2897d'), (28119, '1dcccb98-51b9-49bd-81ec-c5d5c62bea87'), (28120, '0e15845c-8278-4ae1-8b41-403e6698afe5'), (28121, 'f47bdf19-919c-4645-96d7-5bde219f7556'), (11739, '0502e8df-e803-41dc-a2c1-535de5a0ac5e'), (11740, '005c72e8-ed81-43e5-93d7-fc3a05cec140'), (28125, '24144982-b2b1-4f85-a299-ae52b3b80ed5'), (11745, '10784898-608a-4cde-8419-5358ef3928ae'), (11746, 'c1d1e06f-bcd1-4a21-95d5-aacec2df63fc'), (28131, '974a4165-1698-418f-90a1-f8e6e7cbb425'), (28137, '5116f0d1-8fcf-4fed-971e-06051bd8b4b1'), (29676, '0ef5623b-dc0c-44cf-8c91-b53544115a03'), (3566, 'dc14161e-e267-41ea-a4c3-0189e9da1c9a'), (2039, 'a483702a-3d32-49dc-8a1b-cddefa16a85a')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Similarly, when a component is used as an output, Gradio automatically handles the _postprocessing_ needed to convert the data from what is returned by your function (such as a list of image paths) to a form that can be displayed in the user's browser (a gallery of images).

Consider an example demo with three input components (`gr.Textbox`, `gr.Number`, and `gr.Image`) and two outputs (`gr.Number` and `gr.Gallery`) that serve as a UI for your image-to-image generation model. Below is a diagram of what our preprocessing will send to the model and what our postprocessing will require from it.

![](https://github.com/gradio-app/gradio/blob/main/guides/assets/dataflow.svg?raw=true)

In this image, the following preprocessing steps happen to send the data from the browser to your function:
The Gradio library provides several components for building Computer Vision applications on  Spaces such as [Video](https://gradio.app/docs/#video), [Gallery](https://gradio.app/docs/#gallery), and [Model3D](https://gradio.app/docs/#model3d). The community has been hard at work building some amazing Computer Vision applications that are powered by Spaces:
## Components

Gradio includes more than 30 pre-built components (as well as many user-built _custom components_) that can be used as inputs or outputs in your demo with a single line of code. These components correspond to common data types in machine learning and data science, e.g. the `gr.Image` component is designed to handle input or output images, the `gr.Label` component displays classification labels and probabilities, the `gr.Plot` component displays various kinds of plots, and so on.

Each component includes various constructor attributes that control the properties of the component. For example, you can control the number of lines in a `gr.Textbox` using the `lines` argument (which takes a positive integer) in its constructor. Or you can control the way that a user can provide an image in the `gr.Image` component using the `sources` parameter (which takes a list like `[""webcam"", ""upload""]`).

**Static and Interactive Components**
**Static and Interactive Components**

Every component has a _static_ version that is designed to *display* data, and most components also have an _interactive_ version designed to let users input or modify the data. Typically, you don't need to think about this distinction, because when you build a Gradio demo, Gradio automatically figures out whether the component should be static or interactive based on whether it is being used as an input or output. However, you can set this manually using the `interactive` argument that every component supports.

**Preprocessing and Postprocessing**

When a component is used as an input, Gradio automatically handles the _preprocessing_ needed to convert the data from a type sent by the user's browser (such as an uploaded image) to a form that can be accepted by your function (such as a `numpy` array).
```

You can also combine existing Gradio components to create entirely unique experiences.
Like rendering a gallery of chatbot conversations. 
The possibilities are endless, please read the documentation on our javascript packages [here](https://gradio.app/main/docs/js).
We'll be adding more packages and documentation over the coming weeks!

## Matching Gradio Core's Design System

You can explore our component library via Storybook. You'll be able to interact with our components and see them in their various states.

For those interested in design customization, we provide the CSS variables consisting of our color palette, radii, spacing, and the icons we use - so you can easily match up your custom component with the style of our core components. This Storybook will be regularly updated with any new additions or changes.

[Storybook Link](https://gradio.app/main/docs/js/storybook)


## Conclusion

You now how to create delightful frontends for your components!
 
Here is the question: What is the core function of Gradio’s `Gallery` component?",The core function of Gradio's `Gallery` component is to handle and display a collection of images.
What makes Hugging Face’s CleanRL implementation of PPO unique?,"[(10753, '599157ca-e54c-4c41-813e-6b9fb74ccb9f'), (28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (22547, '51e87e51-6544-440b-9057-4201251fc27d'), (23573, '952579f3-7c93-46c5-ae22-ea5e5fd0b30a'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (14876, '45893d20-107c-4db9-a5c6-937c7bfea6c4'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23079, 'd8d7522d-2458-4cb1-8bf1-20c0919ffa71'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (23083, '005603d6-7fc7-4893-8c08-a335d63ca5d9'), (7725, '258a242e-844a-4498-bd2d-4c45f980f5f2'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (10806, 'ea929262-10fb-4c9e-8683-ca15a4e121e2'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (20544, '76805588-a603-400b-856f-1c52463885ec'), (28741, '50733951-b5db-4e9b-83a4-b2911af5c989'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (20558, 'ee3657ca-549a-4f46-9322-da90f5505f63'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (25685, 'e7551698-0040-4251-b71c-50be7bc42a95'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (17509, '56ea90b0-f162-4bf5-8c05-654f3a183cd9'), (21611, '63264b8d-88bb-4d37-b0a4-051d519848ad'), (31340, '1bffbde6-8c01-4f3d-914e-12799d8de47f'), (23153, '49b4c809-71ff-410a-b32d-acc1467103b4'), (19574, 'cc66b1c9-e31b-4dbf-b777-a98a2c4143be'), (19578, 'c39dbfdb-a1e4-46cb-b486-736ca8e047bb'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (27774, '978314d6-ab36-484a-8af5-e7e5eaf4e43b'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25728, 'e9a67dae-3e05-4640-afbb-816d75639376'), (27775, '56108da4-6463-4bb1-8393-42e02e4b2cc9'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (12935, '78e0073f-75a8-4831-8522-3bbca5d23602'), (26253, '6cce3739-bc8b-4bd5-9a49-70bcc59a0d91'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (684, 'a354cdb9-9cc3-4891-99d5-25f115c6901d'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (28335, '994c72af-e053-48a3-9059-b9662f76400a'), (18099, 'b4a101b0-15e0-4ebf-9686-5edca4169313'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (7871, '0e4739f0-67a7-4d4d-9e47-8318130a392e'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (13510, '0e4bf28c-0eed-49c8-a4a5-724ce73aa1fd'), (11975, 'c977182b-d101-4944-95e4-2c832d9bf6fd'), (17610, 'd76795cd-ddfc-4590-87e5-cd888e5d1dca'), (208, 'ca329095-eb87-48a9-9393-7dafb6980978'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (2774, 'a6465c47-57b9-4f9e-928b-222afc2a7181'), (13527, 'bb1d8204-b2c6-4d37-90aa-27b8d0463634'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (9451, '1de3ed1f-8d25-4810-ae33-077cf444d798'), (14577, '03a094b5-b29c-4840-a743-d2ceb896c494'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (3329, '64a72f1a-f303-41c1-97ea-b32aeeac5fbc'), (11526, '994250e0-f714-40fe-b8ff-52c2610db830'), (8970, 'd5af4c95-97c8-4196-bab1-0d74644248f4'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (30994, '79a1b793-603c-4141-87d4-270e01bb91f7'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (31031, '306dd725-df38-43a2-a289-44917df0a6f1'), (12099, '97456597-42ed-4914-bc4a-1146c5fc4b6f'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (30533, '8ca48a83-6a1f-465e-adeb-86112f39f95e'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (11613, '4bb6bcbd-1f45-4ba1-8ea6-468431e61d23'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (13156, '36612860-a540-40dd-bc59-f955f3503a60'), (17767, 'a7af65e5-6000-4846-b469-ecc521b89dfa'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (2924, '4c56317f-5b87-4407-8529-181e6b31a657'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (2940, '41b2ed97-f869-45fa-a2b1-445971ef0afd'), (16252, 'ebc3b30d-233d-43e4-b012-9f5a958063fc'), (17283, '2a05ccb6-e010-4230-bca3-f4caadcd83bd'), (10641, '69905c2d-26d2-4ad5-84ac-f7f6aabdb846'), (3985, 'ecedc47d-0616-441d-a202-07ae59c5cb7a'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (7577, 'a9abfb74-1673-49d8-bd53-27dd99f12582'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (11176, 'ee500edf-4300-4566-a302-c474f1afe5e6'), (30644, '6b3b0112-4a6c-476d-943e-d3202e81f874'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (13239, 'f13538dd-edce-4970-a1cc-83ab6fd1a5d6'), (7096, '3ba57845-c986-4965-b8a2-53c99388658b'), (22966, '8b7b6719-a99f-4081-976d-76c680180f98'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15300, 'ea0bd10e-50d7-46c2-84eb-1307151c2efb'), (5572, 'c7410d55-fc34-4411-841a-9379e5f391d8'), (13767, 'd460916f-0557-46c2-a9d3-da2d7989ac41'), (22471, 'ff8f558a-f015-4d9c-983a-6fadbc649245'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (27607, '477e553f-670e-4d2b-bda2-3ae34b7a5434'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (20456, '2fea8919-613c-4a13-9948-1cf76918fb86'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (27114, 'bc032949-d8d4-4c0e-8898-fdcb416b7e1b'), (25066, '1fd88694-f8b6-4047-913f-dacc1e8bd57c'), (24042, '089a831c-4f38-4cd4-b040-5b7abcd993a9'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (22012, '5c55b2ec-3284-45a4-9f86-cf514fba02e0'), (6143, 'd3e6f6dd-dece-4c80-a8e8-de6ba6331451')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: # Acknowledgement

This work is supported by Hugging Face’s Big Science cluster 🤗. We also thank the helpful discussion with @lewtun and @natolambert.


# Bibtex

```bibtex
@article{Huang2023implementation,
  author = {Huang, Shengyi and Liu, Tianlin and von Werra, Leandro},
  title = {The N Implementation Details of RLHF with PPO},
  journal = {Hugging Face Blog},
  year = {2023},
  note = {https://huggingface.co/blog/the_n_implementation_details_of_rlhf_with_ppo},
}
# Unit 8: Proximal Policy Gradient (PPO) with PyTorch 🤖

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail.png"" alt=""Unit 8""/>


In this notebook, you'll learn to **code your PPO agent from scratch with PyTorch using CleanRL implementation as model**.

To test its robustness, we're going to train it in:

- [LunarLander-v2 🚀](https://www.gymlibrary.dev/environments/box2d/lunar_lander/)

We're constantly trying to improve our tutorials, so **if you find some issues in this notebook**, please [open an issue on the GitHub Repo](https://github.com/huggingface/deep-rl-class/issues).

## Objectives of this notebook 🏆

At the end of the notebook, you will:

- Be able to **code your PPO agent from scratch using PyTorch**.
- Be able to **push your trained agent and the code to the Hub** with a nice video replay and an evaluation score 🔥.


## Prerequisites 🏗️

Before diving into the notebook, you need to:
Introduction to PPO with Sample-Factory

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/thumbnail2.png"" alt=""thumbnail""/>

In this second part of Unit 8, we'll get deeper into PPO optimization by using [Sample-Factory](https://samplefactory.dev/), an **asynchronous implementation of the PPO algorithm**, to train our agent to play [vizdoom](https://vizdoom.cs.put.edu.pl/) (an open source version of Doom).

In the notebook, **you'll train your agent to play the Health Gathering level**, where the agent must collect health packs to avoid dying. After that, you can **train your agent to play more complex levels, such as Deathmatch**.

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit10/environments.png"" alt=""Environment""/>

Sound exciting? Let's get started! 🚀
```

If you don't want to use Google Colab or a Jupyter Notebook, you need to use this command instead: `huggingface-cli login`

## Let's start the training 🔥

⚠️ ⚠️ ⚠️  Don't use **the same repo id with the one you used for the Unit 1**

- Now that you've coded PPO from scratch and added the Hugging Face Integration, we're ready to start the training 🔥

- First, you need to copy all your code to a file you create called `ppo.py`

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step1.png"" alt=""PPO""/>

<img src=""https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit9/step2.png"" alt=""PPO""/>

- Now we just need to run this python script using `python <name-of-python-script>.py` with the additional parameters we defined using `argparse`

- You should modify more hyperparameters otherwise the training will not be super stable.
Demos and GUIs built with Gradio give the power of machine learning to more and more people because they allow non-technical users to access, use, and give feedback on models. And our acquisition by Hugging Face is the next step in this ongoing journey of accessibility. Hugging Face has already radically democratized machine learning so that any software engineer can use state-of-the-art models with a few lines of code. By working together with Hugging Face, we're taking this even further so that machine learning is accessible to literally anyone with an internet connection and a browser. With Hugging Face, we are going to keep growing Gradio and make it the best way to share your machine learning model with anyone, anywhere 🚀
 
Here is the question: What makes Hugging Face’s CleanRL implementation of PPO unique?","The context does not provide specific information about what makes Hugging Face's CleanRL implementation of PPO unique. It only mentions that you will ""code your PPO agent from scratch with PyTorch using CleanRL implementation as model"" in the notebook. It does not provide further details about the CleanRL implementation."
How does Unigram tokenization ensure vocabulary optimization?,"[(23040, 'f17c1799-6922-4207-906f-59ecf4ef6c33'), (29703, 'c39eeb1d-4464-46d4-98a9-c31633860860'), (29704, 'eeb0832a-ecb4-44a7-9fac-ce47c316fd73'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (19463, '72f624a9-b037-428d-9837-de3484ab1799'), (29707, '100df54f-edea-4f8f-a5a1-01f351b1efb0'), (29708, '57d086cd-c094-4b43-998a-9bf67a10c377'), (29709, 'e3ca7c6b-c286-4bf0-ae83-982df4ff7ff6'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (6177, '9a6af5e6-9ab0-4924-a9e2-817c0c8f0901'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (5703, 'deacec8b-e008-4941-922b-71bd1841aa71'), (24656, 'a088c20c-8d7d-4bfd-999d-984c49d82451'), (6225, '95dfb88e-25d4-40c5-8d6f-6d3af07e4e0e'), (6226, '496d95c0-9967-4764-94fe-bd01221c5dc4'), (6227, '87c854b5-8830-4132-9d3b-3c9ad5e7c89a'), (6229, '829198af-0f31-4afe-aae8-358ab99a58c7'), (6232, 'd57b5e65-e59a-4e4b-8aa0-7139e4f39886'), (6236, '19dbd2e0-ebb8-4c12-bbe1-aa0a3af3755d'), (6239, '0624cf0f-179e-441a-bf41-454d44f35506'), (15466, 'bf2582ee-b984-4d41-952a-6497f66b25be'), (15467, '5cc08676-551f-4df5-99d7-18d06b695bd3'), (7789, '206dd674-46b7-4f38-8150-9b15cfd06f87'), (7790, 'a9e5791a-46e7-4308-8134-e24a084a312f'), (7791, '80b0268e-e547-48fb-95d2-622713c6a9db'), (7792, '8d5c7246-3ed2-455b-a56e-538184bdb752'), (7793, '40d93264-9180-45db-b051-ab44da705591'), (6253, 'fa7b9b2e-c67f-4179-b634-54ad01e9f43b'), (7794, '47fc6ab7-bccf-4c40-8b79-a0ebca47c561'), (7796, '673ee2d2-28a6-419c-95fb-ca8cf2d75d9d'), (7795, '54b5e9cb-abbe-4c4f-a09e-9691c4b11923'), (631, 'd8c42629-5fb7-4e48-8de4-57a1a4d10570'), (632, '1ae5445c-2cac-45f5-8d2c-a2d7986b517e'), (15481, '87ce1504-d1e1-49b1-9c72-7fb74be5a8e7'), (15485, '9b5dbea1-d5fa-4385-8c03-f268bd24aea8'), (15487, '211f581a-87ba-479c-bbb7-4bf419ff3c38'), (18566, '5d408cd6-4635-4a46-9971-43575951f389'), (18567, '82b5f6ec-fc59-45e3-8d39-a7e17af46f95'), (18568, '3e6b192d-ac77-405e-b509-7c84053c11a8'), (18569, '09e8a18a-b009-4152-bc84-f97c593e35df'), (18585, '8632ca1c-da27-4f58-a21f-32139490a267'), (18587, '84bdfa2d-cdcf-45f8-9f0a-8ff54b2af410'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (9377, '11462af2-125e-4c1a-91d0-9dba82209e93'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (19621, '2100eee7-6ff0-4550-a16b-6538c747e693'), (7341, 'b601a33f-7e1f-42fd-90b5-1a99b5987075'), (7342, '4e3f892e-b398-4de7-91ee-ce188a823d68'), (17592, '48ede2f2-438c-4cd3-b958-b20d71e0b9aa'), (10455, 'ac623491-7926-46e8-8a45-4c350813dacd'), (10456, 'ecdba708-8f55-48d6-86ee-2ec46b1e2778'), (10457, '34cff3a6-1baa-4a3d-a758-49c4d21bffe5'), (4327, '906c0c93-5ef3-428b-ae43-ee8fb9df0863'), (22288, '26e486c2-43b7-4f99-9d2d-024d2618471d'), (22289, '9e4402d9-ecfe-45c9-bb4e-7e0c7afbfa12'), (22290, 'effab2b0-1b81-4b47-9d10-d1f7245ceb5c'), (22300, '32418488-7979-4eb3-8192-83f21c3a051b'), (22302, 'dad51bd3-e9f8-4ed2-82b1-59534adcc14f'), (11076, 'c68b6b40-c9ad-4e20-abf8-4fe2db7c0f3e'), (11077, '3eba11d4-e601-4b6a-b0f9-0fb7ecced3d3'), (24902, '21707633-c150-463b-a123-71a45823b814'), (16723, 'd4f337a3-994a-44ba-9a0e-70bf73500ee2'), (16724, 'bcd7e1a3-4be5-4f08-9af9-feef218c2d7c'), (18260, '20b06811-c14a-4c0e-b429-99bce86da710'), (16726, 'c0e0ead7-7fb9-4e1a-a8f4-41c13bb96ae1'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (16725, '167730fe-e8e3-492c-a3d4-99715b149522'), (857, '4a58c58e-011b-4c6a-9024-25c07d7755eb'), (16730, '4698f6a9-10ed-4b63-8a16-86a39a5de935'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (16733, 'd41410d5-24e0-4886-8d53-23d71dc2fbf3'), (16734, 'e3d674f7-a138-4375-9fdd-fd3b27312475'), (863, '1c31de44-0abf-41d6-96cd-508363daa9a7'), (864, '5920912b-08ae-4795-a60c-d67481685163'), (16737, '4a37b1c8-d2ed-48a2-a3e8-84f2a1e735c3'), (16738, '33cdddd8-07e1-4371-8c3e-b9f64f9afc3c'), (862, '58dce80f-7a13-41a9-8271-89fa056172d9'), (16740, '567a8c57-7015-46ae-87bb-d7aef479b1ca'), (16736, '14094fc4-1dfe-4a5b-8595-d13531fb8d73'), (16739, '2155e313-e0b0-4dfa-b90d-3dda854127c2'), (24938, '20f0ba7e-1fcc-4eff-9442-3378b6a2b8c7'), (23430, '85213444-0ee5-42eb-947a-16b49dbe2d8c'), (1414, '9772c7cd-c19a-4ee9-abc0-bd27da5ee279'), (1418, 'dcc80b3f-66e8-4b4b-9018-7cd3c8bbea59'), (15247, '9e91e376-e5c3-42b5-822e-a77bfbed897b'), (15248, 'ca77aef7-6039-4c5b-80b1-c76f83ab3f28'), (15251, '12885ed4-98da-492a-a72b-67027a02f6a8'), (4507, '7f03af48-608f-4463-ad30-e8d317782576'), (29610, 'ac1535f9-21ba-44dd-a66f-e962faf6b40f'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (29612, 'ae39f1ab-1b94-4882-b706-1e93f0941895'), (31149, 'debd5c00-01da-49e4-9fac-acfd4e6bcd78'), (29615, '2275ba32-0f01-4d0c-990d-ceaf68cba2af'), (431, '063b8bbb-e761-4492-b59d-d3f01a457327'), (29614, '08a41347-bad7-44ee-a2e3-30600a2d8716'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (31153, 'e4910d18-2062-476e-95cf-13e2844c2ada'), (31154, 'e49097a1-9b93-492c-9e9c-343a02a27e52'), (31155, '4ce7c97c-6ce1-4af5-976f-458bbfed6369'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (29616, '5f1337a0-af97-4b1c-8e08-b3cdfd401241'), (29617, 'ff9c2445-4c03-4813-a426-7902846a33df'), (17362, 'd7daabea-fea0-430b-a3ee-3038eed2cbb8'), (18914, 'a7c8f0c2-85bf-44c3-98d8-dcb026fa61d4'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c'), (29611, '3c70029e-ab2c-422a-81de-13783746fec2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: have our initial vocabulary. The training of the Unigram tokenizer is based on the Expectation-Maximization method: at each iteration. We estimate the probabilities of the tokens of the vocabulary. Then we remove the p percent of tokens that minimize the loss on the corpus and which do not belong to the basic characters as we want to keep in our final vocabulary the basic characters to be able to tokenize any word. Let's go for it! The probability of a token is simply estimated by the number of appearance of this token in our training corpus divided by the total number of appearance of all the tokens. We could use this vocabulary to tokenize our words according to the unigram model. We will do it together to understand two things: how we tokenize a word with a Unigram model and how the loss is calculated on our corpus. The Unigram LM tokenization of our text ""Hug"" will be the one with the highest probability of occurrence according to our Unigram model. To find it, the simplest way to
n this video, we will study together ""the Unigram Language Model subword tokenization algorithm"".

The overall training strategy of a Unigram LM tokenizer is to start with a very large vocabulary and then to remove tokens at each iteration until we reach the desired size. At each iteration, we will calculate a loss on our training corpus thanks to the Unigram model. As the loss calculation depends on the available vocabulary, we can use it to choose how to reduce the vocabulary. So we look at the evolution of the loss by removing in turn each token from the vocabulary. We will choose to remove the p percents which increase the loss the less.
<Question
	choices={[
		{
			text: ""Unigram is a subword tokenization algorithm that starts with a small vocabulary and learns merge rules."",
			explain: ""No, that's the approach taken by a different tokenization algorithm.""
		},
		{
			text: ""Unigram is a subword tokenization algorithm that starts with a big vocabulary and progressively removes tokens from it."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram adapts its vocabulary by minimizing a loss computed over the whole corpus."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram adapts its vocabulary by keeping the most frequent subwords."",
			explain: ""No, this incorrect.""
		},
		{
			text: ""Unigram tokenizes words into subwords by finding the most likely segmentation into tokens, according to the model."",
			explain: ""That's correct!"",
			correct: true
		},
		{
			text: ""Unigram tokenizes words into subwords by splitting them into characters, then applying the merge rules."",
generate the same token, the one which has the greatest probability. Nevertheless, to do tokenization, this model is very useful to us because it can be used to estimate the relative likelihood of different phrases. We are now ready to return to our explanation of the training algorithm. Let's say that we have as a training corpus 10 times the word hug, 12 times the word pug, 5 times the word lug, 4 times bug and 5 times dug. As said at the beginning of the video, the training starts with a big vocabulary. Obviously, as we are using a toy corpus, this vocabulary will not be that big but it should show you the principle. A first method is to list all the possible strict substrings that's what we'll do here. We could also have used the BPE algorithm with a very large vocabulary size. So we have our initial vocabulary. The training of the Unigram tokenizer is based on the Expectation-Maximization method: at each iteration. We estimate the probabilities of the tokens of the vocabulary.
is calculated on our corpus. The Unigram LM tokenization of our text ""Hug"" will be the one with the highest probability of occurrence according to our Unigram model. To find it, the simplest way to proceed would be to list all the possible segmentations of our text ""Hug"", calculate the probability of each of these segmentations and then choose the one with the highest probability. With the current vocabulary, 2 tokenizations get exactly the same probability. So we choose one of them and keep in memory the associated probability. To compute the loss on our training corpus, we need to tokenize as we just did all the remaining words in the corpus. The loss is then the sum over all the words in the corpus of the frequency of occurrence of the word multiplied by the opposite of the log of the probability associated with the tokenization of the word.  We obtain here a loss of one hundred and seventy. Remember, our initial goal was to reduce the vocabulary. To do this, we will remove a token
 
Here is the question: How does Unigram tokenization ensure vocabulary optimization?","Unigram tokenization ensures vocabulary optimization by starting with a large vocabulary and progressively removing tokens from it, at each iteration, by estimating the probabilities of the tokens and removing the p percent of tokens that minimize the loss on the corpus, while keeping the basic characters in the final vocabulary."
What preprocessing step is critical for Proximal Policy Optimization (PPO)?,"[(6144, '34a43938-8523-4217-9140-ca800a80ae2b'), (6145, 'a7fbd7de-a494-43e7-a736-ac47818c05d9'), (6146, 'd950b985-7fb5-46b2-9b05-0af6419fd745'), (6147, '5a0a9bce-c74d-4d84-9cbd-c2b32a3e4e57'), (10753, '599157ca-e54c-4c41-813e-6b9fb74ccb9f'), (6149, 'c49a7f6f-f774-4d7a-8ff4-176a9bbc89ed'), (6150, '3a9ebe50-23b4-42ff-897c-7a593d0b29c6'), (6151, '44101b1a-ca3c-4fc4-85b1-92016126dfe6'), (6152, '879840df-06da-44e5-844f-5819aeba72f9'), (6153, '87f3c0d2-608b-4223-82e9-cfa711b782a9'), (6154, '6312ef17-99ba-4a6a-98eb-8e2daa40a7c3'), (6155, '8e716cf0-8f17-4943-812f-aa1511bf263a'), (6148, '14df6787-76f4-44eb-bae4-4cf91fd2a401'), (6156, '72221968-c8f3-41dd-ac06-841acaa4bc06'), (6158, 'fb4aaf9f-149c-4c41-a3c4-40fdd495655c'), (6159, 'c23932a8-c63d-40e7-8961-c436aeac5bed'), (10772, 'f32c7c97-c222-47d2-a603-afd626c71f5b'), (14888, '5168c8d6-2c50-4147-a76b-28ab20f72d02'), (20017, '7f1adcff-7661-482a-83a6-f88d6491fb7b'), (20018, 'd0dd9c49-d5d7-4e72-b765-717912a3ade0'), (12851, 'c31c286a-542f-4994-b97c-dc916101b6a9'), (12852, 'b241f9da-f9a4-4802-9ba7-8b3acc58a446'), (12850, 'c6e54b94-1bc1-4c66-8959-7c3a3451782c'), (12859, '490a120a-9a4a-4266-b754-fcd3a64022ed'), (30288, 'c5b5f0c9-15ec-4d37-9f6b-5ab6de8f2b12'), (9296, 'b982a595-b413-47c2-a522-d71819d17b20'), (21595, '5efa160a-36d2-4b83-aa17-269b32dddfc6'), (21597, '9d3acce5-8938-42d4-af4e-c785ecf0e2fe'), (30302, '4b7094d4-6aaf-42db-a006-4357c743ab41'), (21599, 'fed774c7-af3a-4c01-92f5-6e974be14e3c'), (21598, 'e113a596-30cc-441f-ac19-90b0009cc7f4'), (21600, '9af1cc4c-21f2-4e01-81b1-d37162920ffb'), (21087, 'a1d9c033-370d-485d-b976-028b9f5d98ca'), (30301, 'e9564e93-e1bd-4577-8ccc-3dd14eaf934b'), (21605, '20ff5585-2f0a-479b-b152-976e32f8ddc6'), (21606, '136ec334-4b38-4957-a9a2-8b75cf91a5d1'), (21608, '6d09190c-d1bb-44f6-9ff0-a0b2d6817266'), (23659, 'b766966d-d274-4d11-91c4-187942ee0f6d'), (27774, '978314d6-ab36-484a-8af5-e7e5eaf4e43b'), (23707, 'e9516a46-e372-416c-a02e-101108ac0354'), (22688, '2d4c0864-3f91-4802-b4f0-d3caca140195'), (22690, '9b2e765e-23fe-4aca-a149-55964e264042'), (22691, '9ef67236-0988-4266-8eb1-aef85de98062'), (22692, '0fead5d4-26f6-4463-9096-eb4233d74149'), (14502, 'a6fde7e9-02c7-4939-96b0-3cc388688a2d'), (22695, '84ad0c76-d7d9-43b7-95a7-c3b740570efe'), (22694, 'd986f4e7-7083-4497-a780-384d8b0c76d8'), (14505, '4e3531c6-8fe1-463f-b0b2-0e9e25382b06'), (14504, 'ae636879-64f6-4cce-be09-2b22b17df887'), (14503, '38e55cbb-04e6-4490-806d-190ff6021fd4'), (169, '94f040f4-fab9-4dba-8171-38e1b4eca3f6'), (8877, '05ddd2e3-d894-4999-8da1-68663d93752e'), (174, '3afa87b4-1cf0-4362-96ad-7ef17bd6382d'), (6142, '71c581d6-8275-4cea-816e-461e7d87c4cd'), (25276, '11672078-96a2-45cb-9a23-4cbf8fca0cd1'), (8893, '65d8236c-8241-48f8-8847-bd972e114aeb'), (2769, 'b6c2166e-75fc-49d5-8c7a-30d140038df1'), (21209, 'ef2404c5-d86a-4e51-b640-ba468ef16ce8'), (21210, 'a7d914db-0a82-4bef-889e-b11938bed886'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (23290, '950c3193-baf6-4806-9048-a55f8ed715b9'), (23291, '34457bff-e882-46a1-8e35-2b91df43fb9b'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (11043, '75ead400-0b13-4330-b53b-6b4e099e4790'), (11044, 'b992427f-b872-4a99-8073-9a4c747b6fc7'), (11045, 'e8358914-ebc4-46c7-9da2-b16c37c3214a'), (11046, '4255ba8c-f71b-4f92-8fdb-5e659d18c084'), (11047, '939d2f75-31f8-4c36-92da-3b8dadf6c83a'), (15654, 'a087c142-5d7a-4b96-b71b-6def7546f9b5'), (3377, '156b12d0-1aa0-4557-a4ca-1e911e59c896'), (15666, '1e2a4f90-5b7f-402e-9d25-7eeaec479dff'), (1342, 'b4875f62-8418-4119-832f-2fa04ff8c912'), (22847, 'de9b7afb-702f-42df-aa12-acc8b66d265a'), (1348, '0054aec1-a356-46ab-805c-ec46a96d4f15'), (24407, 'fd0e1f9b-b81c-4c36-a51d-31ff0fe0d081'), (24408, '83516d8a-58b9-4596-96b4-0333dc196c6a'), (11608, '0615a586-d138-44c6-a942-d38de52463e2'), (13154, '089e8911-3317-49b7-9b6c-503657ea6b65'), (13155, 'feb24b04-1aed-4866-9b73-320a1d8566b1'), (16744, 'ddd4e5f8-05ab-4132-b180-b2bd3006f8ae'), (16745, 'bbcf4d12-db52-4214-bd20-fbb0a28f27e1'), (7016, '262ea3d7-1968-466a-8252-d340a78c884f'), (19318, '2ede5978-1f7e-48ff-8c8c-a2e4cc208c56'), (29053, '4bf872d0-e950-49c1-b35c-7066b8efbc00'), (19325, 'fb7c67ab-594a-487c-ae87-f94d0dbe7c65'), (23950, '3b554714-6622-4da1-8325-fb85a2ecc600'), (31631, 'ff89793b-da5c-481a-97bb-e3d16785ec5a'), (15765, 'b54c2adf-640a-48b9-8002-f91250b6e517'), (23456, 'd1baa005-a401-42d2-924c-aece54f9448a'), (16296, 'da19cd55-f253-4823-b44b-76b00a448890'), (15272, '557eb765-9567-4e6c-aed7-ccf7378079a7'), (963, '6e41de93-e65e-4992-a2b7-1359bd6c0e51'), (964, '65fb34e4-bfe0-4990-b85f-bf4e0f542e42'), (15301, '1eb2e2b0-a68b-4fb8-84ca-85eb3633a2b1'), (13766, '324b6368-b377-4ac8-920b-ae7106ccadf1'), (13767, 'd460916f-0557-46c2-a9d3-da2d7989ac41'), (23498, '4c725dcf-74c2-45a9-8efb-757e4fd0c5b6'), (13772, '9460a058-4737-45bd-bd41-81f40b3f366c'), (13781, '315f918c-0798-4c74-8867-003400d6c986'), (21480, '915c478b-08a6-4d0d-ab41-94a0ba1d95c9'), (21481, '9356a740-eb3f-4eec-975e-bd0b77cbd195'), (21482, 'b4d60595-9fa8-49ed-afe9-c049f28f9646'), (10731, 'df44e1e2-7486-4df3-a811-c05bb5e2cdc7'), (10732, '9d553fdc-e142-40ab-abff-8b9a2a7c81cb'), (21483, '5887ee51-0550-47b1-a9c9-bc00327706c9'), (21484, '780e97be-d635-41ff-a9ad-dc48ac21cb80'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (6141, '37268f07-7a1f-49fb-8486-2fc12e669bce'), (10750, '820e2551-3410-4210-a73c-32820cc67dc8'), (6143, 'd3e6f6dd-dece-4c80-a8e8-de6ba6331451')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The intuition behind PPO [[the-intuition-behind-ppo]]


The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: **we want to avoid having too large of a policy update.**

For two reasons:
- We know empirically that smaller policy updates during training are **more likely to converge to an optimal solution.**
- A too-big step in a policy update can result in falling “off the cliff” (getting a bad policy) **and taking a long time or even having no possibility to recover.**
The idea with Proximal Policy Optimization (PPO) is that we want to improve the training stability of the policy by limiting the change you make to the policy at each training epoch: **we want to avoid having too large policy updates.**

For two reasons:
- We know empirically that smaller policy updates during training are **more likely to converge to an optimal solution.**
- A too big step in a policy update can result in falling “off the cliff” (getting a bad policy) **and having a long time or even no possibility to recover.**

<figure class=""image table text-center m-0 w-full"">
  <img class=""center"" src=""assets/93_deep_rl_ppo/cliff.jpg"" alt=""Policy Update cliff""/>
  <figcaption>Taking smaller policy updates improve the training stability</figcaption>
  <figcaption>Modified version from RL — Proximal Policy Optimization (PPO) Explained by Jonathan Hui: https://jonathan-hui.medium.com/rl-proximal-policy-optimization-ppo-explained-77f014ec3f12</figcaption>
</figure>
- *An Actor* that controls **how our agent behaves** (policy-based method).
- *A Critic* that measures **how good the action taken is** (value-based method).

Today we'll learn about Proximal Policy Optimization (PPO), an architecture that improves our agent's training stability by avoiding too large policy updates. To do that, we use a ratio that will indicates the difference between our current and old policy and clip this ratio from a specific range \\( [1 - \epsilon, 1 + \epsilon] \\) .

Doing this will ensure **that our policy update will not be too large and that the training is more stable.**
## PPO Implementation details

- [The 37 Implementation Details of Proximal Policy Optimization](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/)
- [Part 1 of 3 — Proximal Policy Optimization Implementation: 11 Core Implementation Details](https://www.youtube.com/watch?v=MEt6rrxH8W4)

## Importance Sampling

- [Importance Sampling Explained](https://youtu.be/C3p2wI4RAi8)
--
title: ""Proximal Policy Optimization (PPO)""
thumbnail: /blog/assets/93_deep_rl_ppo/thumbnail.png
authors:
- user: ThomasSimonini
---


# Proximal Policy Optimization (PPO)
<h2>Unit 8, of the <a href=""https://github.com/huggingface/deep-rl-class"">Deep Reinforcement Learning Class with Hugging Face 🤗</a></h2>




⚠️ A **new updated version of this article is available here** 👉 [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)

*This article is part of the Deep Reinforcement Learning Class. A free course from beginner to expert. Check the syllabus [here.](https://huggingface.co/deep-rl-course/unit0/introduction)*
<img src=""assets/93_deep_rl_ppo/thumbnail.png"" alt=""Thumbnail""/>  

---

⚠️ A **new updated version of this article is available here** 👉 [https://huggingface.co/deep-rl-course/unit1/introduction](https://huggingface.co/deep-rl-course/unit8/introduction)
 
Here is the question: What preprocessing step is critical for Proximal Policy Optimization (PPO)?","The context does not explicitly mention a preprocessing step critical for Proximal Policy Optimization (PPO). It discusses the intuition behind PPO, its importance in improving training stability, and its implementation details, but it does not mention a specific preprocessing step. Therefore, I cannot provide an answer to this question based on the provided context."
What feature of the `Gallery` component makes it effective for showcasing datasets?,"[(24583, '3e20db8d-779d-4bed-9130-2436441e7181'), (9737, 'e40aecf5-883b-4bbc-b6a8-a09789434e05'), (9738, '85a238fe-28d8-423e-9794-ec7676be8fa6'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (28185, 'd648591c-3383-45e8-ae71-83b504e2b8bd'), (542, '6b01be74-28fd-45be-904a-18c3c2a319f1'), (20019, 'd69308e0-9e9f-444c-9ef0-195f82325f63'), (4149, 'b9c4eec3-d2b9-41ad-8aea-24b4e8bcecc7'), (28216, '5ef0c5f2-e9f3-49a4-9bda-40af0f54e3d0'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (30279, 'b0fd965a-1a97-4c7d-a68e-94be8fced5dd'), (30282, '43cbb499-9f25-4dfb-9d28-ab35808bf326'), (30285, 'efa5bdfe-f753-4272-a25c-47dcf8254a2b'), (28238, '9f1dbb78-44d1-42bb-89e3-c1e21b6fbeb4'), (1109, 'ba1f0ce5-6f0c-4fba-8fb9-ba5b593c2c7a'), (28250, '31ec90bb-9213-4d5b-9a3c-7fd0daac0f7d'), (24156, 'cfb1685e-60cf-4fb0-a3ef-a2470dbb5881'), (27752, 'ba8de9dc-6631-4a04-ae73-15640a799b35'), (7785, 'ad22e59d-f636-4d84-a65f-6eadf54bb114'), (7786, '700a68c9-54f5-42e1-8d75-a8c915f58d0d'), (27754, '4b78d6ef-9ea6-4e0e-9884-7acbb2ebb25b'), (7788, 'ed6e1abf-00ca-4739-b46a-417ed0673975'), (22124, 'e862ae05-e350-458c-893b-9b4a3d0b9f3f'), (28277, '3948f6d2-7f02-4663-bb3c-b29a67aaad6f'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (11405, '76ecfd95-8c27-4b74-ac14-ad406ad32f56'), (7823, '341c1953-c6b2-408b-9040-3ce33e764469'), (662, 'ae006a35-188f-4f3c-9f30-8297ca43f7f0'), (10400, 'ae1a4054-2e9e-455d-8194-38765cd3cde9'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (9900, 'b0b5ab44-4f8b-4eb6-a4e5-117ec4e467e2'), (9902, 'c62a911f-28f6-45d8-8f75-22595b030d73'), (22704, 'd853fcd0-1387-4353-a361-a5295f36b273'), (28849, '6b9bcdb7-185e-4595-8652-2d050f4caf51'), (28850, '5767e40e-5d9b-47c0-b7de-7f6546e7558a'), (30395, 'ab5263c1-7c85-43ee-bd27-89e9944fe67a'), (709, '68b25497-c5e6-494d-aebf-9f31f7c98cc0'), (710, '01412160-d233-49a5-824c-d832796204fd'), (2247, 'e9c8dd04-774e-481f-a3fe-757c0a391f09'), (2263, 'de23a944-32ae-4aca-9342-ed441d8275b4'), (7386, '1e75d6ef-eb2a-47b6-8bf4-a1960ff737f8'), (13019, '4baef4d0-c3c4-4c0f-9525-be1197621a67'), (13020, '28ef4fc5-53c8-4ea8-aa7e-c936f6151740'), (7389, 'b481fda8-b5de-4d04-8ec6-e36a9f1f4039'), (24798, 'e0f48390-3fe1-4f09-a5e1-dd6f48593b5a'), (24799, '66e2c712-6bc0-4938-891b-596496b93a9d'), (7387, '49e59733-bddf-4e92-be0e-5013437fa51b'), (13025, 'c6fff17b-ccc1-4afa-890a-c900ebca91e2'), (17635, 'cb4c9589-721d-4162-839c-0a37be083184'), (17636, '43108549-f353-47b1-9c09-0cf0cd5e34c6'), (16613, '6d7327c7-0b08-42fb-8c2e-ca0ce3529a62'), (24803, '0d7264b1-bf00-4ff7-b817-27ed08d2fabc'), (17128, 'b8f3678a-4561-4c83-b4eb-b6617ffedb76'), (23274, '20ba3cf8-d2ff-4e5f-8b8b-323410a96fc3'), (13036, 'f060113d-71e0-4537-be71-ea1b13869322'), (13041, '201837af-7121-4b15-b685-62fbfd254278'), (21746, 'a42f5255-7093-43fb-9d0d-87c931483f26'), (23794, 'bfecbc5a-6bad-42e0-ad38-b6d977e57e57'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (2311, '2ddd9274-7512-4bdd-8e03-f54c56c6fdfd'), (7441, '33c38ac2-5bfd-426a-991b-cf41b431fd49'), (21790, 'ad9580c8-18dd-4498-9dc7-b2dd03de689c'), (22304, '3897446b-d474-4a3d-9db5-5b91924cce5e'), (22306, 'b1f36dda-09cd-4806-9f8f-da4135b43503'), (21794, 'a16e5c2f-970a-4136-9788-a80f70f87737'), (2342, '865c895b-34a5-453b-b76f-f5c444c3b2df'), (28981, 'bbc96ca5-bfa6-4390-86cd-17eaa483294a'), (5435, '968cfb3d-c68c-49d0-abe9-61d153b093cd'), (5436, '3eecd54e-6656-44ff-b0a3-3c487b30dc3f'), (2364, 'b88b54eb-4f69-48c4-b9d5-2f15f4857a49'), (18243, '85483643-1630-4d82-bb0d-25ca23df82af'), (2376, '7809bd4a-1a54-4abb-bb3f-068d12f9f4d2'), (5460, 'ec7a9ca7-eb63-40b9-aada-8b77c1fde835'), (5461, 'cd69153a-871d-4cbe-aea0-4e9d67623a75'), (9559, 'a8284fc7-f6b2-4800-baa1-e5b290b17b91'), (2403, '2e49d8f0-4c57-4636-ba08-a9484eb1009f'), (5478, '4fedaad9-8186-4ec9-8940-5025adf8bf40'), (8591, 'da53a557-9d2c-49b1-976b-aaa341d7ad09'), (24975, '84ec31e0-6e0d-40d2-aeed-7c845e768cb2'), (24976, '81b311bd-185b-473c-92bf-e1adf46d9090'), (29077, '3b35b1ca-cceb-4e16-be62-55f19cc9c5d0'), (21909, '8f22022a-61da-425f-b5c2-7ffa56f04c88'), (21914, 'b07f0bed-59f5-4ece-ad75-20bf4316937e'), (16800, '2e36121d-c993-4b67-8fd9-84fa17182afa'), (3493, '8a1040ce-1b60-4e24-90e9-cb4e7127b718'), (25001, '68ccd845-4940-4263-9052-bc0fca1a5c11'), (20394, '5ee7db72-d4a7-43e0-9cc0-f5da6aab5d5c'), (22959, '5be42903-bf52-4f5b-850a-b74653728727'), (21945, 'bb2aad32-1962-44e9-8059-67c527f5ea2b'), (21946, 'b42dae6b-b396-4eb3-a4e6-36f030f827e8'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (16324, '0cb7caef-995c-4aec-9c86-5e22705a71b6'), (21959, 'e07e46db-da0b-45dc-98f8-c77c3ec3b081'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (13265, '5efd7ab7-29e7-43d9-8fff-a81a524d04e4'), (20437, 'fd1f7eff-4ebd-4cf2-857a-9d68e09aff3f'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (25559, '4fdfc851-a389-44bc-ade1-889798f2c676'), (28121, 'f47bdf19-919c-4645-96d7-5bde219f7556'), (1499, '1faf2c53-b46d-4aec-a233-dd712314dc97'), (11740, '005c72e8-ed81-43e5-93d7-fc3a05cec140'), (11746, 'c1d1e06f-bcd1-4a21-95d5-aacec2df63fc'), (25060, 'ebc0d9b5-d75c-44b0-96ab-b8b885fc37f1'), (28137, '5116f0d1-8fcf-4fed-971e-06051bd8b4b1'), (15851, '0bde2ce9-28b0-4ae0-9459-a572cca7f941'), (1516, '18d0dd47-15ad-4d34-865a-9277bc2db97c'), (15853, 'cc203abd-bf92-4785-96c4-97f27b0a8da6'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (15855, '59fd244c-a8fa-4c7c-a422-3f51103f15c1'), (3566, 'dc14161e-e267-41ea-a4c3-0189e9da1c9a'), (15857, 'a948a556-2726-422c-ada1-d6f6cfeaeb9b'), (15861, 'cdbf3fc8-ee9f-4a6a-bfd8-e1fe30982ba1'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (15863, 'e17826a6-d5b3-4729-ad97-387ae765122d'), (13818, '37cd1ef0-d23a-4d2e-a301-79ab3583bded'), (13819, 'b5bed38d-29b0-4d78-84a3-3a189d2995bc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Similarily, you can attach images to a collection item. This is useful for showcasing the output of a model, the content of a dataset, attaching an infographic for context, etc.

To start adding images to your collection, you can click on the image icon in the contextual menu of an item. The menu shows up when you hover over an item with your mouse.

![Collection image icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/collections/collections-image-button.webp)

Then, add images by dragging and dropping images from your computer. You can also click on the gray zone to select image files from your computer's file system.


![Collection image drop zone with images](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/collections/collections-image-gallery.webp)

You can re-order images by drag-and-dropping them. Clicking on an image will open it in full-screen mode.
```

<gradio-app space=""gradio/gradio-analysis-dashboard-minimal""></gradio-app>

We will use the same dataset we used to train our model, but we will make a dashboard to visualize it this time.

- `fn`: The function that will create plots based on data.
- `inputs`: We use the same `Dataframe` component we used above.
- `outputs`: The `Gallery` component is used to keep our visualizations.
- `examples`: We will have the dataset itself as the example.

## Easily load tabular data interfaces with one line of code using skops

`skops` is a library built on top of `huggingface_hub` and `sklearn`. With the recent `gradio` integration of `skops`, you can build tabular data interfaces with one line of code!

```python
import gradio as gr

# title and description are optional
title = ""Supersoaker Defective Product Prediction""
description = ""This model predicts Supersoaker production line failures. Drag and drop any slice from dataset or edit values as you wish in below dataframe component.""
Not only this, but 🤗 Datasets comes prepared with multiple audio-specific features that make working 
with audio datasets easy for researchers and practitioners alike. In this blog, we'll demonstrate these features, showcasing
why 🤗 Datasets is the go-to place for downloading and preparing audio datasets.

## Contents
1. [The Hub](#the-hub)
2. [Load an Audio Dataset](#load-an-audio-dataset)
3. [Easy to Load, Easy to Process](#easy-to-load-easy-to-process)
4. [Streaming Mode: The Silver Bullet](#streaming-mode-the-silver-bullet)
5. [A Tour of Audio Datasets on the Hub](#a-tour-of-audio-datasets-on-the-hub)
6. [Closing Remarks](#closing-remarks)

## The Hub

The Hugging Face Hub is a platform for hosting models, datasets and demos, all open source and publicly available. 
It is home to a growing collection of audio datasets that span a variety of domains, tasks and languages. Through 
tight integrations with 🤗 Datasets, all the datasets on the Hub can be downloaded in one line of code.
Model Card components

**Model Card Components** are special elements that you can inject directly into your Model Card markdown to display powerful custom components in your model page. These components are authored by us, feel free to share ideas about new Model Card component in [this discussion](https://huggingface.co/spaces/huggingface/HuggingDiscussions/discussions/17).

## The Gallery component

Add the `<Gallery />` component to your text-to-image model card to showcase your images generation.

For example, 
```md

<Gallery />

## Model description

TintinIA is fine-tuned version of Stable-Diffusion-xl trained on 125 comics panels from Tintin album.
### Contributors Shoutout:

No changes to highlight.

## 3.4

### New Features:

###### 1. Gallery Captions 🖼️

You can now pass captions to images in the Gallery component. To do so you need to pass a {List} of (image, {str} caption) tuples. This is optional and the component also accepts just a list of the images.

Here's an example:

```python
import gradio as gr

images_with_captions = [
    (""https://images.unsplash.com/photo-1551969014-7d2c4cddf0b6"", ""Cheetah by David Groves""),
    (""https://images.unsplash.com/photo-1546182990-dffeafbe841d"", ""Lion by Francesco""),
    (""https://images.unsplash.com/photo-1561731216-c3a4d99437d5"", ""Tiger by Mike Marrah"")
    ]

with gr.Blocks() as demo:
    gr.Gallery(value=images_with_captions)

demo.launch()
 
Here is the question: What feature of the `Gallery` component makes it effective for showcasing datasets?","The feature of the `Gallery` component that makes it effective for showcasing datasets is its ability to display a collection of images, allowing users to re-order images by drag-and-dropping them, and clicking on an image will open it in full-screen mode. Additionally, the `Gallery` component can also accept captions to images, making it more informative and interactive."
What key challenge does `gradient_checkpointing` address in model training?,"[(515, 'd6ef4662-a49f-4554-839d-838357119d59'), (516, 'ce8c02d3-fb1b-4e21-b424-921eb9a2f16e'), (26118, '54e056e0-c70d-4b7f-a71b-a0cda0567d61'), (17403, '38d35ffd-8bed-4e4d-863e-d2214e1e2d06'), (1558, '27d77579-c43f-45c9-b0cc-9fdb8ef71285'), (3612, '8fabfaf9-03d8-4ebd-94f3-8fed08ef98ea'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (6687, '52408323-66ca-42de-87bc-450459a8465d'), (15911, 'dea3e203-7c16-410e-9667-1ee9ffb39f00'), (5168, 'd8c5756e-8ab0-4af7-a212-b0895ff6ba1e'), (26673, 'acb0c731-b7d9-4884-8760-09bd1882d5a6'), (21556, '4dbfc5e1-05b5-4a57-88db-428197509bbe'), (30262, 'cb2921cb-3f58-4bd3-aa73-00e1462b10c4'), (30266, '26829486-63c9-4351-bd9a-44b3628ba48e'), (1594, '9622f28a-d122-4157-9bd9-2dd99b146101'), (14397, 'c13b3dcf-abd9-47ba-af8f-be1c2fd14508'), (5694, 'dee73012-4ba6-4015-8099-1ff6348fc9b2'), (5695, '556c87ee-1b6b-463d-bdf7-e1a949117c70'), (14398, 'b76f21ff-7ece-400d-b70c-3cde162ac19b'), (17996, 'ac595454-5bfc-40cd-ae9f-d19b0c2e3256'), (12886, '18488b88-987f-476e-9eb1-02c4211c5a10'), (10326, 'bf75e85e-bc28-462e-a306-a52929809f83'), (10330, '6dfbf5a1-fc7e-4d69-9cc3-a54c7658ea1b'), (10331, '3e81c36c-c486-497f-8503-f7e56fb08165'), (10332, 'a0e48a36-723f-4add-b319-aff6ed8392a6'), (4698, 'beeb1c7a-810d-4c97-a763-086c990219b1'), (24163, '6d254de1-aa08-4a87-9094-64313c3a0c4b'), (25188, 'bf31bc87-0988-43d1-bf36-d47a1e56fd34'), (27238, '39b2c1e0-dff0-4879-9bef-52ae8488417e'), (24169, 'c022deee-9096-4e1a-9834-d3e8c60ba81c'), (4204, 'eb9e94d4-02e1-461e-b271-6c8438d90b46'), (10357, '7be76073-fe5c-4db9-b4b8-94410bf2c5ff'), (18038, '0550b415-da02-4ac3-991d-4402606bb708'), (18040, '6972ccd9-aa60-4ddf-a1e6-de6b0d3acb64'), (19064, '95bc1665-922e-432b-81c8-6d235a40160b'), (24705, '80d32f0e-297d-43e5-b765-0a0a79019c92'), (13956, '3ba2f512-964a-4a35-a1a8-47b90825f60c'), (23684, '5278bf06-a69b-482f-a31f-a9e84563cc75'), (21128, '1b0739a8-867b-4a25-bf9e-966b397ff990'), (17034, '3900ad5e-267d-4c2f-bc71-54582b7591c6'), (11917, '65aab8d9-81bf-4e3e-9461-23fab2f48652'), (7321, '79fbe53d-6f19-4a07-953c-f5b8b6d8e1f6'), (30377, '50ae2b60-8d1c-45ff-a4d1-8f0550fb45a4'), (26287, '63f9c6c4-165a-4162-bed3-3270588915cb'), (16560, 'aa6738e7-0dbd-4cbb-892a-fa52d08ae821'), (16568, 'cd08ada0-ddca-40ba-86d6-07bca5b69ab4'), (6844, '0e9c2f66-6441-471e-a0a1-ee5c92fb996f'), (1214, 'c6152586-1c48-41e4-975f-13183809eb4b'), (19646, '6395f7ed-ad70-4ff7-85ca-da08897c72d5'), (16574, 'baac2ff3-e74b-4a82-8254-a505b4f2e25f'), (3774, '8606777c-8876-42f9-8681-dfd312cc167a'), (30917, '963d0459-77e4-4c23-ae6f-2ef4da277f61'), (6343, 'cdbe6a68-4a3b-41d5-a3a1-c4bdf6f677db'), (6346, '97be871b-ee57-495c-bf58-605bb610abd9'), (6347, 'ff784d0d-58ae-482c-935d-fbf1bc3d8a8f'), (6348, 'c6923392-33ba-471c-bcf1-6fd85617b807'), (14552, '93e86ad4-e93b-4540-9f9d-93fdea4dbf22'), (21211, '9d36201a-8071-43cd-96fc-cacf298b9775'), (5855, 'a54d50d8-31ed-416a-9892-7469898e1998'), (6374, '006f6db7-c046-47c2-9ed9-964dafff35c6'), (7911, '1616d972-2df5-46b2-8a5f-47fe82f11fce'), (23272, '6fccbec4-765f-4aea-9f16-e8840626988f'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (2802, '97538279-e719-4506-8d65-56616ebb6594'), (25356, '7fc6f9b3-10fd-48c7-95bd-fd7ba0cab29a'), (26380, '1a1157a2-0073-4470-a582-6fcd4ee954a1'), (19228, '4bf0a776-a5c2-490d-8ab8-44a14da3e59d'), (5931, '3f975e05-d4df-4eae-9878-2b89981c48a3'), (10543, '595b18a2-b752-4644-b8bf-9f5028af4359'), (20784, '34e47be8-1f85-4672-a1e5-78866c65b807'), (22324, 'c51adaaf-b1e1-45a2-b4eb-ff6522d343c8'), (4918, '1e0edbe0-5591-4783-a872-6c2b258e9fd3'), (20279, '17a9857c-7548-4c1d-a304-d96a32e94553'), (22327, '51a6da27-9f86-49e5-8c20-a0566421c142'), (8505, 'fa1d4c6f-2910-4cba-b78a-191386e0bd13'), (22330, 'd82d72b2-a0f2-47dd-b715-da324d3edea1'), (1345, '64645c76-df63-4566-9ea6-fe57d419e168'), (21321, 'd134d4c7-343d-4c6a-986e-dd66ac2e2bad'), (4427, '3a4777b5-58f7-4c71-994f-d317fee8f1db'), (25945, '6f168761-61f4-445d-a9de-fbd0f065cbda'), (25950, '9b5fe5dc-37a3-41d7-abf3-4535bc6b26a9'), (8035, '85373154-23b0-4c8d-826b-c39c88e0e81b'), (7535, '112382bf-ba59-4a80-bce9-ee6ae2715df4'), (28527, '9dd374d0-f99b-4b6b-9b15-5f4ce6382aa4'), (23921, '609c8963-6100-4e46-9705-69991ce228e6'), (20337, '1ac41f0f-b53f-41f2-adc7-29016e775aba'), (20336, 'd2582ee2-dd1b-40d8-ab3e-09b77460db3b'), (11639, '06c18124-efc8-4182-bed4-17021f3967b6'), (7546, '125265e4-f906-4180-bf64-53f1be1bd941'), (8570, '6f60a637-48bf-416e-9912-e30bd7e541e6'), (6030, 'b80847b8-58e2-40de-9633-2c67a5dc619d'), (22422, '2911a091-409f-4d1f-84cf-760819c09a05'), (15767, 'b073d9f7-f678-42db-b6e4-9f73738cae55'), (7064, '284ad4d1-ec71-4b4f-b163-bbf70b5378f9'), (11672, '5cb801f9-b79f-4264-b02e-8146fcbc6268'), (11673, '5ea921e2-015c-49fb-ae9f-6ec9ba75b905'), (17304, '7de11ce0-e619-4eaa-8b2d-16020d24f29e'), (5016, '2c7de3aa-8b01-440b-8b78-5d6d18c84a7d'), (5018, '189b3cba-29f1-48ce-a236-436ddf0f8a62'), (17313, 'f9684d2d-c144-4e91-a2ec-f9a0fcdd92fe'), (24994, 'df96a09a-d542-406f-b0a2-cf2e62ad0431'), (20907, '789c7a86-81f9-481b-a930-b5e65873ccf1'), (16310, '4d0340ca-2008-4c33-93d4-bf10fff43a0f'), (17346, '2c91cfdd-1757-4160-99b2-4ef422291d93'), (964, '65fb34e4-bfe0-4990-b85f-bf4e0f542e42'), (4549, 'f24ee5b9-be57-409d-b6f9-5aa645d8bb4e'), (8652, '9d7d1d5e-5e20-482e-be5a-9a048f3dbd8f'), (6094, '4cdfb0fb-1392-476a-90e8-10ab82ff69d8'), (27605, 'cd1b3318-dc12-4df9-8cba-01f0361dd48c'), (20950, '251402d9-a92c-4800-ba90-095892100773'), (21468, '4a929ab2-ace7-4a93-9676-273b57094c60'), (20447, '87310f99-c045-4f09-b15e-b8efc2843938'), (18912, '0d70a2a9-cf2c-4100-a0c2-aa272e08a67d'), (22499, '186e7c08-bc8c-42e3-92f8-c0884796b602'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (1524, '5de887ce-b7a7-4223-994c-f8f3ad8e7ec3'), (22006, 'be2eb551-b731-4beb-9a2e-1b492536f810'), (22011, 'dde393e4-b99c-47e7-82ff-4724c17236f8'), (31228, '6a3de332-01d3-429a-8f97-b4f5969219dd'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

### Addressing Challenge 3
Flash Attention and enabling gradient checkpointing are required for faster training and reducing VRAM usage to enable fine-tuning and save compute costs. The codebase currently uses monkey patching and the implementation is at [chat_assistant/training/llama_flash_attn_monkey_patch.py](https://github.com/pacman100/DHS-LLM-Workshop/blob/main/chat_assistant/training/llama_flash_attn_monkey_patch.py).

[FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/pdf/2205.14135.pdf) introduces a way to compute exact attention while being faster and memory-efficient by leveraging the knowledge of the memory hierarchy of the underlying hardware/GPUs - The higher the bandwidth/speed of the memory, the smaller its capacity as it becomes more expensive.
For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)
and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).

## Gradient Checkpointing

Some large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. 
This is because there are other components that also require memory storage.

Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in 
significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed 
during the backward pass, would introduce a considerable computational overhead and slow down the training process.
```

First we wrap the dataset in a [`DataLoader`](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). 
Then we can enable gradient checkpointing by calling the model's [`~PreTrainedModel.gradient_checkpointing_enable`] method. 
When we initialize the [`Accelerator`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator) 
we can specify if we want to use mixed precision training and it will take care of it for us in the [`prepare`] call. 
During the [`prepare`](https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.prepare) 
call the dataloader will also be distributed across workers should we use multiple GPUs. We use the same [8-bit optimizer](#8-bit-adam) from the earlier example.
</Tip>

Text-to-image models like Stable Diffusion are conditioned to generate images given a text prompt.

Training a model can be taxing on your hardware, but if you enable `gradient_checkpointing` and `mixed_precision`, it is possible to train a model on a single 24GB GPU. If you're training with larger batch sizes or want to train faster, it's better to use GPUs with more than 30GB of memory. You can reduce your memory footprint by enabling memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing, gradient accumulation or xFormers. A GPU with at least 30GB of memory or a TPU v3 is recommended for training with Flax.
```

Alternatively, use 🤗 Accelerate - find the 🤗 Accelerate example [further in this guide](#using-accelerate). 

<Tip>

While gradient checkpointing may improve memory efficiency, it slows training by approximately 20%.

</Tip>

## Mixed precision training

**Mixed precision training** is a technique that aims to optimize the computational efficiency of training models by 
utilizing lower-precision numerical formats for certain variables. Traditionally, most models use 32-bit floating point 
precision (fp32 or float32) to represent and process variables. However, not all variables require this high precision 
level to achieve accurate results. By reducing the precision of certain variables to lower numerical formats like 16-bit 
floating point (fp16 or float16), we can speed up the computations. Because in this approach some computations are performed 
in half-precision, while some are still in full precision, the approach is called mixed precision training.
 
Here is the question: What key challenge does `gradient_checkpointing` address in model training?","The key challenge that `gradient_checkpointing` addresses in model training is memory issues, specifically the significant memory overhead caused by saving all activations from the forward pass in order to compute the gradients during the backward pass."
How does the Viterbi algorithm improve Unigram tokenization?,"[(19463, '72f624a9-b037-428d-9837-de3484ab1799'), (29703, 'c39eeb1d-4464-46d4-98a9-c31633860860'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (29708, '57d086cd-c094-4b43-998a-9bf67a10c377'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (29709, 'e3ca7c6b-c286-4bf0-ae83-982df4ff7ff6'), (6177, '9a6af5e6-9ab0-4924-a9e2-817c0c8f0901'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (18999, '3a0a5a40-c022-4462-8532-707c6ae35820'), (19005, '08ceddf6-2b38-43dc-9c7d-c85e39600727'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (5703, 'deacec8b-e008-4941-922b-71bd1841aa71'), (5704, 'f5a76ce8-55f0-4aa3-9058-b471d25fed91'), (19017, '63065524-2b10-49ad-b174-3dfd6a6e17e4'), (10823, 'e21b6aee-30a8-41ca-a03b-c9cbbe3e780e'), (24656, 'a088c20c-8d7d-4bfd-999d-984c49d82451'), (6225, '95dfb88e-25d4-40c5-8d6f-6d3af07e4e0e'), (6226, '496d95c0-9967-4764-94fe-bd01221c5dc4'), (6227, '87c854b5-8830-4132-9d3b-3c9ad5e7c89a'), (6229, '829198af-0f31-4afe-aae8-358ab99a58c7'), (6232, 'd57b5e65-e59a-4e4b-8aa0-7139e4f39886'), (6235, 'da68fe2a-d338-45d1-a04c-022e49b5dcd4'), (6236, '19dbd2e0-ebb8-4c12-bbe1-aa0a3af3755d'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (6239, '0624cf0f-179e-441a-bf41-454d44f35506'), (6244, '69a8d278-e494-488d-a2f2-d94fc92f2bd2'), (15466, 'bf2582ee-b984-4d41-952a-6497f66b25be'), (7789, '206dd674-46b7-4f38-8150-9b15cfd06f87'), (7790, 'a9e5791a-46e7-4308-8134-e24a084a312f'), (7791, '80b0268e-e547-48fb-95d2-622713c6a9db'), (7792, '8d5c7246-3ed2-455b-a56e-538184bdb752'), (7793, '40d93264-9180-45db-b051-ab44da705591'), (6253, 'fa7b9b2e-c67f-4179-b634-54ad01e9f43b'), (7795, '54b5e9cb-abbe-4c4f-a09e-9691c4b11923'), (7796, '673ee2d2-28a6-419c-95fb-ca8cf2d75d9d'), (15485, '9b5dbea1-d5fa-4385-8c03-f268bd24aea8'), (15487, '211f581a-87ba-479c-bbb7-4bf419ff3c38'), (18566, '5d408cd6-4635-4a46-9971-43575951f389'), (18567, '82b5f6ec-fc59-45e3-8d39-a7e17af46f95'), (18568, '3e6b192d-ac77-405e-b509-7c84053c11a8'), (18569, '09e8a18a-b009-4152-bc84-f97c593e35df'), (9376, '4aebdbed-0082-4301-a2ce-643093345289'), (9377, '11462af2-125e-4c1a-91d0-9dba82209e93'), (19620, 'bb527b6e-8ac3-4196-b8c6-47c4c0b962fe'), (19621, '2100eee7-6ff0-4550-a16b-6538c747e693'), (7342, '4e3f892e-b398-4de7-91ee-ce188a823d68'), (690, 'b6bf8779-7ba4-44e2-956d-16eaea7da823'), (24764, '645657c8-1e5b-415a-9175-a7b48fbed112'), (17616, 'a59fd552-bc52-4d0c-9aba-aefd3b3839f5'), (10455, 'ac623491-7926-46e8-8a45-4c350813dacd'), (10456, 'ecdba708-8f55-48d6-86ee-2ec46b1e2778'), (10457, '34cff3a6-1baa-4a3d-a758-49c4d21bffe5'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (10460, 'ace98882-3633-40c7-b823-208bda3a4ca1'), (4327, '906c0c93-5ef3-428b-ae43-ee8fb9df0863'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (17708, 'bc875152-3402-4283-9c21-9fd595947fbe'), (11065, 'f276616a-e9b5-4dcd-ae38-4256ec518cf8'), (24382, '0dc8af8e-4936-45ec-af58-d8601bc24113'), (11076, 'c68b6b40-c9ad-4e20-abf8-4fe2db7c0f3e'), (11077, '3eba11d4-e601-4b6a-b0f9-0fb7ecced3d3'), (24900, '4889caeb-6d43-410d-b474-d87d0da7fe19'), (24902, '21707633-c150-463b-a123-71a45823b814'), (18254, 'ae1092dd-508c-4ec9-bd90-bb1e8733e822'), (16719, '7f7a0365-377c-4bfb-a3c8-97ce76ba9618'), (16724, 'bcd7e1a3-4be5-4f08-9af9-feef218c2d7c'), (16725, '167730fe-e8e3-492c-a3d4-99715b149522'), (853, '9512fdcc-bf9d-4dc0-abc0-02b85bfb114e'), (18260, '20b06811-c14a-4c0e-b429-99bce86da710'), (854, 'facc2f1f-14f0-407f-a646-e5a4afeca52e'), (857, '4a58c58e-011b-4c6a-9024-25c07d7755eb'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (16726, 'c0e0ead7-7fb9-4e1a-a8f4-41c13bb96ae1'), (861, '06ceed46-34a1-4772-b0c7-2cc21974805f'), (862, '58dce80f-7a13-41a9-8271-89fa056172d9'), (863, '1c31de44-0abf-41d6-96cd-508363daa9a7'), (864, '5920912b-08ae-4795-a60c-d67481685163'), (16737, '4a37b1c8-d2ed-48a2-a3e8-84f2a1e735c3'), (16738, '33cdddd8-07e1-4371-8c3e-b9f64f9afc3c'), (16739, '2155e313-e0b0-4dfa-b90d-3dda854127c2'), (16740, '567a8c57-7015-46ae-87bb-d7aef479b1ca'), (16741, '78ad9661-d100-44d8-818e-a6130c7ae5ee'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (1414, '9772c7cd-c19a-4ee9-abc0-bd27da5ee279'), (1415, '1411d969-fc20-4d02-9d2f-76d8d2732150'), (12174, '11b1816b-c38a-44f5-ae65-7589d37c8c3f'), (15247, '9e91e376-e5c3-42b5-822e-a77bfbed897b'), (15248, 'ca77aef7-6039-4c5b-80b1-c76f83ab3f28'), (12175, '7d112eff-907a-4a4d-8a99-77bca14c5a4a'), (1424, '7da8fa90-541d-4a40-831d-ce0c019039d4'), (15251, '12885ed4-98da-492a-a72b-67027a02f6a8'), (3479, '6d99cd47-980e-4cae-a7d5-415306bd443d'), (4505, '82ea4097-6dd6-4ae8-b0e9-416160ce9a4a'), (4506, '6381c1d4-8050-42bd-a4f9-137f3a69cf1b'), (4507, '7f03af48-608f-4463-ad30-e8d317782576'), (27545, 'c9fb2425-2ac4-4707-88ad-e11ddfbab842'), (19871, '0a993e9c-5a70-49a8-8b8a-ea79fff01d49'), (31146, 'ef00880f-edbf-4140-ada1-2539d245741e'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (29612, 'ae39f1ab-1b94-4882-b706-1e93f0941895'), (31149, 'debd5c00-01da-49e4-9fac-acfd4e6bcd78'), (31148, 'd00be285-264f-4094-ab8c-a95d9edf5075'), (29615, '2275ba32-0f01-4d0c-990d-ceaf68cba2af'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (31153, 'e4910d18-2062-476e-95cf-13e2844c2ada'), (31154, 'e49097a1-9b93-492c-9e9c-343a02a27e52'), (31155, '4ce7c97c-6ce1-4af5-976f-458bbfed6369'), (29617, 'ff9c2445-4c03-4813-a426-7902846a33df'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (31151, '049c5b5c-6e93-4d3a-9c0b-5ad8a7032aee'), (29616, '5f1337a0-af97-4b1c-8e08-b3cdfd401241'), (18914, 'a7c8f0c2-85bf-44c3-98d8-dcb026fa61d4'), (19441, '63a744d3-8c1c-4b6b-8922-02e881fd1603'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c'), (25595, '1d6aa017-7e13-45e8-b519-5e38bbf64b5b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: them. The token which we will remove is the token which impacts the least the loss: here the token ""bu"". We had mentioned at the beginning of the video that at each iteration we could remove p % of the tokens by iteration. The second token that could be removed at this iteration is the ""du"" token. And that's it, we just have to repeat these steps until we get the vocabulary of the desired size. One last thing, in practice, when we tokenize a word with a Unigram model we don't compute the set of probabilities of the possible splits of a word before comparing them to keep the best one but we use the Viterbi algorithm which is much more efficient. And that's it! I hope that this example has allowed you to better understand the Unigram tokenization algorithm.
To find the path in that graph that is going to have the best score the Viterbi algorithm determines, for each position in the word, the segmentation with the best score that ends at that position. Since we go from the beginning to the end, that best score can be found by looping through all subwords ending at the current position and then using the best tokenization score from the position this subword begins at. Then, we just have to unroll the path taken to arrive at the end.

Let's take a look at an example using our vocabulary and the word `""unhug""`. For each position, the subwords with the best scores ending there are the following:
```

Now the main function is the one that tokenizes words using the Viterbi algorithm. As we saw before, that algorithm computes the best segmentation of each substring of the word, which we will store in a variable named `best_segmentations`. We will store one dictionary per position in the word (from 0 to its total length), with two keys: the index of the start of the last token in the best segmentation, and the score of the best segmentation. With the index of the start of the last token, we will be able to retrieve the full segmentation once the list is completely populated.

Populating the list is done with just two loops: the main loop goes over each start position, and the second loop tries all substrings beginning at that start position. If the substring is in the vocabulary, we have a new segmentation of the word up until that end position, which we compare to what is in `best_segmentations`.
n this video, we will study together ""the Unigram Language Model subword tokenization algorithm"".

The overall training strategy of a Unigram LM tokenizer is to start with a very large vocabulary and then to remove tokens at each iteration until we reach the desired size. At each iteration, we will calculate a loss on our training corpus thanks to the Unigram model. As the loss calculation depends on the available vocabulary, we can use it to choose how to reduce the vocabulary. So we look at the evolution of the loss by removing in turn each token from the vocabulary. We will choose to remove the p percents which increase the loss the less.
At each training step, the Unigram algorithm defines a loss (often defined as the log-likelihood) over the training
data given the current vocabulary and a unigram language model. Then, for each symbol in the vocabulary, the algorithm
computes how much the overall loss would increase if the symbol was to be removed from the vocabulary. Unigram then
removes p (with p usually being 10% or 20%) percent of the symbols whose loss increase is the lowest, *i.e.* those
symbols that least affect the overall loss over the training data. This process is repeated until the vocabulary has
reached the desired size. The Unigram algorithm always keeps the base characters so that any word can be tokenized.

Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of
tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:
 
Here is the question: How does the Viterbi algorithm improve Unigram tokenization?","The Viterbi algorithm improves Unigram tokenization by efficiently determining the best segmentation of a word into subwords. It does this by computing, for each position in the word, the segmentation with the best score that ends at that position, and then using the best tokenization score from the position the subword begins at. This allows the algorithm to find the path in the graph that has the best score, resulting in more efficient and accurate tokenization."
What feature of the Hugging Face `pipeline` API supports zero-shot classification?,"[(13825, 'f1446f89-52a4-4aaf-ab1a-9b92f5010ce7'), (13826, '2a55b5cd-c496-407c-97b0-92e3ab12e0e7'), (11270, 'ed793555-b840-4bd3-b995-45b59931a684'), (3594, '4e1600b8-d386-41a9-bc22-67a03574e9d3'), (3595, 'fc346d5a-1341-4054-aff8-c1e07891108a'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (27689, 'a45172f4-2f68-4ad4-969f-0e32e39df5cb'), (28716, '0e901ab3-56fc-46c6-a3c7-adee2baa6fca'), (24127, '474b8446-9f4d-4ec3-9b0e-78695268902d'), (22079, '16104f25-f547-44dd-9b5f-3798c428cc44'), (22083, 'dd15b594-110b-4773-b128-0658f80e911d'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (14429, '49a2886b-403f-463e-8384-2fb84e5ab062'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10858, '33249571-1ade-46fd-ae89-e32ff806fdcd'), (10859, '7140610f-8a0a-40c5-b6af-12b5af75f3c7'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (2668, 'cf2d43e8-7268-49a8-a30e-520cb997cf07'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (23159, '8df7603d-0d12-428d-8ffd-3b4c55cd0c5f'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (21639, 'f89e686f-ed68-4e75-a452-e5832f6fd820'), (15500, 'faef5ce1-f4bc-4b19-bd94-f0bd6be7eace'), (145, '82bcd7b4-a187-4092-8487-e9a50c1ae2b8'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (150, '3bace117-5049-4192-b247-676e182cf109'), (149, '19d1f1a7-3ff1-4d99-b448-86fda7b1e1a1'), (153, 'a5e94b25-4bd0-4edc-be2a-4167144129cb'), (5789, '01b13fa9-405e-4f18-ad86-5a8549a59c25'), (15012, 'cb4abadf-5fbe-4fc2-a16d-54b0f4981316'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (5810, 'b507f388-1038-4bc8-8559-2d51ffaec850'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (21187, 'd9dcef43-8125-461e-9c4f-131925ad7523'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (712, 'a2d8b244-cc83-4286-af6e-08f1ef96ed7a'), (22223, '2bed839b-dad7-4906-aac6-0fd1dbd515c7'), (21203, '5f38e41b-22a4-4ec8-b993-4864b7079429'), (23251, '23c5e66c-a5d9-4d1b-beee-3e3345488c3c'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (8929, '6416a685-c9f1-4d8e-a261-4d5e3377eb82'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (17638, '2a3bbb6e-24c7-46d2-8b6a-c6fc79c55d6c'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (5888, '7a16f065-8a67-4207-afcb-1487fd8ec145'), (26881, '6a5ea7b7-fc90-49e6-b2be-bb315aabc5f2'), (26883, '2933d4ed-2563-4956-8c7b-5b5afa75f91e'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (18701, '9c065cf2-bb0a-419b-9090-9e75a3bf2630'), (28943, 'dd1112c0-4e24-4bc6-9824-e92c5e1de2bc'), (12050, '67332def-d4e8-4b99-b29c-af15da1631fb'), (3859, 'e092e829-7c5f-48db-b5d9-1cd53e2de720'), (1309, '8adeb3ac-69d3-4c67-88f9-39acd31a5b74'), (24356, '07db076b-feac-48e0-a517-caf251d54bdf'), (31031, '306dd725-df38-43a2-a289-44917df0a6f1'), (5944, 'af31af69-da28-4239-b3f5-59c624ec8d83'), (5945, '82b994e1-d6d9-4269-92fd-5bf9f88254b7'), (5946, '7a54286f-a79f-4ab2-b704-5487dafb7a5b'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (5948, '68fab821-06c3-4a78-96e2-4dfd616f31c0'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (16702, 'cf05cd6b-e33d-4bc7-a11e-5058f0d0676b'), (14165, 'f8e81797-b080-4a41-9f82-86eea16eabe8'), (31586, '4351b921-b4f2-4a0a-a126-07b9ddb21e7f'), (4970, 'c2e7fcf7-8c1c-4593-b9ed-6b554253e421'), (15215, '8b19f2fb-4cae-4528-8a1e-bbe343f82786'), (28539, '15ca206b-e242-4240-9c56-b957d70a1add'), (14206, '5bfbab3f-215b-442a-8bd0-1810e815b192'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (5037, 'bd256078-3488-4b79-aea0-4e8cb9e443d7'), (5041, '812dcc27-ef00-4194-976d-6f9eb30583a9'), (22457, '7963fb33-54d2-4a57-a431-7124a7d34254'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (20420, '8b41c7d9-6430-42dd-ad00-b6de1ccb8e59'), (16337, '2a82c5a8-c446-461e-87a4-57a53425634d'), (6099, '28c7595f-f297-485b-81d1-fee1aab474c0'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (30167, 'e2e5d454-ed15-4973-9ed5-59bd9e36aa28'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (478, '1bac157f-bc66-440f-b56c-736475c789cf'), (2531, 'dec41cf7-889d-4e85-b490-d5f3c8f4ea84'), (13801, 'aa3ed41c-4b41-428a-a670-fc2e12d17add'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Zero-shot image classification pipeline

The simplest way to try out inference with a model supporting zero-shot image classification is to use the corresponding [`pipeline`].
Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads):

```python
>>> from transformers import pipeline

>>> checkpoint = ""openai/clip-vit-large-patch14""
>>> detector = pipeline(model=checkpoint, task=""zero-shot-image-classification"")
```

Next, choose an image you'd like to classify.

```py
>>> from PIL import Image
>>> import requests

>>> url = ""https://unsplash.com/photos/g8oS8-82DxI/download?ixid=MnwxMjA3fDB8MXx0b3BpY3x8SnBnNktpZGwtSGt8fHx8fDJ8fDE2NzgxMDYwODc&force=true&w=640""
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image
**Hugging Face + Google Sheets Demo**

With the [Inference API](https://huggingface.co/landing/inference-api/startups), you can easily use zero-shot classification right into your spreadsheets in Google Sheets. Just [add this script](https://gist.github.com/feconroses/302474ddd3f3c466dc069ecf16bb09d7) in Tools -> Script Editor:

<div class=""aspect-w-16 aspect-h-9"">
<iframe 
src=""https://www.youtube.com/embed/-A-X3aUYkDs"" 
frameborder=""0"" 
allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" 
allowfullscreen></iframe>
</div>



**Few-shot learning in practice**

We wrote a [blog post](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api) about what Few-Shot Learning is and explores how GPT-Neo and 🤗 Accelerated Inference API are used to generate your own predictions.

### **Expert Acceleration Program**
Here is a list of things we’ll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#🤗-🤝-timm)
- [Diffusers](#🧨-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time 👁

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
## Inference Toolkit API

The Inference Toolkit accepts inputs in the `inputs` key, and supports additional [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) parameters in the `parameters` key. You can provide any of the supported `kwargs` from `pipelines` as `parameters`.

Tasks supported by the Inference Toolkit API include:

- **`text-classification`**
- **`sentiment-analysis`**
- **`token-classification`**
- **`feature-extraction`**
- **`fill-mask`**
- **`summarization`**
- **`translation_xx_to_yy`**
- **`text2text-generation`**
- **`text-generation`**
- **`audio-classificatin`**
- **`automatic-speech-recognition`**
- **`conversational`**
- **`image-classification`**
- **`image-segmentation`**
- **`object-detection`**
- **`table-question-answering`**
- **`zero-shot-classification`**
- **`zero-shot-image-classification`**


See the following request examples for some of the tasks:

**`text-classification`**
The Hugging Face Hub has [hundreds of object detection models](https://huggingface.co/models?pipeline_tag=object-detection) pre-trained in different datasets, able to identify and localize various object classes. 

One specific type of object detection models, called _zero-shot_, can receive additional text queries to search for target objects described in the text. These models can detect objects they haven't seen during training, instead of being constrained to the set of classes used during training.

The diversity of detectors goes beyond the range of output classes they can recognize. They vary in terms of underlying architectures, model sizes, processing speeds, and prediction accuracy.

A popular metric used to evaluate the accuracy of predictions made by an object detection model is the **Average Precision (AP)** and its variants, which will be explained later in this blog.
 
Here is the question: What feature of the Hugging Face `pipeline` API supports zero-shot classification?","The feature of the Hugging Face `pipeline` API that supports zero-shot classification is the `task` parameter set to `""zero-shot-image-classification""` when instantiating a pipeline from a checkpoint on the Hugging Face Hub."
What distinguishes BERT from GPT-2 in their training objectives?,"[(19975, '5249658d-eb3d-4033-9ee7-a22195bc6f8c'), (16395, '8227cdab-ed0c-4745-95b8-2cd3c7df5ce4'), (16401, '83e0cf27-fc46-4cc4-974e-c1060f254d10'), (22552, '0bb09945-b6aa-4d64-9393-49cf0c85d158'), (29729, '0a63f681-05ba-403c-beef-479448022871'), (27685, '55404590-f396-4b7d-95b3-a3478cef23b5'), (20006, 'd78b8574-2f4c-4cfd-9142-1c1ed6cec36e'), (24128, '8475f00c-9f15-4992-a30d-b39c6d80bd0c'), (24129, 'd6421501-e91f-4c71-b473-b9d15f389f86'), (13890, '9e6c479f-1408-4216-b559-473f24c5e41a'), (25182, '983a34e8-fffc-47b2-af27-ef060cef3b0b'), (15466, 'bf2582ee-b984-4d41-952a-6497f66b25be'), (20590, '63198484-f72d-4b9f-a549-b750a5590cbf'), (15481, '87ce1504-d1e1-49b1-9c72-7fb74be5a8e7'), (13435, '7a5ab327-9cb4-4389-8775-bbf53854a019'), (13436, 'e2b774e6-5237-4e9f-8716-229315e0fcaf'), (10893, 'f39f880b-b4a5-491f-ae2c-b244e4d26b70'), (13968, 'a4f24ed8-51c0-4e5b-b72e-fc73e05ba0c1'), (7826, 'f72a2714-19ea-4106-b552-9efdcb270198'), (7827, '2b137736-ff22-4a82-a789-80f640d114a9'), (9363, '143d4090-6c74-447b-b404-b27270794a1c'), (23718, '8211bbbb-1371-43e9-b81f-4d5c088e4c59'), (1197, '12a8d1ac-5cd2-427a-a6d2-e14c3591a137'), (4785, 'cbe65301-91d7-4859-b985-fc44d6c3bb68'), (30897, 'c4d781f5-c154-4a61-bdd9-76776461be08'), (23752, '448cbb61-aac1-4dcc-b254-ebf54813788b'), (25290, '16b47a24-aff2-46e9-8b5f-87234f7699d4'), (23765, '4cd54c37-c382-4758-bebd-2b7ff7aae41d'), (23771, '35a287fd-2c35-4cfe-8279-3d6d2f963231'), (8924, '28be2b30-bdcc-4a8c-a385-026b57358177'), (1758, '0ace460f-39fa-4f73-9b29-31d0f72fc7b9'), (18146, '8d880def-73e5-4344-9189-645db0bbec8a'), (6902, '50eda901-3731-439f-9640-49e039f839ed'), (13564, '8ad7ffdf-7d22-41fb-a243-b5930041c5c9'), (19710, 'b89d5455-b007-4aac-808d-12d831e67b7e'), (18692, 'a4f174fa-f21a-4a5e-b591-ae5e955a9ef4'), (17157, 'b9540860-ae17-4e26-b1f5-6ea2ad1ca46e'), (262, 'f0497ae2-095c-40b3-ab07-2a74673af8b2'), (4356, 'c00b9f38-6f5b-4c24-b0fa-b45006f1b2c1'), (20236, 'e4d3333c-81f7-42ae-9a28-0cb22d1c5bce'), (28947, '633f4bd5-8841-4dfe-a590-7980557ba1da'), (28951, 'cd36a4ca-dbbe-4965-bc74-b67cc07e86f6'), (10528, '963907b6-d810-4d44-9614-533799a9faa6'), (31008, '3f401b96-9ff0-4f59-b937-0dead68da5d8'), (30510, 'c72a7174-000b-4d01-9d21-56dd6dc3d5ab'), (30511, '5bf2b9fe-6f07-4d44-9f8d-353116371696'), (21810, '984ed666-72eb-4f58-bc53-465ae53447e2'), (23884, '16adda67-00bd-4c39-bafb-6fa2779497d2'), (6989, '03087320-4dbe-4ba4-8b78-3427b35f3489'), (23885, '43a46fb6-dff8-4a20-8da7-1f6c46088a56'), (6990, '5a2a3311-db49-43c4-b253-f4c25c4b8a2a'), (11086, '2c69e9f6-788f-44ff-8742-db7169f4ffcc'), (11088, '713cfa65-5ce3-4256-a2c0-b96de118eb20'), (11089, '23dcb623-f751-470b-bdda-3a7c2e191213'), (11091, 'bf042742-13b3-4345-aaa5-bd9e891d03fe'), (23891, '3faf98a5-c81a-4897-a244-0da94b1e0761'), (23890, '5d51a959-3b99-4ea4-9614-65e8b5666909'), (8022, '542d128d-72d2-46c5-a25e-8c55ddf820ec'), (6988, '468c7882-e6e3-42c8-88d2-d5202ee073b1'), (8028, '21ebcb85-169a-4633-93aa-9658a1dc285b'), (8031, '33c139a4-ec54-4947-b4f3-3ef43932a64e'), (8032, '01572826-4337-4e15-ad29-e51ee2fe3007'), (7523, '0d2b8e28-067e-4869-89db-3c2183d0d99c'), (11112, 'dc745ed5-f514-4cbf-bc99-349a56a01fdd'), (1899, 'f888ee25-af78-4996-8147-194908c2b4b0'), (1900, '9ac81dc8-63e8-4bb5-b1a2-66f999fdf9c9'), (1901, '30e0cbba-9c95-4e87-ac67-4edee57840a3'), (11120, '591a6a42-5538-4cbf-8bb6-123ad55aeee0'), (8050, 'f718b308-7fed-477e-b7be-0131756a1ab4'), (8051, 'abbfb3fc-8b6f-478f-a25d-569b924bb9e6'), (24947, 'f3fff8ee-8aab-4047-a379-2e9d1a3d8617'), (11125, 'c069e099-d3ce-40a4-bcc5-375c0460fb62'), (8057, '7a650882-2b62-45b7-8011-c145954cd795'), (9082, '60ce2779-9692-49db-b1f3-ab44ae8711ef'), (8059, '46629501-2d75-47d7-9c2f-9d9e97a2f935'), (8060, '9eef21f5-4170-4668-bc2d-bfb804087692'), (15742, '3060302a-236b-4c9e-94f7-f4e0bb06e38a'), (11135, '743e9ff8-f1d6-40c1-ac4e-8f2eb83e3c7c'), (11136, 'c9bf0d63-b2c2-4532-8cfc-f562d16c3432'), (11137, '5f9ecc39-13fa-4c31-ac3d-8d2865593051'), (899, '7d822133-9736-4135-882c-3d3774556565'), (8076, '59fecb92-1150-4db2-8762-ae5a821504a4'), (31637, '4d19baa1-a2bf-4a3e-8789-9f05cf96f733'), (5527, 'bbbafbc0-ab0a-4ab6-8033-d5a951de3f3c'), (8087, '380ce176-83fc-4b43-9912-17d34b2a2be8'), (31640, '83ae1713-8091-41a8-9d58-2d28cde94414'), (20890, '9715d892-84b1-415b-a2a7-180cddf1c895'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (25500, '27b59c24-f5b9-4120-a4d3-75f8231e76af'), (25501, '140e181e-c801-471c-94d3-f6cec80cac7b'), (31652, '70e0a2f0-75da-42e8-ba35-f26d73b35790'), (5541, '6b20e548-35e0-49fc-9aa6-fa3e4b0fa079'), (16813, '84307ab5-f0bb-4bb8-be18-61dc9b9a2dc0'), (16817, '56522f3d-9720-4837-87d4-b42f3242d2ff'), (16820, 'd067336a-be06-4a9e-b551-89d23eb59ebd'), (29627, '759e8e09-8a4b-4916-81f2-8f9b671670aa'), (16827, '9548d4e8-62bb-403e-b4b0-a233ed540940'), (16837, 'ba6c0f25-4741-4c53-a106-814fc3cc5bc4'), (18376, '07f3b9f3-2de7-4f28-b568-eb9fd6f92c2e'), (22474, 'b86e51fa-c463-4437-8d9f-af512824414a'), (30159, 'a98268f0-0b89-4a33-9000-675922e84e2b'), (5073, '7359c8d6-2ea3-4b58-9040-768f0fe43997'), (6614, '8adaa619-97b2-446b-bc9d-86d585ad4d68'), (6615, '4b4eeb94-e396-4c99-a6ba-dd6d9503e03f'), (18908, 'e65fc3e8-d447-4715-8188-6ecd67d5ae7b'), (13292, '2e5c6ac3-19ee-4ddd-9d08-354a10cf7a44'), (25588, '296aa0ef-d372-49c2-ba62-a29c3164b46e'), (15865, '550ad701-7ec0-4e61-ae09-35d40146fb91')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ### Decoder[[nlp-decoder]]

[GPT-2](model_doc/gpt2) is a decoder-only Transformer that predicts the next word in the sequence. It masks tokens to the right so the model can't ""cheat"" by looking ahead. By pretraining on a massive body of text, GPT-2 became really good at generating text, even if the text is only sometimes accurate or true. But GPT-2 lacked the bidirectional context from BERT's pretraining, which made it unsuitable for certain tasks. [XLNET](model_doc/xlnet) combines the best of both BERT and GPT-2's pretraining objectives by using a permutation language modeling objective (PLM) that allows it to learn bidirectionally.
## Language model training

Fine-tuning (or training from scratch) the library models for language modeling on a text dataset for GPT, GPT-2,
ALBERT, BERT, DistilBERT, RoBERTa, XLNet... GPT and GPT-2 are trained or fine-tuned using a causal language modeling
(CLM) loss while ALBERT, BERT, DistilBERT and RoBERTa are trained or fine-tuned using a masked language modeling (MLM)
loss. XLNet uses permutation language modeling (PLM), you can find more information about the differences between those
objectives in our [model summary](https://huggingface.co/transformers/model_summary.html).

There are two sets of scripts provided. The first set leverages the Trainer API. The second set with `no_trainer` in the suffix uses a custom training loop and leverages the 🤗 Accelerate library . Both sets use the 🤗 Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.
the pretraining objective of GPT-2, the fill mask pipeline is the pretraining objective of BERT, which is to guess the value of masked word. In this case, we ask the two most likely values for the missing words (according to the model) and get mathematical or computational as possible answers. Another task Transformers model can perform is to classify each word in the sentence instead of the sentence as a whole. One example of this is Named Entity Recognition, which is the task of identifying entities, such as persons, organizations or locations in a sentence. Here, the model correctly finds the person (Sylvain), the organization (Hugging Face) as well as the location (Brooklyn) inside the input text. The grouped_entities=True argument used is to make the pipeline group together the different words linked to the same entity (such as Hugging and Face here). Another task available with the pipeline API is extractive question answering. Providing a context and a question, the model will
is usually self-supervised, which means it doesn’t require humans annotations for the labels. A very common pretraining objective is to guess the next word in a sentence, which only requires lots and lots of text. GPT-2 for instance, was pretrained this way using the content of 45 millions links posted by users on Reddit. Another example of self-supervised pretraining objective is to predict the value of randomly masked words, which is similar to fill-in-the-blank tests you may have done in school. BERT was pretrained this way using the English Wikipedia and 11,000 unpublished books. In practice, transfer learning is applied on a given model by throwing away its head, that is, its last layers focused on the pretraining objective, and replacing it with a new, randomly initialized, head suitable for the task at hand. For instance, when we fine-tuned a BERT model earlier, we removed the head that classified mask words and replaced it with a classifier with 2 outputs, since our task had
API with the default model associated to each task, but you can use it with any model that has been pretrained or fine-tuned on this task. Going on the model hub (huggingface.co/models), you can filter the available models by task. The default model used in our previous example was gpt2, but there are many more models available, and not just in English! Let's go back to the text generation pipeline and load it with another model, distilgpt2. This is a lighter version of gpt2 created by the Hugging Face team. When applying the pipeline to a given prompt, we can specify several arguments, such as the maximum length of the generated texts, or the number of sentences we want to return (since there is some randomness in the generation). Generating text by guessing the next word in a sentence was the pretraining objective of GPT-2, the fill mask pipeline is the pretraining objective of BERT, which is to guess the value of masked word. In this case, we ask the two most likely values for the
 
Here is the question: What distinguishes BERT from GPT-2 in their training objectives?","According to the context, what distinguishes BERT from GPT-2 in their training objectives is that BERT uses a masked language modeling (MLM) loss, where the model predicts the value of randomly masked words, whereas GPT-2 uses a causal language modeling (CLM) loss, where the model predicts the next word in a sequence."
How does the Hugging Face `Trainer` API simplify model fine-tuning?,"[(28676, '0af11ae0-e244-4a1b-ac29-096ddb00a23d'), (17413, '5c272bf9-ff61-4ced-9930-7c174b92dc98'), (3594, '4e1600b8-d386-41a9-bc22-67a03574e9d3'), (1551, 'b2b7b102-cf63-4933-babf-b2616a5830c9'), (9232, '6757578a-dbe3-4e3c-a9b2-de1fd9fae0a7'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (1561, '2e093792-5611-47fa-9b0f-dccd79bce526'), (18969, 'c88421de-34f5-4579-856b-24781e8372cf'), (4129, '80849eb3-bebc-488f-baee-21d69ca15f2c'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (5155, '86625bb7-792a-426a-9ade-ea98317da2e0'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (23087, '78540d5c-200a-44a6-80b8-ac019e34baf9'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (13874, '3aa09c5f-d782-4417-bf6d-9a16dc742ef1'), (14391, '776da117-491d-40bb-8d06-6276fe14bcf6'), (26693, '74a1767c-cb28-44b5-9581-88def78019d3'), (1609, '335231ef-3cdd-4154-bea0-92054d45edc3'), (20041, '0521172d-93c3-47ab-8eb4-57b9a744de8f'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (82, '9b1467ac-93fd-4e92-ae66-0c561a9fe744'), (13398, '42004fba-1418-4a98-8b40-6539bc0dd8d3'), (10840, '17512ae9-2273-4239-be81-2a79343a68e4'), (24675, 'b0898020-ae5d-4a88-a4bf-4140d6f134c2'), (13421, '794edc9c-81e0-43ec-94cb-c67e7bbe50c7'), (25710, 'd79930fe-1968-4b54-987e-bc987274c7b2'), (3183, '05adffa9-2646-489c-b3ad-2357cb108a8e'), (6769, '0115ab27-2f65-4f55-ac25-301080ce38b6'), (7283, '0dce4d24-dba7-45fc-b775-1aed44862367'), (18549, 'd84bf0de-2759-4c1e-9a94-cc27a2136a9b'), (19064, '95bc1665-922e-432b-81c8-6d235a40160b'), (26745, '509a2418-a582-44fa-9c23-427ca2329092'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (6786, 'bea5187e-94e8-4c28-9338-b31d2da4f110'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (11909, '5a59422a-fad3-4628-a154-f5206bcdaf36'), (7305, 'ae446800-6311-4d46-84b9-7e3088723773'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (21153, '0e8dd5cf-1222-409e-8efa-1a63c0f03561'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (15023, 'cbfb94f6-261d-4bce-981f-2ca1d2339b2d'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (26812, '4220ead8-3c99-4e2c-b3b4-61f5cb7de01d'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (29886, 'ad8e7c8a-7256-4c03-ac9c-fa7b7e149669'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (739, 'cb9b0c29-bf55-4e97-82d9-32783b8d6e54'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (7922, '159410e8-7896-4e2b-93c9-345e8e36a8db'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (4863, '573d2b26-26ef-44f0-8df6-058a342dce58'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (17665, 'daf0b953-1682-4e7b-b581-3054986b069c'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (2832, '998a6650-0b25-4769-81b2-402f8fc9f186'), (28960, '1040c198-e0d5-401e-a9b4-fc013110dcd0'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (20772, '1e69c86c-dc05-4694-bcb5-fc5711c73074'), (24357, '3d30bcf0-69ff-407c-95a6-da0828b87947'), (4907, '57b400bc-9f18-4b0f-8914-e85887b4feda'), (3891, '02ab339e-12bc-4e75-80ad-e89724252f43'), (8504, '143ac2fe-2f79-4c07-bd2e-996611e74123'), (8505, 'fa1d4c6f-2910-4cba-b78a-191386e0bd13'), (29498, 'c7408437-0616-4ade-b32a-0c489529eda1'), (25917, '4e87e174-d954-48bb-ad33-be4edddb51b7'), (30528, '69aa4012-1b33-4373-af24-1d94ec8dd03d'), (16710, 'f734640a-edfc-4987-be06-0b998c1c4861'), (4423, 'dd7ece84-1fee-492d-9192-bf10def5240c'), (4942, '53879912-296a-4865-9cd3-23b21e7e4616'), (9551, 'e1e73c2a-b836-4114-a2fc-bf4c364a2c23'), (15696, 'f5c82a25-56d1-4160-a926-0d697826a6bb'), (6994, '21394a93-d40f-4200-9344-cc9ec5ceeef2'), (4948, '395b39f4-4332-49f3-8d68-c9acecd21bc5'), (21334, '834da3c8-7d6e-4405-9ef3-bd27f264673a'), (7000, '8d065847-3189-4122-97bf-d861398ddd06'), (347, 'd614507c-0464-48b7-bad7-c25df4c56372'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (4960, 'd444769f-6175-4f00-a42b-5059223373ca'), (25955, 'ef44ebd3-e324-4ad4-8b25-fbd6273ad056'), (22381, '45bb3a1c-c7a3-404c-889f-ab2cc824a9d6'), (25456, 'ac2479c5-2682-4db2-a27e-fdf59164519f'), (31128, '0891aee4-6a8a-4e0f-a65e-d4668c18abf8'), (30619, 'aa17c983-fc1c-48fd-af6a-50bfa38933ae'), (23472, 'f143cb65-04bf-4dc6-a99b-008e386f5d9b'), (4530, '006e7cf7-47b3-4211-9fbb-c2a986881355'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (31673, '1483c1a4-0dc9-4497-b744-d59a5373d337'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (30149, 'd5284cd6-0e89-43ab-bbe0-44c9d7f2d010'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (20441, '3fc2ef47-6bb7-49ee-b95e-300483ae096f'), (27098, '4dbebf8f-6846-4cf6-b5c4-39d614dd7a87'), (27097, '666425c1-71c6-44b8-8064-e0bb80375704'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (4070, 'fe807f3d-b428-456c-990e-8f9208dc8217'), (26091, 'eef85ff4-7a3d-44ce-99db-7e1ef9917d94'), (20460, 'f39c222a-ca7b-4789-87e2-f4d8204a7116'), (25582, '4b9fc250-8c86-4ac8-a899-af606ebb9917'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (18932, 'e3182302-a77b-4554-849b-bb3afd5bb673'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Modeling without AutoTrain

We will use [Ray Tune](https://docs.ray.io/en/latest/tune/index.html) and Hugging Face’s Trainer API to search hyper-parameters and fine-tune a pre-trained deep learning model. We have selected [roBERTa base sentiment classification model](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment) which is trained on tweets for fine-tuning. We've fine-tuned the model on google collaboratory and it can be found on the `notebooks` folder in the [GitHub repository](https://github.com/alperiox/user-review-classification-hf-kili).
```

## Set up the Trainer

To fine-tune the model on our data, we'll use Hugging Face's [Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer). We need to set up the training configuration and an evalutation metric to use a Trainer.

First, we'll set up the [`TrainingArguments`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments). This defines all training hyperparameters, such as learning rate and the number of epochs, frequency to save the model and so on. We also specify to push the model to the hub after training (`push_to_hub=True`) and specify a model name (`hub_model_id`).


```python
from transformers import TrainingArguments

epochs = 50
lr = 0.00006
batch_size = 2

hub_model_id = ""segformer-b0-finetuned-segments-sidewalk-2""
later on in this video. When the training is finished, we should do one last push with trainer.push_to_hub for two reasons. One this will make sure we are uploading the final version of our models if we didn't already (for instance if we saved every n steps instead of every epoch). Two, this will draft a model card that will be the landing page of your model repo. Going back to the model page, you can see the Trainer included some metadata that is interpreted by the Hugging Face website in the model card. On top of informations about the training, the intermediate results or the hyperparameter used, we get the values of the metrics automatically displayed in a small widget, and a link to a leaderboard in Paper with Code. The Tensorboard runs have also been pushed to this repo, and we can look at them directly from the Model Hub. If you were not using the Trainer API to fine-tune your model, you can use the push_to_hub method on the model and tokenizer directly. Let's test this to fix
```
    
## Further References 

- [GLUE benchmark homepage](https://gluebenchmark.com/)
- [Fine-tuning a model with the Trainer API](https://huggingface.co/course/chapter3/3?)
```
    
## Further References 

- [GLUE benchmark homepage](https://gluebenchmark.com/)
- [Fine-tuning a model with the Trainer API](https://huggingface.co/course/chapter3/3?)
 
Here is the question: How does the Hugging Face `Trainer` API simplify model fine-tuning?","The Hugging Face `Trainer` API simplifies model fine-tuning by allowing users to set up the training configuration and evaluation metric, and then handling the fine-tuning process, including pushing the model to the hub and drafting a model card with metadata and metrics."
What is the key advantage of LayoutLM over traditional text-based models?,"[(15886, '1f277e8b-db79-4468-af4b-e11deb15d54a'), (15889, 'aad2c393-9f11-40db-b577-03aba97e12a2'), (15893, '05b6f3f7-f2b7-4290-85a9-109ac43a74b1'), (15894, '9be7c721-6636-4795-956e-133562112124'), (46, '93fa1c88-08f0-4d45-bfcd-93c31ccff19a'), (49, '1efa4b78-1a65-4ef9-a7bd-ae0d9ba0fccb'), (28730, 'e9c215d1-838f-480b-89e9-9bffb8f598f8'), (28732, '26f566d8-d9ed-4f9e-a930-8b84075bd6a7'), (28733, 'f836bdf5-abd9-4d78-878b-1eed6e0815c8'), (28734, 'c4312ccc-1453-4779-b880-259e3659a9f2'), (28735, '56165381-810d-434e-9201-84d00575cb88'), (18497, 'e49a1531-a11a-4c31-bd99-b4214e7ad0b6'), (18498, '899a5b0c-bdb5-4f2f-add3-c045afaedafc'), (18499, '9e1839cf-b065-4550-85e5-6eef0c01a02b'), (18500, '601132c7-a766-4738-b8f8-0a14d1084196'), (28740, '5f132404-82a4-4cfb-816b-6ca092d4d7ed'), (18502, '51f3beba-1f28-4dda-8083-00a757ff2875'), (18501, 'dc889574-ebe9-462c-8e67-4cf4a649d73e'), (18504, 'ab801953-53b9-4ef3-9ef4-23ecc3486029'), (18506, '2ab7adfe-f811-4536-a8bc-a20c5a199a61'), (24658, 'f1864d51-a0c4-4b3a-b5b2-89ee1775172d'), (24659, '1ce77664-fd28-405b-b2df-b85564bdba01'), (24662, '00a0aaf0-d10b-4319-9b5b-f247d2cc4d4a'), (20612, 'de424e36-a6d3-4010-b9cb-29aafe220c68'), (20613, 'd1ad2244-95ba-43a6-af74-2b264b02272c'), (20614, '894861fd-6231-4cd8-ab6f-0c19d9c4b753'), (31365, '2f1a7ebc-2583-4d57-9c71-c9dd421d0058'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (5799, '7d6e8705-ceab-43e9-a1de-3f8c4b5e017a'), (21162, '2c577c8e-f71d-49fe-b653-e98024148e66'), (23731, '70cf89a5-3b44-45f3-a4f4-d179871e0419'), (23734, 'c725af5e-eec1-4276-a26c-c4ba0884667e'), (2748, 'c22b3b58-8c0d-48a9-97b2-d8d89b7b5707'), (11964, 'f62ba1ac-f4a7-4acd-a5fc-ddf410ab1d47'), (11966, 'd7d54d2d-6cb7-4459-9ee5-6091ab1da2a7'), (11967, '1731736c-2b48-4e47-aefe-b7ce94cf7fee'), (11968, '339f2d9a-04bc-45b1-ba09-2cb0ee42c1d3'), (5823, '07fcf0fb-ec91-4401-a419-8ec49d41d591'), (4807, '52de4c92-bd98-4936-ae2a-bb0b9e3493be'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (4809, 'abf2fc26-cdb5-482e-af69-0eb9e6c18794'), (18134, 'edec057b-dc14-495b-893a-7a04a35f89ac'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (13552, '74848e30-fb78-4cca-afe2-1d027939cfbf'), (29428, '173a121e-d8f8-4781-80c0-982f5775a7fe'), (1780, 'd58c641f-0b84-48bd-a841-a94c022d81b6'), (1782, 'ab7d79dc-b5d4-4148-a520-a2456cbf8605'), (24823, 'fc6e31f1-cb15-461c-bf95-d4637763257d'), (18168, '3012d4d4-e60f-4363-ae7a-fa0b832c41f6'), (29433, '54590b2d-6c0c-4175-97fa-5c59657c11a2'), (250, 'efa79a0b-ef9a-44ee-8dbc-9370a0757031'), (29432, 'cfe216f3-87f3-4423-845d-6b97b3ceff48'), (18169, '5533b481-295a-42cb-a639-6b7535db6a13'), (18170, 'c664e159-5f97-4cd3-9176-bc322662a4ba'), (29436, '7fc8e06a-e13f-4a56-9b66-6a6dba9c67ac'), (29437, 'a9873ed2-2cf8-44cc-9b63-9ba1c9d4a818'), (29442, 'a2fb8394-94b3-447e-a512-9461a99a583e'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (29445, 'bbcef900-4efc-40e4-a995-d0d601c26cdf'), (23813, 'e529e90a-7751-49cc-8418-262a566b79d9'), (29448, 'de680f2b-2364-4825-bd55-d130bdbe185f'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (13585, 'dc97e6e4-5a6a-41b4-8851-91e3a0e3af3f'), (13586, '29bffc25-131a-4a3c-acd1-0b0609c13d25'), (13587, '05b7e6a6-0d90-4abc-b1d2-dfabde4133eb'), (284, '5e72c7ed-cf69-4a85-8460-5d28bf60ba77'), (285, 'fff20960-c258-4ff5-baf2-cfc2ce558b96'), (286, 'da6a4ed0-790f-4f58-aa39-6df5013da3f6'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (30546, '76a2065f-10de-44f8-b8f2-b60455bedfe1'), (9070, 'f6396021-41a6-42e7-b12c-de89e8c6a0ea'), (16759, '1b62ceba-cef9-48f4-9d42-37f27b4c1c9c'), (7549, '5d8534a4-8f48-47c1-8dc9-b1c30ede1830'), (23427, '44ebafae-3114-46b5-bbfa-3f2d81df8eb8'), (9104, 'cf198758-ade7-47bb-82de-51e1e4e0bc2a'), (9105, '8b68bec4-21d7-407a-8ccf-1eb33fe9062a'), (9106, '0d5f7175-c2cd-4335-9084-dcdbb3bf43a5'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (10144, '78a0d3a7-2154-41f9-b87d-ee66babd1020'), (10145, 'ce96ae5b-5345-4742-bb66-c625e53c53e0'), (28577, 'e8423519-1354-4682-ae08-875751388b76'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (2491, 'aefe200c-ad39-44b6-a12e-575c5fcbf47f'), (2492, '40497a51-4b9d-4870-87b8-6d58dfc9001e'), (2493, '4b6116cb-7892-4969-80b1-11fcc811c1ca'), (2494, 'a522e4ba-2d7d-40bf-9a9c-18de74cc137f'), (16319, '3f64c3db-fe7d-4c58-9506-1df4f37aa265'), (2495, 'c831ecfe-8ba0-4247-97bd-3379c2df25ec'), (2496, '88889674-e469-4f11-967b-ecaa5db509a2'), (16320, 'aceded5d-8291-4ec6-937b-4ffa5e060758'), (18364, '82091703-8ce2-4abe-bc4a-5a4beda4c923'), (5061, '5ebc4085-43ae-4488-9355-8e1b17dfd415'), (2507, 'e3274c16-e60e-45a3-85d6-b3c1800f42e7'), (2508, '03e961b4-71a0-4d72-9397-e2eb7cc4a1c8'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (2516, '02dc7eba-815e-47f3-a8bc-731fc3d4a427'), (2515, '369e23c3-72b0-444c-9267-f700f03a9ca4'), (18398, '881db9a6-a3a5-4163-a27d-d4319313f716'), (18399, '398c917a-6871-4126-85d0-056ee5dbe30a'), (18400, 'ef9b2afc-e962-412d-a475-22cc316750d3'), (5095, '2c960b15-fe52-4cc0-8902-eb912dfbf0e0'), (5096, '628ed62a-7410-4937-86e9-8e980e3d9920'), (5097, '2d07dc17-5032-44db-97e2-731bf7bfca9b'), (15861, 'cdbf3fc8-ee9f-4a6a-bfd8-e1fe30982ba1')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to
its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. In this
paper, we present LayoutLMv2 by pre-training text, layout and image in a multi-modal framework, where new model
architectures and pre-training tasks are leveraged. Specifically, LayoutLMv2 not only uses the existing masked
visual-language modeling task but also the new text-image alignment and text-image matching tasks in the pre-training
stage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware self-attention
mechanism into the Transformer architecture, so that the model can fully understand the relative positional
relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms strong baselines and
*Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the
widespread use of pretraining models for NLP applications, they almost exclusively focus on text-level manipulation,
while neglecting layout and style information that is vital for document image understanding. In this paper, we propose
the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is
beneficial for a great number of real-world document image understanding tasks such as information extraction from
scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM.
To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for
document-level pretraining. It achieves new state-of-the-art results in several downstream tasks, including form
That's where models like [LayoutLM](https://huggingface.co/docs/transformers/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut) come into play. By incorporating not only text but also visual information, these models can dramatically increase accuracy. For comparison, on [RVL-CDIP](https://huggingface.co/datasets/rvl_cdip), an important benchmark for document image classification, a BERT-base model achieves 89% accuracy by using the text. A [DiT](https://huggingface.co/docs/transformers/main/en/model_doc/dit) (Document Image Transformer) is a pure vision model (i.e., it does not take text as input) and can reach 92% accuracy. But models like [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) and [Donut](https://huggingface.co/docs/transformers/model_doc/donut), which use the text and visual information together using a multimodal Transformer, can achieve 95% accuracy! These multimodal models are changing how
-->

# LayoutLM

<a id='Overview'></a>

## Overview

The LayoutLM model was proposed in the paper [LayoutLM: Pre-training of Text and Layout for Document Image
Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and
Ming Zhou. It's a simple but effective pretraining method of text and layout for document image understanding and
information extraction tasks, such as form understanding and receipt understanding. It obtains state-of-the-art results
on several downstream tasks:
The first version of LayoutLM (now known as LayoutLMv1) was released in 2020 and dramatically improved over existing benchmarks, and it's still one of the most popular models on the Hugging Face Hub for Document AI. [LayoutLMv2](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv2) and [LayoutLMv3](https://huggingface.co/docs/transformers/main/en/model_doc/layoutlmv3) incorporate visual features during pre-training, which provides an improvement. The LayoutLM family produced a step change in Document AI performance. For example, on the [FUNSD](https://guillaumejaume.github.io/FUNSD/) benchmark dataset, a BERT model has an F1 score of 60%, but with LayoutLM, it is possible to get to 90%!
 
Here is the question: What is the key advantage of LayoutLM over traditional text-based models?","The key advantage of LayoutLM over traditional text-based models is its ability to jointly model interactions between text and layout information across scanned document images, incorporating visual information and understanding the relative positional relationship among different text blocks, which is vital for document image understanding tasks."
What task is best suited for the TAPAS model in the Hugging Face ecosystem?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (23573, '952579f3-7c93-46c5-ae22-ea5e5fd0b30a'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (31340, '1bffbde6-8c01-4f3d-914e-12799d8de47f'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (27775, '56108da4-6463-4bb1-8393-42e02e4b2cc9'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (6276, '2ce65e1c-ab26-4801-985a-e588681baaae'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (26753, 'b73ff6ee-b7b8-4cdf-9fbf-d80ff9ea9130'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (4283, '53c57917-cb79-4192-a594-583da7166657'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (5827, 'a050f7b4-db8b-44a9-a1dc-19e832bc6cc3'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (25299, 'f6c0a00d-d54d-4ae5-a0b9-af2a5eba1630'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (15064, 'd2be339c-6a22-428d-a082-f7c51fd357ff'), (15578, '4d5e8ece-7925-4dcd-8ef9-c0411120c161'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (15070, 'b79c805d-f30e-4344-a16b-e77f25c0719f'), (22752, '847c0672-9e44-445e-b14d-84817ad403ee'), (10472, 'a81886d9-4860-4589-b7c5-67ead9379d53'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (3368, '17715a5e-5f2c-4df5-94da-06ac43fe341f'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (29040, 'ee3b3272-4ce8-45ce-bb3a-7d0ad91ca9a1'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (22902, 'e34d8c1c-70e2-4b9e-a4e1-7d97d0a2e614'), (16252, 'ebc3b30d-233d-43e4-b012-9f5a958063fc'), (2944, 'd7daee80-75bc-4ee2-bcf3-3cf024d1a940'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (412, '94474cf0-d09f-4652-89ac-bf87619f3cfd'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11174, '626555b1-ecba-4b6b-bb92-b2185675bfb5'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (3024, '6582a1c7-c7c9-4fcb-93db-b4c90e6e31f9'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (27607, '477e553f-670e-4d2b-bda2-3ae34b7a5434'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (30685, 'a039eae8-64c2-468f-9dfd-126ca871a519'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: elcome to the Hugging Face Course! This course has been designed to teach you all about the Hugging Face ecosystem: how to use the dataset and model hub as well as all our open source libraries. Here is the Table of Contents. As you can see, it's divided in three sections which become progressively more advanced. At this stage, the first two sections have been released. The first will teach you the basics of how to use a Transformer model, fine-tune it on your own dataset and share the result with the community. The second will dive deeper into our libraries and teach you how to tackle any NLP task. We are actively working on the last one and hope to have it ready for you for the spring of 2022. The first chapter requires no technical knowledge and is a good introduction to learn what Transformers models can do and how they could be of use to you or your company. The next chapters require a good knowledge of Python and some basic knowledge of Machine Learning and Deep Learning. If you
```

<Tip>

There are more than 200k models on the Hugging Face Hub! Each task in the [`InferenceClient`] comes with a recommended
model. Be aware that the HF recommendation can change over time without prior notice. Therefore it is best to explicitly
set a model once you are decided. Also, in most cases you'll be interested in finding a model specific to _your_ needs.
Visit the [Models](https://huggingface.co/models) page on the Hub to explore your possibilities.

</Tip>

### Using a specific URL
We use the most efficient methods built into Hugging Face model [pipelines](https://huggingface.co/transformers/main_classes/pipelines.html) to reduce the amount of computation during each forward pass. These methods are specific to the architecture of the model and the target task, for instance for a text-generation task on a GPT architecture, we reduce the dimensionality of the attention matrices computation by focusing on the new attention of the last token in each pass:
After a first test on Google Colab, the Hugging Face experts guided Witty Works on deploying the model on Azure. No optimization was necessary as the model was fast enough.

```diff
Elena Nazarenko, Lead Data Scientist at Witty Works: 
“Working with Hugging Face saved us a lot of time and money. 
One can feel lost when implementing complex text classification use cases. 
As it is one of the most popular tasks, there are a lot of models on the Hub. 
The Hugging Face experts guided me through the massive amount of transformer-based 
models to choose the best possible approach. 
Plus, I felt very well supported during the model deployment”
Alright, the next step now is to find a suitable pretrained model to be used for fine-tuning. This is actually more difficult than it seems given the large amount of pretrained and fine-tuned models that are on the [Hugging Face Hub](https://huggingface.co/models). The best option is usually to simply try out a variety of different models to see which one performs best.
We still haven't found the perfect way of comparing different model checkpoints to each other at Hugging Face, but we provide some resources that are worth looking into:

-   The [model summary](https://huggingface.co/docs/transformers/model_summary) gives a short overview of different model architectures.
-   A task-specific search on the Hugging Face Hub, *e.g.* [a search on text-classification models](https://huggingface.co/models), shows you the most downloaded checkpoints which is also an indication of how well those checkpoints perform.
 
Here is the question: What task is best suited for the TAPAS model in the Hugging Face ecosystem?","The context does not mention the TAPAS model, so it is not possible to determine what task is best suited for it in the Hugging Face ecosystem."
How does CLIP support multimodal tasks in the Hugging Face library?,"[(11270, 'ed793555-b840-4bd3-b995-45b59931a684'), (15371, 'bd8140d0-5f35-4354-95e0-dddc95efe4dc'), (15375, 'e332f0d9-263a-4ecd-b2fe-4c3a681a7cda'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (5140, '9c749309-ea99-43d6-875c-cb62f74db39f'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (15395, '71a93765-78eb-42ad-b0a5-3203d5f8995b'), (16932, 'd165fea3-524c-4a1f-8627-923c16bdcb4f'), (5170, '9bac6e65-279d-46be-9505-6b87ade77f71'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (20579, '1cadc685-b43e-4779-9427-c24086103ace'), (20580, 'd3179689-1789-4157-83a3-9dfab3333b49'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (7805, '0e09b839-24f1-4883-bb8c-4921cb04ebda'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (7809, '173bc460-5338-4f7c-9b69-916e9e03ecfb'), (7812, 'd2ee2299-ee53-476e-9f83-1c5ab9120907'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (6286, '17043260-ccb0-41be-a8d7-cd50fdf8f0c8'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (667, '466a4c47-d6a2-4439-8124-9da2dea99f39'), (5793, '358177a4-5892-4a9b-9356-18b7f3fb59d8'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (4774, '331613c0-519f-4a91-aa47-3f15174210ed'), (4775, 'dece872c-e754-4f2b-b021-07367964cf41'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (20661, '3d03f793-112e-4835-89e0-d0c9ab6a236d'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (1748, '867c8140-6d94-4898-9809-7d0a4048ff6a'), (1749, '22147376-467e-46ca-bd5a-e6e7fd40222e'), (11477, 'c5876285-5fa1-4172-9c57-bef923092cfa'), (18136, 'ea71868e-38ba-42e9-9d5c-3ac4745b0a68'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (3823, '1b242c88-9fa6-4480-8d4a-167fc47e1885'), (13554, 'f239a4cb-1368-4161-bb6c-d4d76b74b26d'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (252, '1d1458f5-0779-4f41-8eab-579cf8943202'), (21759, 'dbfc4df5-b25f-4178-9029-97d1d8c84f9a'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (28943, 'dd1112c0-4e24-4bc6-9824-e92c5e1de2bc'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (18217, '4e0278ab-2929-47df-8ac5-54be5d98a007'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (13634, '63e875a9-4fad-4f80-8454-e567ece5a4ac'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (333, 'b89f73b6-20ff-473e-9c00-802ac258ce6c'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (9072, '7a516a26-4603-45f9-b69a-f48eb9d9c67e'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (14203, '63a95699-e58b-45ee-9516-c8e461b3b930'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (31615, '2a9d091b-0c54-45c7-aed4-9ec246ffca0a'), (31619, '15b09247-3696-4abe-b6ab-82b08cf31da4'), (31620, 'f5fe6877-60cd-4f03-8287-da18e67df6dc'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (22928, 'a8321404-e3d8-420e-b234-a78cf2bdc969'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (22930, '25b5e287-3b54-40ee-9fc0-06927f1cd7cd'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (22951, '3aac8015-6063-4901-8593-99dfff0250a1'), (22966, '8b7b6719-a99f-4081-976d-76c680180f98'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (22970, '36b2f7d0-d2f2-4773-a1a0-4253ebb02ddc'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (9149, '7946e92e-6ff7-420f-9b65-34ff8498ebeb'), (18366, '8a7e2dee-f14d-4ea1-819d-851a6affb887'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15299, 'c141a388-b78f-49f1-aefc-636b7195e91c'), (5063, 'f4cfdc74-4cbc-4213-a3fb-a400b432dc40'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (29652, '250e9a0d-0efe-494b-a5ce-ed0c9f5008c4'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (23014, 'd29e7466-2ce3-4018-8252-e65865458a74'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## What can you find on the Hugging Face Hub?

### Models 

The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.

### Datasets
The Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.

### Spaces

Hugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications.
Here is a list of things we’ll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#🤗-🤝-timm)
- [Diffusers](#🧨-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time 👁

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
This model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).

## Usage tips and example

CLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image
classification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text
features. Both the text and visual features are then projected to a latent space with identical dimension. The dot
product between the projected image and text features is then used as a similar score.
Using OpenCLIP at Hugging Face

[OpenCLIP](https://github.com/mlfoundations/open_clip) is an open-source implementation of OpenAI's CLIP.

## Exploring OpenCLIP on the Hub

You can find OpenCLIP models by filtering at the left of the [models page](https://huggingface.co/models?library=open_clip&sort=trending).

OpenCLIP models hosted on the Hub have a model card with useful information about the models. Thanks to OpenCLIP Hugging Face Hub integration, you can load OpenCLIP models with a few lines of code. You can also deploy these models using [Inference Endpoints](https://huggingface.co/inference-endpoints).


## Installation

To get started, you can follow the [OpenCLIP installation guide](https://github.com/mlfoundations/open_clip#usage).
You can also use the following one-line install through pip:

```
$ pip install open_clip_torch
 
Here is the question: How does CLIP support multimodal tasks in the Hugging Face library?","CLIP supports multimodal tasks in the Hugging Face library by using a ViT-like transformer to get visual features and a causal language model to get text features. Both the text and visual features are then projected to a latent space with identical dimension, and the dot product between the projected image and text features is used as a similarity score. This allows CLIP to be used for image-text similarity and zero-shot image classification tasks."
What distinguishes DETR from traditional object detection models?,"[(24064, 'b6a47dbd-f2c7-495b-be9d-1df30f14a395'), (17920, '9c587d42-fc22-4562-91cb-4e195b4c2fb1'), (17924, '5b0a1275-3317-4b6f-a5d9-84a7c3d71e4f'), (17926, 'fcc6fc34-3edd-43e4-b14b-e4de28ee9254'), (17927, '17a6d0e4-44b5-4cc2-b617-c98049e07917'), (17929, 'b18ebb24-46f7-4e3c-b4e4-defd1ce4ecad'), (17913, '24d44c2f-f45a-48ea-9b6f-ffa568fe3800'), (17918, '614edb3a-1dab-44db-b368-77c9229645d1'), (15957, '3e6a66a0-6f14-4ee6-87f9-e0007e231de0'), (15958, '4f4d7669-cdab-47bc-91aa-46441b0ada5a'), (22105, '303bca85-f93b-4c4b-beea-dfa3885fd809'), (22107, '7016d826-b9d4-4fc2-a1eb-765565f4a335'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (22112, '72ebbfd3-6f7e-4e49-aeee-7692ef491d72'), (15968, '6e12c477-8d00-447a-a4f5-3ea190633d3d'), (22114, '0a5ba4bb-e50a-4171-82ba-b95d1f4b573c'), (22115, '9150d9a8-8fa7-4306-b55e-506c43d5351f'), (22113, '14d11c06-e512-45ae-8775-9c31eed1e9d7'), (22117, '8cff68d8-a4af-44d2-a8ff-5456711c3d39'), (22118, '36a5c490-635c-46e4-bbff-9f2186d8cb06'), (15976, '091b3322-c4ef-4224-93cd-cc8201ba4ba9'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (15991, '6f8b7e92-77b4-401d-90e0-b58d203a3010'), (145, '82bcd7b4-a187-4092-8487-e9a50c1ae2b8'), (146, '76f72a72-531b-4daa-bb90-f35cf2112614'), (14996, '9baa6f8d-58bd-4683-a23a-6edac5da7e9b'), (5789, '01b13fa9-405e-4f18-ad86-5a8549a59c25'), (5797, 'dc2eff85-d919-407a-8b02-26a1fd7f55d0'), (4783, '0c737a83-0ee0-4d98-8a7f-fd07ddb178c4'), (23742, 'ef2a9f76-e54c-4a7d-b20b-cbdb06199aca'), (23743, '1e8468a0-3b9a-43dd-8d98-034bd897f4b4'), (24056, '3a5f4de8-a3ff-40d2-a789-a355054190a6'), (1756, '7dc789c1-95db-4be7-be41-2a9eb4705706'), (18142, '6432e78a-9379-42f5-ae76-b4e21cea4701'), (18144, 'ee4af106-0a20-453f-9d13-f2869e1d57e7'), (16611, '5ba26cb6-0890-4615-9f28-45a8ba3c5ded'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (16619, '0c9d0670-8b19-4167-97e9-4418dc3bd375'), (9452, '59aaeec8-b95f-4c82-89b3-f8905ba521d8'), (9453, '0763f28d-22c2-4948-ba1b-2cad01c04513'), (9454, '9de797fe-8ad7-43f0-86df-89345e1c4416'), (9455, '19e9e822-6304-4055-acec-9cff189e5fc4'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (6897, 'd88a3afc-4525-4ce0-9982-6f08b8d65534'), (24063, '68c3b5af-92ed-4063-9471-b9fce32331ca'), (13560, '838f91f6-9316-4ade-94c0-ca8e99840812'), (13562, '161d1786-5dd5-4883-af2d-6d40b0f613e7'), (4861, '79210cef-fada-44fb-a563-86b3d8b879d2'), (258, 'ccf8ce5c-aeab-4f18-8d26-8afda967080a'), (260, '14fc3235-cc9c-40f2-b951-b3002fd58852'), (24864, '9fd43470-89e8-4e23-a66c-e9ae2ac66f1c'), (13630, '5f595309-fb4e-4261-92ad-938f4eb058f2'), (23873, '6259700a-095f-4a14-8048-95fb1df7f296'), (28482, '3e98ed61-e405-42ac-8bd4-63da353dcace'), (23875, '7504618b-e7a7-4763-90ae-7702a4d617aa'), (23876, '9ec09514-e912-4251-a49b-f760d9b16f84'), (23877, '31509c88-8837-4b9d-ad51-5135d853ebd1'), (23879, '33efa917-ec34-42f9-9dba-22b827c59769'), (329, 'e5a52187-6e6d-47ba-bffa-cde854b2df46'), (2914, 'a437440c-998a-4d47-95de-352ecbe40b57'), (2915, '2db2eb55-2dc6-4ef1-aa46-55c914e880a1'), (2916, 'bf914d00-c1cf-4ab4-8b22-8641787bb13c'), (2917, 'd2246bb6-552f-4781-bcd8-fa71d6444c8d'), (9080, '8b8f599f-5243-408d-a6b9-d30288bf980b'), (2941, '8c483416-34d1-4d29-ba5b-878414f53927'), (2942, '6dd1087e-50d4-488b-b71c-0be7ed0bec5d'), (2944, 'd7daee80-75bc-4ee2-bcf3-3cf024d1a940'), (2945, 'd457e654-81bf-4d98-b51d-16c77c925417'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (29081, 'fb92a989-9e09-4948-b2bc-d0c04fb1372f'), (29083, 'c31c65e7-4ff0-4f9a-a1a6-454212b45856'), (5034, '22b398e9-2cf4-46b0-8719-f39bbc9fb3ff'), (2498, 'd50bb439-a45b-4d6a-9a35-187b48d23c9c'), (18374, '20d9b64b-d0f4-4ffa-b3f2-0f7c1b52710e'), (5071, '51839c46-b9cb-4584-81c1-e2dca0138c66'), (17879, '0febef04-6ab5-44f6-bc89-962863e3aea9'), (17880, 'b0df342e-5867-45e5-95f6-3db08814a8c4'), (17881, '23452cf6-a9ba-41d4-9f58-b8f65df16caa'), (17882, '4c166992-ffd3-4c48-86ef-0f6fd102934d'), (17883, '2b9035d9-f069-450d-b557-24fcf97b2910'), (17884, 'e49a7762-796b-444b-a445-a5879dafc4f2'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (17886, 'd30ae109-0b56-489e-98da-428f04978d14'), (7131, '9f48e28e-96e7-41e3-90f7-84a75a36ac4e'), (17893, 'e7b7d3d3-6779-4c81-9fd1-bf9db34d9bc4'), (17896, 'a716dda4-5dd1-4680-b0b3-7457ac80e32e'), (17897, 'b2c293ab-1e67-4b13-9656-273b0328e15b'), (24042, '089a831c-4f38-4cd4-b040-5b7abcd993a9'), (24043, '513d6d7b-52da-4acf-b55a-dcde6b2e7a0c'), (24044, 'a1f11a6f-a069-4ecc-b93e-337ac0ed14a3'), (24045, '2d1bae87-5feb-4e78-b32e-37ab2c42ebe8'), (24046, '7c5e6cbc-6fe8-49e1-ac93-4876e7a67454'), (24047, '010894cd-a4e9-43be-b277-7315f80d3141'), (24048, 'ae498ee6-ab8e-42d5-9d25-171274f6e755'), (24049, 'f5eaf2b3-155c-4d8f-ab52-aaa991cef16c'), (24050, '685c99e5-057d-4b3e-99b4-197c3ae5730f'), (24051, 'fbdebca1-822a-4463-be0f-9037e34f8342'), (17909, '0f26210c-2ee8-4e3f-9846-831bb49812bf'), (24054, 'e3a12540-4fda-4015-9a12-49a3dc50cdf8'), (17912, '6c000270-627d-44ae-bac1-9ab0640951e5'), (24057, 'd292faa1-de56-402c-878a-e2d243e5db03'), (17914, 'e3f62a9f-6c82-48c3-84c5-d5b2d2cdf56b'), (24059, '8730cadc-dd12-4971-8ffe-4c3b9a5d8056'), (17917, '3e26c0d6-3750-4005-9663-ab1c4105004a'), (24062, '17a005d1-952b-439c-bf0d-09520cded7bf'), (17919, '292976be-c487-4f13-b1a9-79e90c6369a9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Detection Transformer (DETR) directly transforms queries to unique objects by using one-to-one bipartite matching during training and enables end-to-end object detection. Recently, these models have surpassed traditional detectors on COCO with undeniable elegance. However, they differ from traditional detectors in multiple designs, including model architecture and training schedules, and thus the effectiveness of one-to-one matching is not fully understood. In this work, we conduct a strict comparison between the one-to-one Hungarian matching in DETRs and the one-to-many label assignments in traditional detectors with non-maximum supervision (NMS). Surprisingly, we observe one-to-many assignments with NMS consistently outperform standard one-to-one matching under the same setting, with a significant gain of up to 2.5 mAP. Our detector that trains Deformable-DETR with traditional IoU-based label assignment achieved 50.2 COCO mAP within 12 epochs (1x schedule) with ResNet50 backbone,
*We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the
detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression
procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the
new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via
bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries,
DETR reasons about the relations of the objects and the global image context to directly output the final set of
predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many
other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and
other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and
highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily
generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive
baselines.*
- DETR uses so-called **object queries** to detect objects in an image. The number of queries determines the maximum
  number of objects that can be detected in a single image, and is set to 100 by default (see parameter
  `num_queries` of [`~transformers.DetrConfig`]). Note that it's good to have some slack (in COCO, the
  authors used 100, while the maximum number of objects in a COCO image is ~70).
- The decoder of DETR updates the query embeddings in parallel. This is different from language models like GPT-2,
  which use autoregressive decoding instead of parallel. Hence, no causal attention mask is used.
- DETR adds position embeddings to the hidden states at each self-attention and cross-attention layer before projecting
  to queries and keys. For the position embeddings of the image, one can choose between fixed sinusoidal or learned
  absolute position embeddings. By default, the parameter `position_embedding_type` of
  [`~transformers.DetrConfig`] is set to `""sine""`.
The abstract from the paper is the following:

*DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/deformable_detr_architecture.png""
alt=""drawing"" width=""600""/>
 
Here is the question: What distinguishes DETR from traditional object detection models?","According to the context, DETR distinguishes itself from traditional object detection models in multiple designs, including:

1. **Model architecture**: DETR uses a transformer encoder-decoder architecture, whereas traditional detectors use other architectures.
2. **Training schedules**: DETR has a different training schedule compared to traditional detectors.
3. **One-to-one bipartite matching**: DETR uses one-to-one bipartite matching during training, whereas traditional detectors use one-to-many label assignments with non-maximum suppression"
What is the primary role of the `AutoTokenizer` in the Transformers library?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (19463, '72f624a9-b037-428d-9837-de3484ab1799'), (16399, 'dfd20272-a4fb-40e6-8c4a-eb2e8ac83370'), (7699, '107319f2-ea06-4a6c-83c7-1e8baec954bf'), (1045, '9e6832cc-4ade-48ef-b453-6bf847dda000'), (8229, '8f21e539-a202-474e-81a6-443e2d0a7115'), (2603, 'd20d955f-cf79-4625-9929-f3c3548d700f'), (20523, 'bb4780d2-fb0c-4468-855d-13ac0fc45c7c'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (14897, 'f38486c8-9c53-4985-a2dd-f9524c95e9c0'), (24126, 'f6d0b6d4-13c2-4d59-a59d-6b341a5a1508'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (10823, 'e21b6aee-30a8-41ca-a03b-c9cbbe3e780e'), (10824, '6d96b69f-c6af-4766-b64b-337c86dd7faf'), (18511, '4f3566db-bced-43d1-ad6b-57ce9577de94'), (18524, '044070e6-dcff-4f56-a6ba-28a3c0b23c17'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (12897, '78a4117e-544a-4fa9-9c46-63ee897d9e66'), (30312, 'd0a9520c-4b27-41e5-80a5-684f3b054f1b'), (30314, '062790b0-8e33-4435-ac93-85790f7ffb85'), (15478, 'b235b5c0-0d88-46a1-83d2-d72e1f226a7b'), (15479, '6daa1c77-9403-4dec-94ef-3f7f4160b632'), (15480, 'e8dd4427-60bd-48a5-89aa-e3ea494636d4'), (11899, 'c242e864-053c-427b-8359-fe3179308fe8'), (15491, '87084643-cfb0-4d05-8ced-5a742b953434'), (16524, '82e8de62-f2ea-4b9f-b238-49daaff66aa0'), (2709, '1d0d3208-e930-4ddc-a49c-73fadf6dcb78'), (10390, '4675c741-058a-4e20-9b79-1a956319b212'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (5285, '01793fa2-e61d-46c3-b590-c59c08dec418'), (1192, '48d5754e-ee39-4825-aa44-fc362b042ee3'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (1195, '3e7433ba-fae2-4e13-8802-9ff9da0b8900'), (30892, 'c7c7e937-8715-4300-9fb3-896a9568ab94'), (15531, '1388b94e-750c-4306-9d61-6f0ae373ddf3'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (17587, 'fd31d5fb-3745-4e63-a1ce-28974559d05e'), (19639, 'ee54b9b5-fa3c-4457-9bc6-160b732b815c'), (17592, '48ede2f2-438c-4cd3-b958-b20d71e0b9aa'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (24763, '81ad470e-e933-4a27-9979-d29627c04cf1'), (9916, '38adbc72-3efe-45f2-8caa-4702f7539b73'), (14029, '6bcbaa99-102f-4b45-b6b4-b58d00e4604a'), (19664, '2872ed56-b851-405f-8601-3748f47a00e1'), (13532, '8fd50109-f5ba-4939-b029-7d52ed8bd5bf'), (1249, '087ac398-f52c-42bc-96c6-a651dd21b098'), (1250, '46a46533-cfb4-4033-a0a2-ad4d9066e462'), (22245, '3590cbc4-b51f-432a-8dfe-5c293ec5646f'), (229, '8d6c749e-aba2-4e9c-86ed-fe336bbd3992'), (14567, '3330776a-c8c7-4367-b728-fba1c1251ddc'), (7921, 'dfb1162e-a303-4815-ba4a-22e68d333e5b'), (26866, '7c0456b0-8159-4ce8-b3f7-26b6dbed84fd'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (4862, '63ae6b24-af6d-47b4-8986-588ba0ca55f7'), (20738, '180ef799-bbe9-421a-97db-0919371c476e'), (11016, 'e4b1b7b7-839c-4405-86ab-e31e02c670b7'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (17161, '5a419712-79a3-4f57-926e-74f9da484cf5'), (21259, '4b7416f8-d3ac-48cf-ae7f-cc6f5afbb494'), (19722, '3876686d-2be2-4dc4-a048-968b26e05815'), (21261, '9fc8bb55-a9e1-487a-9702-6f727d330ef6'), (27406, '954a0b75-0188-4266-84c0-4a2f946626bf'), (3346, '032420bb-66aa-4f3e-b636-fe7b4d17188b'), (28952, '79800c43-9871-4516-b042-0c6ed54ed114'), (16666, '26627d88-bba5-49fd-a9a1-5f8958fa344f'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (15132, '8dd8343f-285b-436f-84a7-620e9144edce'), (11052, '086572b7-4e0c-446b-a514-4bc43efab281'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (26926, '3c507921-05b4-4626-bf7a-e8162a34e3a9'), (25907, '4c651379-1c81-48ed-9f9b-a9d7060ae2e3'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (21817, '931f3d19-2f6b-426e-9e7b-2d20ef96b089'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (24901, '20ed6577-9065-4b17-a364-b1463389e837'), (8518, '85a977af-6725-4583-a9c2-3cdacab3087c'), (18759, 'a4f9107f-b29d-42b9-adf1-a7d1b294c213'), (24904, '0d263af5-23ad-4f96-a3b0-81bee9b6bd23'), (855, 'b464c94c-ff07-41a8-bcbf-b75efd8dea91'), (10090, '8113f1c3-4c93-4dd2-b961-f3b61ffc9e0e'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (24949, '83efc35a-a86a-47fb-9f8c-04d42a95a7a8'), (24951, '518851bb-0a7b-4098-993b-556790a3803b'), (31101, 'c943ed4f-1ab8-411a-a1f2-28e45430aea2'), (20862, 'd3fd15a9-7f09-44c6-8342-ef2c61cba290'), (22399, '8c31bbea-69a2-4f83-afb3-796762dba928'), (3968, 'a33b79e1-7414-4e9f-b531-b1b20c2fc2f8'), (8576, '774d75b2-2b47-4c0a-b365-891414ebb5df'), (9610, 'b094bdc0-3783-4e56-aa01-2bb05d7808ef'), (18316, '477b02f9-3b34-4924-bd6b-dc1f478a1d5b'), (28562, '672617a2-bf46-4632-9cb2-8d0187372dc3'), (26004, '24ff7e08-15b1-4eb9-99e6-4ff1379bb974'), (20884, 'f94ec739-e144-4f11-9a61-c704eb112ba1'), (11677, 'b65b3085-bff1-4fe2-9cbe-69e92c3410b8'), (19870, '359a7e1c-0599-4279-9610-c730b74ad737'), (29619, '228cf10d-872b-4bd7-b2e8-6087e98e55fa'), (5044, '3347ade6-a88b-4653-97f6-3b4beff0fa0c'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (23519, '640f7b60-5c27-4391-bfcc-ae58b3011c63'), (3555, '59f59d85-8d73-4831-a23d-9040e6170b39'), (8680, '31b2a8b6-db02-4422-9575-f8708e6cf9ab'), (29163, 'b0d6c22e-e5b5-4bdc-a535-3f76e4217be2'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (3056, 'd4d04eec-32a0-4471-9e0a-6a0802f35104'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (9205, '813fcccf-2923-465b-ab73-2cfe6cc521fd'), (17404, '35034f8e-d7fb-40a9-b538-e5b7820c0f21'), (1534, 'fb44d1f3-3c02-4d77-afaf-99392c4d8752')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The 🤗 Transformers `tokenizer` has an attribute called `backend_tokenizer` that provides access to the underlying tokenizer from the 🤗 Tokenizers library:

```py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
print(type(tokenizer.backend_tokenizer))
## Introduction

If you're familiar with the 🤗 Transformers library, you've probably written code like this:

```python
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModel.from_pretrained(checkpoint)
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")


def tokenize_function(examples):
    return tokenizer(examples[""review""], truncation=True)
{/if}

In the last few sections, we've been trying our best to do most of the work by hand. We've explored how tokenizers work and looked at tokenization, conversion to input IDs, padding, truncation, and attention masks.

However, as we saw in section 2, the 🤗 Transformers API can handle all of this for us with a high-level function that we'll dive into here. When you call your `tokenizer` directly on the sentence, you get back inputs that are ready to pass through your model:

```py
from transformers import AutoTokenizer

checkpoint = ""distilbert-base-uncased-finetuned-sst-2-english""
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

sequence = ""I've been waiting for a HuggingFace course my whole life.""

model_inputs = tokenizer(sequence)
Let's return to the example from the previous section and see how you can use the `AutoClass` to replicate the results of the [`pipeline`].

### AutoTokenizer

A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the [tokenizer summary](./tokenizer_summary)). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.

Load a tokenizer with [`AutoTokenizer`]:

```py
>>> from transformers import AutoTokenizer

>>> model_name = ""nlptown/bert-base-multilingual-uncased-sentiment""
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
 
Here is the question: What is the primary role of the `AutoTokenizer` in the Transformers library?","The primary role of the `AutoTokenizer` in the Transformers library is to preprocess text into an array of numbers as inputs to a model, following specific tokenization rules that are consistent with the pretraining of the model."
What task does VideoMAE excel at within the Transformers ecosystem?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (18438, '46782e4c-a0e6-4646-81b0-50890671322d'), (18443, '00876f43-8b96-47cb-ba09-71690498db3d'), (5136, 'd9840db6-0933-4364-81bb-b7ab396b1651'), (16402, '3394d7df-18b7-43be-ac56-8b2d36c32975'), (5141, 'aa79a23d-5dce-4f0c-905f-502f514390e9'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (22563, 'a104d910-0ce1-4c60-ac15-3602d55952cb'), (4646, '13611a54-20bb-45b0-89e7-31b0bb22b402'), (4647, '8ee752d6-19a5-4a65-80bb-42e15b1a7b17'), (27688, 'a073f2c4-979d-44a2-9ead-1fd732d3029a'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (29270, '64b5ebd2-1350-4d53-aa2c-55ca9cd27f45'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (20654, '0dfc500c-a1cd-464b-917d-6d60a2832b61'), (20655, 'c9b81110-ed83-4bf6-8c1a-06c7469faa44'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (18621, 'a8c4b4e5-1ba0-4ee7-8ce6-fa9bb2fee140'), (18623, '015910db-fd0a-4c1f-8420-c137cb6a5a86'), (18624, '355b21ab-d432-4cbc-80cf-045354644b16'), (23745, 'ea883ea3-b0e0-43b6-b43b-b0dc66eb7422'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (13526, '594c0819-9996-42bd-8329-7cec367352a5'), (18140, '2b5de1ef-6517-4c54-b9b2-3d94550ca7a5'), (18143, '12f61c7f-c269-4136-b67f-a5742e09c011'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (4845, '940b1611-e095-4f8e-8f1c-183c7c7752cd'), (6893, '3e3a63e2-90f5-4d24-8729-3673b42d713d'), (4850, '3a950f12-3c2d-47ce-a6a8-1abdfb1befa0'), (1269, 'bdeff915-4d5c-4751-bddf-6637aaed895b'), (13558, '415ec308-3b10-4657-a9b2-e4f3aa21d028'), (1272, '704b6c46-a654-40a3-a06d-5d323c8e318f'), (13561, '3692733a-59d7-45fc-944a-f9bf8f1c8f57'), (19199, '8d274b83-a879-4bcf-98fb-bffc83ab3692'), (2816, 'dca14a8b-6394-41b5-9c6d-a623fed443df'), (256, '154c2da9-4985-48e3-86b8-642c818c758e'), (259, '57b1e5d5-014f-4166-a8c3-af7f7acfafc2'), (28424, '118d80b7-d822-4e36-87a3-b04479f5676e'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (1817, '1c818452-3fbe-49d9-a499-ac6ab04ee510'), (18713, '94baefac-bc00-4b79-8894-4b98315b4237'), (18206, '8db1e991-a927-49a1-bf19-bfdc54622518'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (18211, '85fcb405-d941-422c-9b78-e62a4ab3e684'), (2854, '44bd768a-6581-41af-9f6f-6a6fb30e2f15'), (17190, '03819edd-dcd1-4e8c-baa0-f847b307fe77'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (23858, '7b468a56-8c89-4769-9647-5f02a89e2c30'), (23859, 'e47fd052-4097-479b-95f3-f65bc65a3c2c'), (13623, '61a8c7a1-9ad3-4d9a-aa84-06e57daeda3a'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (13628, '4866bd5b-5e30-4a48-9d14-747c0530f831'), (322, 'f79309af-2846-48e5-a187-020a8da015fe'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (327, 'a865fbd9-1b73-4e87-a13f-96a95c8b6c8c'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (16216, '973de4b9-d6cb-42ab-9274-482f4fc83fef'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (7013, '7c2e994f-665b-4e39-a477-19fa189595ac'), (11113, '0eb2c9d8-ae9d-440e-a2d1-ae063ee9e84d'), (11114, '933fa587-6033-469c-b633-a24147cec80c'), (27499, 'd198fa61-745f-4431-8bc3-ce4c1e7ba81b'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (9079, 'a92d8741-f372-482b-a859-65f7ad04b0e1'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (22915, '1e3e0c59-a367-481b-9800-7d8f63c21391'), (22918, '201f6149-0a67-49ea-81bd-e07891bdb9c1'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (10133, '7735b57a-5b30-4c4f-9ab3-c1e84672a8be'), (10134, 'ee5fe619-627f-4f73-a64d-e403fc8167c8'), (27038, '2bd6c10f-c5ca-4311-9c98-85b902dcda5a'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (5026, 'c4a2a4da-22f1-4d91-9179-1923ed762c86'), (27047, '5e5049e0-dc61-4f46-88a1-f386272cfe4b'), (5036, '0b5c3f03-bc2e-47d6-9eda-a8474681507a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (9145, 'a9a8d501-4863-4964-b90e-1c9b79a97a29'), (20411, '63503326-c8fb-4aa9-ad45-d38809271cee'), (10172, 'd627ad07-e686-4214-8212-b0cadd98deb4'), (9150, 'de3eb793-c6de-4fd2-9f8c-1b0b686cf6d1'), (22462, 'b3785b03-9898-4e88-a5ce-0b5ae380249c'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (26051, 'c87cc8ee-cf0d-4ddd-af75-53d9ebcfc577'), (18373, '995a4fac-0cfe-428c-851d-1d3d6e3902bf'), (10185, '7824c364-4a2f-4032-a5e8-10723b216cbe'), (10190, '92ba9e39-8a19-408b-92df-63347b640043'), (13263, '5e5d0239-c669-4ea3-82ab-571e42b0f577'), (5070, '678cfa30-db3a-4716-b273-0dba91f82881'), (7122, '0c3bc29c-a51a-42a2-98c5-275b173fdd9d'), (470, '61735e28-6388-4763-8d04-89f6c4bf7bfc'), (6615, '4b4eeb94-e396-4c99-a6ba-dd6d9503e03f'), (6616, 'c8496f16-33f0-4cf4-a35c-3dc3f0fefa1b'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (7145, '0559763f-9a94-4590-bf9a-97945749c2cd'), (31733, 'e4039e2d-a446-47f0-95ca-14630768b30b'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (31742, '972b3dba-dbb9-4b63-aca8-a766d766de03')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Transformer models can also perform tasks on **several modalities combined**, such as table question answering, optical character recognition, information extraction from scanned documents, video classification, and visual question answering.

🤗 Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets and then share them with the community on our [model hub](https://huggingface.co/models). At the same time, each python module defining an architecture is fully standalone and can be modified to enable quick research experiments.

🤗 Transformers is backed by the three most popular deep learning libraries — [Jax](https://jax.readthedocs.io/en/latest/), [PyTorch](https://pytorch.org/) and [TensorFlow](https://www.tensorflow.org/) — with a seamless integration between them. It's straightforward to train your models with one before loading them for inference with the other.

## Online demos
1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.
1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of Wisconsin–Madison) released with the paper [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.
1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.
1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of Wisconsin–Madison) released with the paper [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.
1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (from Multimedia Computing Group, Nanjing University) released with the paper [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) by Zhan Tong, Yibing Song, Jue Wang, Limin Wang.
1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (from NAVER AI Lab/Kakao Enterprise/Kakao Brain) released with the paper [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) by Wonjae Kim, Bokyung Son, Ildoo Kim.
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (from University of Wisconsin–Madison) released with the paper [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) by Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee.
1. **[VideoMAE](https://huggingface.co/docs/transformers/model_doc/videomae)** (来自 Multimedia Computing Group, Nanjing University) 伴随论文 [VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training](https://arxiv.org/abs/2203.12602) 由 Zhan Tong, Yibing Song, Jue Wang, Limin Wang 发布。
1. **[ViLT](https://huggingface.co/docs/transformers/model_doc/vilt)** (来自 NAVER AI Lab/Kakao Enterprise/Kakao Brain) 伴随论文 [ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision](https://arxiv.org/abs/2102.03334) 由 Wonjae Kim, Bokyung Son, Ildoo Kim 发布。
1. **[VipLlava](https://huggingface.co/docs/transformers/model_doc/vipllava)** (来自 University of Wisconsin–Madison) 伴随论文 [Making Large Multimodal Models Understand Arbitrary Visual Prompts](https://arxiv.org/abs/2312.00784) 由 Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory P. Meyer, Yuning Chai, Dennis Park, Yong Jae Lee 发布。
 
Here is the question: What task does VideoMAE excel at within the Transformers ecosystem?","According to the context, VideoMAE excels at self-supervised video pre-training, as stated in the paper ""VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training""."
Why is the Transformers library considered framework-agnostic?,"[(11779, '6addf5b8-7cdb-47f9-9f4e-7bb05b91fa14'), (11780, 'b728994c-1148-4ed4-aba8-e9857a16ae49'), (17412, '1d9b136f-d20c-4989-8975-80625fa8da45'), (24074, 'c7902235-64ce-4d97-b746-5682c76f72ec'), (24075, 'af6adf8d-5d92-4a5a-bc07-5c711964db79'), (7699, '107319f2-ea06-4a6c-83c7-1e8baec954bf'), (4633, '3b34b63e-5eb9-4941-a5ac-d4ed41dc2454'), (22556, 'f87b1d1b-4399-4adc-9de7-3039dc9df116'), (17441, '2b2d10bf-e7c0-47cc-903e-fc709dcedd32'), (22561, 'de933eb3-9bbf-4dfa-b7fe-ba4874081807'), (22562, '4ea9cd3d-f17a-4e44-9d4d-04ed89e920d0'), (6701, '03b27867-b940-4578-8dee-63bc47e83023'), (6703, '89e54911-4e03-4c77-8766-a35bc7746bb5'), (24631, '1cafa663-09f0-4b92-9f19-efd956944da3'), (24636, '34c928bf-700f-4de6-b158-b590ff679457'), (24126, 'f6d0b6d4-13c2-4d59-a59d-6b341a5a1508'), (19519, '0a2a03e3-7510-4135-a0ce-faa323000ba6'), (18511, '4f3566db-bced-43d1-ad6b-57ce9577de94'), (10837, '5eab99ff-b40b-47ec-afa5-7bfdebb97d28'), (9301, '55e1eaee-0f8d-4363-9870-a60c76f7f4cb'), (28762, '21cd688d-99de-48bf-ab8b-b482ceaef386'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (10852, '46665772-4611-41ea-a131-43ef68fc79e1'), (24691, 'e2088a54-3bfc-40df-b1bd-8d4fb59815b9'), (18571, 'e150c6a6-1333-4175-944b-28873073d9c6'), (12947, '8890d1af-9df8-4229-a9ce-37479fb8c9d3'), (1183, '83219e89-2f3e-4e38-8e76-34ba4626dc2d'), (1184, '723b43d2-3d1d-400f-8942-0ffea0603feb'), (7841, 'd54cbc48-e006-43f1-b78c-818d2db58a91'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (30884, 'cca93cf6-12a9-4084-9d47-97a3d2761b55'), (24745, '3be32488-5723-4850-ba4a-ad6e8308396e'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (1195, '3e7433ba-fae2-4e13-8802-9ff9da0b8900'), (6316, '979e27ae-ecb6-421a-9259-ab1fcb97b027'), (5804, '8e1fc3d4-a991-4cea-9c02-51c62d3475c0'), (30895, 'd9909539-e9b8-4ba7-a169-17d2b6a76435'), (19634, '02d4e747-6302-4bf1-831f-dbc2c22bdd5e'), (19639, 'ee54b9b5-fa3c-4457-9bc6-160b732b815c'), (20667, 'b0b180ee-2478-48b8-9446-bf6a9feabb3b'), (1212, 'cd2054ec-f9d3-4753-ad7f-b668bbdf6450'), (28351, '8e21f6b4-fb9c-471a-ac89-c30e760888ef'), (30915, '8ed2c202-2476-4d9f-b935-28d3cd7be506'), (28357, 'a9255cf7-c778-44dd-82fa-06901205d8c4'), (30922, '150e7f7e-438f-472c-b0d2-d43c51ed72c0'), (21717, '8c131a2b-09e9-4e2c-8595-235b0899fe23'), (214, 'ced565fd-8f3c-4e7b-a0f9-9e46e7ed2922'), (26327, '976c9296-1e6f-471f-954f-eabdd55bd672'), (15589, 'a3edd040-f1eb-423d-a235-b0ee3702fe70'), (6889, 'f9703e3f-789b-4d40-bb13-0dcb8de3863a'), (19691, '4b313b1e-2cb2-452a-aa19-380b20fa6be1'), (1268, '1089efcd-cb52-41f2-bd36-dae165bb1e06'), (6404, '385c95df-7b33-44c2-b8bb-5eec023e9f4e'), (28424, '118d80b7-d822-4e36-87a3-b04479f5676e'), (28425, 'd8cd1795-7e07-4dc4-953a-4c57241d871d'), (8975, 'bd732547-839f-4846-9884-4eedc392d46b'), (3355, '037abe8e-e3b5-43d4-878b-f3c7d3759f71'), (15132, '8dd8343f-285b-436f-84a7-620e9144edce'), (6943, 'ad8c7727-69f8-472d-b29c-cffc054a6777'), (16674, 'a05d2b01-8fd3-48e1-8077-85f9ef33864d'), (2854, '44bd768a-6581-41af-9f6f-6a6fb30e2f15'), (15659, '54679008-9ef1-4235-9d99-b2a9666e3d13'), (28972, '65ee9890-02a6-4ef0-9e94-a742399f7efc'), (5933, 'e8977096-c2bb-4f13-9f6e-6bfd1dbeb9bf'), (14638, '61cb344f-bba6-48d1-8a4b-abfa78050d48'), (21811, '470364a3-fa9e-427f-afc0-7d75e5a99a7d'), (14654, '2115c3bb-1660-4f63-928b-e80775234089'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (18758, 'addc4b13-f6b1-447d-96e4-c23ffbc01340'), (9035, 'ff9bf8d9-09bf-407b-b40d-290b88386205'), (9036, 'dd8bd821-e6b0-4036-92bb-098a5480159d'), (340, 'f260b640-4e93-47fd-9cf4-b3ad7c90bd35'), (10591, '8d2a8fe3-da86-48ed-abdd-6964e5b35016'), (11618, 'ef4b1093-8878-415b-8fc7-a89a9cf5c35b'), (22380, '978b8e56-9171-4db6-9c73-e98d52b06531'), (14704, 'd152c193-d62e-412a-9a73-fad192c92a19'), (17785, '6f2d27a1-e231-45b4-a129-855030d7addf'), (28539, '15ca206b-e242-4240-9c56-b957d70a1add'), (17787, 'ce2aba08-c447-4397-8e52-af796596c2f3'), (25980, '44d9eb20-ac45-46e9-b4ea-310683db7670'), (14718, '745dc549-1d83-4aea-a80b-26ae31887c1a'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (8576, '774d75b2-2b47-4c0a-b365-891414ebb5df'), (22401, '4bd4fe39-031e-426c-9d9e-c4cde0365999'), (17791, '829ffbbd-d8db-4775-a439-40803f37146f'), (17797, '67bbb946-21ee-4def-8627-56bd8001dcf1'), (17799, '1a9b6e64-5c5c-4f5b-ac37-76af0490b888'), (20874, 'b5bf7e12-cca2-4502-b81c-6cb78f787e38'), (5003, '2d53d359-788d-456d-b523-0b8f16b91897'), (24972, '9084f511-5580-4546-944c-05245a37f48b'), (11671, '326e73c9-19d5-4603-a3ad-32e2aa106c07'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (16300, '76eda82c-7ee1-4c29-9cfd-968d101f307a'), (5045, 'ad18dded-fb4f-4735-a7ec-131971916c04'), (5046, 'e483ebd0-26b3-4491-bd82-067efe507897'), (5047, 'b14667c0-0253-45c1-9c7d-59945b988201'), (16311, 'ff1c1d88-bc88-43e0-9d16-50398857fbc0'), (11712, '177a724e-3546-4865-a0ac-5221590a62c6'), (11209, '3b5ff375-2e1c-4a92-8077-43f3d30f62ae'), (13263, '5e5d0239-c669-4ea3-82ab-571e42b0f577'), (7121, 'd4717635-bd13-4d6f-99ae-c00c694d443b'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (15835, '888ea0a4-ad95-4449-9478-48ba7798a983'), (31720, '8cbb6021-102e-469a-a00f-a1dc88cafe0d'), (29163, 'b0d6c22e-e5b5-4bdc-a535-3f76e4217be2'), (11763, '41c7384a-f304-4b4d-b199-eb78ace75511'), (31734, '13a6a27a-6ab0-4b6a-95e5-cdf93137aae2'), (30198, '744655f5-8938-4651-ad69-c1f70f4e20b0'), (13821, '2e6bacd8-94da-4331-9f51-94b8005dc599')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

This is just a taste of the library, of course - if you want more, you can check out our [notebooks](https://huggingface.co/docs/transformers/notebooks), or our [code examples](https://github.com/huggingface/transformers/tree/main/examples/tensorflow). There are also several other [examples of the library in action at keras.io](https://keras.io/examples/#natural-language-processing)!

At this point, you now understand some of the basic concepts and classes in `transformers`. Everything I’ve written above is framework-agnostic (with the exception of the “TF” in `TFAutoModel`), but when you want to actually train and serve your model, that’s when things will start to diverge between the frameworks. And that brings us to the main focus of this article: As a TensorFlow engineer, what should you expect from `transformers`?

#### Philosophy #1: All TensorFlow models should be Keras Model objects, and all TensorFlow layers should be Keras Layer objects.
So what I've been working on for the last few months on the transformers library is providing the functionality to export these models into a format that lets you run them much more efficiently using tools that we have at Hugging Face, but also just general tools in the open-source ecosystem.

In a way, the philosophy of the transformers library is like writing lots of code so that the users don't have to write that code.

In this particular example, what we're talking about is something called the ONNX format. It's a special format that is used in industry where you can basically have a model that's written in PyTorch but you can then convert it to TensorFlow or you can run it on some very dedicated hardware.

And if you actually look at what's needed to make this conversion happen in the transformers library, it's fairly gnarly. But we make it so that you only really have to run one line of code and the library will take care of you.
-->

# Philosophy

🤗 Transformers is an opinionated library built for:

- machine learning researchers and educators seeking to use, study or extend large-scale Transformers models.
- hands-on practitioners who want to fine-tune those models or serve them in production, or both.
- engineers who just want to download a pretrained model and use it to solve a given machine learning task.

The library was designed with two strong goals in mind:

1. Be as easy and fast to use as possible:
To start, let's try to get a general overview of the Transformers
library.

General overview of 🤗 Transformers
----------------------------------

First, you should get a general overview of 🤗 Transformers. Transformers 
is a very opinionated library, so there is a chance that
you don't agree with some of the library's philosophies or design
choices. From our experience, however, we found that the fundamental
design choices and philosophies of the library are crucial to
efficiently scale Transformers while keeping maintenance costs at a
reasonable level.

A good first starting point to better understand the library is to read
the [documentation of our philosophy](https://huggingface.co/transformers/philosophy.html).
As a result of our way of working, there are some choices that we try to apply to all models:
For large parts of the library, Diffusers adopts an important design principle of the [Transformers library](https://github.com/huggingface/transformers), which is to prefer copy-pasted code over hasty abstractions. This design principle is very opinionated and stands in stark contrast to popular design principles such as [Don't repeat yourself (DRY)](https://en.wikipedia.org/wiki/Don%27t_repeat_yourself).
In short, just like Transformers does for modeling files, Diffusers prefers to keep an extremely low level of abstraction and very self-contained code for pipelines and schedulers.
Functions, long code blocks, and even classes can be copied across multiple files which at first can look like a bad, sloppy design choice that makes the library unmaintainable.
**However**, this design has proven to be extremely successful for Transformers and makes a lot of sense for community-driven, open-source machine learning libraries because:
 
Here is the question: Why is the Transformers library considered framework-agnostic?","The Transformers library is considered framework-agnostic because everything written about it (in the context) is framework-agnostic, with the exception of the ""TF"" in `TFAutoModel`. This implies that the library can be used with different frameworks, such as TensorFlow, PyTorch, etc., without being tied to a specific one."
What makes the CLIP model effective for zero-shot multimodal tasks?,"[(23554, '52119c19-3173-4838-93a4-3fde72e9b37b'), (6151, '44101b1a-ca3c-4fc4-85b1-92016126dfe6'), (15375, 'e332f0d9-263a-4ecd-b2fe-4c3a681a7cda'), (11283, '729f6a59-d667-445b-9197-bd8970806506'), (15908, 'c51d64ff-cbc2-4026-a49f-5b75de1c045c'), (15918, 'c7ba6127-5921-4286-880d-489225907f4f'), (15924, '0fa9c0d3-9d36-4b4c-9711-b6faf189ad89'), (3127, '3603fd2c-d457-4a01-893d-9d26a38ff933'), (31292, 'b37711fa-ba5e-4d7c-9d3d-8ff4635ba254'), (4670, '082e9d09-351a-48e9-9013-71721f935119'), (11329, 'b5c39d95-7e52-4220-b14e-6cdcd2ee5964'), (30785, 'fe7b98b8-14ab-4e54-8af2-68f748159690'), (22085, 'f6050fc4-183f-46c7-9934-5cdcc7d6aa7f'), (15946, 'dba83616-b339-4f07-b0d5-3922d4f09e86'), (587, 'd2ef4b9b-f115-4853-823e-4f5394d3d34c'), (27730, 'ca5cf0ee-5a70-4daa-9d14-10ead06c9b1c'), (89, '89c32447-7b9b-4dbd-8cb7-1e1f1394809d'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (20579, '1cadc685-b43e-4779-9427-c24086103ace'), (20580, 'd3179689-1789-4157-83a3-9dfab3333b49'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (10858, '33249571-1ade-46fd-ae89-e32ff806fdcd'), (10859, '7140610f-8a0a-40c5-b6af-12b5af75f3c7'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (7806, '6d7638e5-caf5-4427-a7e6-a138c90c7e9c'), (7808, 'f7d82006-5b58-485f-b5ca-2e5043ac18d8'), (7809, '173bc460-5338-4f7c-9b69-916e9e03ecfb'), (7810, '6464d47d-5a3f-4e13-b8a1-5d521cf34e11'), (7812, 'd2ee2299-ee53-476e-9f83-1c5ab9120907'), (7814, '59fc1afc-32d4-4401-b7a7-f31ff5bf1424'), (7815, 'ffedd8d3-edcc-43cc-9cec-a604d063dc3e'), (1677, 'ae024f27-e2c3-43b9-b174-47ebc2b1a20d'), (29838, '4085395a-76c9-44e4-bb5d-139a2ac8f1ef'), (6287, '89e924a6-29d0-47d3-96a0-a0d16dfc1ce2'), (6286, '17043260-ccb0-41be-a8d7-cd50fdf8f0c8'), (153, 'a5e94b25-4bd0-4edc-be2a-4167144129cb'), (670, '577aa401-7ed7-400a-a023-60b33766e240'), (671, '11cdddb6-4b04-4b7d-be04-b2efd538cdc5'), (5793, '358177a4-5892-4a9b-9356-18b7f3fb59d8'), (675, 'e6d47d5b-9841-4de7-a93f-86385047865b'), (4774, '331613c0-519f-4a91-aa47-3f15174210ed'), (4775, 'dece872c-e754-4f2b-b021-07367964cf41'), (20661, '3d03f793-112e-4835-89e0-d0c9ab6a236d'), (29373, 'e19e41fa-2890-4dc5-9c37-8fe73d4a7d8e'), (1749, '22147376-467e-46ca-bd5a-e6e7fd40222e'), (3799, '1651e17d-3706-47fa-9cf4-a9f8a9ebb754'), (16090, 'de34769a-90f0-4c82-b82e-6d9a5a0e6f92'), (3825, '4917fc56-25d8-4ee8-903c-494addb76edd'), (3826, '256f85ac-16ed-48b7-8a96-c986ccb76483'), (3831, '320e50a7-2690-4798-86c2-921ef51a4cac'), (3833, '3069c003-9ca5-49a2-b414-b97231da8a77'), (3834, 'df9d737a-e452-4e71-9f9b-48cb3c6baaa6'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (3837, '2b3f9754-3c83-4140-8261-a686b11fd43d'), (6910, '49d3e17d-b9ae-4481-9c7a-4c59ba0a76f5'), (19204, 'e5374f8a-a738-473d-9541-a851c339f045'), (8996, '2cfc69fd-58d5-4ede-ba16-0ce14d0215c1'), (18217, '4e0278ab-2929-47df-8ac5-54be5d98a007'), (5945, '82b994e1-d6d9-4269-92fd-5bf9f88254b7'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (26433, '7a9b6698-cc9a-4c20-8f14-c6e3414592c1'), (13634, '63e875a9-4fad-4f80-8454-e567ece5a4ac'), (12103, 'a445155c-2e8e-4785-8f2e-3a6c20775a86'), (333, 'b89f73b6-20ff-473e-9c00-802ac258ce6c'), (5474, '1158d6f0-fad9-4bbb-babf-d6178165afc7'), (5475, '57047f2d-2af3-4afd-87c7-5e4f83f2eaaf'), (25962, '103e5b4c-2a99-4fa7-860b-8a1d96ced460'), (20849, '4367c0f4-7a62-4a12-b57c-73342255b66c'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (4983, '044a8313-5785-4ad9-a376-13fc8c2b2cfa'), (25978, '650f0907-0697-4a99-b904-f43e16a875e0'), (14204, '23ef5336-0d61-4a22-b398-3349e2d163ca'), (14205, '2c1444e7-9718-4eec-8840-4cb117e9e104'), (14206, '5bfbab3f-215b-442a-8bd0-1810e815b192'), (31616, 'f6da62fc-c26d-4422-99b3-031dc14e92df'), (31619, '15b09247-3696-4abe-b6ab-82b08cf31da4'), (31621, '833b507c-b23d-45b8-936f-a85896b6367d'), (16262, 'ded5c5e0-e2cc-4e35-a782-46787d779148'), (16264, '5d166a05-2d80-4201-ae2f-b71993797a24'), (22923, 'c7ff6a20-33ae-49f3-a056-4612d0bb44cb'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (22926, 'f448c8d4-9e0f-4837-af00-f4aa2144feaa'), (22927, '85eb5b44-5e25-43cc-9e27-6e1aad8b3d58'), (22928, 'a8321404-e3d8-420e-b234-a78cf2bdc969'), (22930, '25b5e287-3b54-40ee-9fc0-06927f1cd7cd'), (22931, 'ece59397-b752-40a0-a704-c3bd65da52a5'), (21400, '637da23f-83b4-4655-83e1-bd9d7d7048ee'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (28579, 'faeb2a24-8c7e-43f7-b3f4-3ae4017753be'), (22953, 'e5b6a425-e3e0-4e50-8c94-9a4d3896c8a5'), (22954, '4239756d-2474-4166-b13c-fec6cc66c150'), (11691, 'dd3ff1e4-0aa8-42d6-b141-406f1df3f143'), (22967, 'e33301fd-bcf6-4639-a5b1-f164b2d685f9'), (22970, '36b2f7d0-d2f2-4773-a1a0-4253ebb02ddc'), (29122, '81bc2e07-9a53-43ff-a046-a73c2a5c8858'), (29124, '2a4c40e0-2e01-47f4-a1b0-a84901d0c22e'), (29127, '82149023-2091-41fa-98e9-b702609ed535'), (30677, 'a25bef54-3520-4f86-a7af-0451578e6208'), (5599, 'dd67de1c-aeff-4a2f-8d93-77772f57bfcd'), (13286, '7ee1ad39-4258-4a5d-81b7-e8d7746cca3a'), (23015, 'd9177564-c31b-4316-8d08-cb422482dd8b'), (23014, 'd29e7466-2ce3-4018-8252-e65865458a74'), (29160, 'f3509beb-f693-42c8-bd7e-3bcf5819c4b4'), (21485, '1ff92749-dc34-4f1f-94b1-ec18b41dd726'), (7149, '59d72bbc-36db-4460-9bb1-280b7e865c48')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: While models such as CLIP, FLAVA, BridgeTower, BLIP, LiT and `VisionEncoderDecoder` models provide joint image-text embeddings that can be used for downstream tasks such as zero-shot image classification, other models are trained on interesting downstream tasks. In addition, FLAVA is trained with both unimodal and multi-modal pre-training objectives and can be used for both unimodal vision or language tasks and multi-modal tasks.
```

## Conclusion

There have been incredible advances in multi-modal models in recent years, with models such as CLIP and ALIGN unlocking various downstream tasks such as image captioning, zero-shot image classification, and open vocabulary object detection. In this blog, we talked about the latest open source ViT and ALIGN models contributed to the Hub by Kakao Brain, as well as the new COYO text-image dataset. We also showed how you can use these models to perform various tasks with a few lines of code both on their own or as a part of 🤗 Transformers pipelines.
This model was contributed by [valhalla](https://huggingface.co/valhalla). The original code can be found [here](https://github.com/openai/CLIP).

## Usage tips and example

CLIP is a multi-modal vision and language model. It can be used for image-text similarity and for zero-shot image
classification. CLIP uses a ViT like transformer to get visual features and a causal language model to get the text
features. Both the text and visual features are then projected to a latent space with identical dimension. The dot
product between the projected image and text features is then used as a similar score.
## Usage example

OWLv2 is, just like its predecessor [OWL-ViT](owlvit), a zero-shot text-conditioned object detection model. OWL-ViT uses [CLIP](clip) as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. To use CLIP for detection, OWL-ViT removes the final token pooling layer of the vision model and attaches a lightweight classification and box head to each transformer output token. Open-vocabulary classification is enabled by replacing the fixed classification layer weights with the class-name embeddings obtained from the text model. The authors first train CLIP from scratch and fine-tune it end-to-end with the classification and box heads on standard detection datasets using a bipartite matching loss. One or multiple text queries per image can be used to perform zero-shot text-conditioned object detection.
![Generated 3D object with NeRF](assets/27_summer_at_huggingface/diet_nerf.png)

3. [CLIP RSIC](https://huggingface.co/spaces/sujitpal/clip-rsicd-demo). CLIP RSIC is a CLIP model fine-tuned on remote sensing image data to enable zero-shot satellite image classification and captioning. This project demonstrates how effective fine-tuned CLIP models can be for specialized domains.

    ![CLIP search](assets/27_summer_at_huggingface/clip.png)

Apart from these very cool projects, we're excited about how these community events enable training large and multi-modal models for multiple languages. For example, we saw the first ever Open Source big LMs for some low-resource languages like [Swahili](https://huggingface.co/models?language=sw), [Polish](https://huggingface.co/flax-community/papuGaPT2) and [Marathi](https://huggingface.co/spaces/flax-community/roberta-base-mr).

## Bonus
 
Here is the question: What makes the CLIP model effective for zero-shot multimodal tasks?","According to the context, the CLIP model is effective for zero-shot multimodal tasks because it uses a ViT-like transformer to get visual features and a causal language model to get the text features, and then projects both the text and visual features to a latent space with identical dimension. The dot product between the projected image and text features is then used as a similarity score."
How does LayoutLM improve document understanding compared to text-only models?,"[(15886, '1f277e8b-db79-4468-af4b-e11deb15d54a'), (15888, '9d894a2a-217b-4732-9a14-b74028b1380b'), (15889, 'aad2c393-9f11-40db-b577-03aba97e12a2'), (46, '93fa1c88-08f0-4d45-bfcd-93c31ccff19a'), (49, '1efa4b78-1a65-4ef9-a7bd-ae0d9ba0fccb'), (28730, 'e9c215d1-838f-480b-89e9-9bffb8f598f8'), (28731, '85e9fbea-6d30-4bab-84e9-166bb1cc8edc'), (28732, '26f566d8-d9ed-4f9e-a930-8b84075bd6a7'), (28734, 'c4312ccc-1453-4779-b880-259e3659a9f2'), (28735, '56165381-810d-434e-9201-84d00575cb88'), (28736, 'e998dbb9-1571-43be-9178-96f1a0a0f55e'), (28737, '32486ef1-3b91-4208-805e-da987d4c42d7'), (18498, '899a5b0c-bdb5-4f2f-add3-c045afaedafc'), (28739, '50948ec6-1039-4799-9d5c-f657b72f8d39'), (18500, '601132c7-a766-4738-b8f8-0a14d1084196'), (18499, '9e1839cf-b065-4550-85e5-6eef0c01a02b'), (18502, '51f3beba-1f28-4dda-8083-00a757ff2875'), (18501, 'dc889574-ebe9-462c-8e67-4cf4a649d73e'), (18504, 'ab801953-53b9-4ef3-9ef4-23ecc3486029'), (18505, 'be272b48-013a-4fd6-bc00-3da6c111c0be'), (18506, '2ab7adfe-f811-4536-a8bc-a20c5a199a61'), (28738, 'a52d4111-ff1a-43ef-8d26-44506f5907a6'), (24658, 'f1864d51-a0c4-4b3a-b5b2-89ee1775172d'), (24659, '1ce77664-fd28-405b-b2df-b85564bdba01'), (25730, '2939ee0f-e1f7-4024-8090-a9fad3f59c7f'), (20612, 'de424e36-a6d3-4010-b9cb-29aafe220c68'), (20613, 'd1ad2244-95ba-43a6-af74-2b264b02272c'), (20614, '894861fd-6231-4cd8-ab6f-0c19d9c4b753'), (5799, '7d6e8705-ceab-43e9-a1de-3f8c4b5e017a'), (4263, '3fa528e4-728b-40f1-bf37-f9cdba53ed19'), (23731, '70cf89a5-3b44-45f3-a4f4-d179871e0419'), (23734, 'c725af5e-eec1-4276-a26c-c4ba0884667e'), (11964, 'f62ba1ac-f4a7-4acd-a5fc-ddf410ab1d47'), (11965, 'c0758bab-7fe0-4ec8-ad7c-d42e0e7ee829'), (11966, 'd7d54d2d-6cb7-4459-9ee5-6091ab1da2a7'), (11967, '1731736c-2b48-4e47-aefe-b7ce94cf7fee'), (11968, '339f2d9a-04bc-45b1-ba09-2cb0ee42c1d3'), (2748, 'c22b3b58-8c0d-48a9-97b2-d8d89b7b5707'), (4807, '52de4c92-bd98-4936-ae2a-bb0b9e3493be'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (4809, 'abf2fc26-cdb5-482e-af69-0eb9e6c18794'), (18134, 'edec057b-dc14-495b-893a-7a04a35f89ac'), (13552, '74848e30-fb78-4cca-afe2-1d027939cfbf'), (29428, '173a121e-d8f8-4781-80c0-982f5775a7fe'), (1780, 'd58c641f-0b84-48bd-a841-a94c022d81b6'), (1781, 'baccd6ea-7883-4702-8c88-d3beb5093793'), (18167, '298e7b8b-8ca9-4081-aa18-6a89f90e9951'), (29432, 'cfe216f3-87f3-4423-845d-6b97b3ceff48'), (29433, '54590b2d-6c0c-4175-97fa-5c59657c11a2'), (18170, 'c664e159-5f97-4cd3-9176-bc322662a4ba'), (1782, 'ab7d79dc-b5d4-4148-a520-a2456cbf8605'), (18168, '3012d4d4-e60f-4363-ae7a-fa0b832c41f6'), (18169, '5533b481-295a-42cb-a639-6b7535db6a13'), (250, 'efa79a0b-ef9a-44ee-8dbc-9370a0757031'), (29436, '7fc8e06a-e13f-4a56-9b66-6a6dba9c67ac'), (29437, 'a9873ed2-2cf8-44cc-9b63-9ba1c9d4a818'), (29438, 'ae614832-218f-409e-b17a-bf956c69691d'), (29445, 'bbcef900-4efc-40e4-a995-d0d601c26cdf'), (29448, 'de680f2b-2364-4825-bd55-d130bdbe185f'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (20239, '54e57242-e451-457e-93e5-47af02aeeb09'), (13584, '66959fae-87ce-4a42-994d-2340635e4863'), (13585, 'dc97e6e4-5a6a-41b4-8851-91e3a0e3af3f'), (13586, '29bffc25-131a-4a3c-acd1-0b0609c13d25'), (13587, '05b7e6a6-0d90-4abc-b1d2-dfabde4133eb'), (283, 'dc991605-c043-43f1-9c3b-390378a7a834'), (284, '5e72c7ed-cf69-4a85-8460-5d28bf60ba77'), (285, 'fff20960-c258-4ff5-baf2-cfc2ce558b96'), (286, 'da6a4ed0-790f-4f58-aa39-6df5013da3f6'), (25377, '11930567-432f-44a9-b953-a45bd742357d'), (25378, '12a72bcd-5c44-46f6-b23f-0d04a75b3737'), (9070, 'f6396021-41a6-42e7-b12c-de89e8c6a0ea'), (7549, '5d8534a4-8f48-47c1-8dc9-b1c30ede1830'), (7551, 'd4335603-9f39-479c-a2ff-3595c944921d'), (9104, 'cf198758-ade7-47bb-82de-51e1e4e0bc2a'), (9105, '8b68bec4-21d7-407a-8ccf-1eb33fe9062a'), (9106, '0d5f7175-c2cd-4335-9084-dcdbb3bf43a5'), (10144, '78a0d3a7-2154-41f9-b87d-ee66babd1020'), (10145, 'ce96ae5b-5345-4742-bb66-c625e53c53e0'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (2486, '2d8ba467-ab34-4a64-b9e7-badb0b503981'), (2487, '14ae9625-97a8-4df9-a44c-fc87d442ca3e'), (2488, 'b50685c1-3fd6-4499-8e52-852bdbb2fe48'), (2490, 'd4f5e452-c337-48d1-9e39-de8ee6b76285'), (2491, 'aefe200c-ad39-44b6-a12e-575c5fcbf47f'), (2492, '40497a51-4b9d-4870-87b8-6d58dfc9001e'), (2493, '4b6116cb-7892-4969-80b1-11fcc811c1ca'), (2494, 'a522e4ba-2d7d-40bf-9a9c-18de74cc137f'), (2495, 'c831ecfe-8ba0-4247-97bd-3379c2df25ec'), (2496, '88889674-e469-4f11-967b-ecaa5db509a2'), (16320, 'aceded5d-8291-4ec6-937b-4ffa5e060758'), (18364, '82091703-8ce2-4abe-bc4a-5a4beda4c923'), (5061, '5ebc4085-43ae-4488-9355-8e1b17dfd415'), (2501, '1b748e58-f472-4e30-8e4b-26e263813b25'), (2507, 'e3274c16-e60e-45a3-85d6-b3c1800f42e7'), (2508, '03e961b4-71a0-4d72-9397-e2eb7cc4a1c8'), (2512, '922c050f-9165-4d33-9529-8a72a7e9ed40'), (2516, '02dc7eba-815e-47f3-a8bc-731fc3d4a427'), (18398, '881db9a6-a3a5-4163-a27d-d4319313f716'), (18399, '398c917a-6871-4126-85d0-056ee5dbe30a'), (18400, 'ef9b2afc-e962-412d-a475-22cc316750d3'), (5095, '2c960b15-fe52-4cc0-8902-eb912dfbf0e0'), (5096, '628ed62a-7410-4937-86e9-8e980e3d9920'), (5097, '2d07dc17-5032-44db-97e2-731bf7bfca9b'), (13305, '565bfc08-7c70-4b30-bfd3-ad92dddb3ad6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to
its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. In this
paper, we present LayoutLMv2 by pre-training text, layout and image in a multi-modal framework, where new model
architectures and pre-training tasks are leveraged. Specifically, LayoutLMv2 not only uses the existing masked
visual-language modeling task but also the new text-image alignment and text-image matching tasks in the pre-training
stage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware self-attention
mechanism into the Transformer architecture, so that the model can fully understand the relative positional
relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms strong baselines and
*Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the
widespread use of pretraining models for NLP applications, they almost exclusively focus on text-level manipulation,
while neglecting layout and style information that is vital for document image understanding. In this paper, we propose
the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is
beneficial for a great number of real-world document image understanding tasks such as information extraction from
scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM.
To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for
document-level pretraining. It achieves new state-of-the-art results in several downstream tasks, including form
The abstract from the paper is the following:

*Multimodal pre-training with text, layout, and image has made significant progress for Visually-rich Document
Understanding (VrDU), especially the fixed-layout documents such as scanned document images. While, there are still a
large number of digital documents where the layout information is not fixed and needs to be interactively and
dynamically rendered for visualization, making existing layout-based pre-training approaches not easy to apply. In this
paper, we propose MarkupLM for document understanding tasks with markup languages as the backbone such as
HTML/XML-based documents, where text and markup information is jointly pre-trained. Experiment results show that the
pre-trained MarkupLM significantly outperforms the existing strong baseline models on several document understanding
tasks. The pre-trained model and code will be publicly available.*
The abstract from the paper is the following:

*Multimodal pre-training with text, layout, and image has achieved SOTA performance for visually-rich document
understanding tasks recently, which demonstrates the great potential for joint learning across different modalities. In
this paper, we present LayoutXLM, a multimodal pre-trained model for multilingual document understanding, which aims to
bridge the language barriers for visually-rich document understanding. To accurately evaluate LayoutXLM, we also
introduce a multilingual form understanding benchmark dataset named XFUN, which includes form understanding samples in
7 languages (Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and key-value pairs are manually labeled
for each language. Experiment results show that the LayoutXLM model has significantly outperformed the existing SOTA
cross-lingual pre-trained models on the XFUN dataset.*
-->

# LayoutLM

<a id='Overview'></a>

## Overview

The LayoutLM model was proposed in the paper [LayoutLM: Pre-training of Text and Layout for Document Image
Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and
Ming Zhou. It's a simple but effective pretraining method of text and layout for document image understanding and
information extraction tasks, such as form understanding and receipt understanding. It obtains state-of-the-art results
on several downstream tasks:
 
Here is the question: How does LayoutLM improve document understanding compared to text-only models?","LayoutLM improves document understanding compared to text-only models by jointly modeling interactions between text and layout information across scanned document images, which is beneficial for a great number of real-world document image understanding tasks such as information extraction from scanned documents. Additionally, it incorporates words' visual information into the model by leveraging image features. This allows the model to fully understand the relative positional relationship among different text blocks, which is not possible with text-only models."
What is the key distinction between DETR and traditional object detection models?,"[(24064, 'b6a47dbd-f2c7-495b-be9d-1df30f14a395'), (17920, '9c587d42-fc22-4562-91cb-4e195b4c2fb1'), (17924, '5b0a1275-3317-4b6f-a5d9-84a7c3d71e4f'), (17926, 'fcc6fc34-3edd-43e4-b14b-e4de28ee9254'), (17927, '17a6d0e4-44b5-4cc2-b617-c98049e07917'), (17929, 'b18ebb24-46f7-4e3c-b4e4-defd1ce4ecad'), (17918, '614edb3a-1dab-44db-b368-77c9229645d1'), (15957, '3e6a66a0-6f14-4ee6-87f9-e0007e231de0'), (15958, '4f4d7669-cdab-47bc-91aa-46441b0ada5a'), (22105, '303bca85-f93b-4c4b-beea-dfa3885fd809'), (22107, '7016d826-b9d4-4fc2-a1eb-765565f4a335'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (22112, '72ebbfd3-6f7e-4e49-aeee-7692ef491d72'), (15968, '6e12c477-8d00-447a-a4f5-3ea190633d3d'), (22114, '0a5ba4bb-e50a-4171-82ba-b95d1f4b573c'), (22115, '9150d9a8-8fa7-4306-b55e-506c43d5351f'), (22113, '14d11c06-e512-45ae-8775-9c31eed1e9d7'), (22116, 'e561f1a9-e1b1-4da2-9db7-3485e7719c79'), (22118, '36a5c490-635c-46e4-bbff-9f2186d8cb06'), (15976, '091b3322-c4ef-4224-93cd-cc8201ba4ba9'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (15991, '6f8b7e92-77b4-401d-90e0-b58d203a3010'), (145, '82bcd7b4-a187-4092-8487-e9a50c1ae2b8'), (146, '76f72a72-531b-4daa-bb90-f35cf2112614'), (14996, '9baa6f8d-58bd-4683-a23a-6edac5da7e9b'), (31383, 'ebe05c0d-29a2-4128-af9c-2924be0bcabe'), (5789, '01b13fa9-405e-4f18-ad86-5a8549a59c25'), (5797, 'dc2eff85-d919-407a-8b02-26a1fd7f55d0'), (4783, '0c737a83-0ee0-4d98-8a7f-fd07ddb178c4'), (23742, 'ef2a9f76-e54c-4a7d-b20b-cbdb06199aca'), (23743, '1e8468a0-3b9a-43dd-8d98-034bd897f4b4'), (24056, '3a5f4de8-a3ff-40d2-a789-a355054190a6'), (24057, 'd292faa1-de56-402c-878a-e2d243e5db03'), (1756, '7dc789c1-95db-4be7-be41-2a9eb4705706'), (18142, '6432e78a-9379-42f5-ae76-b4e21cea4701'), (18144, 'ee4af106-0a20-453f-9d13-f2869e1d57e7'), (16611, '5ba26cb6-0890-4615-9f28-45a8ba3c5ded'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (16619, '0c9d0670-8b19-4167-97e9-4418dc3bd375'), (9452, '59aaeec8-b95f-4c82-89b3-f8905ba521d8'), (9453, '0763f28d-22c2-4948-ba1b-2cad01c04513'), (9454, '9de797fe-8ad7-43f0-86df-89345e1c4416'), (9455, '19e9e822-6304-4055-acec-9cff189e5fc4'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (6897, 'd88a3afc-4525-4ce0-9982-6f08b8d65534'), (24063, '68c3b5af-92ed-4063-9471-b9fce32331ca'), (13560, '838f91f6-9316-4ade-94c0-ca8e99840812'), (13562, '161d1786-5dd5-4883-af2d-6d40b0f613e7'), (4861, '79210cef-fada-44fb-a563-86b3d8b879d2'), (258, 'ccf8ce5c-aeab-4f18-8d26-8afda967080a'), (260, '14fc3235-cc9c-40f2-b951-b3002fd58852'), (24864, '9fd43470-89e8-4e23-a66c-e9ae2ac66f1c'), (13630, '5f595309-fb4e-4261-92ad-938f4eb058f2'), (23873, '6259700a-095f-4a14-8048-95fb1df7f296'), (28482, '3e98ed61-e405-42ac-8bd4-63da353dcace'), (23875, '7504618b-e7a7-4763-90ae-7702a4d617aa'), (23876, '9ec09514-e912-4251-a49b-f760d9b16f84'), (23877, '31509c88-8837-4b9d-ad51-5135d853ebd1'), (23879, '33efa917-ec34-42f9-9dba-22b827c59769'), (329, 'e5a52187-6e6d-47ba-bffa-cde854b2df46'), (2914, 'a437440c-998a-4d47-95de-352ecbe40b57'), (2915, '2db2eb55-2dc6-4ef1-aa46-55c914e880a1'), (2916, 'bf914d00-c1cf-4ab4-8b22-8641787bb13c'), (2917, 'd2246bb6-552f-4781-bcd8-fa71d6444c8d'), (9080, '8b8f599f-5243-408d-a6b9-d30288bf980b'), (2941, '8c483416-34d1-4d29-ba5b-878414f53927'), (2942, '6dd1087e-50d4-488b-b71c-0be7ed0bec5d'), (2944, 'd7daee80-75bc-4ee2-bcf3-3cf024d1a940'), (2945, 'd457e654-81bf-4d98-b51d-16c77c925417'), (23441, 'c6068114-3596-41cd-a5a9-7b573a830970'), (29081, 'fb92a989-9e09-4948-b2bc-d0c04fb1372f'), (29083, 'c31c65e7-4ff0-4f9a-a1a6-454212b45856'), (5034, '22b398e9-2cf4-46b0-8719-f39bbc9fb3ff'), (2498, 'd50bb439-a45b-4d6a-9a35-187b48d23c9c'), (18374, '20d9b64b-d0f4-4ffa-b3f2-0f7c1b52710e'), (5071, '51839c46-b9cb-4584-81c1-e2dca0138c66'), (17879, '0febef04-6ab5-44f6-bc89-962863e3aea9'), (17880, 'b0df342e-5867-45e5-95f6-3db08814a8c4'), (17881, '23452cf6-a9ba-41d4-9f58-b8f65df16caa'), (17882, '4c166992-ffd3-4c48-86ef-0f6fd102934d'), (17883, '2b9035d9-f069-450d-b557-24fcf97b2910'), (17884, 'e49a7762-796b-444b-a445-a5879dafc4f2'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (17886, 'd30ae109-0b56-489e-98da-428f04978d14'), (17893, 'e7b7d3d3-6779-4c81-9fd1-bf9db34d9bc4'), (17896, 'a716dda4-5dd1-4680-b0b3-7457ac80e32e'), (17897, 'b2c293ab-1e67-4b13-9656-273b0328e15b'), (24042, '089a831c-4f38-4cd4-b040-5b7abcd993a9'), (24043, '513d6d7b-52da-4acf-b55a-dcde6b2e7a0c'), (24044, 'a1f11a6f-a069-4ecc-b93e-337ac0ed14a3'), (24045, '2d1bae87-5feb-4e78-b32e-37ab2c42ebe8'), (24046, '7c5e6cbc-6fe8-49e1-ac93-4876e7a67454'), (24047, '010894cd-a4e9-43be-b277-7315f80d3141'), (24048, 'ae498ee6-ab8e-42d5-9d25-171274f6e755'), (24049, 'f5eaf2b3-155c-4d8f-ab52-aaa991cef16c'), (24050, '685c99e5-057d-4b3e-99b4-197c3ae5730f'), (24051, 'fbdebca1-822a-4463-be0f-9037e34f8342'), (17909, '0f26210c-2ee8-4e3f-9846-831bb49812bf'), (24054, 'e3a12540-4fda-4015-9a12-49a3dc50cdf8'), (17912, '6c000270-627d-44ae-bac1-9ab0640951e5'), (17913, '24d44c2f-f45a-48ea-9b6f-ffa568fe3800'), (17914, 'e3f62a9f-6c82-48c3-84c5-d5b2d2cdf56b'), (24059, '8730cadc-dd12-4971-8ffe-4c3b9a5d8056'), (17917, '3e26c0d6-3750-4005-9663-ab1c4105004a'), (24062, '17a005d1-952b-439c-bf0d-09520cded7bf'), (17919, '292976be-c487-4f13-b1a9-79e90c6369a9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *Detection Transformer (DETR) directly transforms queries to unique objects by using one-to-one bipartite matching during training and enables end-to-end object detection. Recently, these models have surpassed traditional detectors on COCO with undeniable elegance. However, they differ from traditional detectors in multiple designs, including model architecture and training schedules, and thus the effectiveness of one-to-one matching is not fully understood. In this work, we conduct a strict comparison between the one-to-one Hungarian matching in DETRs and the one-to-many label assignments in traditional detectors with non-maximum supervision (NMS). Surprisingly, we observe one-to-many assignments with NMS consistently outperform standard one-to-one matching under the same setting, with a significant gain of up to 2.5 mAP. Our detector that trains Deformable-DETR with traditional IoU-based label assignment achieved 50.2 COCO mAP within 12 epochs (1x schedule) with ResNet50 backbone,
*We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the
detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression
procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the
new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via
bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries,
DETR reasons about the relations of the objects and the global image context to directly output the final set of
predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many
other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and
- DETR uses so-called **object queries** to detect objects in an image. The number of queries determines the maximum
  number of objects that can be detected in a single image, and is set to 100 by default (see parameter
  `num_queries` of [`~transformers.DetrConfig`]). Note that it's good to have some slack (in COCO, the
  authors used 100, while the maximum number of objects in a COCO image is ~70).
- The decoder of DETR updates the query embeddings in parallel. This is different from language models like GPT-2,
  which use autoregressive decoding instead of parallel. Hence, no causal attention mask is used.
- DETR adds position embeddings to the hidden states at each self-attention and cross-attention layer before projecting
  to queries and keys. For the position embeddings of the image, one can choose between fixed sinusoidal or learned
  absolute position embeddings. By default, the parameter `position_embedding_type` of
  [`~transformers.DetrConfig`] is set to `""sine""`.
2. The feature vectors are passed to the encoder, which learns the image representations using its attention layers. Next, the encoder hidden states are combined with *object queries* in the decoder. Object queries are learned embeddings that focus on the different regions of an image, and they're updated as they progress through each attention layer. The decoder hidden states are passed to a feedforward network that predicts the bounding box coordinates and class label for each object query, or `no object` if there isn't one.

    DETR decodes each object query in parallel to output *N* final predictions, where *N* is the number of queries. Unlike a typical autoregressive model that predicts one element at a time, object detection is a set prediction task (`bounding box`, `class label`) that makes *N* predictions in a single pass.
The abstract from the paper is the following:

*DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/deformable_detr_architecture.png""
alt=""drawing"" width=""600""/>
 
Here is the question: What is the key distinction between DETR and traditional object detection models?","The key distinction between DETR and traditional object detection models is that DETR views object detection as a direct set prediction problem, eliminating the need for hand-designed components like non-maximum suppression (NMS) and anchor generation, whereas traditional detectors use one-to-many label assignments with NMS."
What preprocessing step does the `AutoTokenizer` handle in the Transformers library?,"[(3078, 'acfcc590-5b42-42b0-ac4b-46dae799f50a'), (25612, '01868de2-601f-4b56-9680-7fff3bd713aa'), (16399, 'dfd20272-a4fb-40e6-8c4a-eb2e8ac83370'), (1045, '9e6832cc-4ade-48ef-b453-6bf847dda000'), (30749, '1f54578e-bf90-4efe-b3d3-48ac13a8a2f9'), (8229, '8f21e539-a202-474e-81a6-443e2d0a7115'), (2603, 'd20d955f-cf79-4625-9929-f3c3548d700f'), (20523, 'bb4780d2-fb0c-4468-855d-13ac0fc45c7c'), (14894, '48d55ded-d67e-4be3-82e3-ce3bb2bb3785'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (14897, 'f38486c8-9c53-4985-a2dd-f9524c95e9c0'), (10823, 'e21b6aee-30a8-41ca-a03b-c9cbbe3e780e'), (10824, '6d96b69f-c6af-4766-b64b-337c86dd7faf'), (5718, 'e5c1a399-0835-4635-841b-8d00c339e0ac'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (12897, '78a4117e-544a-4fa9-9c46-63ee897d9e66'), (12903, '44dc538d-ecf5-47f4-9862-c6179d76af59'), (30312, 'd0a9520c-4b27-41e5-80a5-684f3b054f1b'), (30314, '062790b0-8e33-4435-ac93-85790f7ffb85'), (15478, 'b235b5c0-0d88-46a1-83d2-d72e1f226a7b'), (15479, '6daa1c77-9403-4dec-94ef-3f7f4160b632'), (11899, 'c242e864-053c-427b-8359-fe3179308fe8'), (15491, '87084643-cfb0-4d05-8ced-5a742b953434'), (13961, '8a21322f-5ee4-452b-88b0-2c8aff291f84'), (16524, '82e8de62-f2ea-4b9f-b238-49daaff66aa0'), (17042, '92ec80ab-ab9a-426b-aa02-aba5eeb4b527'), (2709, '1d0d3208-e930-4ddc-a49c-73fadf6dcb78'), (10390, '4675c741-058a-4e20-9b79-1a956319b212'), (13979, 'de950627-bfa4-44a7-aded-c6a93c8ee195'), (1192, '48d5754e-ee39-4825-aa44-fc362b042ee3'), (1194, '176e6b07-3a4c-4e46-94bd-150d14b57c69'), (1195, '3e7433ba-fae2-4e13-8802-9ff9da0b8900'), (30892, 'c7c7e937-8715-4300-9fb3-896a9568ab94'), (30894, 'cbc1ee58-63f8-4d36-b721-c2f5c32a7c39'), (688, 'dd7ba60b-ae7e-4773-acba-8c59fe920da9'), (17587, 'fd31d5fb-3745-4e63-a1ce-28974559d05e'), (19639, 'ee54b9b5-fa3c-4457-9bc6-160b732b815c'), (26809, '2936c0b4-ef39-49e1-8c33-c015934769f2'), (24763, '81ad470e-e933-4a27-9979-d29627c04cf1'), (9916, '38adbc72-3efe-45f2-8caa-4702f7539b73'), (24764, '645657c8-1e5b-415a-9175-a7b48fbed112'), (14029, '6bcbaa99-102f-4b45-b6b4-b58d00e4604a'), (17614, 'a4127615-83f7-416f-8953-35d7a99ae728'), (19664, '2872ed56-b851-405f-8601-3748f47a00e1'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (13532, '8fd50109-f5ba-4939-b029-7d52ed8bd5bf'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (1250, '46a46533-cfb4-4033-a0a2-ad4d9066e462'), (22245, '3590cbc4-b51f-432a-8dfe-5c293ec5646f'), (229, '8d6c749e-aba2-4e9c-86ed-fe336bbd3992'), (14567, '3330776a-c8c7-4367-b728-fba1c1251ddc'), (7921, 'dfb1162e-a303-4815-ba4a-22e68d333e5b'), (26866, '7c0456b0-8159-4ce8-b3f7-26b6dbed84fd'), (24830, 'f72b739a-f083-4a87-bd51-7c1becd2358b'), (20738, '180ef799-bbe9-421a-97db-0919371c476e'), (3330, '773f4fd3-6b91-450e-9baf-cc691b845874'), (17161, '5a419712-79a3-4f57-926e-74f9da484cf5'), (19722, '3876686d-2be2-4dc4-a048-968b26e05815'), (21259, '4b7416f8-d3ac-48cf-ae7f-cc6f5afbb494'), (21261, '9fc8bb55-a9e1-487a-9702-6f727d330ef6'), (27406, '954a0b75-0188-4266-84c0-4a2f946626bf'), (3343, 'a583dfae-ba2d-47d5-83f4-089b36deba7c'), (3346, '032420bb-66aa-4f3e-b636-fe7b4d17188b'), (28947, '633f4bd5-8841-4dfe-a590-7980557ba1da'), (3351, '6a70bba3-b7d2-4b60-a6c5-5adf098dffcf'), (16666, '26627d88-bba5-49fd-a9a1-5f8958fa344f'), (3364, '28218e01-4fde-45a7-a4cc-4b166f6af333'), (11052, '086572b7-4e0c-446b-a514-4bc43efab281'), (26926, '3c507921-05b4-4626-bf7a-e8162a34e3a9'), (28976, 'faae52fd-7dbe-4472-80e7-24d5e5104cce'), (25907, '4c651379-1c81-48ed-9f9b-a9d7060ae2e3'), (31544, 'eb25ff06-0119-48a6-822a-15328549c28a'), (11065, 'f276616a-e9b5-4dcd-ae38-4256ec518cf8'), (21817, '931f3d19-2f6b-426e-9e7b-2d20ef96b089'), (10560, '573b41d5-0219-46dd-a582-e72660c54732'), (24904, '0d263af5-23ad-4f96-a3b0-81bee9b6bd23'), (12105, 'e4f29e10-0755-4ad0-87ea-18507f153a17'), (7507, 'ccc6d78e-a524-4c60-afef-5b359b51febf'), (855, 'b464c94c-ff07-41a8-bcbf-b75efd8dea91'), (18271, 'd8a0b21a-3bfa-4527-aea2-525c77aeba86'), (10090, '8113f1c3-4c93-4dd2-b961-f3b61ffc9e0e'), (24949, '83efc35a-a86a-47fb-9f8c-04d42a95a7a8'), (24951, '518851bb-0a7b-4098-993b-556790a3803b'), (31101, 'c943ed4f-1ab8-411a-a1f2-28e45430aea2'), (20862, 'd3fd15a9-7f09-44c6-8342-ef2c61cba290'), (22399, '8c31bbea-69a2-4f83-afb3-796762dba928'), (3968, 'a33b79e1-7414-4e9f-b531-b1b20c2fc2f8'), (9610, 'b094bdc0-3783-4e56-aa01-2bb05d7808ef'), (9611, '23cfc180-57d0-403b-be80-7ecf9be74322'), (18316, '477b02f9-3b34-4924-bd6b-dc1f478a1d5b'), (28562, '672617a2-bf46-4632-9cb2-8d0187372dc3'), (26004, '24ff7e08-15b1-4eb9-99e6-4ff1379bb974'), (11677, 'b65b3085-bff1-4fe2-9cbe-69e92c3410b8'), (19870, '359a7e1c-0599-4279-9610-c730b74ad737'), (19871, '0a993e9c-5a70-49a8-8b8a-ea79fff01d49'), (2477, 'e57f43f0-16e6-46b0-b3ba-dcc7f8c68e15'), (29619, '228cf10d-872b-4bd7-b2e8-6087e98e55fa'), (5044, '3347ade6-a88b-4653-97f6-3b4beff0fa0c'), (29621, 'eefc33cd-1766-424e-a0fd-e40657b872d7'), (27093, '49e7d98a-c253-4dcb-a8ab-1282bc24cff5'), (23510, '6235b762-c6b0-4de0-b0e5-9327a31aa0bf'), (3555, '59f59d85-8d73-4831-a23d-9040e6170b39'), (8680, '31b2a8b6-db02-4422-9575-f8708e6cf9ab'), (14829, '2e961f5e-1054-4fe6-9a18-fcf4df5489ea'), (3056, 'd4d04eec-32a0-4471-9e0a-6a0802f35104'), (14833, '0a2e6b4c-9bb4-405a-bd06-61b9c4b7497a'), (8182, '7613c1c3-12f8-4ce6-ac52-0a4820e8f467'), (17404, '35034f8e-d7fb-40a9-b538-e5b7820c0f21'), (1534, 'fb44d1f3-3c02-4d77-afaf-99392c4d8752')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: When using transfer learning, however, it's very important that you process inputs to the model the same way that they were processed during training. This ensures that the model has to relearn as little as possible when we transfer its knowledge to a new problem. In `transformers`, this preprocessing is often handled with **tokenizers**. Tokenizers can be loaded in the same way as models, using the `AutoTokenizer` class. Be sure that you load the tokenizer that matches the model you want to use!

```py
from transformers import TFAutoModel, AutoTokenizer

# Make sure to always load a matching tokenizer and model!
tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")
model = TFAutoModel.from_pretrained(""bert-base-cased"")

# Let's load some data and tokenize it
test_strings = [""This is a sentence!"", ""This is another one!""]
tokenized_inputs = tokenizer(test_strings, return_tensors=""np"", padding=True)
</Tip>

Generally, we recommend using the `AutoTokenizer` class and the `AutoModelFor` class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next [tutorial](preprocessing), learn how to use your newly loaded tokenizer, image processor, feature extractor and processor to preprocess a dataset for fine-tuning.
</pt>
<tf>
Finally, the `TFAutoModelFor` classes let you load a pretrained model for a given task (see [here](model_doc/auto) for a complete list of available tasks). For example, load a model for sequence classification with [`TFAutoModelForSequenceClassification.from_pretrained`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(""distilbert-base-uncased"")
<Youtube id=""nvBXf7s7vTI""/>

🤗 Transformers provides a `Trainer` class to help you fine-tune any of the pretrained models it provides on your dataset. Once you've done all the data preprocessing work in the last section, you have just a few steps left to define the `Trainer`. The hardest part is likely to be preparing the environment to run `Trainer.train()`, as it will run very slowly on a CPU. If you don't have a GPU set up, you can get access to free GPUs or TPUs on [Google Colab](https://colab.research.google.com/).

The code examples below assume you have already executed the examples in the previous section. Here is a short summary recapping what you need:

```py
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

raw_datasets = load_dataset(""glue"", ""mrpc"")
checkpoint = ""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

2. Load a preprocessing class like a tokenizer, image processor, feature extractor, or processor:

   ```py
   >>> from transformers import AutoTokenizer

   >>> tokenizer = AutoTokenizer.from_pretrained(""distilbert-base-uncased"")
   ```

3. Create a function to tokenize the dataset:

   ```py
   >>> def tokenize_dataset(dataset):
   ...     return tokenizer(dataset[""text""])  # doctest: +SKIP
   ```

4. Apply the tokenizer over the entire dataset with [`~datasets.Dataset.map`] and then pass the dataset and tokenizer to [`~TFPreTrainedModel.prepare_tf_dataset`]. You can also change the batch size and shuffle the dataset here if you'd like:

   ```py
   >>> dataset = dataset.map(tokenize_dataset)  # doctest: +SKIP
   >>> tf_dataset = model.prepare_tf_dataset(
   ...     dataset[""train""], batch_size=16, shuffle=True, tokenizer=tokenizer
   ... )  # doctest: +SKIP
```

## Preprocess dataset

Let's prepare the dataset for training. Load a feature extractor, tokenizer, and processor. You should also pass the language and task to the tokenizer and processor so they know how to process the inputs:

```py
from transformers import AutoFeatureExtractor, AutoTokenizer, AutoProcessor

feature_extractor = AutoFeatureExtractor.from_pretrained(model_name_or_path)
tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, language=language, task=task)
processor = AutoProcessor.from_pretrained(model_name_or_path, language=language, task=task)
 
Here is the question: What preprocessing step does the `AutoTokenizer` handle in the Transformers library?",The `AutoTokenizer` handles the preprocessing step of tokenizing the input data in the Transformers library.
What task is TAPAS optimized for in the Hugging Face library?,"[(10245, '022ae3cf-1084-40e3-b8c7-875a87e42cb4'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (14876, '45893d20-107c-4db9-a5c6-937c7bfea6c4'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (31271, 'e84ce4da-70f4-419c-8992-b825495bbf7b'), (23082, 'ffc45be3-6ef6-419f-a2b1-4a90fb78a421'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (21136, '6940c909-8fdc-4dfe-bd8c-d617a9c1cb8a'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21138, 'c2c5b354-94af-4fd1-bbc5-65ca22e909cb'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (21153, '0e8dd5cf-1222-409e-8efa-1a63c0f03561'), (23714, 'a2d69303-a006-44f0-8008-1accb10a3697'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (18083, '44c0febf-121b-48ea-afb6-fb9879e52104'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5806, '5054cac8-6958-4df8-9986-f0bb2157877f'), (19117, 'a570815f-38b6-47ca-a5c8-1bd2674040a4'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (8892, '91b7caf7-7012-4878-b2e4-e5e45737faa9'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (25299, 'f6c0a00d-d54d-4ae5-a0b9-af2a5eba1630'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (15064, 'd2be339c-6a22-428d-a082-f7c51fd357ff'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (15070, 'b79c805d-f30e-4344-a16b-e77f25c0719f'), (28895, 'a9db2a61-1c6e-4bf8-adba-ba8bd4fd18b2'), (22752, '847c0672-9e44-445e-b14d-84817ad403ee'), (7912, '93b75fbc-1720-442c-8161-9f4a66e82e5e'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (21760, '273c70db-fb08-4e2d-88cd-25720c4f2c6f'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (8462, '2c893bf2-57c0-475a-93c5-62cd31957575'), (1299, '2060a92d-6b84-4555-9fd9-8c570bd74c4c'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (3368, '17715a5e-5f2c-4df5-94da-06ac43fe341f'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (9564, '70c97e3e-c7da-4237-84d9-68cd10dda983'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (15215, '8b19f2fb-4cae-4528-8a1e-bbe343f82786'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (7026, 'a54a170b-414e-4326-bff0-925d0cf07fab'), (16252, 'ebc3b30d-233d-43e4-b012-9f5a958063fc'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (25476, '3fcbb85d-17ea-4cf1-b253-7994b353a67e'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (23465, '0b03992a-9d11-4d6f-9f2b-6f512e3007c1'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29107, '90120fe5-ecd2-4fc7-b8d6-52f400cebcc5'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (24522, '84973e34-c9c8-4efe-97f2-d3f8efb83196'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (27607, '477e553f-670e-4d2b-bda2-3ae34b7a5434'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (20452, '2c34139d-f9af-47ac-b2f0-67be904d0c39'), (31717, 'e243cc16-22ac-4aa1-b77a-973332494a6f'), (20456, '2fea8919-613c-4a13-9948-1cf76918fb86'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (15851, '0bde2ce9-28b0-4ae0-9459-a572cca7f941'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (19438, '1b351e34-f0ef-408a-b6b2-209c3c330c69'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (1523, '07ef543d-eb22-404b-8aad-103be2f770bf'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Optimising Transformers for Production

Transformers have completely transformed (pun intended) the field of AI. Models such as BERT are widely used by Graphcore customers in a huge array of applications, across NLP and beyond. These multi-talented models can perform feature extraction, text generation, sentiment analysis, translation and many more functions.

Already, Hugging Face plays host to hundreds of Transformers, from the French-language CamemBERT to ViT which applies lessons learned in NLP to computer vision. The Transformers library is downloaded an average of 2 million times every month and demand is growing.

With a user base of more than 50,000 developers – Hugging Face has seen the fastest ever adoption of an open-source project.

Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.
```

## Optimum Library

Hugging Face is a fast-growing open community and platform aiming to democratize good machine learning. We extended modalities from NLP to audio and vision, and now covers use cases across Machine Learning to meet our community's needs following the success of the [Transformers library](https://huggingface.co/docs/transformers/index). Now on [Hugging Face Hub](https://huggingface.co/models), there are more than 120K free and accessible model checkpoints for various machine learning tasks, 18K datasets, and 20K ML demo apps. However, scaling transformer models into production is still a challenge for the industry. Despite high accuracy, training and inference of transformer-based models can be time-consuming and expensive.
Now, with its Hardware Partner Program, Hugging Face is connecting the ultimate Transformer toolset with today's most advanced AI hardware.

Using Optimum, a new open-source library and toolkit, developers will be able to access hardware-optimized models certified by Hugging Face.

These are being developed in a collaboration between Graphcore and Hugging Face, with the first IPU-optimized models appearing on Optimum later this year. Ultimately, these will cover a wide range of applications, from vision and speech to translation and text generation.

Hugging Face CEO Clément Delangue said: “Developers all want access to the latest and greatest hardware – like the Graphcore IPU, but there’s always that question of whether they’ll have to learn new code or processes. With Optimum and the Hugging Face Hardware Program, that’s just not an issue. It’s essentially plug-and-play"".

## SOTA Models meet SOTA Hardware
### Tools for exploring and working with datasets

There are a growing number of tools being created which make it easier to understand datasets hosted on the Hugging Face Hub. 

### Tools for loading datasets hosted on the Hugging Face Hub 

Datasets shared on the Hugging Face Hub can be loaded via a variety of tools. The [`datasets`](https://huggingface.co/docs/datasets/) library is a Python library which can directly load datasets from the huggingface hub via a `load_dataset` command. The `datasets` library is optimized for working with large datasets (including datasets which won't fit into memory) and supporting machine learning workflows.
4. [LLM Optimization](https://huggingface.co/text-generation-inference): Optimized for LLMs, enabling high throughput with Paged Attention and low latency through custom transformers code and Flash Attention power by Text Generation Inference
5. [Comprehensive Task Support](https://huggingface.co/docs/inference-endpoints/supported_tasks): Out of the box support for 🤗 Transformers, Sentence-Transformers, and Diffusers tasks and models, and easy customization to enable advanced tasks like speaker diarization or any Machine Learning task and library.
 
Here is the question: What task is TAPAS optimized for in the Hugging Face library?","The answer cannot be deduced from the context, as TAPAS is not mentioned in the provided text."
What is the primary purpose of VideoMAE in the Hugging Face ecosystem?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (7178, '086ecf90-fb2d-4fb0-9e37-23a45c3703ea'), (18443, '00876f43-8b96-47cb-ba09-71690498db3d'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (5141, 'aa79a23d-5dce-4f0c-905f-502f514390e9'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (28702, '543ba877-2cf5-455d-ada4-0723af0d74dc'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (28712, '02630cd6-b1a8-4bf9-b2a1-09a7d7d4d5fd'), (23593, 'c4cad64a-26a2-4318-b2bf-37b79d234f09'), (28716, '0e901ab3-56fc-46c6-a3c7-adee2baa6fca'), (30767, '456bf7dc-6ce9-4319-8a17-9096e901ce89'), (1584, '0d8bad5c-d9db-4574-b7f2-7e45c57460a3'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (1587, '91ad012d-28c8-4d60-9505-d50047646ce0'), (564, '3cb31992-cb17-4b45-88c5-6ab44194256f'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (7224, '49503e5a-ba18-43c6-9722-173bbeee8dd9'), (16954, 'b3f25eff-c105-4e0f-a20c-da1be8daf35e'), (580, '220b772e-41f0-4265-9916-a11b8180f66d'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (17509, '56ea90b0-f162-4bf5-8c05-654f3a183cd9'), (31340, '1bffbde6-8c01-4f3d-914e-12799d8de47f'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (19578, 'c39dbfdb-a1e4-46cb-b486-736ca8e047bb'), (11386, 'b62622b1-cbec-4754-9710-20faddd1d382'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (5776, '8001f35a-9ae7-4b87-b42a-486643837817'), (26773, '36a72c91-d9e1-4a83-8b43-432e315d3dd5'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (20654, '0dfc500c-a1cd-464b-917d-6d60a2832b61'), (19117, 'a570815f-38b6-47ca-a5c8-1bd2674040a4'), (18620, '3c01384a-579e-4c80-8f40-2877e8079213'), (18621, 'a8c4b4e5-1ba0-4ee7-8ce6-fa9bb2fee140'), (18624, '355b21ab-d432-4cbc-80cf-045354644b16'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (18625, 'f698c5d3-9306-49b5-b647-aeae545d29ee'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (14536, '35c97d08-cce8-4a82-8a9d-bcd7d45bc9ca'), (16076, '86d0bad6-da7c-4137-84bc-4ab20f21d914'), (716, '3ac83e0f-ae99-4341-a5c8-b350a45098ad'), (205, 'b799eadc-2980-4a69-9219-6422c5b0a54e'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (26836, '9d0493e6-8f23-4c53-b979-3cfab7d1e205'), (11477, 'c5876285-5fa1-4172-9c57-bef923092cfa'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (22756, '583a445a-eaf6-4e7a-8f28-8707a7e1cbf7'), (17642, '65fcc7ea-697c-444e-8b1c-9b81f01558fe'), (4850, '3a950f12-3c2d-47ce-a6a8-1abdfb1befa0'), (24819, '0857c8b1-5b56-46ba-ac6d-6f09a3ed5011'), (19191, 'e9283c36-b474-44ec-bf62-24457f376faa'), (19193, '47b2a217-5aaf-43bf-90c5-6431f02321bd'), (21243, '4c853a7f-56ab-4489-93ba-acedd42e9107'), (12028, 'da0f499e-0b89-4a96-8974-e7bbb9af3fff'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (1822, '4f77f068-d03b-4652-a24f-de37a88161a9'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (11581, '2601a0d2-947c-40f9-bc88-35febc4ce941'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (12099, '97456597-42ed-4914-bc4a-1146c5fc4b6f'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (348, 'a2e55383-c0a3-42c9-bc84-8a5018baf440'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (897, 'f387e8a2-b1bf-48fa-a6f3-c7a015a21ae7'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (3510, '4e9735e1-0a03-42b4-a237-4ea82a96730d'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9150, 'de3eb793-c6de-4fd2-9f8c-1b0b686cf6d1'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (19916, 'e7d06c45-10f0-4bd2-8d3f-e2478dd9da51'), (10190, '92ba9e39-8a19-408b-92df-63347b640043'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (11228, '52d60165-8b9b-4209-93b8-39681023ebce'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (11235, '8558ce8a-5be5-4c58-91a4-e034fd8f061d'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (13801, 'aa3ed41c-4b41-428a-a670-fc2e12d17add'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Support for third-party libraries

Central to the Hugging Face ecosystem is the [Hugging Face Hub](https://huggingface.co/docs/hub), which lets people collaborate effectively on Machine Learning. As mentioned earlier, we not only support models from 🤗 Transformers on the Hub but also models from other third-party libraries. To this end, we provide [several utilities](https://huggingface.co/docs/hub/models-adding-libraries) so that you can integrate your own library with the Hub. One of the primary advantages of doing this is that it becomes very easy to share artifacts (such as models and datasets) with the community, thereby making it easier for your users to try out your models.

When you have your models hosted on the Hub, you can also [add custom inference widgets](https://github.com/huggingface/api-inference-community) for them. Inference widgets allow users to quickly check out the models. This helps with improving user engagement.
![You can now find interactive demos under ArXiv papers](/blog/assets/arxiv/recording.gif)

Since its launch in October 2021, Hugging Face Spaces has been used to build and share over 12,000 open-source machine learning demos crafted by the community. With Spaces, Hugging Face users can share, explore, discuss models, and build interactive applications that enable anyone with a browser to try them out without having to run any code. These demos are built using open-source tools such as the Gradio and Streamlit Python libraries, and leverage models and datasets available on the Hugging Face Hub.
### Hugging Face Demos
At Hugging Face, our goal is to make it easier to use and build upon state-of-the-art research. Head over to our hub to see and play around with Spaces demos contributed by the 🤗 team, countless community contributors and research authors. At the moment, we host demos for [VideoGPT](https://huggingface.co/spaces/akhaliq/VideoGPT), [CogVideo](https://huggingface.co/spaces/THUDM/CogVideo), [ModelScope Text-to-Video](https://huggingface.co/spaces/damo-vilab/modelscope-text-to-video-synthesis), and [Text2Video-Zero](https://huggingface.co/spaces/PAIR/Text2Video-Zero) with many more to come. To see what we can do with these models, let's take a look at the Text2Video-Zero demo. This demo not only illustrates text-to-video generation but also enables multiple other generation modes for text-guided video editing and joint conditional video generation using pose, depth and edge inputs along with text prompts.
he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which
he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which
 
Here is the question: What is the primary purpose of VideoMAE in the Hugging Face ecosystem?","There is no mention of VideoMAE in the provided context, so it is not possible to determine its primary purpose in the Hugging Face ecosystem."
How does the Hugging Face `pipeline` API assist with zero-shot classification?,"[(13825, 'f1446f89-52a4-4aaf-ab1a-9b92f5010ce7'), (13826, '2a55b5cd-c496-407c-97b0-92e3ab12e0e7'), (3594, '4e1600b8-d386-41a9-bc22-67a03574e9d3'), (3595, 'fc346d5a-1341-4054-aff8-c1e07891108a'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (27689, 'a45172f4-2f68-4ad4-969f-0e32e39df5cb'), (28716, '0e901ab3-56fc-46c6-a3c7-adee2baa6fca'), (23087, '78540d5c-200a-44a6-80b8-ac019e34baf9'), (24127, '474b8446-9f4d-4ec3-9b0e-78695268902d'), (22079, '16104f25-f547-44dd-9b5f-3798c428cc44'), (22083, 'dd15b594-110b-4773-b128-0658f80e911d'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (4178, 'fae17654-76a4-48ba-8a62-9b7bc23a7d8a'), (10838, 'e6530fa4-03e3-4fec-bb90-0c3e249eb971'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (4186, 'e6d642f9-4872-48cf-9eb5-dcdaf5c2a380'), (14429, '49a2886b-403f-463e-8384-2fb84e5ab062'), (1630, '8b61309f-c135-4283-bd28-0a9b2a9eaed7'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (10858, '33249571-1ade-46fd-ae89-e32ff806fdcd'), (10860, '31a71cbd-9055-4cee-9989-720671b2902c'), (2668, 'cf2d43e8-7268-49a8-a30e-520cb997cf07'), (13421, '794edc9c-81e0-43ec-94cb-c67e7bbe50c7'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (14961, '116e77dc-af15-4a1c-a806-9c18bf7c534b'), (23159, '8df7603d-0d12-428d-8ffd-3b4c55cd0c5f'), (6268, 'fb12cc65-f485-483b-b997-bff673358726'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (21639, 'f89e686f-ed68-4e75-a452-e5832f6fd820'), (15500, 'faef5ce1-f4bc-4b19-bd94-f0bd6be7eace'), (145, '82bcd7b4-a187-4092-8487-e9a50c1ae2b8'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (150, '3bace117-5049-4192-b247-676e182cf109'), (149, '19d1f1a7-3ff1-4d99-b448-86fda7b1e1a1'), (153, 'a5e94b25-4bd0-4edc-be2a-4167144129cb'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (5810, 'b507f388-1038-4bc8-8559-2d51ffaec850'), (3771, 'cba7f1b1-a6ee-4540-a1ad-79f082e1175c'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (21187, 'd9dcef43-8125-461e-9c4f-131925ad7523'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16072, 'da66f4cb-8456-478e-acc4-29a3ea7aac65'), (22223, '2bed839b-dad7-4906-aac6-0fd1dbd515c7'), (21203, '5f38e41b-22a4-4ec8-b993-4864b7079429'), (23251, '23c5e66c-a5d9-4d1b-beee-3e3345488c3c'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (8929, '6416a685-c9f1-4d8e-a261-4d5e3377eb82'), (29410, '1750462a-8592-4830-8fb0-5ba4e629cf2c'), (5888, '7a16f065-8a67-4207-afcb-1487fd8ec145'), (26881, '6a5ea7b7-fc90-49e6-b2be-bb315aabc5f2'), (26883, '2933d4ed-2563-4956-8c7b-5b5afa75f91e'), (17159, 'bd14f784-c8e8-425a-a848-98cb832d2e60'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (28427, 'c3e7f0fe-7c4b-430d-a0ad-42e0abcf5f96'), (18701, '9c065cf2-bb0a-419b-9090-9e75a3bf2630'), (28943, 'dd1112c0-4e24-4bc6-9824-e92c5e1de2bc'), (12050, '67332def-d4e8-4b99-b29c-af15da1631fb'), (3859, 'e092e829-7c5f-48db-b5d9-1cd53e2de720'), (1309, '8adeb3ac-69d3-4c67-88f9-39acd31a5b74'), (24356, '07db076b-feac-48e0-a517-caf251d54bdf'), (5944, 'af31af69-da28-4239-b3f5-59c624ec8d83'), (5945, '82b994e1-d6d9-4269-92fd-5bf9f88254b7'), (5946, '7a54286f-a79f-4ab2-b704-5487dafb7a5b'), (5947, 'a8cc8df6-3050-419f-9611-d91ef95d2660'), (5948, '68fab821-06c3-4a78-96e2-4dfd616f31c0'), (5949, '2f839d59-b35e-48bf-bf7a-7b0a0328a9f2'), (16702, 'cf05cd6b-e33d-4bc7-a11e-5058f0d0676b'), (14165, 'f8e81797-b080-4a41-9f82-86eea16eabe8'), (31586, '4351b921-b4f2-4a0a-a126-07b9ddb21e7f'), (18789, '14581cc7-6ec8-445f-b40c-75d29e153b4a'), (15215, '8b19f2fb-4cae-4528-8a1e-bbe343f82786'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (28539, '15ca206b-e242-4240-9c56-b957d70a1add'), (14206, '5bfbab3f-215b-442a-8bd0-1810e815b192'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (6045, 'f8f84514-0fc4-4472-bc43-fe12c1366193'), (24478, '698a8b1f-969a-454c-bca5-7b8c2c6f6a59'), (2466, '79ca1d45-83b8-48a9-9a04-eca71f2b1ba1'), (5041, '812dcc27-ef00-4194-976d-6f9eb30583a9'), (29110, 'cb901626-ecea-43a5-aca3-4f794968bd03'), (22457, '7963fb33-54d2-4a57-a431-7124a7d34254'), (30141, '8d2521d3-81d3-444f-a8e4-bb7fb542cd5b'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (31682, '20d03ac7-1e99-48dc-b324-0cbfc547d99d'), (20420, '8b41c7d9-6430-42dd-ad00-b6de1ccb8e59'), (16337, '2a82c5a8-c446-461e-87a4-57a53425634d'), (6099, '28c7595f-f297-485b-81d1-fee1aab474c0'), (30167, 'e2e5d454-ed15-4973-9ed5-59bd9e36aa28'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (478, '1bac157f-bc66-440f-b56c-736475c789cf'), (2531, 'dec41cf7-889d-4e85-b490-d5f3c8f4ea84'), (13801, 'aa3ed41c-4b41-428a-a670-fc2e12d17add'), (15857, 'a948a556-2726-422c-ada1-d6f6cfeaeb9b'), (30196, 'fe9fe11a-b733-478c-bb0a-e2955b6b5a39'), (30199, '7f526aff-0ecc-4763-806e-7ad5f531d814'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: **Hugging Face + Google Sheets Demo**

With the [Inference API](https://huggingface.co/landing/inference-api/startups), you can easily use zero-shot classification right into your spreadsheets in Google Sheets. Just [add this script](https://gist.github.com/feconroses/302474ddd3f3c466dc069ecf16bb09d7) in Tools -> Script Editor:

<div class=""aspect-w-16 aspect-h-9"">
<iframe 
src=""https://www.youtube.com/embed/-A-X3aUYkDs"" 
frameborder=""0"" 
allow=""accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture"" 
allowfullscreen></iframe>
</div>



**Few-shot learning in practice**

We wrote a [blog post](https://huggingface.co/blog/few-shot-learning-gpt-neo-and-inference-api) about what Few-Shot Learning is and explores how GPT-Neo and 🤗 Accelerated Inference API are used to generate your own predictions.

### **Expert Acceleration Program**
```

## Zero-shot image classification pipeline

The simplest way to try out inference with a model supporting zero-shot image classification is to use the corresponding [`pipeline`].
Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads):

```python
>>> from transformers import pipeline

>>> checkpoint = ""openai/clip-vit-large-patch14""
>>> detector = pipeline(model=checkpoint, task=""zero-shot-image-classification"")
```

Next, choose an image you'd like to classify.

```py
>>> from PIL import Image
>>> import requests

>>> url = ""https://unsplash.com/photos/g8oS8-82DxI/download?ixid=MnwxMjA3fDB8MXx0b3BpY3x8SnBnNktpZGwtSGt8fHx8fDJ8fDE2NzgxMDYwODc&force=true&w=640""
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image
```

## Zero-shot image classification by hand

Now that you've seen how to use the zero-shot image classification pipeline, let's take a look how you can run zero-shot
image classification manually.

Start by loading the model and associated processor from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=zero-shot-image-classification&sort=downloads).
Here we'll use the same checkpoint as before:

```py
>>> from transformers import AutoProcessor, AutoModelForZeroShotImageClassification

>>> model = AutoModelForZeroShotImageClassification.from_pretrained(checkpoint)
>>> processor = AutoProcessor.from_pretrained(checkpoint)
```

Let's take a different image to switch things up.

```py
>>> from PIL import Image
>>> import requests

>>> url = ""https://unsplash.com/photos/xBRQfR2bqNI/download?ixid=MnwxMjA3fDB8MXxhbGx8fHx8fHx8fHwxNjc4Mzg4ODEx&force=true&w=640""
>>> image = Image.open(requests.get(url, stream=True).raw)

>>> image
The output is a list of individual results, in the same order as the input texts. Here we find the same label and score for the first text, and the second text is judged positive with a confidence of 99.99%. The zero-shot classification pipeline is a more general text-classification pipeline: it allows you to provide the labels you want. Here we want to classify our input text along the labels ""education"", ""politics"" and ""business"". The pipeline successfully recognizes it's more about education than the other labels, with a confidence of 84%. Moving on to other tasks, the text generation pipeline will auto-complete a given prompt. The output is generated with a bit of randomness, so it changes each time you call the generator object on a given prompt. Up until now, we have used the pipeline API with the default model associated to each task, but you can use it with any model that has been pretrained or fine-tuned on this task. Going on the model hub (huggingface.co/models), you can
Here is a list of things we’ll cover:

- [Supported vision tasks and Pipelines](#support-for-pipelines)
- [Training your own vision models](#training-your-own-models)
- [Integration with `timm`](#🤗-🤝-timm)
- [Diffusers](#🧨-diffusers)
- [Support for third-party libraries](#support-for-third-party-libraries)
- [Deployment](#deployment)
- and much more!

## Enabling the community: One task at a time 👁

The Hugging Face Hub is home to over 100,000 public models for different tasks such as next-word prediction, mask filling, token classification, sequence classification, and so on. As of today, we support [8 core vision tasks](https://huggingface.co/tasks) providing many model checkpoints:

- Image classification
- Image segmentation
- (Zero-shot) object detection
- Video classification
- Depth estimation
- Image-to-image synthesis
- Unconditional image generation
- Zero-shot image classification
 
Here is the question: How does the Hugging Face `pipeline` API assist with zero-shot classification?","The Hugging Face `pipeline` API assists with zero-shot classification by providing a simple way to use zero-shot image classification models in a pipeline. It allows users to instantiate a pipeline from a checkpoint on the Hugging Face Hub, and then use it to classify images without requiring any additional training or fine-tuning. The pipeline takes care of the underlying complexity of the model, allowing users to easily integrate zero-shot image classification into their applications."
What feature of DETR simplifies object detection workflows?,"[(24064, 'b6a47dbd-f2c7-495b-be9d-1df30f14a395'), (17927, '17a6d0e4-44b5-4cc2-b617-c98049e07917'), (17929, 'b18ebb24-46f7-4e3c-b4e4-defd1ce4ecad'), (17918, '614edb3a-1dab-44db-b368-77c9229645d1'), (15957, '3e6a66a0-6f14-4ee6-87f9-e0007e231de0'), (15958, '4f4d7669-cdab-47bc-91aa-46441b0ada5a'), (22104, '069ad32b-c04c-40f7-a7dc-02c674ed408e'), (22105, '303bca85-f93b-4c4b-beea-dfa3885fd809'), (10842, '76705614-ca04-439a-bd22-96988b1c2e44'), (22107, '7016d826-b9d4-4fc2-a1eb-765565f4a335'), (22110, '60c67161-62ba-4bb8-88e1-6a0592f80287'), (22111, '6cb41ff3-1249-49c2-b70e-3668ab947b7a'), (15968, '6e12c477-8d00-447a-a4f5-3ea190633d3d'), (22113, '14d11c06-e512-45ae-8775-9c31eed1e9d7'), (22114, '0a5ba4bb-e50a-4171-82ba-b95d1f4b573c'), (22112, '72ebbfd3-6f7e-4e49-aeee-7692ef491d72'), (22116, 'e561f1a9-e1b1-4da2-9db7-3485e7719c79'), (22117, '8cff68d8-a4af-44d2-a8ff-5456711c3d39'), (10854, '3cae5d2f-8bf0-43cd-8609-1f7a2de7db58'), (22115, '9150d9a8-8fa7-4306-b55e-506c43d5351f'), (22118, '36a5c490-635c-46e4-bbff-9f2186d8cb06'), (15976, '091b3322-c4ef-4224-93cd-cc8201ba4ba9'), (10857, '4cb2e390-8c4e-4ec7-b071-4244c40ac28b'), (15991, '6f8b7e92-77b4-401d-90e0-b58d203a3010'), (15992, '83f100af-399e-463b-a61f-8ead65e5b0cb'), (145, '82bcd7b4-a187-4092-8487-e9a50c1ae2b8'), (146, '76f72a72-531b-4daa-bb90-f35cf2112614'), (14996, '9baa6f8d-58bd-4683-a23a-6edac5da7e9b'), (5797, 'dc2eff85-d919-407a-8b02-26a1fd7f55d0'), (4783, '0c737a83-0ee0-4d98-8a7f-fd07ddb178c4'), (23742, 'ef2a9f76-e54c-4a7d-b20b-cbdb06199aca'), (23743, '1e8468a0-3b9a-43dd-8d98-034bd897f4b4'), (24059, '8730cadc-dd12-4971-8ffe-4c3b9a5d8056'), (1756, '7dc789c1-95db-4be7-be41-2a9eb4705706'), (18142, '6432e78a-9379-42f5-ae76-b4e21cea4701'), (16095, '02d79523-ad0c-4ecc-b5d7-465408fcb8cf'), (18144, 'ee4af106-0a20-453f-9d13-f2869e1d57e7'), (16094, '90e8a230-a59f-4644-b799-3e81823cb581'), (16611, '5ba26cb6-0890-4615-9f28-45a8ba3c5ded'), (19177, '69ba1df0-d437-4646-8421-c788fe8f152e'), (16619, '0c9d0670-8b19-4167-97e9-4418dc3bd375'), (9452, '59aaeec8-b95f-4c82-89b3-f8905ba521d8'), (9453, '0763f28d-22c2-4948-ba1b-2cad01c04513'), (9454, '9de797fe-8ad7-43f0-86df-89345e1c4416'), (9455, '19e9e822-6304-4055-acec-9cff189e5fc4'), (9456, '656645bc-25b2-457a-a50b-7a6d293dce8d'), (6897, 'd88a3afc-4525-4ce0-9982-6f08b8d65534'), (24063, '68c3b5af-92ed-4063-9471-b9fce32331ca'), (13560, '838f91f6-9316-4ade-94c0-ca8e99840812'), (13562, '161d1786-5dd5-4883-af2d-6d40b0f613e7'), (4861, '79210cef-fada-44fb-a563-86b3d8b879d2'), (258, 'ccf8ce5c-aeab-4f18-8d26-8afda967080a'), (260, '14fc3235-cc9c-40f2-b951-b3002fd58852'), (24864, '9fd43470-89e8-4e23-a66c-e9ae2ac66f1c'), (18213, '076f6581-4145-4ca5-93e8-9e744b0574a7'), (13630, '5f595309-fb4e-4261-92ad-938f4eb058f2'), (23873, '6259700a-095f-4a14-8048-95fb1df7f296'), (28482, '3e98ed61-e405-42ac-8bd4-63da353dcace'), (23875, '7504618b-e7a7-4763-90ae-7702a4d617aa'), (23876, '9ec09514-e912-4251-a49b-f760d9b16f84'), (23877, '31509c88-8837-4b9d-ad51-5135d853ebd1'), (23879, '33efa917-ec34-42f9-9dba-22b827c59769'), (329, 'e5a52187-6e6d-47ba-bffa-cde854b2df46'), (2914, 'a437440c-998a-4d47-95de-352ecbe40b57'), (2915, '2db2eb55-2dc6-4ef1-aa46-55c914e880a1'), (2916, 'bf914d00-c1cf-4ab4-8b22-8641787bb13c'), (2917, 'd2246bb6-552f-4781-bcd8-fa71d6444c8d'), (2918, '17c41e0d-a375-4338-834a-318249151bb9'), (9080, '8b8f599f-5243-408d-a6b9-d30288bf980b'), (2941, '8c483416-34d1-4d29-ba5b-878414f53927'), (2942, '6dd1087e-50d4-488b-b71c-0be7ed0bec5d'), (2943, 'ce63a19b-4958-4982-befc-fd45f084ce90'), (2944, 'd7daee80-75bc-4ee2-bcf3-3cf024d1a940'), (2945, 'd457e654-81bf-4d98-b51d-16c77c925417'), (29081, 'fb92a989-9e09-4948-b2bc-d0c04fb1372f'), (29083, 'c31c65e7-4ff0-4f9a-a1a6-454212b45856'), (29084, '0f46e0ac-4e8c-47b7-9117-2cb32566dde7'), (21403, '48cbb85a-c202-43b1-819b-f39b70fe5c5b'), (5034, '22b398e9-2cf4-46b0-8719-f39bbc9fb3ff'), (2498, 'd50bb439-a45b-4d6a-9a35-187b48d23c9c'), (18374, '20d9b64b-d0f4-4ffa-b3f2-0f7c1b52710e'), (15967, '4b616701-f77b-4bde-acf3-2362ebdc292d'), (5071, '51839c46-b9cb-4584-81c1-e2dca0138c66'), (17879, '0febef04-6ab5-44f6-bc89-962863e3aea9'), (7131, '9f48e28e-96e7-41e3-90f7-84a75a36ac4e'), (17884, 'e49a7762-796b-444b-a445-a5879dafc4f2'), (17885, '5b5c9913-c54d-4c53-937c-8ff20a4c37e4'), (17883, '2b9035d9-f069-450d-b557-24fcf97b2910'), (17886, 'd30ae109-0b56-489e-98da-428f04978d14'), (24043, '513d6d7b-52da-4acf-b55a-dcde6b2e7a0c'), (24044, 'a1f11a6f-a069-4ecc-b93e-337ac0ed14a3'), (24045, '2d1bae87-5feb-4e78-b32e-37ab2c42ebe8'), (24046, '7c5e6cbc-6fe8-49e1-ac93-4876e7a67454'), (24047, '010894cd-a4e9-43be-b277-7315f80d3141'), (24048, 'ae498ee6-ab8e-42d5-9d25-171274f6e755'), (24049, 'f5eaf2b3-155c-4d8f-ab52-aaa991cef16c'), (24050, '685c99e5-057d-4b3e-99b4-197c3ae5730f'), (24051, 'fbdebca1-822a-4463-be0f-9037e34f8342'), (17909, '0f26210c-2ee8-4e3f-9846-831bb49812bf'), (24054, 'e3a12540-4fda-4015-9a12-49a3dc50cdf8'), (24056, '3a5f4de8-a3ff-40d2-a789-a355054190a6'), (24057, 'd292faa1-de56-402c-878a-e2d243e5db03'), (17914, 'e3f62a9f-6c82-48c3-84c5-d5b2d2cdf56b'), (17915, '68a9aaf8-a123-46fe-9b65-a8b6006ea652'), (17916, '25fdbee4-bd2f-411d-8846-f229957a8a05'), (17917, '3e26c0d6-3750-4005-9663-ab1c4105004a'), (24062, '17a005d1-952b-439c-bf0d-09520cded7bf'), (17919, '292976be-c487-4f13-b1a9-79e90c6369a9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# DETR

## Overview

The DETR model was proposed in [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) by
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov and Sergey Zagoruyko. DETR
consists of a convolutional backbone followed by an encoder-decoder Transformer which can be trained end-to-end for
object detection. It greatly simplifies a lot of the complexity of models like Faster-R-CNN and Mask-R-CNN, which use
things like region proposals, non-maximum suppression procedure and anchor generation. Moreover, DETR can also be
naturally extended to perform panoptic segmentation, by simply adding a mask head on top of the decoder outputs.

The abstract from the paper is the following:
*We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the
detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression
procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the
new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via
bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries,
DETR reasons about the relations of the objects and the global image context to directly output the final set of
predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many
other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and
*Detection Transformer (DETR) directly transforms queries to unique objects by using one-to-one bipartite matching during training and enables end-to-end object detection. Recently, these models have surpassed traditional detectors on COCO with undeniable elegance. However, they differ from traditional detectors in multiple designs, including model architecture and training schedules, and thus the effectiveness of one-to-one matching is not fully understood. In this work, we conduct a strict comparison between the one-to-one Hungarian matching in DETRs and the one-to-many label assignments in traditional detectors with non-maximum supervision (NMS). Surprisingly, we observe one-to-many assignments with NMS consistently outperform standard one-to-one matching under the same setting, with a significant gain of up to 2.5 mAP. Our detector that trains Deformable-DETR with traditional IoU-based label assignment achieved 50.2 COCO mAP within 12 epochs (1x schedule) with ResNet50 backbone,
The abstract from the paper is the following:

*DETR has been recently proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance. However, it suffers from slow convergence and limited feature spatial resolution, due to the limitation of Transformer attention modules in processing image feature maps. To mitigate these issues, we proposed Deformable DETR, whose attention modules only attend to a small set of key sampling points around a reference. Deformable DETR can achieve better performance than DETR (especially on small objects) with 10 times less training epochs. Extensive experiments on the COCO benchmark demonstrate the effectiveness of our approach.*

<img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/deformable_detr_architecture.png""
alt=""drawing"" width=""600""/>
*The recently-developed DETR approach applies the transformer encoder and decoder architecture to object detection and achieves promising performance. In this paper, we handle the critical issue, slow training convergence, and present a conditional cross-attention mechanism for fast DETR training. Our approach is motivated by that the cross-attention in DETR relies highly on the content embeddings for localizing the four extremities and predicting the box, which increases the need for high-quality content embeddings and thus the training difficulty. Our approach, named conditional DETR, learns a conditional spatial query from the decoder embedding for decoder multi-head cross-attention. The benefit is that through the conditional spatial query, each cross-attention head is able to attend to a band containing a distinct region, e.g., one object extremity or a region inside the object box. This narrows down the spatial range for localizing the distinct regions for object classification
 
Here is the question: What feature of DETR simplifies object detection workflows?","According to the context, the feature of DETR that simplifies object detection workflows is that it ""greatly simplifies a lot of the complexity of models like Faster-R-CNN and Mask-R-CNN, which use things like region proposals, non-maximum suppression procedure and anchor generation."""
Why is LayoutLM effective for invoice processing tasks?,"[(17922, '335ca9e2-22f9-44fe-9a0c-f78aecd43637'), (15894, '9be7c721-6636-4795-956e-133562112124'), (20511, 'e3f039c3-f4ff-41a6-8680-ee6405668c98'), (29222, 'f47e0b8c-3526-4297-b05b-4cb3e1e6c6f7'), (46, '93fa1c88-08f0-4d45-bfcd-93c31ccff19a'), (49, '1efa4b78-1a65-4ef9-a7bd-ae0d9ba0fccb'), (30776, 'a5497ed7-9df9-460a-9ab8-8fffde7862ad'), (28730, 'e9c215d1-838f-480b-89e9-9bffb8f598f8'), (28731, '85e9fbea-6d30-4bab-84e9-166bb1cc8edc'), (28732, '26f566d8-d9ed-4f9e-a930-8b84075bd6a7'), (28733, 'f836bdf5-abd9-4d78-878b-1eed6e0815c8'), (28734, 'c4312ccc-1453-4779-b880-259e3659a9f2'), (28735, '56165381-810d-434e-9201-84d00575cb88'), (28736, 'e998dbb9-1571-43be-9178-96f1a0a0f55e'), (28737, '32486ef1-3b91-4208-805e-da987d4c42d7'), (18498, '899a5b0c-bdb5-4f2f-add3-c045afaedafc'), (28739, '50948ec6-1039-4799-9d5c-f657b72f8d39'), (18500, '601132c7-a766-4738-b8f8-0a14d1084196'), (18501, 'dc889574-ebe9-462c-8e67-4cf4a649d73e'), (28740, '5f132404-82a4-4cfb-816b-6ca092d4d7ed'), (18502, '51f3beba-1f28-4dda-8083-00a757ff2875'), (18504, 'ab801953-53b9-4ef3-9ef4-23ecc3486029'), (28738, 'a52d4111-ff1a-43ef-8d26-44506f5907a6'), (18506, '2ab7adfe-f811-4536-a8bc-a20c5a199a61'), (18499, '9e1839cf-b065-4550-85e5-6eef0c01a02b'), (18505, 'be272b48-013a-4fd6-bc00-3da6c111c0be'), (24658, 'f1864d51-a0c4-4b3a-b5b2-89ee1775172d'), (27758, '898227c9-7326-4f07-9ef7-298c3ac2a2c4'), (25730, '2939ee0f-e1f7-4024-8090-a9fad3f59c7f'), (20612, 'de424e36-a6d3-4010-b9cb-29aafe220c68'), (23703, '9a68e484-49d0-45bf-a969-355f784f0b60'), (28219, '53ccc20d-0caf-44e6-ba84-9d617960607b'), (29345, 'f47133db-9b18-4693-8903-5afbd2c09067'), (5799, '7d6e8705-ceab-43e9-a1de-3f8c4b5e017a'), (24234, 'e9da4eca-e7b5-4a74-94f9-bd8ffcc53d29'), (24753, '78197f5b-3e81-4a11-8324-5aff91118459'), (23731, '70cf89a5-3b44-45f3-a4f4-d179871e0419'), (179, '89c2a93f-84b0-4e59-9a27-0d006c45280e'), (4277, 'cc9c3804-1e2e-4ad2-b1d3-53f67d66923d'), (23734, 'c725af5e-eec1-4276-a26c-c4ba0884667e'), (11963, '19b593f9-7961-4a03-a6f7-ee73f5009d31'), (11964, 'f62ba1ac-f4a7-4acd-a5fc-ddf410ab1d47'), (11966, 'd7d54d2d-6cb7-4459-9ee5-6091ab1da2a7'), (11967, '1731736c-2b48-4e47-aefe-b7ce94cf7fee'), (11968, '339f2d9a-04bc-45b1-ba09-2cb0ee42c1d3'), (4807, '52de4c92-bd98-4936-ae2a-bb0b9e3493be'), (199, '9d317a5b-6e78-4522-8546-2b795b2e3947'), (4808, '89b4f365-75d9-4c0f-be0f-d765cacebfc1'), (24273, '9d89b90d-0743-475c-8c30-c52c8b3feee9'), (8405, '487e1d5b-fe95-44cb-81b7-be84c90f0025'), (1244, '5360b900-8d5f-41af-84f2-4f57a06034d1'), (30948, '08cc6d26-8f38-4dfd-a6de-85779f3373e0'), (19696, '43e599c7-2548-47c5-9039-cfa92c630506'), (29427, 'e43e1578-ba4b-4acf-8d7e-75f74561fe80'), (29428, '173a121e-d8f8-4781-80c0-982f5775a7fe'), (1780, 'd58c641f-0b84-48bd-a841-a94c022d81b6'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (18676, '19268cbe-5f8f-44b7-9603-fd3d64a79dc2'), (18168, '3012d4d4-e60f-4363-ae7a-fa0b832c41f6'), (29432, 'cfe216f3-87f3-4423-845d-6b97b3ceff48'), (29433, '54590b2d-6c0c-4175-97fa-5c59657c11a2'), (29434, 'a2830d94-ca0e-4ae3-af52-2fe38f66fc61'), (29436, '7fc8e06a-e13f-4a56-9b66-6a6dba9c67ac'), (29437, 'a9873ed2-2cf8-44cc-9b63-9ba1c9d4a818'), (29438, 'ae614832-218f-409e-b17a-bf956c69691d'), (23807, '9d7cdc35-737d-4b2c-9e58-0c6898cdfc31'), (29442, 'a2fb8394-94b3-447e-a512-9461a99a583e'), (29445, 'bbcef900-4efc-40e4-a995-d0d601c26cdf'), (29448, 'de680f2b-2364-4825-bd55-d130bdbe185f'), (29449, '44d89b85-4667-4793-8c73-fda2d31b7155'), (29450, 'a2c6a91e-86ac-423f-bd97-726831c5ecb1'), (29457, '0a4a2d1d-c69f-448e-a4e8-9d1337382099'), (13585, 'dc97e6e4-5a6a-41b4-8851-91e3a0e3af3f'), (284, '5e72c7ed-cf69-4a85-8460-5d28bf60ba77'), (27423, 'cdb54c20-dca4-48f4-96a7-f000fa351601'), (21289, 'ecfafe21-5618-424b-8d9a-fddfb1952265'), (2345, '3bd3225a-2064-438c-80c1-e672696ab2a2'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (29515, '18fb13c6-8761-4aea-ac01-1b771967088f'), (7549, '5d8534a4-8f48-47c1-8dc9-b1c30ede1830'), (7551, 'd4335603-9f39-479c-a2ff-3595c944921d'), (26501, '6e9da8f8-10a7-43e3-9d77-af2e3222359d'), (14726, 'c5fe841e-3954-4091-9714-f1ad50c6681b'), (18497, 'e49a1531-a11a-4c31-bd99-b4214e7ad0b6'), (19855, '9800f1ae-0b10-4f47-8b9b-9a97f69d8a08'), (9104, 'cf198758-ade7-47bb-82de-51e1e4e0bc2a'), (7582, '2542fc8f-6e53-4c29-b66b-a0c7df5ab0ff'), (10144, '78a0d3a7-2154-41f9-b87d-ee66babd1020'), (10145, 'ce96ae5b-5345-4742-bb66-c625e53c53e0'), (22435, '2cb4720c-dc7b-487b-8244-1eb135abdcb2'), (18868, '4283a613-b4bb-4cc1-b94b-361a545de787'), (2490, 'd4f5e452-c337-48d1-9e39-de8ee6b76285'), (2491, 'aefe200c-ad39-44b6-a12e-575c5fcbf47f'), (2492, '40497a51-4b9d-4870-87b8-6d58dfc9001e'), (11197, '96a27a04-dbd2-4852-ac2c-7319365b5024'), (2493, '4b6116cb-7892-4969-80b1-11fcc811c1ca'), (2495, 'c831ecfe-8ba0-4247-97bd-3379c2df25ec'), (2494, 'a522e4ba-2d7d-40bf-9a9c-18de74cc137f'), (2496, '88889674-e469-4f11-967b-ecaa5db509a2'), (2498, 'd50bb439-a45b-4d6a-9a35-187b48d23c9c'), (2497, '1707ae47-9dc9-49dc-b3fa-e34dfc7bbd9e'), (16321, '3ed75aea-51b3-4c72-a3d7-7b7be11183b2'), (16320, 'aceded5d-8291-4ec6-937b-4ffa5e060758'), (20937, 'bc8955bf-129c-4b11-91ac-b38eb73b23ac'), (2507, 'e3274c16-e60e-45a3-85d6-b3c1800f42e7'), (2510, '7d4dc0bf-199e-49ec-8219-cc474ae04cc7'), (5583, 'c8d81eac-ebd5-4d5a-8dc9-3203b916c95f'), (13779, '1b48c519-9d0e-451c-862b-eff29665e127'), (18398, '881db9a6-a3a5-4163-a27d-d4319313f716'), (5095, '2c960b15-fe52-4cc0-8902-eb912dfbf0e0'), (15861, 'cdbf3fc8-ee9f-4a6a-bfd8-e1fe30982ba1')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: A step beyond layout analysis is document parsing. Document parsing is identifying and extracting key information from a document, such as names, items, and totals from an invoice form. This [LayoutLMv2 Space](https://huggingface.co/spaces/nielsr/LayoutLMv2-FUNSD) shows to parse a document to recognize questions, answers, and headers.
-->

# LayoutLM

<a id='Overview'></a>

## Overview

The LayoutLM model was proposed in the paper [LayoutLM: Pre-training of Text and Layout for Document Image
Understanding](https://arxiv.org/abs/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and
Ming Zhou. It's a simple but effective pretraining method of text and layout for document image understanding and
information extraction tasks, such as form understanding and receipt understanding. It obtains state-of-the-art results
on several downstream tasks:
*Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to
its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. In this
paper, we present LayoutLMv2 by pre-training text, layout and image in a multi-modal framework, where new model
architectures and pre-training tasks are leveraged. Specifically, LayoutLMv2 not only uses the existing masked
visual-language modeling task but also the new text-image alignment and text-image matching tasks in the pre-training
stage, where cross-modality interaction is better learned. Meanwhile, it also integrates a spatial-aware self-attention
mechanism into the Transformer architecture, so that the model can fully understand the relative positional
relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms strong baselines and
text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis.*
*Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the
widespread use of pretraining models for NLP applications, they almost exclusively focus on text-level manipulation,
while neglecting layout and style information that is vital for document image understanding. In this paper, we propose
the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is
beneficial for a great number of real-world document image understanding tasks such as information extraction from
scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM.
To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for
document-level pretraining. It achieves new state-of-the-art results in several downstream tasks, including form
 
Here is the question: Why is LayoutLM effective for invoice processing tasks?","LayoutLM is effective for invoice processing tasks because it jointly models interactions between text and layout information across scanned document images, which is beneficial for extracting key information such as names, items, and totals from an invoice form."
What distinguishes CLIP’s approach to multimodal tasks in the Hugging Face library?,"[(18442, 'fe6bfb29-6431-48f5-9705-9fe95cf62319'), (15371, 'bd8140d0-5f35-4354-95e0-dddc95efe4dc'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (18449, 'c9c37efb-6971-4e7c-99dc-eb4cd285b1e2'), (5140, '9c749309-ea99-43d6-875c-cb62f74db39f'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (16932, 'd165fea3-524c-4a1f-8627-923c16bdcb4f'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (11337, 'f576bd7f-1d54-4986-86e9-a81a88db2fc1'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10833, '9903aff9-bcb5-4b1d-85d5-53715728f1e0'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10835, 'b193dc36-1e74-49bd-b1a9-b9c54a521559'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (20579, '1cadc685-b43e-4779-9427-c24086103ace'), (20580, 'd3179689-1789-4157-83a3-9dfab3333b49'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (7805, '0e09b839-24f1-4883-bb8c-4921cb04ebda'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (7812, 'd2ee2299-ee53-476e-9f83-1c5ab9120907'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (667, '466a4c47-d6a2-4439-8124-9da2dea99f39'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (4774, '331613c0-519f-4a91-aa47-3f15174210ed'), (4775, 'dece872c-e754-4f2b-b021-07367964cf41'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (22204, 'c0ad89f5-de39-4443-9c10-c3966736995b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (1748, '867c8140-6d94-4898-9809-7d0a4048ff6a'), (1749, '22147376-467e-46ca-bd5a-e6e7fd40222e'), (11477, 'c5876285-5fa1-4172-9c57-bef923092cfa'), (18135, '9044d94c-8fa3-4b9c-9dac-1b8df585b38a'), (18136, 'ea71868e-38ba-42e9-9d5c-3ac4745b0a68'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (20204, 'ba5f90cf-a46f-4ea5-8d5d-00642b205e98'), (3823, '1b242c88-9fa6-4480-8d4a-167fc47e1885'), (13553, 'c62d15cd-73a2-4288-8852-93fe0d381706'), (13554, 'f239a4cb-1368-4161-bb6c-d4d76b74b26d'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (1271, '3a06894b-a5c3-4d2e-9823-cd6289f614ef'), (251, '2e5afb66-671a-4b04-add5-c282d78ed8d8'), (3835, '4ed663f4-fb18-42ac-9cd4-b199746d0242'), (252, '1d1458f5-0779-4f41-8eab-579cf8943202'), (30976, '32e5deab-8676-4ced-b5ae-2ce47401ab9b'), (3842, '388c9bda-b8d1-4c67-a63c-f00f93b922f9'), (21762, '29921dc2-2b5f-4a18-b45f-665a7ae4b698'), (18184, 'a53114c0-96cb-42ef-a44e-4a6ea9778ff1'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (13601, 'd2fcf63a-6647-41a1-8530-613f53b57a88'), (18210, 'fd28ff79-028b-422b-b63f-ff3029b46bc6'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (18217, '4e0278ab-2929-47df-8ac5-54be5d98a007'), (299, '7f2a2f1a-f503-44f9-b87a-583789044a1f'), (13627, '343b5480-7373-420c-a9d1-d1cfb60100ee'), (13634, '63e875a9-4fad-4f80-8454-e567ece5a4ac'), (14658, '7d610c52-5163-4003-84e8-2c2144abc98e'), (326, '4720a122-5147-49f6-b5aa-b859fb8ad714'), (333, 'b89f73b6-20ff-473e-9c00-802ac258ce6c'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (9071, 'c3b1963f-2791-4bf8-90ef-97a07ce6137f'), (9072, '7a516a26-4603-45f9-b69a-f48eb9d9c67e'), (10111, 'a097658d-c7fd-44ee-964a-4221af4a56a5'), (31615, '2a9d091b-0c54-45c7-aed4-9ec246ffca0a'), (8578, 'b6835785-bd61-41a6-94d0-a59988d7384f'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (31620, 'f5fe6877-60cd-4f03-8287-da18e67df6dc'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (31619, '15b09247-3696-4abe-b6ab-82b08cf31da4'), (16266, '418e8b42-a3ca-4adf-8bbc-5024b449f5eb'), (22924, '976e45d6-f33e-4ae3-a01c-1e3a2fc7c512'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (22928, 'a8321404-e3d8-420e-b234-a78cf2bdc969'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (22966, '8b7b6719-a99f-4081-976d-76c680180f98'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (22970, '36b2f7d0-d2f2-4773-a1a0-4253ebb02ddc'), (9149, '7946e92e-6ff7-420f-9b65-34ff8498ebeb'), (18366, '8a7e2dee-f14d-4ea1-819d-851a6affb887'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (18365, '2e4e0c37-84a9-4d1d-84ed-472f926d395b'), (9156, 'a2fce173-0b3a-4ce8-a687-71931ba05ec3'), (5062, 'e19ce9e8-b551-4ddf-9d90-e04193b2b5f9'), (5063, 'f4cfdc74-4cbc-4213-a3fb-a400b432dc40'), (27594, 'e63f712a-e87d-4651-a762-597319389df4'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (23014, 'd29e7466-2ce3-4018-8252-e65865458a74'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (5111, 'e87c80c3-d7d3-4478-88c3-e8dc5a994da2')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## What can you find on the Hugging Face Hub?

### Models 

The Hugging Face Hub provides access to machine learning models covering various tasks and domains. Many machine learning libraries have integrations with the Hugging Face Hub, allowing you to directly use or share models to the Hub via these libraries.

### Datasets
The Hugging Face hub hosts over 30,000 datasets. These datasets cover a range of domains and modalities, including text, image, audio and multi-modal datasets. These datasets are valuable for training and evaluating machine learning models.

### Spaces

Hugging Face [Spaces](https://huggingface.co/docs/hub/spaces) is a platform that allows you to host machine learning demos and applications. These Spaces range from simple demos allowing you to explore the predictions made by a machine learning model to more involved applications.
**Evaluating and documenting bias:** At Hugging Face, we are big proponents of [model cards](https://huggingface.co/docs/hub/model-card-guidebook) and other forms of documentation (e.g., [datasheets](https://arxiv.org/abs/1803.09010), READMEs, etc). In the case of text-to-image (and other multimodal) models, the result of explorations made using explorer tools and red-teaming efforts such as the ones described above can be shared alongside model checkpoints and weights. One of the issues is that we currently don't have standard benchmarks or datasets for measuring the bias in multimodal models (and indeed, in text-to-image generation systems specifically), but as more [work](https://arxiv.org/abs/2306.05949) in this direction is carried out by the community, different bias metrics can be reported in parallel in model documentation.

## Values and Bias
# Optimum + ONNX Runtime: Easier, Faster training for your Hugging Face models


## Introduction

Transformer based models in language, vision and speech are getting larger to support complex multi-modal use cases for the end customer. Increasing model sizes directly impact the resources needed to train these models and scale them as the size increases. Hugging Face and Microsoft’s ONNX Runtime teams are working together to build advancements in finetuning large Language, Speech and Vision models. Hugging Face’s [Optimum library](https://huggingface.co/docs/optimum/index), through its integration with ONNX Runtime for training, provides an open solution to __improve training times by 35% or more__ for many popular Hugging Face models. We present details of both Hugging Face Optimum and the ONNX Runtime Training ecosystem, with performance numbers highlighting the benefits of using the Optimum library.

## Performance results
he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which
he Hugging Face Datasets library: A Quick overview. The Hugging Face Datasets library is a library that provides an API to quickly download many public datasets and preprocess them. In this video we will explore how to do that. The downloading part is easy: with the load_dataset function, you can directly download and cache a dataset from its identifier on the Dataset hub. Here we fetch the MRPC dataset from the GLUE benchmark, which is a dataset containing pairs of sentences where the task is to determine the paraphrases. The object returned by the load_dataset function is a DatasetDict, which is a sort of dictionary containing each split of our dataset. We can access each split by indexing with its name. This split is then an instance of the Dataset class, with columns (here sentence1, sentence2. label and idx) and rows. We can access a given element by its index. The amazing thing about the Hugging Face Datasets library is that everything is saved to disk using Apache Arrow, which
 
Here is the question: What distinguishes CLIP’s approach to multimodal tasks in the Hugging Face library?","There is no mention of CLIP's approach to multimodal tasks in the provided context. The context discusses the Hugging Face Hub, its features, and related libraries, but it does not specifically mention CLIP or its approach to multimodal tasks. Therefore, I cannot provide an answer to this question based on the given context."
What distinguishes Denoising Diffusion Probabilistic Models (DDPMs) from GANs?,"[(5632, 'e494173b-85c5-437c-a6b9-dcdfcadef8f4'), (22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (5634, '59a4e6a6-39fc-415a-9e3b-3691933b4ace'), (5645, '1b732b6c-6263-4c40-bfd5-e3fe9e80bf13'), (7184, '8450f91c-e0e1-4e31-99c8-57f8d43d9cdb'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (15898, 'b6f9af12-8c93-4907-8584-6063e7adc079'), (7206, 'b78e1a91-c2d0-411c-ac86-3b8ef5a071ab'), (3115, 'b9589646-16ee-46e7-9031-d0f697b1e6d8'), (3116, '8d103b9e-a858-41e3-ae2a-510b15458255'), (3128, 'cd116de5-9c26-431b-aacf-ee4e4b6e9331'), (15932, '2653d81f-316d-4342-aaea-37030a01bde8'), (26693, '74a1767c-cb28-44b5-9581-88def78019d3'), (14417, '369ec804-4ae1-4468-89f5-49f780b0ba5d'), (14418, 'f50616ce-ba15-4e58-9fc9-30dbc62fb3c6'), (13396, 'e9acadfa-60e3-4c46-a14b-5b042e105d94'), (11373, '03316170-9bc8-479c-8653-d3b46bddaf64'), (11374, 'dc4c2f8f-1a15-4e07-a621-c3e962d6c89b'), (14452, 'd602c8f8-3830-4f9e-8b34-1bea97e99242'), (26237, '4d63839d-2a2f-48a6-a9ec-8b5736c5450e'), (11392, '5dfaeba9-636e-4cc0-bbc9-63a401ed60fb'), (20125, '9bc4c44b-7772-416d-a50b-6e38fc9e06e8'), (20126, '21fa70df-4206-49b0-9c9f-0f65995f9ea2'), (24742, '9aa45ed3-0c7b-4df2-8474-398a70a68998'), (10920, 'f18f64ad-7a3c-461d-9f41-0ffa5327fb77'), (20142, '8acb5b27-257d-4765-acaa-2068a1bb9923'), (20144, '48868250-7cd6-416a-999d-65c68f28be99'), (10933, 'b2789e50-f10a-4c41-9023-3c64d5949b58'), (20149, '44c9682e-0ce4-4b25-98a7-519107c43120'), (4280, 'c91cc95e-4c1b-4e75-a090-e529f4e4cb58'), (20152, '0680c6e7-1e57-4e37-9236-de4f5e13e357'), (20155, '04e7e95a-9eb0-4302-a0ec-07716aa382f2'), (29374, '221cd942-7551-4df5-9a02-d0f73e055431'), (26304, 'f09ef0c4-c54a-4f32-a228-9fbfc29d0954'), (4298, '52a7967e-5caa-40c1-b74b-72d11b1e1ae3'), (4299, 'fa200320-e0ea-4961-b45c-b5ec32991353'), (22740, '42178967-045d-4482-95a3-1453ae35af86'), (9960, '1f60684e-f37c-4a17-a2b9-0ff7529cebec'), (9961, 'f4b19bf3-358d-4b4d-84ba-3831e2c2f81c'), (9962, '49cc7639-d6b4-4703-8068-e5764e6ccce6'), (9964, 'dc56023c-7e5a-42d1-95c8-a33b112491e3'), (9459, '50af37d9-9d34-4347-9602-65d90571a951'), (25332, 'b27bb4a7-cee9-4ccc-ad5e-bb3287aa008c'), (25333, 'de776bd8-a2ca-424f-b495-fc498f857db9'), (19194, 'f2337207-7b15-4acc-8d68-6059df62cd35'), (4352, 'c1795653-81fd-490c-ad75-40b680b89a8a'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (10000, '11b6cc75-c6fd-4e28-86d3-d42657a01b52'), (29459, '5f27b86d-2a59-42a9-b881-0eb4893c8a7e'), (30995, '2482d3c6-479e-411a-9276-d86969cedbe9'), (30996, 'fc3155b3-30ea-41c1-97b1-d8f4534cd97d'), (10013, '9df20a69-85c2-439c-8d2e-67e96e29b0ec'), (23839, 'aac7f57f-4263-4e9e-bbee-db3bfed8ee29'), (23840, '3714cf17-faab-4e7b-ba97-cc35cea06600'), (23841, '5a99e4d4-7fec-4135-b408-ad9cae391ea4'), (10018, '3df3416e-1ad6-4849-a54e-65555d532b6e'), (10029, 'd1a5f9fc-00c4-4974-b71e-14d74422db37'), (10031, 'd311da63-e4f2-4661-9207-d5accca2e35e'), (26432, 'bad5a360-cdaa-4da7-a6ac-bdcbaacffd05'), (12104, '91971395-1e61-4f8d-bc69-6e8b6191c233'), (24906, '4f6d9603-05a1-44d8-b626-d6aeed01a2f5'), (10579, 'c5c569a7-6ff7-430d-adbd-b77fe1021f0d'), (5975, 'c2372c16-a0f7-4f31-b095-79df0d949f6e'), (26458, '2fac6939-29e7-473f-b941-a99b086bc5f3'), (24412, 'f7ed4212-a236-46c7-b1d5-d77a9167437d'), (27486, '8d60bbbd-8d6a-4b1d-a7bc-a7b44dd8b420'), (27487, 'fd4a0188-8341-4b74-8a7c-90f57ba20a22'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (13153, 'ceaaeca1-e37c-45bd-870c-17fc8edf6ab6'), (13154, '089e8911-3317-49b7-9b6c-503657ea6b65'), (13155, 'feb24b04-1aed-4866-9b73-320a1d8566b1'), (5989, 'b6e237c3-82b5-47e4-b29a-a75446c3d291'), (5990, 'e1ee08c9-bc5e-455e-b0c1-4a99a63d54da'), (5991, 'ac3ddeae-3274-4fe4-8d77-2b3c3b80f768'), (19816, 'db4527c9-5c35-4cdd-87db-f3992908e083'), (5992, 'f9f87c8a-b463-4465-8bb2-190abe9a2c8d'), (5996, '95d44852-e6f1-4950-8ede-d7cf96cd61ec'), (5997, '59b12a73-bd36-4948-92df-483316bf393a'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (22904, 'a678407d-0ca1-4128-a53e-8ab8dccdddf1'), (14715, '1d442336-f43d-4c89-84ca-a202701b740c'), (22910, '94bd2f01-8327-4f2c-9cee-d36f8b154579'), (22911, 'beaca8ce-2939-41e0-b621-52375e694804'), (22912, '76059485-1ebf-444f-97ba-17fcdb6926b5'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (22914, '779719c4-a9bc-4ab7-bbd5-541c9e1906ce'), (22920, '500cc8f3-3dc5-41e4-a57e-78372d38d835'), (22921, '28568e53-fad9-4e99-96dd-6932ad9d07a3'), (22922, 'f7b5b230-09cb-483f-9a29-efce17dee03a'), (6030, 'b80847b8-58e2-40de-9633-2c67a5dc619d'), (26514, '9fcaf21e-79bb-4839-864d-0d857fa15240'), (26515, 'c3f7dc34-4221-41a4-9dae-14d180d6d7ed'), (26517, 'ceda17d3-0a97-4b17-bb3d-70fcef59b982'), (8600, '269c11e4-e332-4535-8735-0da74c456141'), (4012, 'b1e161a0-d490-4161-880d-9be316427ee3'), (14767, 'db0fc7e3-d997-4659-abf7-2fb7983ab41e'), (1478, 'e9cb9ec9-6fa8-4e1e-832b-73c1d9d9f2d1'), (17364, '375858e6-fad8-49d7-9f08-70b4ff973b3e'), (19928, '1966d4d3-33cc-44da-aee9-0df795e0bff6'), (19929, 'cf4b92d2-40d3-40e0-b637-e172a5518fb6'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (17378, '2275b94a-b644-42e1-8ec3-e89326e8993f'), (10212, '0d28df65-2cae-4253-8b4c-c834f7ec7e1f'), (19944, '5fb1084c-cdb3-4c12-90d8-ef3744dfe8b5'), (19945, 'e8883fa6-cf8d-4183-beba-8d7f435a4e81'), (13296, '1deb15a7-660c-4670-836e-ea98398cf41b'), (5628, '67c8fa95-6c8b-4122-8f5c-1074d364c480')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The abstract from the paper is:

*Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample.
To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models
with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process.
We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from.
We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.*
The abstract from the paper is:

*Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples 10× to 50× faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.*
In this blog post, we'll take a deeper look into **Denoising Diffusion Probabilistic Models** (also known as DDPMs, diffusion models, score-based generative models or simply [autoencoders](https://benanne.github.io/2022/01/31/diffusion.html)) as researchers have been able to achieve remarkable results with them for (un)conditional image/audio/video generation. Popular examples (at the time of writing) include [GLIDE](https://arxiv.org/abs/2112.10741) and [DALL-E 2](https://openai.com/dall-e-2/) by OpenAI, [Latent Diffusion](https://github.com/CompVis/latent-diffusion) by the University of Heidelberg and [ImageGen](https://imagen.research.google/) by Google Brain.
The abstract from the paper is:

*We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at [this https URL](https://github.com/hojonathanho/diffusion).*

## DDPMScheduler
[[autodoc]] DDPMScheduler

## DDPMSchedulerOutput
[[autodoc]] schedulers.scheduling_ddpm.DDPMSchedulerOutput
*Diffusion probabilistic models (DPMs) have demonstrated a very promising ability in high-resolution image synthesis. However, sampling from a pre-trained DPM is time-consuming due to the multiple evaluations of the denoising network, making it more and more important to accelerate the sampling of DPMs. Despite recent progress in designing fast samplers, existing methods still cannot generate satisfying images in many applications where fewer steps (e.g., <10) are favored. In this paper, we develop a unified corrector (UniC) that can be applied after any existing DPM sampler to increase the order of accuracy without extra model evaluations, and derive a unified predictor (UniP) that supports arbitrary order as a byproduct. Combining UniP and UniC, we propose a unified predictor-corrector framework called UniPC for the fast sampling of DPMs, which has a unified analytical form for any order and can significantly improve the sampling quality over previous methods, especially in
 
Here is the question: What distinguishes Denoising Diffusion Probabilistic Models (DDPMs) from GANs?","The context does not explicitly state what distinguishes Denoising Diffusion Probabilistic Models (DDPMs) from GANs. However, it can be inferred that DDPMs do not require adversarial training, which is a key characteristic of GANs. This is mentioned in the first abstract: ""Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training..."""
How does the U-Net architecture enhance diffusion models?,"[(22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (5634, '59a4e6a6-39fc-415a-9e3b-3691933b4ace'), (4611, '9ba60db2-6853-40a1-88bd-22525fbdb8e9'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (15896, '533a0a9e-34b4-449c-bb00-101804823eab'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (11291, '9e06936c-6b43-4df3-8c7a-1c5d502857b2'), (14360, '1097aabb-9876-417f-85a0-d8b4863820bc'), (29732, '23e38ff1-32ee-4ec1-be96-ae1f89a1758f'), (559, '71d541e3-219e-4d47-9e9e-9e89d3d0aa60'), (3128, 'cd116de5-9c26-431b-aacf-ee4e4b6e9331'), (11840, '002155e5-ca0d-4f17-bd65-852799de0b8d'), (11841, '6a43bb30-c717-4720-9f76-11b3df6f7412'), (15944, '805fc77a-02da-4894-9717-7755d57cbe70'), (7244, '285ab510-3ccc-4789-a279-59038db4de4c'), (589, '5f6c042b-5281-483f-b557-84661e4b6441'), (590, '6523fa0b-894c-49e0-bfb1-fb3213623bf3'), (591, '06236243-318b-48f5-8a7d-cfb9b4f1e0d5'), (21074, '808ca6dc-63be-4d6c-b1e5-63d43d71fae2'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (9824, 'dd81f094-0a63-41f4-9d2d-7fcedd15ebb6'), (14436, 'b6c8ba09-18f8-4b76-9196-bdead257e416'), (14440, 'e0ab7c36-83a2-4564-8607-0dac39b536e7'), (14452, 'd602c8f8-3830-4f9e-8b34-1bea97e99242'), (14973, 'eb381477-f68c-4c84-ac11-531c7b03e72c'), (6797, '3160d312-6e44-41aa-9a17-ee3fbb2b0aa2'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (20126, '21fa70df-4206-49b0-9c9f-0f65995f9ea2'), (20128, 'e3f94bda-7ff4-4e13-b614-239b43914824'), (24737, '6f3ed7f6-e427-4bec-8a99-737d311a82b6'), (24742, '9aa45ed3-0c7b-4df2-8474-398a70a68998'), (24743, '7c967f07-5e4b-4076-896b-2d21640ef09e'), (29863, '3b09e565-988c-407e-8b7b-4a84180a05cb'), (10920, 'f18f64ad-7a3c-461d-9f41-0ffa5327fb77'), (19121, 'a3952b10-3afe-4c9f-8b38-a1460ba3c96e'), (19124, '6ba95fb6-7338-4800-ae17-1210fa23398d'), (10933, 'b2789e50-f10a-4c41-9023-3c64d5949b58'), (20149, '44c9682e-0ce4-4b25-98a7-519107c43120'), (19125, '15ecc634-7e99-4166-a9b6-adcf5d7c6024'), (20152, '0680c6e7-1e57-4e37-9236-de4f5e13e357'), (19128, '7822f723-18d9-492d-b2c1-5d48cb7de5cb'), (19129, '0741f5fa-69ff-44d3-98e9-8f2f90060b9b'), (29374, '221cd942-7551-4df5-9a02-d0f73e055431'), (19135, 'd743a82b-46bc-41fd-98b0-40e850c520b4'), (26304, 'f09ef0c4-c54a-4f32-a228-9fbfc29d0954'), (4297, '141bfea2-71c0-48b6-8a06-0ca33c6b56f7'), (4299, 'fa200320-e0ea-4961-b45c-b5ec32991353'), (14034, '37ae5ab3-64a3-411a-82cb-1381ad832a7d'), (9429, 'f9b10760-2b6c-4577-999d-9ce25869b8f0'), (25814, '1938526d-56ae-43fe-850c-1f0af6bb61fe'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (10970, 'cde9f8dd-eb88-4591-a16d-d69f96640dec'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (14560, 'eabb1851-01c3-427e-94d1-4cfe9008657f'), (9961, 'f4b19bf3-358d-4b4d-84ba-3831e2c2f81c'), (9962, '49cc7639-d6b4-4703-8068-e5764e6ccce6'), (30449, 'ab376947-fe7b-450f-a5b6-aea1cf6f4ed4'), (23285, 'b125ab4d-5491-40a0-8499-97b79a61538d'), (30455, '30b49335-6092-4e0b-876d-b396ad058840'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (9984, '6c3ad997-641d-4020-bcc6-5e8e6570b864'), (19205, '5e659bb0-88f3-4251-9a17-78e0d28fa04d'), (6412, 'e491859b-0c3a-451c-8d54-e5536a56bf5f'), (24333, 'fa763bfb-3bb5-4a40-81b9-1853bd3ab9d6'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (20754, '3f10ec13-d217-4e30-9b39-be5617b57262'), (29459, '5f27b86d-2a59-42a9-b881-0eb4893c8a7e'), (9503, '5cde6fee-ebef-4bb3-85cb-ed9d8558d468'), (10029, 'd1a5f9fc-00c4-4974-b71e-14d74422db37'), (10030, '6fb9b07f-5a83-4be2-b3f5-96b0abee993f'), (10031, 'd311da63-e4f2-4661-9207-d5accca2e35e'), (26438, 'f329b776-81fd-470b-b8e7-ab77e1d208d0'), (12104, '91971395-1e61-4f8d-bc69-6e8b6191c233'), (10579, 'c5c569a7-6ff7-430d-adbd-b77fe1021f0d'), (5975, 'c2372c16-a0f7-4f31-b095-79df0d949f6e'), (5976, 'e8e3e65c-67a5-40a4-9e0e-3764eba99f00'), (13150, 'da5b6fd3-746b-4e1a-a44a-549e924acbc6'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (5991, 'ac3ddeae-3274-4fe4-8d77-2b3c3b80f768'), (5992, 'f9f87c8a-b463-4465-8bb2-190abe9a2c8d'), (5993, '6e16964e-c3be-43d1-b214-1db2ae5be924'), (5996, '95d44852-e6f1-4950-8ede-d7cf96cd61ec'), (4462, '14e529ce-e387-47f4-9d38-a454c23e8bd7'), (5999, '910374c2-7ddf-4de9-a684-62147c8d1782'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (13169, '3cd0b600-024b-4841-89f2-c5d09b3b1789'), (14710, 'c6ac5276-70e1-47d6-afe5-9fe19c25afce'), (6008, 'f1ddc133-b500-4631-9de5-add5ec3ca0f4'), (22904, 'a678407d-0ca1-4128-a53e-8ab8dccdddf1'), (13178, '51262099-96cc-44dc-97e7-e541341cac30'), (14715, '1d442336-f43d-4c89-84ca-a202701b740c'), (14716, '493155e3-f96f-48c6-be10-b3d2c158a052'), (27005, 'f27afacb-d027-4567-b47d-c01f0f4ad924'), (22910, '94bd2f01-8327-4f2c-9cee-d36f8b154579'), (22908, '20cf93bb-0193-4309-9639-b4265308de33'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (10626, '3167dc55-164d-44b9-89a7-954043ddd7d9'), (10627, 'cd118b8a-513a-41f4-946e-d570e88a67ae'), (22920, '500cc8f3-3dc5-41e4-a57e-78372d38d835'), (22921, '28568e53-fad9-4e99-96dd-6932ad9d07a3'), (22922, 'f7b5b230-09cb-483f-9a29-efce17dee03a'), (17302, '10950558-fd37-474e-868f-4f3fca1d098a'), (3997, '2662a32f-d93d-48c2-afe0-d243cab54e4c'), (4012, 'b1e161a0-d490-4161-880d-9be316427ee3'), (27585, '06691433-dd73-4b81-ae71-7fbb769a70c9'), (14796, '65ed6c5b-f389-454c-9309-16f111f60c54'), (14797, 'f4ac038a-4c8f-4f8b-be3a-d372722a3272'), (17363, '509f9576-4f0e-4f9e-a4f6-a2956a1427f9'), (17364, '375858e6-fad8-49d7-9f08-70b4ff973b3e'), (980, '02cdd715-f11d-4e48-937f-49571f80f726'), (982, '4b1ed7e5-38de-47d6-b5e5-a776db6fd6bc'), (10212, '0d28df65-2cae-4253-8b4c-c834f7ec7e1f'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: - Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance
- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis
- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models by improving the U-Net architecture, as well as introducing classifier guidance
In terms of architecture, the DDPM authors went for a **U-Net**, introduced by ([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in [He et al., 2015](https://arxiv.org/abs/1512.03385)).

<p align=""center"">
    <img src=""assets/78_annotated-diffusion/unet_architecture.jpg"" width=""400"" />
</p>

As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.

Below, we implement this network, step-by-step.

### Network helpers
# DiT

[Scalable Diffusion Models with Transformers](https://huggingface.co/papers/2212.09748) (DiT) is by William Peebles and Saining Xie.

The abstract from the paper is:

*We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.*
The AR models section provides additional context on VQ-Diffusion's architecture in comparison to AR transformer based models.

[Taming Transformers](https://arxiv.org/abs/2012.09841) provides a good discussion on converting raw pixels to discrete tokens in a compressed latent space so that transformers become computationally feasible for image data.

### VQ-Diffusion in Context

#### Diffusion Models

Contemporary diffusion models are mostly continuous. In the forward process, continuous diffusion models iteratively add Gaussian noise. The reverse process is approximated via \\( p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t)) \\). In the simpler case of [DDPM](https://arxiv.org/abs/2006.11239), the covariance matrix is fixed, a U-Net is trained to predict the noise in \\( x_t \\), and \\( x_{t-1} \\) is derived from the noise.
To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.
Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers. The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks.

**3. The Text-encoder**

The text-encoder is responsible for transforming the input prompt, *e.g.* ""An astronaut riding a horse"" into an embedding space that can be understood by the U-Net. It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings.
 
Here is the question: How does the U-Net architecture enhance diffusion models?","The U-Net architecture enhances diffusion models in several ways:

1. **Learning important information**: The bottleneck in the middle of the U-Net ensures that the network learns only the most important information, preventing the loss of important details during downsampling.
2. **Improved gradient flow**: The residual connections between the encoder and decoder, inspired by ResNet, improve gradient flow, allowing the network to learn more effectively.
3. **Preserving information during downsampling**: The short-cut connections between the downs"
What is the primary purpose of JAX in machine learning?,"[(30734, '0d46b742-8add-4a55-b98f-21a1e075e656'), (30735, '912fcfca-3558-4870-8270-ee1f10468220'), (3600, 'a5251b45-4980-4841-a1da-d832c0aa4e34'), (17939, 'eb167bfc-4875-47c0-84b4-14d6a25744e6'), (17943, 'b24c7665-16d4-463b-81bf-731e4b322785'), (30744, '85546f48-12fb-4929-8f5f-4e0317439b79'), (17944, 'c6846f7e-7529-4813-b0ab-6c6cbf84a425'), (16416, '1b6290e2-d918-4435-af5a-004a7c4401ca'), (18996, '6a19e0bf-7a11-49c8-9186-3e6ff36b3640'), (17475, '496f81a1-3559-4e5f-ae4f-25727935e2df'), (78, '2fa28e05-ce19-491b-8367-3692ef4f3852'), (19535, '66073b8b-617e-48b6-8911-9bff81d7df7a'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (7258, '60ffe65d-54e6-4c62-a466-854687cd7645'), (4717, '3a237531-c4e8-4ab9-a982-09465c5ed278'), (5743, 'c1d8b14c-c8ac-4b43-a04c-1e061196fc77'), (5744, '8a324259-975e-4404-a067-26a272142939'), (5745, '77c39426-5363-43cb-9fee-bfc8d242613d'), (24180, '6fd43eed-f7b8-4151-808f-472f264f2aa2'), (13945, 'a94b0abd-2e7a-44ce-ad86-49755f562e31'), (13949, '19c1ac0e-ea20-4f0d-9183-4a6c4fe31c37'), (21635, 'dc2413c5-b29d-4185-bd39-a90d0f73d0f1'), (13957, '6452d1b9-cf35-4d81-aa93-772441ecdc1d'), (13958, 'ed200096-8374-40a5-9473-b55aae1e78f8'), (663, '8c59f71c-cf1b-4c2b-920d-9ad20ca14c6c'), (668, '249a03e0-c503-4df7-824c-7f0c50892b55'), (1181, '2e574621-b2e7-4ffb-ad69-7aa65d1e16fb'), (18589, '1b6a9bc8-bc9d-4c7d-8c7d-a90f1b043552'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (24746, 'fb0ba88d-c0a4-4d54-a5ba-ace8f014dc82'), (18098, '8ccdabe2-25b8-434c-8ed1-410a960610e9'), (18100, '337d1350-3df6-4643-9dde-2aa3d475aa17'), (23746, '6bcd61fc-cad2-440d-9af4-d5420c6dea2f'), (6852, '6314e627-a6f6-4b5b-81e6-111df0abd8a8'), (16585, 'd4b8ee95-06d5-497c-b989-560e42d85808'), (13515, '9d555f5e-ed90-44ba-bc2a-a8f9bc0941fe'), (17119, '3362d998-e1dd-4fe9-838e-5136a991278a'), (18670, '52152586-5012-49cb-bc98-ffaee3f6db05'), (19695, '2b63b560-d4ac-4b5f-a51b-76f6465bf883'), (19696, '43e599c7-2548-47c5-9039-cfa92c630506'), (19697, '2bb5d1ca-7bcf-4074-b6a2-32572672320c'), (18674, '79b5c722-2889-43be-b08a-a5799e9159f3'), (19699, '10a481ad-92e0-4ff7-8bf5-1ec64a1e708f'), (18676, '19268cbe-5f8f-44b7-9603-fd3d64a79dc2'), (18675, '1da20ff6-a9fc-47bd-beda-e5b849c25412'), (18678, '024c9ef4-b4db-48b5-af25-45f6f55f2032'), (18679, '940bea7f-982b-4d25-af18-345fac8b5ccb'), (19703, 'cee8ceac-098f-4297-bc31-778901f94db6'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (1274, '1fa4cb29-360a-47e7-a8d4-b3282777ee5a'), (16122, '7950be94-ecd4-4eda-b3d6-1fc80aacafd4'), (5890, '196d6213-976c-4e03-ab43-414d50d862f3'), (5891, 'e903bbaa-0306-4955-8b4b-3f01e947eb7c'), (21778, 'acec3fd7-6af1-4a2d-a7f6-80646ac73ff1'), (16147, '27008f92-4674-4cb1-b22a-6468630dc026'), (21779, 'cb8cf65e-d491-4dfa-b4f8-9d7d9e918dbc'), (21783, 'e00654c3-6d4e-43f7-9af7-a315aa0e72c3'), (21785, '0f42515e-4921-41d6-a8f2-18c62768982e'), (21786, 'c4b02c4e-9c4d-437d-ba8f-fa2e3ed2ba61'), (21787, '1dc5808f-b83c-4ca5-94dd-ff9446297817'), (16157, 'e8e24e1f-95a9-4f56-bd14-0192a666c31d'), (16168, '8720f9c5-bd5a-4961-b1b7-7dd1d69cb57d'), (16169, '28f13635-acb7-4098-9b99-064a295d026c'), (14121, 'c62b84e0-8e2d-4903-80c4-6e2af3aa7531'), (16172, 'b7a87971-1bc4-4771-bdaa-c67e3b637648'), (16176, 'a7578109-e271-40f8-aa02-f89b5dddab55'), (16179, '1d124f89-41e1-4023-a15b-937a496b7152'), (10040, '41bd4b8c-cb79-4a90-b723-46c86076ed1f'), (10041, '61256d30-f3fb-4a77-b783-c6a4a918dd0e'), (16186, 'd28a9bb2-1d44-4685-89a2-827e11eeb7f4'), (16187, '35be4a8b-fae7-46ed-bacf-a5a045c602cf'), (10044, 'bb24d82d-8c5b-4e60-a20b-488cfff229bb'), (10045, '8f8befd3-1f42-449f-9b31-97e81ad871fc'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (16185, '91f950e5-c9c1-40fb-a64e-ffaa632b3f4d'), (16192, 'ead14a30-856c-4c68-b676-f32d311a4375'), (10047, '6ad5d7a1-4fad-4e6c-a3f8-3ff5071a694b'), (2880, '130238c5-0fac-4497-9834-def294af26c6'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (16196, '70e6ae83-91ce-42c2-abe6-64b6f74358cc'), (16197, '9a286ec4-bd86-44d0-839f-bdbc41b5d244'), (16198, 'ca1b20da-9f54-4873-91f7-44f1ef190ac3'), (16199, '0d5f4326-c5ee-425d-a054-2c6febdb709b'), (16201, '74a34dcf-6a1b-4918-a45a-b5f73a7e5d91'), (9034, '835c1664-8c85-4b8d-be66-5ffe6e5b9a4f'), (16209, 'd0d2483e-c27b-4df7-aa5f-4a72dee19295'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (16213, '2996caec-c6fd-4c7d-83c9-3fe4006c4dca'), (28504, '4920cb0c-ca5a-408f-9ee5-eeeac00bd90f'), (16225, '8209b394-4270-47c3-af24-bd1f76be97dc'), (16226, 'fba86705-5d78-4380-b949-e56866aaf769'), (354, '2a268797-e79e-4063-843a-7cbe133cb03e'), (16239, '948fe2a6-ea63-4bdf-ba5e-f3291774774f'), (7547, '836d3b3f-ca29-4139-a241-bd2400998d51'), (14719, '77e3df3c-17d4-4639-808e-82441cf32fd2'), (7569, '767086c3-8554-4dac-a175-2097c23fe2dd'), (23455, 'a3815f91-b11e-4a12-b214-845902d94b3f'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (5025, '4bba4d42-2a40-417a-af10-f1e9d550b595'), (25511, 'df21c244-c02c-41dc-a6c7-2347bb115460'), (25514, '96ce2d77-9a93-400e-a9d1-5e20bef9b690'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (16843, '926901bf-b2bd-4869-8ac4-10833c3d0dd1'), (16844, '14804510-d68b-43ef-b630-fea72e07be9a'), (13282, '65364d14-5bde-4e50-87e2-a33023028d02'), (27116, 'ee44caad-d368-4c27-b4ad-92f672f5e3da'), (22517, '67ea3b27-8027-4a45-8051-d1f4af05f97f'), (22518, '3830e20a-fda4-48d4-81c9-a357a6a40a7d'), (25080, '94e831a1-6d84-4acf-8b58-b55946872ec4')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Quickstart flax and jax

[JAX](https://jax.readthedocs.io/en/latest/index.html) is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more. A great place for getting started with JAX is the [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html).
### Talks & Speakers

#### Skye Wanderman-Milne, JAX developer, Google Brain
- Talk: Intro to JAX on Cloud TPUs
- Abstract: JAX is a system for high-performance machine-learning research that combines the familiarity of Python + NumPy together with the power of hardware acceleration on CPUs, GPUs, and TPUs. It offers composable function transformations for automatic differentiation, automatic batching, end-to-end compilation, and both data and model parallelism. This talk will show you how to get up and running with JAX on a Cloud TPU VM. 
- Speaker info: Skye Wanderman-Milne is a software engineer working on JAX. She has previously worked on TensorFlow and Apache Impala, a high-performance distributed database.
- Speaker info:
  - Iurii Kemaev is a Research Engineer at DeepMind. He has been using JAX for 2 years advancing RL research. Iurii is one of the DM JAX ecosystem leads.
  - Soňa Mokrá is a Research Engineer at DeepMind. She has a background in machine translation and has been using JAX as the main ML framework for the past 6 months.
  - Junhyuk Oh is a Research Scientist at DeepMind, working on reinforcement learning and meta-learning. More information is available at https://junhyuk.com/
<h3 align=""center"">
    <p>State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow</p>
</h3>

<h3 align=""center"">
    <a href=""https://hf.co/course""><img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/course_banner.png""></a>
</h3>

🤗 Transformers provides thousands of pretrained models to perform tasks on different modalities such as text, vision, and audio.

These models can be applied on:

* 📝 Text, for tasks like text classification, information extraction, question answering, summarization, translation, and text generation, in over 100 languages.
* 🖼️ Images, for tasks like image classification, object detection, and segmentation.
* 🗣️ Audio, for tasks like speech recognition and audio classification.
#### Suraj Patil & Patrick von Platen, Machine Learning Engineers at Hugging Face
- Talk: How to use JAX/Flax with Transformers
- Abstract: Transformers is one of the most popular open-source ML libraries and supports PyTorch, Tensorflow, and JAX/Flax. In this talk, we will explain how JAX/Flax models should be used in Transformers and compare their design in Transformers with the design of PyTorch models in Transformers. In the second part, we will give you a hands-on presentation of how a model can be trained end-to-end with the official JAX/Flax example scripts using Transformers & Datasets. Along the way, we want to give you some tips and tricks on how to best realize your project.
- Speaker info: Suraj and Patrick are part of Hugging Face’s open source team and lead the integration of JAX/Flax into Transformers.
- GitHub: https://github.com/patil-suraj & https://github.com/patrickvonplaten
 
Here is the question: What is the primary purpose of JAX in machine learning?","The primary purpose of JAX (Jacobian-Automatic eX.Methods) in machine learning research is high-performance numerical computing that combines familiarity of Python + NumPy together with power hardware acceleration enabling composability transformations such as differentiation (i.e., automatic computation or reverse mode automatic differentiation), vectorization (i.e., batch compilation), parallelization (e.g., multi-GPU or multi-accelerator execution), just-in-time compilation targeting accelerated architectures like GPUs or TPUs etc., primarily focusing towards"
What benefit does 8-bit quantization offer for large-scale language models?,"[(12289, 'fe670b2c-6435-4e7f-8e1f-e1058894e410'), (7238, 'eef72ab8-ee77-42ff-8c37-2520ee38ae8b'), (5716, 'a521babf-2a86-410a-9a2c-16cd35ecd60f'), (2645, 'c0529244-876b-4941-83d4-3c85320ab532'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (21089, '9e8976c4-72d9-49eb-9b17-5078f33abec4'), (5731, '0d967011-f6bb-453c-94d9-0c8031acf313'), (14437, '6f4d674a-5ac8-464d-8dde-f98b8f555e87'), (21099, '7e7d4ba2-8c67-4e57-9e48-70cb0d5cb756'), (14443, '5ddb8814-0af0-4228-ad38-7a298ab49646'), (30319, 'a04b6e09-b517-497d-bf98-2432285b7d43'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (30322, '7aab1247-fa08-4335-a051-0b4737639821'), (30325, '16c886b9-b19c-44ea-9e7a-e5bd22b82ced'), (30326, '9527ab2d-2568-40f2-8af9-d9d87d77583c'), (30327, 'a62abcc1-500d-4f5e-a758-64fcef6ee3e7'), (21109, '0626c7f9-dffb-4329-86ab-21a3d6000823'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (21497, '9302803f-14ef-4e6b-a4b8-0888e4a9bb43'), (2695, '38e95994-7560-49d4-8884-6a69c4b9f459'), (18059, '0c61ecab-c341-4e5e-9b6b-11f05c7bf24e'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (22163, '8d167b0c-14d6-4205-a383-f37c9f717a64'), (22167, '27793adb-fb93-47e7-bbf6-90ccaa62cc27'), (3736, 'e3e83e2c-6c23-41d7-8f7e-c1919c0feea6'), (22168, 'e6c58e6f-1ad9-42ec-beab-d54096522c9e'), (3738, '95bed131-0d04-468e-b476-46221cb94b4b'), (22169, '1eefa675-c3d2-4400-8957-842b99fce9c4'), (22173, 'c6da122a-96dc-44f4-931e-80f5975b4687'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (28353, '841b803d-8a56-40f9-a606-deac23e25ac8'), (24267, '17a4733e-95f5-4a59-bf83-0455673132e8'), (5839, 'd97b59e5-561b-4a61-bcff-51b7f943fa47'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (6358, 'c5b8b78f-d9a9-483d-8f87-70642fe0c191'), (25303, '9215e63c-2a98-4f8e-988b-31c2d71d12a2'), (25304, 'b80ca7fa-2902-4f9b-bfe3-492030f9ce0c'), (25305, '6126e14a-e501-46c0-8773-957993663f6d'), (25306, '9d30be4f-6f9f-4452-84ee-5219508fa7be'), (25307, 'a1dddf5c-6872-4923-9d66-941863c5e2e6'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (27362, 'fcf527e3-17fe-43ea-9f64-d5d198cf9cbc'), (25318, '7f293204-2059-483b-9146-91aa485398d2'), (25321, 'b7befe0e-86b5-4603-8086-b358c0606f7c'), (28910, '645a05e6-4d85-49f2-92c8-8181916201d3'), (25841, '944d2878-310e-4406-a580-e27546617a5b'), (27378, '69fd6e93-802f-4dfc-a4ff-6caf775f803d'), (6899, '5fc72f0c-9870-4f07-bc4f-c8e16dc0b4f2'), (6903, '807eb0ed-deee-4627-8c1d-d65ce5f2f592'), (4357, '34f6ba11-eab8-4f0b-a037-c1fc34b53915'), (21266, '92adf75c-feec-4fca-a3d6-d7ec3ab3c977'), (21269, '78ae6612-508c-47ad-9a00-a5911c9e8ecd'), (21272, '76efa72a-7e7a-4994-8958-c6de170f0c1c'), (21273, '6e7d8ca1-c6a3-4f3d-b7f4-d961cc88716d'), (21274, '6dc1af30-a439-4f39-baba-fb652d70f065'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (17703, 'd695e0ce-5708-4684-a7a3-60e3f01ae575'), (16683, '8816fbb3-bf82-4453-886f-29f192a503f6'), (1839, '2e1f7f32-7b5b-44e1-b786-cda596c27fee'), (21807, 'b3346506-6c4a-48ac-9d75-c1258f287641'), (21810, '984ed666-72eb-4f58-bc53-465ae53447e2'), (21818, '73d48f15-6f37-4445-aa1f-d7942fd7ca06'), (19259, 'f519d5f3-91ef-4576-8c06-268c476e2894'), (19260, '5426cc73-0d90-44a0-b3b1-ee7e31d8605c'), (19262, 'cb4dea97-cfcf-4d5d-827a-c683ab6dcaf2'), (1858, 'ebbabbf7-d895-47f9-abba-d2685cb8f307'), (1859, '52a06e39-5439-41fa-b672-c8652dd2ef41'), (14662, '1f8ef827-8905-47df-97a4-47924dde939a'), (1864, '9ba52445-c296-42f0-8c37-a67e6f6728e9'), (14664, '10a5d466-9698-40d2-aa8e-c005dc0608d1'), (14670, '4934aa67-784a-4d9b-b8a9-e356c97d7c55'), (14671, 'f63072f2-da40-472c-a639-1b77176258b8'), (14672, 'e0d05374-3982-41cc-91ba-8f6d65c9832c'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (14676, '888122bb-5af6-412f-b7ef-7cc898f025ea'), (14679, '941c9eda-09fb-46bf-a0c8-12c756f40a6a'), (14685, 'f6385cc5-2ea5-401f-bace-7cbe69ca1700'), (22883, '9c589b58-aee6-48cb-9a23-df9d06a81fc1'), (30572, 'd78077e0-9c1c-40c0-b1f9-3ca8b834414f'), (18307, 'c8662bd0-7bd4-46fa-975d-11bfc7146272'), (20361, '38a40d70-31f1-4631-97f6-df1d876a9b8d'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (3999, '09d1277f-d722-4457-b060-831350863007'), (4010, 'f7201c35-6a48-44a3-b8a2-3b51387783a3'), (10667, '99592116-e7df-436c-aefb-86283541f025'), (7599, 'c64e7824-8c85-482f-a5a9-75513d131cd6'), (7600, '035f5b2b-a0fa-409f-bca1-b121604c1b57'), (7601, '3ecbf4ed-0cd3-483b-88d7-128fe443adff'), (23475, '68a40f24-cf09-4158-a626-b4357e796ceb'), (7604, '76a18b88-7a2a-4930-8f66-1b41521667f6'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (31737, '1db4c46e-0450-4584-a576-b34a60bf4b35'), (4031, '2b10b56f-b719-4f10-b0a1-04970839e977'), (11714, '81248f40-6e7d-4225-8017-1c87a3d5c64a'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (2525, '24835b20-c162-4932-9a49-726b89e54e7d'), (2526, '17411b67-d805-441d-a2e1-1374871f0ab1'), (6624, 'd3cc1a59-80a9-4bf1-876a-3e285f82e13f'), (22521, '0a44fe1c-dad7-4651-9607-3524ac837384'), (21498, 'a0f42abf-15b3-4093-ae5f-bf71a5993cf0'), (31739, 'ae5209af-c84b-4c9d-8cad-45339d0bf030'), (21503, 'c1792f1c-c950-4331-b5c4-949b2c0a2708')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Joint Megatron-LM and Deepspeeed:

- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/abs/2201.11990).

ALiBi:

-  [Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation](https://arxiv.org/abs/2108.12409)
- [What Language Model to Train if You Have One Million GPU Hours?](https://openreview.net/forum?id=rI7BL3fHIZq) - there you will find the experiments that lead to us choosing ALiBi.

BitsNBytes:

- [8-bit Optimizers via Block-wise Quantization](https://arxiv.org/abs/2110.02861) (in the context of Embedding LayerNorm but the rest of the paper and the technology is amazing - the only reason were weren't using the 8-bit optimizer is because we were already saving the optimizer memory with DeepSpeed-ZeRO).

## Blog credits
The picture above shows the results of image generation and some model characteristics. As you can see, just conversion to OpenVINO brings a significant decrease in the inference latency ( **1.9x** ). Applying 8-bit quantization boosts inference speed further leading to **3.9x** speedup compared to PyTorch. Another benefit of quantization is a significant reduction of model footprint, **0.25x** of PyTorch checkpoint, which also improves the model load time. Applying Token Merging (ToME) (with a **merging ratio of 0.4** ) on top of quantization brings **5.1x** performance speedup while keeping the footprint at the same level. We didn't provide a thorough analysis of the visual quality of the optimized models, but, as you can see, the results are quite solid.
The abstract from the paper is the following:

*Quantization techniques can reduce the size of Deep Neural Networks and improve inference latency and throughput by
taking advantage of high throughput integer instructions. In this paper we review the mathematical aspects of
quantization parameters and evaluate their choices on a wide range of neural network models for different application
domains, including vision, speech, and language. We focus on quantization techniques that are amenable to acceleration
by processors with high-throughput integer math pipelines. We also present a workflow for 8-bit quantization that is
able to maintain accuracy within 1% of the floating-point baseline on all networks studied, including models that are
more difficult to quantize, such as MobileNets and BERT-large.*

This model was contributed by [shangz](https://huggingface.co/shangz).

## Usage tips
```

### 8-bit

<Tip>

Learn more about the details of 8-bit quantization in this [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration)!

</Tip>

This section explores some of the specific features of 8-bit models, such as offloading, outlier thresholds, skipping module conversion, and finetuning.

#### Offloading

8-bit models can offload weights between the CPU and GPU to support fitting very large models into memory. The weights dispatched to the CPU are actually stored in **float32**, and aren't converted to 8-bit. For example, to enable offloading for the [bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7) model, start by creating a [`BitsAndBytesConfig`]:

```py
from transformers import AutoModelForCausalLM, BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(llm_int8_enable_fp32_cpu_offload=True)
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Quantization

Quantization techniques reduces memory and computational costs by representing weights and activations with lower-precision data types like 8-bit integers (int8). This enables loading larger models you normally wouldn't be able to fit into memory, and speeding up inference. Transformers supports the AWQ and GPTQ quantization algorithms and it supports 8-bit and 4-bit quantization with bitsandbytes.

<Tip>

Learn how to quantize models in the [Quantization](../quantization) guide.

</Tip>

## AwqConfig

[[autodoc]] AwqConfig

## GPTQConfig

[[autodoc]] GPTQConfig

## BitsAndBytesConfig

[[autodoc]] BitsAndBytesConfig
 
Here is the question: What benefit does 8-bit quantization offer for large-scale language models?","According to the context, 8-bit quantization offers several benefits for large-scale language models, including:

1. **Inference speedup**: 8-bit quantization can lead to a **3.9x** speedup compared to PyTorch.
2. **Model footprint reduction**: 8-bit quantization can reduce the model footprint to **0.25x** of the PyTorch checkpoint, which also improves the model load time.
3. **Memory savings**: 8-bit"
How does the variance schedule affect the forward diffusion process in DDPMs?,"[(5633, '08f3dc55-d96f-41a2-9247-79e9e57e0a2b'), (5634, '59a4e6a6-39fc-415a-9e3b-3691933b4ace'), (4611, '9ba60db2-6853-40a1-88bd-22525fbdb8e9'), (5645, '1b732b6c-6263-4c40-bfd5-e3fe9e80bf13'), (15896, '533a0a9e-34b4-449c-bb00-101804823eab'), (29723, '645b6c64-d629-4e91-a35d-b6f31b7b5147'), (3115, 'b9589646-16ee-46e7-9031-d0f697b1e6d8'), (15932, '2653d81f-316d-4342-aaea-37030a01bde8'), (9792, 'c23887e9-c847-42df-b582-76d15b62b7e1'), (7244, '285ab510-3ccc-4789-a279-59038db4de4c'), (26701, '3c9d16cf-eb89-4c9e-9e76-5bcbda107d24'), (7246, 'f2f24efc-81fd-40de-9450-3d9bc633abf7'), (13396, 'e9acadfa-60e3-4c46-a14b-5b042e105d94'), (598, '8c1684a0-1743-4eb1-aeb6-2c2e62f2083e'), (601, '8cac6f94-0281-42c2-83c8-e850306c4c98'), (9838, '24e469f0-838c-4b10-8de3-82174bdc04da'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (6799, '7fdaa5a2-96ae-4629-98d8-4011d0f44647'), (25749, 'b5c23583-19c3-48f4-a569-f766c165894a'), (10920, 'f18f64ad-7a3c-461d-9f41-0ffa5327fb77'), (20136, 'ebf447fc-7e03-4766-9e76-3c4a14c4f572'), (29866, '60c2f47e-2fd2-4b84-8003-c4cdc2aedcc0'), (20142, '8acb5b27-257d-4765-acaa-2068a1bb9923'), (20143, 'd869b9f1-cd4d-4d20-955b-89d5173b1e6e'), (10933, 'b2789e50-f10a-4c41-9023-3c64d5949b58'), (4280, 'c91cc95e-4c1b-4e75-a090-e529f4e4cb58'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (19128, '7822f723-18d9-492d-b2c1-5d48cb7de5cb'), (20155, '04e7e95a-9eb0-4302-a0ec-07716aa382f2'), (19134, 'e3688ad7-d66d-424f-8be8-c607f1178ce8'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (7874, '9157ec2c-4628-4f73-8e40-85b3d1e9a454'), (28374, '2f02209a-dbaf-4432-b002-393b2b850745'), (25816, '0aee80dc-2820-4f8b-a131-2042458d62e0'), (9961, 'f4b19bf3-358d-4b4d-84ba-3831e2c2f81c'), (9962, '49cc7639-d6b4-4703-8068-e5764e6ccce6'), (9965, '9ce19ae7-e5ff-4d53-ac46-bfe57d86d3da'), (9966, '250cfd25-8c4d-4880-9b39-a99e4debe016'), (19695, '2b63b560-d4ac-4b5f-a51b-76f6465bf883'), (25332, 'b27bb4a7-cee9-4ccc-ad5e-bb3287aa008c'), (8957, 'd821fc5e-a896-4f11-b3ca-931a9c70ca93'), (4354, '23d6e114-d7c6-4b5d-b123-10b9575f7a47'), (6412, 'e491859b-0c3a-451c-8d54-e5536a56bf5f'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (10000, '11b6cc75-c6fd-4e28-86d3-d42657a01b52'), (30995, '2482d3c6-479e-411a-9276-d86969cedbe9'), (30996, 'fc3155b3-30ea-41c1-97b1-d8f4534cd97d'), (10003, 'e4f7af88-4b26-49b2-85af-d98722f2636e'), (10009, '117f363f-359c-4598-9aa4-7542e1af2afe'), (23839, 'aac7f57f-4263-4e9e-bbee-db3bfed8ee29'), (7456, '25b4b0d5-f2ef-4f3b-be33-88790e160ed4'), (23841, '5a99e4d4-7fec-4135-b408-ad9cae391ea4'), (3366, '67907026-cc7d-4490-8556-498e7e15245a'), (10535, 'a92216b1-3959-48ee-80b2-f50cec8a3460'), (10538, '1bfaf9c3-f829-4171-8dc9-e762693b30c5'), (26410, '67e40473-eb86-4bc5-aadc-9904f3c372cf'), (19250, '8c61f212-aee7-4c19-9c1b-c2e21bfd2b05'), (27455, 'd6460a51-1d18-45d0-8192-6f2463639df5'), (12100, '1bbf45ef-90da-4af0-9bc6-54ff09763654'), (12101, 'bc396373-21b5-4b13-a876-c3c4dd9f1ded'), (24906, '4f6d9603-05a1-44d8-b626-d6aeed01a2f5'), (24908, 'e5fe80b3-8b91-4599-8072-37edf5ea77fd'), (24909, '82290a82-2504-4656-a125-e4ba230d33ea'), (24910, '93139bb7-0ac6-44de-b998-dccfe267409d'), (24911, '8d336559-acfd-4d67-a0fb-e707975cad03'), (29009, '517b0bd9-f6a2-4e14-8423-495ff426a3c1'), (24914, 'fea84df6-95a4-47fb-9d7b-e7715c6895a3'), (10579, 'c5c569a7-6ff7-430d-adbd-b77fe1021f0d'), (5975, 'c2372c16-a0f7-4f31-b095-79df0d949f6e'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (13153, 'ceaaeca1-e37c-45bd-870c-17fc8edf6ab6'), (24931, '6001761b-915b-47a4-88d1-9a3dc126d236'), (19814, 'd93ac1aa-7066-47be-9649-9e2fc04a3100'), (24935, '3ae70e6a-bbb4-47bd-986b-93416ef571a9'), (13158, 'd5de80e9-25fb-4ef5-8868-7bb0688c1611'), (19818, 'd3f85fc4-89c0-48b3-80cf-4a44612c2dc2'), (5996, '95d44852-e6f1-4950-8ede-d7cf96cd61ec'), (5998, '6eb6b062-9d93-4028-8277-3d1bed1c5f96'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (13169, '3cd0b600-024b-4841-89f2-c5d09b3b1789'), (13174, 'a62f183d-35f1-48bf-a9b1-178a591da0e3'), (22904, 'a678407d-0ca1-4128-a53e-8ab8dccdddf1'), (6009, 'c5f4509c-9eed-4234-abdb-5ebb21193480'), (22908, '20cf93bb-0193-4309-9639-b4265308de33'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (22911, 'beaca8ce-2939-41e0-b621-52375e694804'), (14211, '23b7c816-0f90-4aa4-91fb-9518fda4894f'), (6024, '597afa6c-ee7a-4d5b-b5fb-087459f615a7'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (19855, '9800f1ae-0b10-4f47-8b9b-9a97f69d8a08'), (31633, 'fb67f82a-d3d9-46de-897f-1dbe80d597ba'), (2966, '0a9b8161-daf7-4627-9847-cb113b207f7b'), (17302, '10950558-fd37-474e-868f-4f3fca1d098a'), (8600, '269c11e4-e332-4535-8735-0da74c456141'), (6565, '7cc358fa-67fa-4da6-8331-9465d5bb3852'), (17323, 'c05c8d93-d9fd-4eda-aea0-2daab9c5a902'), (4012, 'b1e161a0-d490-4161-880d-9be316427ee3'), (17335, 'b353fdcd-0079-445d-94db-4f3cce7e2a9f'), (17344, 'dd623682-ea28-458a-8129-4731e7425a06'), (15301, '1eb2e2b0-a68b-4fb8-84ca-85eb3633a2b1'), (15302, 'e4baa76c-3e22-4be0-b525-70fa6339f3af'), (4047, '1a54fabd-203c-4e44-9c9a-efc4a3b0004a'), (17363, '509f9576-4f0e-4f9e-a4f6-a2956a1427f9'), (15317, 'c82c3719-3a25-4978-a7f3-404b7bb3da1d'), (19931, '5464a5f6-419b-471b-882d-500d8380993a'), (17380, '75a53abe-6122-45ed-b350-7c9d763b9e3c'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b'), (19944, '5fb1084c-cdb3-4c12-90d8-ef3744dfe8b5'), (19945, 'e8883fa6-cf8d-4183-beba-8d7f435a4e81'), (10222, 'a13b8c95-cbad-4812-ad41-a22be173a110'), (26614, 'ef74dcbf-21f2-4413-b74f-69aee8ff4bc3'), (5626, 'd6277e09-2a4f-42b8-ae53-edde58d332e7')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Defining the forward diffusion process

The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps \\(T\\). This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:

> We set the forward process variances to constants
increasing linearly from \\(\beta_1 = 10^{−4}\\)
to \\(\beta_T = 0.02\\).

However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. 

Below, we define various schedules for the \\(T\\) timesteps (we'll choose one later on).
<p align=""center"">
    <img src=""assets/78_annotated-diffusion/diffusion_figure.png"" width=""600"" />
</p>

Both the forward and reverse process indexed by \\(t\\) happen for some number of finite time steps \\(T\\) (the DDPM authors use \\(T=1000\\)). You start with \\(t=0\\) where you sample a real image \\(\mathbf{x}_0\\) from your data distribution (let's say an image of a cat from ImageNet), and the forward process samples some noise from a Gaussian distribution at each time step \\(t\\), which is added to the image of the previous time step. Given a sufficiently large \\(T\\) and a well behaved schedule for adding noise at each time step, you end up with what is called an [isotropic Gaussian distribution](https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic) at \\(t=T\\) via a gradual process.

## In more mathematical form
# Schedulers

🤗 Diffusers provides many scheduler functions for the diffusion process. A scheduler takes a model's output (the sample which the diffusion process is iterating on) and a timestep to return a denoised sample. The timestep is important because it dictates where in the diffusion process the step is; data is generated by iterating forward *n* timesteps and inference occurs by propagating backward through the timesteps. Based on the timestep, a scheduler may be *discrete* in which case the timestep is an `int` or *continuous* in which case the timestep is a `float`.

Depending on the context, a scheduler defines how to iteratively add noise to an image or how to update a sample based on a model's output:

- during *training*, a scheduler adds noise (there are different algorithms for how to add noise) to a sample to train a diffusion model
- during *inference*, a scheduler defines how to update a sample based on a pretrained model's output
## In more mathematical form

Let's write this down more formally, as ultimately we need a tractable loss function which our neural network needs to optimize. 

Let \\(q(\mathbf{x}_0)\\) be the real data distribution, say of ""real images"". We can sample from this distribution to get an image, \\(\mathbf{x}_0 \sim q(\mathbf{x}_0)\\). We define the forward diffusion process \\(q(\mathbf{x}_t | \mathbf{x}_{t-1})\\) which adds Gaussian noise at each time step \\(t\\), according to a known variance schedule \\(0 < \beta_1 < \beta_2 < ... < \beta_T < 1\\) as
$$
q(\mathbf{x}_t | \mathbf{x}_{t-1}) = \mathcal{N}(\mathbf{x}_t; \sqrt{1 - \beta_t} \mathbf{x}_{t-1}, \beta_t \mathbf{I}). 
$$
</Tip>

For the quicktour, you'll instantiate the [`DDPMScheduler`] with its [`~diffusers.ConfigMixin.from_config`] method:

```py
>>> from diffusers import DDPMScheduler

>>> scheduler = DDPMScheduler.from_pretrained(repo_id)
>>> scheduler
DDPMScheduler {
  ""_class_name"": ""DDPMScheduler"",
  ""_diffusers_version"": ""0.21.4"",
  ""beta_end"": 0.02,
  ""beta_schedule"": ""linear"",
  ""beta_start"": 0.0001,
  ""clip_sample"": true,
  ""clip_sample_range"": 1.0,
  ""dynamic_thresholding_ratio"": 0.995,
  ""num_train_timesteps"": 1000,
  ""prediction_type"": ""epsilon"",
  ""sample_max_value"": 1.0,
  ""steps_offset"": 0,
  ""thresholding"": false,
  ""timestep_spacing"": ""leading"",
  ""trained_betas"": null,
  ""variance_type"": ""fixed_small""
}
 
Here is the question: How does the variance schedule affect the forward diffusion process in DDPMs?","The variance schedule affects the forward diffusion process in DDPMs by controlling the amount of noise added to the image at each time step. The schedule determines the variance of the Gaussian noise added at each step, with higher variances resulting in more noise being added. The original DDPM authors used a linear schedule, where the variance increases linearly from β1 = 10^−4 to βT = 0.02, while a cosine schedule has been shown to achieve better results. The choice"
What key feature makes Flax distinct in its handling of neural networks?,"[(17931, '9ca66b23-26fc-451a-95bc-f56dca1857b2'), (6676, '0d231cb5-d381-44c6-9669-18d5df65d10d'), (18455, '00b11bfb-a9b7-4473-8c5b-032887064102'), (14880, 'a8dd5add-4531-4051-8819-039db852d991'), (18996, '6a19e0bf-7a11-49c8-9186-3e6ff36b3640'), (26684, '1efbd9e1-3025-4ea8-b6e6-ebe753a2f8a5'), (30783, '520b29e4-62d5-450c-87c6-95e8040615df'), (26688, '06c43668-d169-4991-9447-1d6ea35000d1'), (30785, 'fe7b98b8-14ab-4e54-8af2-68f748159690'), (30784, '5abe754b-87a4-4fc2-9103-0a0a0aab8e09'), (17475, '496f81a1-3559-4e5f-ae4f-25727935e2df'), (14407, 'a79ebe19-8cc0-40d8-ac73-4855d7cb570a'), (25159, '658fd35c-cc52-4a38-8685-95e85220eda8'), (7258, '60ffe65d-54e6-4c62-a466-854687cd7645'), (7259, '3a45b1bb-76a2-4e36-8e07-2845da111f5a'), (24163, '6d254de1-aa08-4a87-9094-64313c3a0c4b'), (25192, '8f098902-5f74-45e9-9728-e6fab498c2c4'), (25193, '03a63ec7-afd1-45dd-acea-f1087aadab69'), (13934, 'db89e881-3a64-4f86-b624-3f67721d2160'), (5745, '77c39426-5363-43cb-9fee-bfc8d242613d'), (24178, '3c4d4424-cb56-479f-a8b9-d87c9e045138'), (13938, '46f349ea-1419-4e09-bfd7-e42051ddde5d'), (4724, 'b46e8527-efd9-4564-96a5-72c8818d4bf0'), (24180, '6fd43eed-f7b8-4151-808f-472f264f2aa2'), (24177, '35b3eea1-aabc-4ecb-a399-ec042cd227aa'), (20600, '0e8f178d-148b-48bf-be1c-95d3f20535be'), (18047, '88a9e52b-eec2-4a57-933d-f10b57098f70'), (24706, 'b8a44ff2-1624-4d7e-8af2-7d48b4e7a549'), (21635, 'dc2413c5-b29d-4185-bd39-a90d0f73d0f1'), (13958, 'ed200096-8374-40a5-9473-b55aae1e78f8'), (20634, 'b26ceca9-16f3-491c-a4e2-ce69a77a514c'), (666, '87ef7cf7-253c-4e2c-a8a4-64a6170a4cfc'), (668, '249a03e0-c503-4df7-824c-7f0c50892b55'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (11456, '3a19ee7b-41ea-4782-adbc-37d919e3d488'), (23750, 'c2856f93-0376-48a3-8544-fd75e08e618a'), (18123, 'bc8959d8-bf85-4f91-8c70-99876447e055'), (23763, '0331ca31-194a-49b0-aa36-b9a61017c85b'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (14556, 'a4bee4d0-485b-48d7-8bb4-2ee651d45c15'), (4829, '2327c019-21d7-4362-844c-4ec05792b517'), (23777, 'e8d6bb71-ce43-4ba7-bdcb-8d0d203337a7'), (1768, 'e4a9204b-8ed1-4d38-bb48-f929611413a0'), (26862, 'edbd63a0-d666-4cdb-84c2-26609a8ae5d7'), (19699, '10a481ad-92e0-4ff7-8bf5-1ec64a1e708f'), (16122, '7950be94-ecd4-4eda-b3d6-1fc80aacafd4'), (26875, '09d6e559-df0c-401d-ae66-2f3358d9b39a'), (26876, '4dfe8cb4-fe81-4e63-855e-6028faa0d7b5'), (5894, '7ff4ce6e-4ce6-4ff8-a452-877f4c80abdb'), (18694, '14ec5ba0-bd18-4aa7-a57f-338402d25194'), (9994, '828e8a71-5859-4c61-868d-07d0f8ec652f'), (28429, '9c6fb59e-1d9f-48bd-be33-bd99c86d369d'), (18190, 'e8ba51cb-eaf2-4926-b117-a9ed9172c009'), (10511, 'e13e2a1f-46bf-4f51-9fd3-d963a35dddf2'), (18704, '153572ea-4ba1-43c6-9820-c834edb506aa'), (16147, '27008f92-4674-4cb1-b22a-6468630dc026'), (16150, 'ea3c23db-9446-48cf-9cff-b124d174ccdf'), (13607, 'a7f2128c-ee6c-496e-a752-b4497da88f26'), (16168, '8720f9c5-bd5a-4961-b1b7-7dd1d69cb57d'), (16169, '28f13635-acb7-4098-9b99-064a295d026c'), (1834, '2f9f1ca1-34ca-4d7f-912c-1c6fc32e2590'), (16171, '7690120c-12d1-46f3-9772-f3b699f83fc8'), (16172, 'b7a87971-1bc4-4771-bdaa-c67e3b637648'), (16173, '021868a6-6880-43b4-aba8-34b48a2ffff3'), (16175, '593e60d6-0896-4f95-985c-fe42f835237b'), (16176, 'a7578109-e271-40f8-aa02-f89b5dddab55'), (16177, '68477c3c-eb28-4238-ba86-32d0aa9939f5'), (305, '5d87ba97-734a-4266-ad39-fb1514c254c6'), (16179, '1d124f89-41e1-4023-a15b-937a496b7152'), (16181, 'dd336379-e09c-4dee-810c-fedf585d9e36'), (16184, '08e02bad-54da-4851-82ae-02218bac7497'), (16185, '91f950e5-c9c1-40fb-a64e-ffaa632b3f4d'), (10041, '61256d30-f3fb-4a77-b783-c6a4a918dd0e'), (10040, '41bd4b8c-cb79-4a90-b723-46c86076ed1f'), (10043, '030c756f-7c61-4302-b450-f25c5d990cf9'), (16189, '3b6faf7f-1527-4d6d-a357-0b3c68d67e71'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (16191, '9fbef11c-ebf4-4a39-99e1-dddb5fd7c23f'), (16193, 'dc1889b7-9c05-4074-ac23-4bc47fe1125c'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (16196, '70e6ae83-91ce-42c2-abe6-64b6f74358cc'), (16197, '9a286ec4-bd86-44d0-839f-bdbc41b5d244'), (16201, '74a34dcf-6a1b-4918-a45a-b5f73a7e5d91'), (16715, 'e3eb35de-68db-4c66-b9da-4c622cd55f2a'), (16210, '9fe165b2-82c7-4cf0-9c38-d9d6b7d37fe6'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (28504, '4920cb0c-ca5a-408f-9ee5-eeeac00bd90f'), (16225, '8209b394-4270-47c3-af24-bd1f76be97dc'), (21865, 'aed3a1e4-ca73-4239-a584-5c016709d32b'), (16234, '5eb5e210-4b97-4623-9dfa-d3d8e2330b55'), (16239, '948fe2a6-ea63-4bdf-ba5e-f3291774774f'), (24458, 'f7480eb5-a90b-4bb4-b923-1a0adb0171ff'), (9127, '80c781ca-38c4-453f-aeea-e80ee16dc3f7'), (16815, 'eef3bcbc-cbb5-4733-aa46-b4a36551b90b'), (3506, '4e992d07-de67-463d-9af5-8cad259a8ab0'), (22454, '1d7b084d-cdda-46cb-9415-b22155a22500'), (19387, 'fd8aa818-6add-4bbf-a661-88e05a30e489'), (29634, 'd756cc17-3381-4c73-99de-56f9d281f772'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (16844, '14804510-d68b-43ef-b630-fea72e07be9a'), (3030, 'e3812643-6ba2-4e81-afe0-bcc63edd1257'), (3031, '87db4b4e-191b-4b1f-ab44-7e5caf12b17f'), (30166, '95692b7e-3886-4c57-80d1-d78d81223c31'), (27608, 'ab21f240-f322-4fda-aa4b-237296a60eca'), (8667, '83f4e0e5-f70b-4a54-acef-2b200d5689bb'), (20955, '35ddbe51-134a-4010-8f1a-99919ac666fd'), (8669, '8f464916-06ad-48e4-85c4-3ae8a88c6ac2'), (2530, '7a44759c-f730-4e27-af93-41a5169acd88'), (22517, '67ea3b27-8027-4a45-8051-d1f4af05f97f'), (19958, 'a5878f32-2a7f-4f07-b78b-35d9e33c6d22'), (24567, 'c943037d-8961-4b3b-9362-dec6325b1bc6'), (24568, '81edcf95-34e4-4a32-8561-1bf4cb75b32c'), (18421, '7b5f0c1d-0849-407b-ba6b-352a5f7c5213'), (14331, '50cf2132-6037-4a00-a088-51aa58b6f2c5'), (5118, 'c5ddf224-1b18-420d-8a45-9bf0bb131699')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: [Flax](https://flax.readthedocs.io/en/latest/index.html) is a high-performance neural network library designed for flexibility built on top of JAX. It aims to provide users with full control of their training code and is carefully designed to work well with JAX transformations such as `grad` and `pmap` (see the [Flax philosophy](https://flax.readthedocs.io/en/latest/philosophy.html)). For an introduction to Flax see the [Flax Basics Colab](https://flax.readthedocs.io/en/latest/notebooks/flax_basics.html) or the list of curated [Flax examples](https://flax.readthedocs.io/en/latest/examples.html).

## Quickstart flax and jax in transformers

Currently, we support the following models in Flax. 
Note that some models are about to be merged to `main` and will 
be available in a couple of days.
#### Marc van Zee, Research SWE, Google Brain (Flax team)
- Talk: Introduction to Flax
- Abstract: In this talk I will provide a high-level introduction to the neural network library Flax. I will discuss the Flax philosophy, talk about the ecosystem around Flax and provide a high-level introduction to the code. I explain the Module abstraction and how to use it to train your models.
- Speaker info: Marc is at Google Research for over 4 years. First he worked on conceptual AI, developing a next generation language understanding and reasoning prototype and he authored the CFQ dataset for compositional generalization. Currently, Marc works as a research software engineer in the Flax team.
Participants used Tensor Processing Units (TPUs) with [Flax](https://github.com/google/flax) and [JAX](https://github.com/google/jax). JAX is a linear algebra library (like `numpy`) that can do automatic differentiation ([Autograd](https://github.com/hips/autograd)) and compile down to [XLA](https://www.tensorflow.org/xla), and Flax is a neural network library and ecosystem for JAX. TPU compute time was provided free by [Google Cloud](https://cloud.google.com/), who co-sponsored the event.

Over the next two weeks, teams participated in lectures from Hugging Face and Google, trained one or more models using JAX/Flax, shared them with the community, and provided a  [Hugging Face Spaces](https://huggingface.co/spaces) demo showcasing the capabilities of their model. Approximately 100 teams participated in the event, and it resulted in 170 models and 36 demos.
Several other projects were born from the Nx initiative. [Axon](https://github.com/elixir-nx/axon) brings functional composable Neural Networks to Elixir, taking inspiration from projects such as [Flax](https://github.com/google/flax) and [PyTorch Ignite](https://pytorch.org/ignite/index.html). The [Explorer](https://github.com/elixir-nx/explorer) project borrows from [dplyr](https://dplyr.tidyverse.org/) and [Rust's Polars](https://www.pola.rs/) to provide expressive and performant dataframes to the Elixir community.

[Bumblebee](https://github.com/elixir-nx/bumblebee) and [Tokenizers](https://github.com/elixir-nx/tokenizers) are our most recent releases. We are thankful to Hugging Face for enabling collaborative Machine Learning across communities and tools, which played an essential role in bringing the Elixir ecosystem up to speed.
```

First, we import the necessary libraries:
- `jax` is provides the primitives for TPU operations
- `flax.jax_utils` contains some useful utility functions for `Flax`, a neural network library built on top of JAX
- `diffusers` has all the code that is relevant for SDXL.
- We also initialize a cache to speed up the JAX model compilation.
- We automatically determine the number of available TPU devices.

**1. Downloading Model and Loading Pipeline**

```python
pipeline, params = FlaxStableDiffusionXLPipeline.from_pretrained(
    ""stabilityai/stable-diffusion-xl-base-1.0"", revision=""refs/pr/95"", split_head_dim=True
)
```
Here, a pre-trained model `stable-diffusion-xl-base-1.0` from the namespace `stabilityai` is loaded. It returns a pipeline for inference and its parameters.

**2. Casting Parameter Types**

```python
scheduler_state = params.pop(""scheduler"")
params = jax.tree_util.tree_map(lambda x: x.astype(jnp.bfloat16), params)
params[""scheduler""] = scheduler_state
 
Here is the question: What key feature makes Flax distinct in its handling of neural networks?","According to the Flax philosophy, the key feature that makes Flax distinct in its handling of neural networks is that it provides users with full control of their training code."
What problem does gradient checkpointing address in diffusion models?,"[(4609, 'b27a1438-b788-4f0f-8269-2c66f61be2cc'), (5634, '59a4e6a6-39fc-415a-9e3b-3691933b4ace'), (4611, '9ba60db2-6853-40a1-88bd-22525fbdb8e9'), (516, 'ce8c02d3-fb1b-4e21-b424-921eb9a2f16e'), (5633, '08f3dc55-d96f-41a2-9247-79e9e57e0a2b'), (7184, '8450f91c-e0e1-4e31-99c8-57f8d43d9cdb'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (11291, '9e06936c-6b43-4df3-8c7a-1c5d502857b2'), (15911, 'dea3e203-7c16-410e-9667-1ee9ffb39f00'), (3134, '82cefd2e-89c3-4b22-96f5-de5df1f3fe97'), (3141, 'ba5cdaea-0eb1-42ae-a62d-9d7fc61c1de5'), (7242, '7d32318b-4f2f-4493-8af3-ce5f81a7811a'), (30794, '60649a7f-802b-4687-8b03-4c75a01eca78'), (7244, '285ab510-3ccc-4789-a279-59038db4de4c'), (589, '5f6c042b-5281-483f-b557-84661e4b6441'), (26701, '3c9d16cf-eb89-4c9e-9e76-5bcbda107d24'), (591, '06236243-318b-48f5-8a7d-cfb9b4f1e0d5'), (7246, 'f2f24efc-81fd-40de-9450-3d9bc633abf7'), (7245, 'f97562de-c76f-4d07-b5d2-84f9663e0010'), (13396, 'e9acadfa-60e3-4c46-a14b-5b042e105d94'), (30805, '8d9b07a6-4ba5-40b1-a223-4f12f9892100'), (11862, '666001a9-512c-463b-bbb1-4fa509f3dd92'), (9816, '5a10bbff-24a7-4d06-b2b9-87ca115873d4'), (601, '8cac6f94-0281-42c2-83c8-e850306c4c98'), (10331, '3e81c36c-c486-497f-8503-f7e56fb08165'), (24163, '6d254de1-aa08-4a87-9094-64313c3a0c4b'), (14436, 'b6c8ba09-18f8-4b76-9196-bdead257e416'), (11373, '03316170-9bc8-479c-8653-d3b46bddaf64'), (14452, 'd602c8f8-3830-4f9e-8b34-1bea97e99242'), (14453, 'a004344c-285b-467d-91d0-a2ba08171153'), (14454, 'ce330ae8-8dec-4720-92e6-052305cb40dc'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (13956, '3ba2f512-964a-4a35-a1a8-47b90825f60c'), (6799, '7fdaa5a2-96ae-4629-98d8-4011d0f44647'), (20128, 'e3f94bda-7ff4-4e13-b614-239b43914824'), (25761, '8d9a0522-9ef4-43b2-8895-9601081937fd'), (10924, '1f072b2d-e84e-4ab8-bdff-6bbe2a12b0aa'), (10933, 'b2789e50-f10a-4c41-9023-3c64d5949b58'), (6837, '3a458d4b-7608-459d-80ef-a04f6c606f10'), (19128, '7822f723-18d9-492d-b2c1-5d48cb7de5cb'), (6844, '0e9c2f66-6441-471e-a0a1-ee5c92fb996f'), (17597, '8e21cdc2-2daf-4ce2-b812-2ad19cdb6870'), (26304, 'f09ef0c4-c54a-4f32-a228-9fbfc29d0954'), (6343, 'cdbe6a68-4a3b-41d5-a3a1-c4bdf6f677db'), (4297, '141bfea2-71c0-48b6-8a06-0ca33c6b56f7'), (6346, '97be871b-ee57-495c-bf58-605bb610abd9'), (6347, 'ff784d0d-58ae-482c-935d-fbf1bc3d8a8f'), (4299, 'fa200320-e0ea-4961-b45c-b5ec32991353'), (17609, '4cdc2094-1a54-40b7-9a6e-05ea32af5ee7'), (30414, '42a3e3a1-1e5a-4dae-b43b-56ac3ecc0061'), (31439, '44a37647-64bf-46ad-8a21-01bb8a1c4465'), (6374, '006f6db7-c046-47c2-9ed9-964dafff35c6'), (23272, '6fccbec4-765f-4aea-9f16-e8840626988f'), (19695, '2b63b560-d4ac-4b5f-a51b-76f6465bf883'), (6412, 'e491859b-0c3a-451c-8d54-e5536a56bf5f'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (6419, '42aef9bd-e9bd-469a-ac13-036843c85c48'), (30996, 'fc3155b3-30ea-41c1-97b1-d8f4534cd97d'), (25375, '4a82a223-150f-4b59-9144-ad72f1697f2f'), (23841, '5a99e4d4-7fec-4135-b408-ad9cae391ea4'), (10029, 'd1a5f9fc-00c4-4974-b71e-14d74422db37'), (10543, '595b18a2-b752-4644-b8bf-9f5028af4359'), (10031, 'd311da63-e4f2-4661-9207-d5accca2e35e'), (26415, 'c36eb535-fc79-44b4-b747-4cf29cc693f2'), (19256, '4307afee-c6a2-4b98-9213-6ad94155d798'), (26427, 'd5213d1c-651a-420e-9353-5b752301fe26'), (12101, 'bc396373-21b5-4b13-a876-c3c4dd9f1ded'), (26438, 'f329b776-81fd-470b-b8e7-ab77e1d208d0'), (19269, '2b788324-03f3-4218-95db-15f2077f16ff'), (10579, 'c5c569a7-6ff7-430d-adbd-b77fe1021f0d'), (5975, 'c2372c16-a0f7-4f31-b095-79df0d949f6e'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (5983, 'd398359b-b46c-4efd-978a-507676dbbea2'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (13153, 'ceaaeca1-e37c-45bd-870c-17fc8edf6ab6'), (5989, 'b6e237c3-82b5-47e4-b29a-a75446c3d291'), (5994, '393743ce-a942-4417-a7ed-66302d16103f'), (5995, '65a70509-08c3-4ba5-8543-038d3685b26e'), (4462, '14e529ce-e387-47f4-9d38-a454c23e8bd7'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (6008, 'f1ddc133-b500-4631-9de5-add5ec3ca0f4'), (22908, '20cf93bb-0193-4309-9639-b4265308de33'), (26494, '0993cfc5-93cf-445f-af49-92995c8906c2'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (26498, '8d241fd8-6c1c-4c2f-a561-694ae53eaa19'), (14211, '23b7c816-0f90-4aa4-91fb-9518fda4894f'), (14212, '4b03c79b-3dd8-400a-a8d2-e793ad43770b'), (10629, '00c5a0a0-02c4-4466-b0de-8105660ce253'), (22921, '28568e53-fad9-4e99-96dd-6932ad9d07a3'), (6030, 'b80847b8-58e2-40de-9633-2c67a5dc619d'), (19858, 'faa94907-c6f5-412b-aea1-d75f8b7fe3ce'), (6035, '39eaf788-d50c-41b3-afca-a2bc9e0f5915'), (9621, 'b886005d-bf70-423c-ac80-118efe1f9377'), (17304, '7de11ce0-e619-4eaa-8b2d-16020d24f29e'), (3998, 'd69ff847-eb07-4bc3-8707-899d59ed3e9b'), (4000, 'ea805f3c-9994-4780-9d7a-b121ffed6c14'), (17313, 'f9684d2d-c144-4e91-a2ec-f9a0fcdd92fe'), (26528, '5f55bea4-0eee-4351-981a-0f1c903294d0'), (14753, '02332114-d3f0-4ffd-906c-68000226fe4f'), (10681, 'e3b4aaa0-b3d9-4082-910d-271dd4fa6a71'), (17346, '2c91cfdd-1757-4160-99b2-4ef422291d93'), (15302, 'e4baa76c-3e22-4be0-b525-70fa6339f3af'), (8650, '19f4e06e-36ef-41c6-bd20-a5e373e02f13'), (8652, '9d7d1d5e-5e20-482e-be5a-9a048f3dbd8f'), (4047, '1a54fabd-203c-4e44-9c9a-efc4a3b0004a'), (15315, '7e744e9a-2d02-41eb-8d2f-acb54e6336bf'), (17363, '509f9576-4f0e-4f9e-a4f6-a2956a1427f9'), (982, '4b1ed7e5-38de-47d6-b5e5-a776db6fd6bc'), (17366, '477951c1-c4dd-429d-8afe-7a5c69bb20af'), (15319, '30aee605-cc98-4531-bd2a-b53f3911f36d'), (10212, '0d28df65-2cae-4253-8b4c-c834f7ec7e1f'), (10213, 'bc603003-9845-4bf8-8b72-7ce0fa07fe77'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b'), (10216, '14dcb926-8bff-4561-8c79-5999949b18ed'), (490, '50d82520-464a-48ce-8d09-3b309c927bbc'), (10219, '4d8a7b69-b7de-4cdb-a3e5-89aa2464d7b4')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: For additional information, please refer to batch size and gradient accumulation benchmarks for [RTX-3090](https://github.com/huggingface/transformers/issues/14608#issuecomment-1004392537)
and [A100](https://github.com/huggingface/transformers/issues/15026#issuecomment-1005033957).

## Gradient Checkpointing

Some large models may still face memory issues even when the batch size is set to 1 and gradient accumulation is used. 
This is because there are other components that also require memory storage.

Saving all activations from the forward pass in order to compute the gradients during the backward pass can result in 
significant memory overhead. The alternative approach of discarding the activations and recalculating them when needed 
during the backward pass, would introduce a considerable computational overhead and slow down the training process.
**Gradient checkpointing** offers a compromise between these two approaches and saves strategically selected activations 
throughout the computational graph so only a fraction of the activations need to be re-computed for the gradients. For 
an in-depth explanation of gradient checkpointing, refer to [this great article](https://medium.com/tensorflow/fitting-larger-networks-into-memory-583e3c758ff9).

To enable gradient checkpointing in the [`Trainer`], pass the corresponding a flag to [`TrainingArguments`]:

```py
training_args = TrainingArguments(
    per_device_train_batch_size=1, gradient_accumulation_steps=4, gradient_checkpointing=True, **default_args
)
This script is experimental, and it's easy to overfit and run into issues like catastrophic forgetting. Try exploring different hyperparameters to get the best results on your dataset.

</Tip>

[Stable Diffusion XL (SDXL)](https://hf.co/papers/2307.01952) is a larger and more powerful iteration of the Stable Diffusion model, capable of producing higher resolution images.

SDXL's UNet is 3x larger and the model adds a second text encoder to the architecture. Depending on the hardware available to you, this can be very computationally intensive and it may not run on a consumer GPU like a Tesla T4. To help fit this larger model into memory and to speedup training, try enabling `gradient_checkpointing`, `mixed_precision`, and `gradient_accumulation_steps`. You can reduce your memory-usage even more by enabling memory-efficient attention with [xFormers](../optimization/xformers) and using [bitsandbytes'](https://github.com/TimDettmers/bitsandbytes) 8-bit optimizer.
</Tip>

Text-to-image models like Stable Diffusion are conditioned to generate images given a text prompt.

Training a model can be taxing on your hardware, but if you enable `gradient_checkpointing` and `mixed_precision`, it is possible to train a model on a single 24GB GPU. If you're training with larger batch sizes or want to train faster, it's better to use GPUs with more than 30GB of memory. You can reduce your memory footprint by enabling memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing, gradient accumulation or xFormers. A GPU with at least 30GB of memory or a TPU v3 is recommended for training with Flax.
### Activation Checkpointing or Gradient Checkpointing

Activation checkpointing and gradient checkpointing are two distinct terms that refer to the same methodology. It's very confusing but this is how it is.

Gradient checkpointing allows one to trade speed for GPU memory, which either allows one to overcome a GPU OOM, or increase their batch size, which often leads to a better performance.

HF Transformers models don't know anything about DeepSpeed's activation checkpointing, so if you try to enable that feature in the DeepSpeed config file, nothing will happen.

Therefore you have two ways to take advantage of this very beneficial feature:
 
Here is the question: What problem does gradient checkpointing address in diffusion models?","Gradient checkpointing addresses the problem of significant memory overhead in diffusion models, which occurs when saving all activations from the forward pass to compute gradients during the backward pass."
What is the role of the `bitsandbytes` library in model quantization?,"[(29700, 'c1318e99-a283-4914-a7fa-45ad6c5e5852'), (8750, '269f08cc-32e9-4eea-9f7b-f529f1cc3c6a'), (8751, '8c359c4e-1177-4beb-9c6b-a20fac796132'), (17484, '478254f3-20db-4347-befe-c597c03a3811'), (17490, '7b2e415b-7669-4951-9061-4876c1558784'), (2645, 'c0529244-876b-4941-83d4-3c85320ab532'), (17498, '96fd2e8a-9317-4a82-9532-188a66603186'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (17499, '2341d406-75b9-48c6-9dfc-d86b8e87f7c7'), (21088, 'fb49fb03-082d-499d-ae0d-08b8fa2132f0'), (21089, '9e8976c4-72d9-49eb-9b17-5078f33abec4'), (5731, '0d967011-f6bb-453c-94d9-0c8031acf313'), (21101, '75d9cf28-7d39-4780-a31a-e9af5054cad5'), (30319, 'a04b6e09-b517-497d-bf98-2432285b7d43'), (21104, '4be870c7-46ed-4d02-81ad-3907f774e50d'), (21105, '4ac6228f-f922-4412-8d9f-4530484eb6b0'), (21106, '3f4d3e3a-83a1-46a8-90a5-473b57d5e518'), (30322, '7aab1247-fa08-4335-a051-0b4737639821'), (21108, '9da25174-bc1e-4da2-aa83-96c28c795297'), (21109, '0626c7f9-dffb-4329-86ab-21a3d6000823'), (30325, '16c886b9-b19c-44ea-9e7a-e5bd22b82ced'), (30327, 'a62abcc1-500d-4f5e-a758-64fcef6ee3e7'), (30326, '9527ab2d-2568-40f2-8af9-d9d87d77583c'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (2695, '38e95994-7560-49d4-8884-6a69c4b9f459'), (18059, '0c61ecab-c341-4e5e-9b6b-11f05c7bf24e'), (22163, '8d167b0c-14d6-4205-a383-f37c9f717a64'), (3736, 'e3e83e2c-6c23-41d7-8f7e-c1919c0feea6'), (22168, 'e6c58e6f-1ad9-42ec-beab-d54096522c9e'), (3738, '95bed131-0d04-468e-b476-46221cb94b4b'), (22169, '1eefa675-c3d2-4400-8957-842b99fce9c4'), (3739, 'a381d597-5605-449b-93b3-ff3cb73e810d'), (18084, '3aceb203-82f1-4360-b43a-cb660b8fb638'), (19110, '474cbf15-e581-41e8-99eb-61fe8a4c1c5d'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (28353, '841b803d-8a56-40f9-a606-deac23e25ac8'), (5839, 'd97b59e5-561b-4a61-bcff-51b7f943fa47'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (25302, 'bcf9a9f1-7a6b-4a62-87f7-68506603417d'), (25303, '9215e63c-2a98-4f8e-988b-31c2d71d12a2'), (25304, 'b80ca7fa-2902-4f9b-bfe3-492030f9ce0c'), (25305, '6126e14a-e501-46c0-8773-957993663f6d'), (21208, 'b21de659-6d35-4f86-be6d-7f65a8150522'), (25307, 'a1dddf5c-6872-4923-9d66-941863c5e2e6'), (25308, '8b6f2168-8617-45b3-a551-d649b58807c6'), (25309, '7255614e-d1fd-488d-8704-d06b7bedb54d'), (6359, 'd3604ded-fdc8-4b98-8dfc-e74be70af11d'), (25306, '9d30be4f-6f9f-4452-84ee-5219508fa7be'), (25310, '030268a0-245f-44fe-92c2-9d1acd73350d'), (27362, 'fcf527e3-17fe-43ea-9f64-d5d198cf9cbc'), (25317, 'e251888b-7855-4192-8a77-60936c25167a'), (25318, '7f293204-2059-483b-9146-91aa485398d2'), (25321, 'b7befe0e-86b5-4603-8086-b358c0606f7c'), (27379, '88aea9d0-3b36-4f80-986f-5040b1529bab'), (28917, 'efc6d6d5-fe91-453d-bbb1-a04031fcc67e'), (28919, '3e5a0f8b-f492-437e-94b3-8d7956333af5'), (25856, '00f40f52-58be-45e5-b0bb-73fca74a1420'), (21266, '92adf75c-feec-4fca-a3d6-d7ec3ab3c977'), (21269, '78ae6612-508c-47ad-9a00-a5911c9e8ecd'), (21272, '76efa72a-7e7a-4994-8958-c6de170f0c1c'), (21273, '6e7d8ca1-c6a3-4f3d-b7f4-d961cc88716d'), (21274, '6dc1af30-a439-4f39-baba-fb652d70f065'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (6946, '9c56617c-a3b0-434c-aa47-5100679e7ee5'), (16681, 'fc9a2a86-bdfe-4b9d-8e65-72359a9c1b04'), (16683, '8816fbb3-bf82-4453-886f-29f192a503f6'), (21804, '9912e115-c51c-4885-ad29-5a67bcb2572b'), (1839, '2e1f7f32-7b5b-44e1-b786-cda596c27fee'), (19259, 'f519d5f3-91ef-4576-8c06-268c476e2894'), (19260, '5426cc73-0d90-44a0-b3b1-ee7e31d8605c'), (19261, '828a7548-bebe-48da-bd43-915ef22bbb9b'), (19262, 'cb4dea97-cfcf-4d5d-827a-c683ab6dcaf2'), (1858, 'ebbabbf7-d895-47f9-abba-d2685cb8f307'), (19266, '0e059998-e258-4fbd-b3d1-8130a2c38a88'), (1859, '52a06e39-5439-41fa-b672-c8652dd2ef41'), (1861, '1fceace0-5b7f-450f-af60-1b82c2f8e316'), (1863, 'dc10a6f2-1b16-4604-beaa-3c00b3fbb844'), (1864, '9ba52445-c296-42f0-8c37-a67e6f6728e9'), (1867, 'ecfeb56f-f1a7-497d-a969-0db6183f60ed'), (1868, '2e91e1c2-545c-4276-b542-91f9bf500752'), (1869, 'c19409fc-e034-4449-82ff-40bb8a0531e0'), (14670, '4934aa67-784a-4d9b-b8a9-e356c97d7c55'), (14671, 'f63072f2-da40-472c-a639-1b77176258b8'), (1871, 'af5812d9-acbc-4fcc-be8c-9bde8b337014'), (14672, 'e0d05374-3982-41cc-91ba-8f6d65c9832c'), (1872, '4ab11bb3-d5e2-4c59-ac01-a9f3d8773989'), (14689, 'dc1279fd-9d80-4303-bb67-de1f9302e520'), (16752, 'b9ea5b0b-e8ef-459b-9671-c9a1f6973e32'), (15216, '3d8016fa-e696-4e96-9c3c-a99b70c16a07'), (16753, '74c08fca-b7ed-4bf0-8c9b-8b1bd698cc86'), (23939, '0c15a3e9-0bd1-48d4-ad86-562d0f7e7b19'), (18308, '3aa1ce96-60da-432f-b499-3b4d6c91a79d'), (20361, '38a40d70-31f1-4631-97f6-df1d876a9b8d'), (3999, '09d1277f-d722-4457-b060-831350863007'), (4010, 'f7201c35-6a48-44a3-b8a2-3b51387783a3'), (10667, '99592116-e7df-436c-aefb-86283541f025'), (14841, 'a52d04fc-e112-4cbf-9105-ae202b83ac4c'), (7599, 'c64e7824-8c85-482f-a5a9-75513d131cd6'), (7600, '035f5b2b-a0fa-409f-bca1-b121604c1b57'), (23475, '68a40f24-cf09-4158-a626-b4357e796ceb'), (7611, '209bae2d-e7fd-4a65-b444-97ce1d8e2034'), (31737, '1db4c46e-0450-4584-a576-b34a60bf4b35'), (4031, '2b10b56f-b719-4f10-b0a1-04970839e977'), (11714, '81248f40-6e7d-4225-8017-1c87a3d5c64a'), (11722, '1c4509b6-9894-4c45-8e40-b0099ab2f9ef'), (24023, 'bcd43be2-5718-48a7-891e-7e52f6ce3f48'), (26105, '2eacd23b-3305-4e74-9fc3-e986a9db3a0b'), (31739, 'ae5209af-c84b-4c9d-8cad-45339d0bf030')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

# Quantization

Quantization represents data with fewer bits, making it a useful technique for reducing memory-usage and accelerating inference especially when it comes to large language models (LLMs). There are several ways to quantize a model including:

* optimizing which model weights are quantized with the [AWQ](https://hf.co/papers/2306.00978) algorithm
* independently quantizing each row of a weight matrix with the [GPTQ](https://hf.co/papers/2210.17323) algorithm
* quantizing to 8-bit and 4-bit precision with the [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) library
In this guide, you'll see how to quantize a model to 4-bits and train it with LoRA.

## Quantize a model

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) is a quantization library with a Transformers integration. With this integration, you can quantize a model to 8 or 4-bits and enable many other options by configuring the [`~transformers.BitsAndBytesConfig`] class. For example, you can:

* set `load_in_4bit=True` to quantize the model to 4-bits when you load it
* set `bnb_4bit_quant_type=""nf4""` to use a special 4-bit data type for weights initialized from a normal distribution
* set `bnb_4bit_use_double_quant=True` to use a nested quantization scheme to quantize the already quantized weights
* set `bnb_4bit_compute_dtype=torch.bfloat16` to use bfloat16 for faster computation

```py
import torch
from transformers import BitsAndBytesConfig
## Comparing bitsandbytes and auto-gptq
In this section, we will go over the pros and cons of bitsandbytes and gptq quantization. Note that these are based on the feedback from the community and they can evolve over time as some of these features are in the roadmap of the respective libraries.

### What are the benefits of bitsandbytes?
**easy**: bitsandbytes still remains the easiest way to quantize any model as it does not require calibrating the quantized model with input data (also called zero-shot quantization). It is possible to quantize any model out of the box as long as it contains `torch.nn.Linear` modules. Whenever a new architecture is added in transformers, as long as they can be loaded with accelerate’s `device_map=”auto”`, users can benefit from bitsandbytes quantization straight out of the box with minimal performance degradation. Quantization is performed on model load, no need to run any post-processing or preparation step.
```

## bitsandbytes

[bitsandbytes](https://github.com/TimDettmers/bitsandbytes) is the easiest option for quantizing a model to 8 and 4-bit. 8-bit quantization multiplies outliers in fp16 with non-outliers in int8, converts the non-outlier values back to fp16, and then adds them together to return the weights in fp16. This reduces the degradative effect outlier values have on a model's performance. 4-bit quantization compresses a model even further, and it is commonly used with [QLoRA](https://hf.co/papers/2305.14314) to finetune quantized LLMs.

To use bitsandbytes, make sure you have the following libraries installed:

<hfoptions id=""bnb"">
<hfoption id=""8-bit"">

```bash
pip install transformers accelerate bitsandbytes>0.37.0
```

</hfoption>
<hfoption id=""4-bit"">

```bash
pip install bitsandbytes>=0.39.0
pip install --upgrade accelerate
pip install --upgrade transformers
```

You can return the original Transformers model with the [`~PreTrainedModel.reverse_bettertransformer`] method. You should use this before saving your model to use the canonical Transformers modeling:

```py
model = model.reverse_bettertransformer()
model.save_pretrained(""saved_model"")
```

## bitsandbytes

bitsandbytes is a quantization library that includes support for 4-bit and 8-bit quantization. Quantization reduces your model size compared to its native full precision version, making it easier to fit large models onto GPUs with limited memory.

Make sure you have bitsandbytes and 🤗 Accelerate installed:

```bash
# these versions support 8-bit and 4-bit
pip install bitsandbytes>=0.39.0 accelerate>=0.20.0

# install Transformers
pip install transformers
 
Here is the question: What is the role of the `bitsandbytes` library in model quantization?","The `bitsandbytes` library is a quantization library that allows for 8-bit and 4-bit quantization of models, making it a useful technique for reducing memory usage and accelerating inference, especially for large language models (LLMs). It provides an easy way to quantize any model out of the box, without requiring calibration with input data, and can be used to quantize models to 4-bits and train them with LoRA."
How does JAX’s `pmap` transformation aid distributed computing?,"[(8193, 'b151e333-2b84-4dc6-b21a-8331ad5dd90e'), (17933, 'c40f9fed-2aa0-440a-b827-f14dab4cc652'), (17934, 'a346e6f1-8dc6-4546-85c3-dac1062bf6ff'), (17936, 'ae3a8fdf-f301-49d1-b921-e4797db8704e'), (17939, 'eb167bfc-4875-47c0-84b4-14d6a25744e6'), (17943, 'b24c7665-16d4-463b-81bf-731e4b322785'), (17944, 'c6846f7e-7529-4813-b0ab-6c6cbf84a425'), (17945, 'f634cebc-3913-40a2-bb04-e90e1a825e90'), (27, 'b28f131a-f631-4757-8989-27d4dee5109f'), (12329, '61d3d00d-84f9-4e18-8942-1c97db6f41d3'), (18996, '6a19e0bf-7a11-49c8-9186-3e6ff36b3640'), (17475, '496f81a1-3559-4e5f-ae4f-25727935e2df'), (19526, '790c4385-8da2-4412-bf50-64e4a5ffadd3'), (19960, '9514cb32-fe98-4601-902d-2a0305125b86'), (29769, 'b7739621-6687-45e5-964d-cb69e022c6c8'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19535, '66073b8b-617e-48b6-8911-9bff81d7df7a'), (19536, 'fbbacd1f-478e-4c12-96f1-94f962921780'), (19539, 'd3738631-3ca2-4343-90d9-eb8b90b4e497'), (7258, '60ffe65d-54e6-4c62-a466-854687cd7645'), (19554, '393955e9-a5fd-4d28-828f-4a2e8a006c3e'), (19560, '2052afa9-9548-48e4-b160-94d2cd8fa764'), (19561, 'bd6d1959-f94f-4a44-b2c4-a94c6950eb8c'), (23661, '7627cc33-7b00-4214-b28d-a9e143e9b18c'), (13934, 'db89e881-3a64-4f86-b624-3f67721d2160'), (5743, 'c1d8b14c-c8ac-4b43-a04c-1e061196fc77'), (23662, 'da32bec6-7ab6-47fd-a980-4c6c292d570f'), (13942, '1df7a85d-88ed-4c36-be1e-842a7b62c454'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (13945, 'a94b0abd-2e7a-44ce-ad86-49755f562e31'), (13949, '19c1ac0e-ea20-4f0d-9183-4a6c4fe31c37'), (13950, 'f0690a10-f5cb-40cd-9add-24e45ac0cb39'), (13951, 'c8a5aeb5-996e-44aa-b45b-404446ed5208'), (21635, 'dc2413c5-b29d-4185-bd39-a90d0f73d0f1'), (13958, 'ed200096-8374-40a5-9473-b55aae1e78f8'), (17045, 'e20ede76-eeae-4f93-b5d6-e6926918562c'), (8344, 'a36aef0a-ec28-4a70-a315-653400a6cb8e'), (668, '249a03e0-c503-4df7-824c-7f0c50892b55'), (18589, '1b6a9bc8-bc9d-4c7d-8c7d-a90f1b043552'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (6852, '6314e627-a6f6-4b5b-81e6-111df0abd8a8'), (14539, '9665d62e-053d-4587-99f5-9bb20435af05'), (10976, '3512b41d-4bc5-4cfd-9d47-aeb0f8ecafb4'), (10982, 'df95faf9-a5f2-4147-8ee6-3fd41d74091b'), (18670, '52152586-5012-49cb-bc98-ffaee3f6db05'), (19695, '2b63b560-d4ac-4b5f-a51b-76f6465bf883'), (19696, '43e599c7-2548-47c5-9039-cfa92c630506'), (18673, '01aa33e5-c464-48e9-8045-2f268b684921'), (19698, '778b47fe-c721-4b84-bd17-18c49f7ade13'), (19697, '2bb5d1ca-7bcf-4074-b6a2-32572672320c'), (18674, '79b5c722-2889-43be-b08a-a5799e9159f3'), (19699, '10a481ad-92e0-4ff7-8bf5-1ec64a1e708f'), (18677, '224cde0b-46f2-44c1-b119-e01e490c24ef'), (19703, 'cee8ceac-098f-4297-bc31-778901f94db6'), (18678, '024c9ef4-b4db-48b5-af25-45f6f55f2032'), (19705, 'f9b19032-1bc7-4038-a9f5-4153aa763d07'), (16122, '7950be94-ecd4-4eda-b3d6-1fc80aacafd4'), (18679, '940bea7f-982b-4d25-af18-345fac8b5ccb'), (18681, '4c6e3cf8-615d-42a1-9e9a-0876eae1d3a5'), (18676, '19268cbe-5f8f-44b7-9603-fd3d64a79dc2'), (16126, 'c5005127-8511-4f0a-b4e9-bd0bc8366072'), (18686, '931719b8-bd25-4edd-baa3-fa21598c9bd0'), (5891, 'e903bbaa-0306-4955-8b4b-3f01e947eb7c'), (18675, '1da20ff6-a9fc-47bd-beda-e5b849c25412'), (13578, 'cb671a15-7afc-42fd-8983-3f6a6aa4e48e'), (21778, 'acec3fd7-6af1-4a2d-a7f6-80646ac73ff1'), (16147, '27008f92-4674-4cb1-b22a-6468630dc026'), (16150, 'ea3c23db-9446-48cf-9cff-b124d174ccdf'), (21785, '0f42515e-4921-41d6-a8f2-18c62768982e'), (16157, 'e8e24e1f-95a9-4f56-bd14-0192a666c31d'), (16162, '85d5b632-e607-41c6-b9dd-f35152ef7f21'), (16168, '8720f9c5-bd5a-4961-b1b7-7dd1d69cb57d'), (16169, '28f13635-acb7-4098-9b99-064a295d026c'), (16176, 'a7578109-e271-40f8-aa02-f89b5dddab55'), (19701, '35576b10-0e2e-436c-99d4-b50eddc1355c'), (19702, '1fcede80-72d5-462e-9e4d-bfc96074a009'), (10040, '41bd4b8c-cb79-4a90-b723-46c86076ed1f'), (10041, '61256d30-f3fb-4a77-b783-c6a4a918dd0e'), (16184, '08e02bad-54da-4851-82ae-02218bac7497'), (10044, 'bb24d82d-8c5b-4e60-a20b-488cfff229bb'), (10045, '8f8befd3-1f42-449f-9b31-97e81ad871fc'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (10047, '6ad5d7a1-4fad-4e6c-a3f8-3ff5071a694b'), (16196, '70e6ae83-91ce-42c2-abe6-64b6f74358cc'), (16197, '9a286ec4-bd86-44d0-839f-bdbc41b5d244'), (18759, 'a4f9107f-b29d-42b9-adf1-a7d1b294c213'), (16201, '74a34dcf-6a1b-4918-a45a-b5f73a7e5d91'), (29515, '18fb13c6-8761-4aea-ac01-1b771967088f'), (16209, 'd0d2483e-c27b-4df7-aa5f-4a72dee19295'), (16212, 'fe962485-2e4f-4020-a771-0916567be485'), (1364, 'f0e823a1-099a-4e51-a569-6a4ac48f1c04'), (16224, '9d437157-28f7-42be-a54d-c75d73e9dcd4'), (16225, '8209b394-4270-47c3-af24-bd1f76be97dc'), (16226, 'fba86705-5d78-4380-b949-e56866aaf769'), (16239, '948fe2a6-ea63-4bdf-ba5e-f3291774774f'), (19842, '177c114f-7a6c-4116-9c04-d60f549b7fac'), (19856, '1779d346-5968-4964-a7b4-39c87c76dd12'), (19388, '141d4b90-0e49-4309-98b8-fb6890009430'), (16843, '926901bf-b2bd-4869-8ac4-10833c3d0dd1'), (25048, '83289d50-3b81-466d-8fea-01489a0b35de'), (25063, '789ac356-c388-4d15-9aea-d440d41248e9'), (13287, 'cbc5c499-bc5e-4765-846b-17e1bf023c43'), (27116, 'ee44caad-d368-4c27-b4ad-92f672f5e3da'), (4088, 'd9fe20f3-82a5-4a79-b974-9c908fd23625'), (4090, 'b3adac99-bff6-43d3-9681-7c5e1b3ffb37'), (10750, '820e2551-3410-4210-a73c-32820cc67dc8')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Intro: JAX and Flax

[JAX](https://github.com/google/jax) is a numerical computation library that exposes a NumPy-like API with tracing capabilities. With JAX's `jit`, you can
trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. JAX
supports additional transformations such as `grad` (for arbitrary gradients), `pmap` (for parallelizing computation on multiple devices), `remat` (for gradient checkpointing), `vmap` (automatic
efficient vectorization), and `pjit` (for automatically sharded model parallelism). All JAX transformations compose arbitrarily with each other -- e.g., efficiently
computing per-example gradients is simply `vmap(grad(f))`.
```

## Quickstart flax and jax

[JAX](https://jax.readthedocs.io/en/latest/index.html) is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more. A great place for getting started with JAX is the [JAX 101 Tutorial](https://jax.readthedocs.io/en/latest/jax-101/index.html).
- **Parallelization**. Workloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute in parallel on XLA devices. For text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance.

👉 Try it out for yourself:

[![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/google/sdxl)

## Stable Diffusion XL pipeline in JAX

Upon having access to a TPU VM (TPUs higher than version 3), you should first install
a TPU-compatible version of JAX:
#### High-performance throughput for high batch sizes

Workloads can be scaled across multiple devices using JAX's [pmap](https://jax.readthedocs.io/en/latest/_autosummary/jax.pmap.html), which expresses single-program multiple-data (SPMD) programs. Applying pmap to a function will compile a function with XLA, then execute it in parallel on various XLA devices. 
For text-to-image generation workloads this means that increasing the number of images rendered simultaneously is straightforward to implement and doesn't compromise performance. For example, running SDXL on a TPU with 8 chips will generate 8 images in the same time it takes for 1 chip to create a single image.

TPU v5e instances come in multiple shapes, including 1, 4 and 8-chip shapes, all the way up to 256 chips (a full TPU v5e pod), with ultra-fast ICI links between chips. This allows you to choose the TPU shape that best suits your use case and easily take advantage of the parallelism that JAX and TPUs provide.
### Talks & Speakers

#### Skye Wanderman-Milne, JAX developer, Google Brain
- Talk: Intro to JAX on Cloud TPUs
- Abstract: JAX is a system for high-performance machine-learning research that combines the familiarity of Python + NumPy together with the power of hardware acceleration on CPUs, GPUs, and TPUs. It offers composable function transformations for automatic differentiation, automatic batching, end-to-end compilation, and both data and model parallelism. This talk will show you how to get up and running with JAX on a Cloud TPU VM. 
- Speaker info: Skye Wanderman-Milne is a software engineer working on JAX. She has previously worked on TensorFlow and Apache Impala, a high-performance distributed database.
 
Here is the question: How does JAX’s `pmap` transformation aid distributed computing?","JAX's `pmap` transformation aids distributed computing by expressing single-program multiple-data (SPMD) programs, which allows workloads to be scaled across multiple devices. Applying `pmap` to a function compiles it with XLA and executes it in parallel on various XLA devices, enabling parallelization of computation on multiple devices. This enables easy implementation of parallelism, such as increasing the number of images rendered simultaneously, without compromising performance."
What is the significance of the U-Net’s bottleneck layer in diffusion models?,"[(22017, 'b7368177-5672-4a62-92e3-e9ddd889c4a2'), (5634, '59a4e6a6-39fc-415a-9e3b-3691933b4ace'), (4611, '9ba60db2-6853-40a1-88bd-22525fbdb8e9'), (3593, '1985987b-6c12-4a20-89af-f6e17405b17c'), (12312, 'c4118a7b-9b80-4f95-971d-fca3336fda2d'), (15897, '11c88f2d-d482-4d24-9477-722810684ed0'), (15896, '533a0a9e-34b4-449c-bb00-101804823eab'), (1065, '8ea1935b-01c5-43c5-bcaf-64cbdd549f04'), (17456, '3d0b462d-4f6f-4e29-bdf7-d84ce48e8560'), (3128, 'cd116de5-9c26-431b-aacf-ee4e4b6e9331'), (15932, '2653d81f-316d-4342-aaea-37030a01bde8'), (11841, '6a43bb30-c717-4720-9f76-11b3df6f7412'), (15944, '805fc77a-02da-4894-9717-7755d57cbe70'), (7242, '7d32318b-4f2f-4493-8af3-ce5f81a7811a'), (26701, '3c9d16cf-eb89-4c9e-9e76-5bcbda107d24'), (589, '5f6c042b-5281-483f-b557-84661e4b6441'), (591, '06236243-318b-48f5-8a7d-cfb9b4f1e0d5'), (13396, 'e9acadfa-60e3-4c46-a14b-5b042e105d94'), (601, '8cac6f94-0281-42c2-83c8-e850306c4c98'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (3165, '02526a17-696d-4c06-80df-fba7911d02c9'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (14436, 'b6c8ba09-18f8-4b76-9196-bdead257e416'), (9831, 'b83751d5-6be9-4596-8944-5bf488470bc7'), (11373, '03316170-9bc8-479c-8653-d3b46bddaf64'), (14452, 'd602c8f8-3830-4f9e-8b34-1bea97e99242'), (14454, 'ce330ae8-8dec-4720-92e6-052305cb40dc'), (3195, '8ad2c051-95b7-4e6d-ba37-44db801a9218'), (6797, '3160d312-6e44-41aa-9a17-ee3fbb2b0aa2'), (6798, '9af83609-4ab8-4c19-9006-da849bf77897'), (6799, '7fdaa5a2-96ae-4629-98d8-4011d0f44647'), (9887, 'd038f03f-34b1-42f4-baae-413ab5287074'), (10933, 'b2789e50-f10a-4c41-9023-3c64d5949b58'), (10934, 'aaf1dd82-9ab3-4cbe-9630-792102f53a87'), (19128, '7822f723-18d9-492d-b2c1-5d48cb7de5cb'), (20152, '0680c6e7-1e57-4e37-9236-de4f5e13e357'), (19130, '10b2046b-33b3-4a66-9b66-dcdc83ec6e9b'), (29374, '221cd942-7551-4df5-9a02-d0f73e055431'), (19135, 'd743a82b-46bc-41fd-98b0-40e850c520b4'), (26304, 'f09ef0c4-c54a-4f32-a228-9fbfc29d0954'), (4297, '141bfea2-71c0-48b6-8a06-0ca33c6b56f7'), (17609, '4cdc2094-1a54-40b7-9a6e-05ea32af5ee7'), (4299, 'fa200320-e0ea-4961-b45c-b5ec32991353'), (31439, '44a37647-64bf-46ad-8a21-01bb8a1c4465'), (9429, 'f9b10760-2b6c-4577-999d-9ce25869b8f0'), (28374, '2f02209a-dbaf-4432-b002-393b2b850745'), (25815, 'd2945123-c528-4603-82cb-328b25d31cd5'), (25814, '1938526d-56ae-43fe-850c-1f0af6bb61fe'), (10972, '7eea2dce-2427-4dc8-8716-869bd8b7dfec'), (14560, 'eabb1851-01c3-427e-94d1-4cfe9008657f'), (9962, '49cc7639-d6b4-4703-8068-e5764e6ccce6'), (19695, '2b63b560-d4ac-4b5f-a51b-76f6465bf883'), (16626, '0f1fbca2-1b75-41e4-8f72-91511808c65a'), (24822, 'f4a6dad1-aea2-4131-a1fe-ebd9e58195d2'), (30455, '30b49335-6092-4e0b-876d-b396ad058840'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (9984, '6c3ad997-641d-4020-bcc6-5e8e6570b864'), (24333, 'fa763bfb-3bb5-4a40-81b9-1853bd3ab9d6'), (24335, '5c8d3462-bd2f-4fee-a05a-040246eb5ab8'), (9493, '8c059764-2d44-47c9-a65e-0c7442efc12e'), (23839, 'aac7f57f-4263-4e9e-bbee-db3bfed8ee29'), (23841, '5a99e4d4-7fec-4135-b408-ad9cae391ea4'), (3366, '67907026-cc7d-4490-8556-498e7e15245a'), (26410, '67e40473-eb86-4bc5-aadc-9904f3c372cf'), (10029, 'd1a5f9fc-00c4-4974-b71e-14d74422db37'), (10031, 'd311da63-e4f2-4661-9207-d5accca2e35e'), (26424, '768e99c8-37a5-4af5-babc-6888b57ffc55'), (26427, 'd5213d1c-651a-420e-9353-5b752301fe26'), (26432, 'bad5a360-cdaa-4da7-a6ac-bdcbaacffd05'), (26438, 'f329b776-81fd-470b-b8e7-ab77e1d208d0'), (26440, 'dd15fbf3-4799-4f4c-a2dc-34e82bccc429'), (12104, '91971395-1e61-4f8d-bc69-6e8b6191c233'), (10579, 'c5c569a7-6ff7-430d-adbd-b77fe1021f0d'), (5975, 'c2372c16-a0f7-4f31-b095-79df0d949f6e'), (27486, '8d60bbbd-8d6a-4b1d-a7bc-a7b44dd8b420'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (19814, 'd93ac1aa-7066-47be-9649-9e2fc04a3100'), (5991, 'ac3ddeae-3274-4fe4-8d77-2b3c3b80f768'), (5992, 'f9f87c8a-b463-4465-8bb2-190abe9a2c8d'), (5993, '6e16964e-c3be-43d1-b214-1db2ae5be924'), (5994, '393743ce-a942-4417-a7ed-66302d16103f'), (5996, '95d44852-e6f1-4950-8ede-d7cf96cd61ec'), (4462, '14e529ce-e387-47f4-9d38-a454c23e8bd7'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (13169, '3cd0b600-024b-4841-89f2-c5d09b3b1789'), (22904, 'a678407d-0ca1-4128-a53e-8ab8dccdddf1'), (6008, 'f1ddc133-b500-4631-9de5-add5ec3ca0f4'), (22908, '20cf93bb-0193-4309-9639-b4265308de33'), (22910, '94bd2f01-8327-4f2c-9cee-d36f8b154579'), (22911, 'beaca8ce-2939-41e0-b621-52375e694804'), (22912, '76059485-1ebf-444f-97ba-17fcdb6926b5'), (22913, '612f4cf1-11c1-4c60-a277-04526e7b4db4'), (22921, '28568e53-fad9-4e99-96dd-6932ad9d07a3'), (22922, 'f7b5b230-09cb-483f-9a29-efce17dee03a'), (26514, '9fcaf21e-79bb-4839-864d-0d857fa15240'), (17302, '10950558-fd37-474e-868f-4f3fca1d098a'), (17814, 'b31d2dac-db5f-4be3-b6f5-ef844ffc9f36'), (3997, '2662a32f-d93d-48c2-afe0-d243cab54e4c'), (4000, 'ea805f3c-9994-4780-9d7a-b121ffed6c14'), (4012, 'b1e161a0-d490-4161-880d-9be316427ee3'), (17363, '509f9576-4f0e-4f9e-a4f6-a2956a1427f9'), (17364, '375858e6-fad8-49d7-9f08-70b4ff973b3e'), (15315, '7e744e9a-2d02-41eb-8d2f-acb54e6336bf'), (982, '4b1ed7e5-38de-47d6-b5e5-a776db6fd6bc'), (21465, '2d5e5ed9-2cc4-48ea-9db1-ba33c3f993a7'), (10212, '0d28df65-2cae-4253-8b4c-c834f7ec7e1f'), (10213, 'bc603003-9845-4bf8-8b72-7ce0fa07fe77'), (10214, 'e03caf42-9c1a-43dc-b3eb-b895a3ef2f2b')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: In terms of architecture, the DDPM authors went for a **U-Net**, introduced by ([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in [He et al., 2015](https://arxiv.org/abs/1512.03385)).

<p align=""center"">
    <img src=""assets/78_annotated-diffusion/unet_architecture.jpg"" width=""400"" />
</p>

As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.

Below, we implement this network, step-by-step.

### Network helpers
To prevent the U-Net from losing important information while downsampling, short-cut connections are usually added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.
Additionally, the stable diffusion U-Net is able to condition its output on text-embeddings via cross-attention layers. The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks.

**3. The Text-encoder**

The text-encoder is responsible for transforming the input prompt, *e.g.* ""An astronaut riding a horse"" into an embedding space that can be understood by the U-Net. It is usually a simple *transformer-based* encoder that maps a sequence of input tokens to a sequence of latent text-embeddings.
The AR models section provides additional context on VQ-Diffusion's architecture in comparison to AR transformer based models.

[Taming Transformers](https://arxiv.org/abs/2012.09841) provides a good discussion on converting raw pixels to discrete tokens in a compressed latent space so that transformers become computationally feasible for image data.

### VQ-Diffusion in Context

#### Diffusion Models

Contemporary diffusion models are mostly continuous. In the forward process, continuous diffusion models iteratively add Gaussian noise. The reverse process is approximated via \\( p_{\theta}(x_{t-1} | x_t) = N(x_{t-1}; \mu_{\theta}(x_t, t), \Sigma_{\theta}(x_t, t)) \\). In the simpler case of [DDPM](https://arxiv.org/abs/2006.11239), the covariance matrix is fixed, a U-Net is trained to predict the noise in \\( x_t \\), and \\( x_{t-1} \\) is derived from the noise.
From the description above, the latency bottleneck in text generation is clear: running a model forward pass for large models is slow, and you may need to do hundreds of them in a sequence. But let’s dive deeper: why are forward passes slow? Forward passes are typically dominated by matrix multiplications and, after a quick visit to the [corresponding wikipedia section](https://en.wikipedia.org/wiki/Matrix_multiplication_algorithm#Communication-avoiding_and_distributed_algorithms), you can tell that memory bandwidth is the limitation in this operation (e.g. from the GPU RAM to the GPU compute cores). In other words, *the bottleneck in the forward pass comes from loading the model layer weights into the computation cores of your device, not from performing the computations themselves*.
There are three main components in latent diffusion.

1. An autoencoder (VAE).
2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).
3. A text-encoder, *e.g.* [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).

**1. The autoencoder (VAE)**

The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the *U-Net* model.
The decoder, conversely, transforms the latent representation back into an image.
 
Here is the question: What is the significance of the U-Net’s bottleneck layer in diffusion models?",The significance of the U-Net's bottleneck layer in diffusion models is that it ensures the network learns only the most important information.
Why is the cosine schedule used in DDPMs?,"[(4610, '91bf9f33-527d-4a97-adb6-7cdcece4bf07'), (21010, '70ec8d78-e610-4197-a105-1af1c66df8af'), (29724, 'a843f61a-3301-45a6-9603-b1bc63e38f86'), (7711, 'e4b0e922-63ff-4afa-8d0b-9cc18df28ed7'), (10288, '35f2aeaa-7fc6-4c71-a486-d8f3b84c7027'), (3129, 'eb945bfb-2047-4a55-a268-2f7b9efc42f6'), (9791, 'ce574839-7f39-42b8-a9d1-3afd2011cc48'), (9792, 'c23887e9-c847-42df-b582-76d15b62b7e1'), (9793, '86da3069-1a46-483f-ade8-1bff25b33a5e'), (9794, 'a588cee3-37ac-472d-9b7f-80645fe6441e'), (10819, '9fb4e5bf-b2d1-4c28-ac14-c7b7fd6f77ae'), (26693, '74a1767c-cb28-44b5-9581-88def78019d3'), (10319, '470961b7-eb81-40c7-90e6-f3f7d301f59f'), (7249, 'f9cd9cd6-3fa2-4f34-9adc-fa2a88591a19'), (10321, 'f303db9f-073b-4ede-835a-a64e0d8de2c7'), (10324, '3ce38d9e-1405-46d9-9ea9-78ae13f3f5d4'), (598, '8c1684a0-1743-4eb1-aeb6-2c2e62f2083e'), (601, '8cac6f94-0281-42c2-83c8-e850306c4c98'), (10337, '02bff3e1-743b-4cc3-8bc1-032c1727470a'), (97, 'c110d4b5-e404-4749-8851-600a26eec5e4'), (24172, '4f8609c4-09c5-42aa-a3f7-3fddea20bbd2'), (22128, '9dc88224-d7e5-4e97-85a2-af881342560f'), (1149, '77759709-4e75-47e6-a833-e9b8e4b7a27a'), (22143, 'fd6c94ce-f2ca-497d-837c-43f6371df07c'), (8833, '0cf36ca7-5234-4b5d-9605-db7972ebe611'), (13471, '49ab1615-bd1a-4824-8e15-7308d94ff607'), (10920, 'f18f64ad-7a3c-461d-9f41-0ffa5327fb77'), (29865, 'df2de640-3fc7-4776-823b-4a2e97b33336'), (20143, 'd869b9f1-cd4d-4d20-955b-89d5173b1e6e'), (20144, '48868250-7cd6-416a-999d-65c68f28be99'), (24759, '7d4b5301-47a7-43b7-80ac-321205ef5402'), (24760, 'c49ebc66-d22e-413f-9fb9-b16fceb219ac'), (4281, 'd190bfa4-359f-41ab-881e-ddb36b3307dd'), (29370, 'ca968d3d-5098-450d-88b3-1aa6aa81fa32'), (4280, 'c91cc95e-4c1b-4e75-a090-e529f4e4cb58'), (24761, 'fbfcbbfc-71b8-450b-8253-d38bfe2126d7'), (19134, 'e3688ad7-d66d-424f-8be8-c607f1178ce8'), (7874, '9157ec2c-4628-4f73-8e40-85b3d1e9a454'), (7875, 'a763d775-7294-4d44-8464-f9d0ed3d3740'), (29380, '64537a00-0a4c-47b1-a56a-c01303abbe99'), (17602, '02d83b80-c573-483d-803d-5418fb631225'), (8905, 'da3a0a41-1061-454a-b081-9bf8282a3cc5'), (25816, '0aee80dc-2820-4f8b-a131-2042458d62e0'), (2794, '7f53439e-3da7-4e0d-a687-691e2a07c5ae'), (15595, 'ffbb7210-d87c-4059-ac9f-65eeb9d458ad'), (9457, '2a6577a6-538d-48fa-a6dd-f51131667055'), (9460, '2a8ccede-e4a9-4f50-b6cb-f0e496b15028'), (25334, '33427231-4e65-4935-ba92-1cda98b9c5fa'), (8957, 'd821fc5e-a896-4f11-b3ca-931a9c70ca93'), (6924, '07867a49-23ec-4818-bb67-47e96c80acd7'), (10000, '11b6cc75-c6fd-4e28-86d3-d42657a01b52'), (10001, 'c5de7ffb-cc03-4862-8a62-4a088a4c4470'), (30995, '2482d3c6-479e-411a-9276-d86969cedbe9'), (3367, 'f2781c19-ab56-4d16-9bad-9ef7f4c970f1'), (10537, 'dba8ae3c-a95e-4638-aa41-2e2ea7884bd2'), (10538, '1bfaf9c3-f829-4171-8dc9-e762693b30c5'), (19250, '8c61f212-aee7-4c19-9c1b-c2e21bfd2b05'), (826, 'fdefe76b-2b8f-43ca-9f3b-5ec8c236c70a'), (27457, '020fc58b-f7e4-483c-b336-a484a010112b'), (12100, '1bbf45ef-90da-4af0-9bc6-54ff09763654'), (12101, 'bc396373-21b5-4b13-a876-c3c4dd9f1ded'), (24906, '4f6d9603-05a1-44d8-b626-d6aeed01a2f5'), (31051, '220ea5d2-c24c-4a78-92ac-b85dc4aa2649'), (24907, '1eae9929-97de-41e0-8204-1e713785cb91'), (24909, '82290a82-2504-4656-a125-e4ba230d33ea'), (24910, '93139bb7-0ac6-44de-b998-dccfe267409d'), (24911, '8d336559-acfd-4d67-a0fb-e707975cad03'), (24908, 'e5fe80b3-8b91-4599-8072-37edf5ea77fd'), (24913, 'af339238-6e3a-486f-a56d-373f29afefef'), (24914, 'fea84df6-95a4-47fb-9d7b-e7715c6895a3'), (29009, '517b0bd9-f6a2-4e14-8423-495ff426a3c1'), (9556, '4ebcb0ec-bfc7-44ca-92cd-96ee50f00f5f'), (10580, '5b2828bb-b3d9-4084-bbf1-9a510e142f1f'), (24414, '4b4fcbc2-95df-49ad-bed5-96229886d628'), (24935, '3ae70e6a-bbb4-47bd-986b-93416ef571a9'), (5996, '95d44852-e6f1-4950-8ede-d7cf96cd61ec'), (6001, 'c6e36804-ea90-4217-a933-f50539a76b4b'), (13174, 'a62f183d-35f1-48bf-a9b1-178a591da0e3'), (4473, 'b3950e16-b914-45c3-b8d8-44747bb14f36'), (15228, '511e53d3-26de-49ea-9af5-149d1dd1600a'), (17279, '5faa6213-4a1f-423e-b057-1bd034cbd5b9'), (24451, '6d1f1c1e-57b0-4897-9db1-acd4c0385df8'), (24452, '24f7284d-65f7-4323-8260-c5c056cc1ede'), (10630, '5cce9ad3-d5cc-4672-915a-e0266aa5d36f'), (14732, 'ab8b7633-875a-4767-8f8f-5fa64020c6cf'), (14733, '021331ca-6368-4f8c-8770-4a5163394244'), (29068, 'b7ffe6ea-3757-459f-876e-2b0f489486cc'), (14734, '9f167c0f-fe33-4e9e-9b55-f5744e17ed77'), (17308, '67caf671-206f-4914-84b0-6b4997f8d43b'), (14243, 'ba11a1ce-1bb3-4063-81de-fb499ec3dca3'), (6565, '7cc358fa-67fa-4da6-8331-9465d5bb3852'), (6566, '7e754e32-6e2e-49f0-813c-ee2d0328b432'), (17322, '32fa00cd-d7cb-4a4b-be23-b9b6cc645954'), (17323, 'c05c8d93-d9fd-4eda-aea0-2daab9c5a902'), (429, '37891b99-81aa-4352-8a5e-a2de4c2a3a6b'), (15279, 'e865e23f-6b79-4d30-9dbf-77fd945bbdf9'), (17338, 'a0b3b3cd-27fe-400e-b4e7-f1457df2f6a9'), (22972, '66d2f02e-7cda-4b16-857c-abb040bd13dc'), (22973, '83ae4353-8f08-4b09-810b-7070fdf20989'), (15301, '1eb2e2b0-a68b-4fb8-84ca-85eb3633a2b1'), (15302, 'e4baa76c-3e22-4be0-b525-70fa6339f3af'), (12771, '570ac182-24ba-4ab1-b9c7-dd9e116176e1'), (7656, '2096fae3-5c5f-4d71-bc5c-119a58ffa760'), (12266, '72ef4d58-fd26-49dd-9120-c763209691e4'), (22509, '6347b9ca-d5cf-4268-90d0-931bfc1400dd'), (19950, '61f98945-5fc0-4952-850e-216527180361'), (19951, 'a9e8c627-a75f-47ee-8b76-a48fe1e5c86c'), (19949, 'e039d6f8-4d3e-483d-be54-a1f3ba60842a'), (19954, '22a16eed-47fb-4d60-839e-218906969e9a'), (19957, '2575cfc3-15d9-458d-a8c4-39544f731e1b'), (26614, 'ef74dcbf-21f2-4413-b74f-69aee8ff4bc3'), (19958, 'a5878f32-2a7f-4f07-b78b-35d9e33c6d22'), (508, '5fc182c0-bab6-4aa4-b5b5-b075bc875330'), (5629, '4607a00e-38e1-452d-843d-5b57232b547e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Defining the forward diffusion process

The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps \\(T\\). This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:

> We set the forward process variances to constants
increasing linearly from \\(\beta_1 = 10^{−4}\\)
to \\(\beta_T = 0.02\\).

However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. 

Below, we define various schedules for the \\(T\\) timesteps (we'll choose one later on).
```

Alright! We've got a long list of schedulers to choose from 📝. By default, AudioLDM 2 uses the [`DDIMScheduler`](https://huggingface.co/docs/diffusers/api/schedulers/ddim), 
and requires 200 inference steps to get good quality audio generations. However, more performant schedulers, like [`DPMSolverMultistepScheduler`](https://huggingface.co/docs/diffusers/main/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler), 
require only **20-25 inference steps** to achieve similar results.
Below, we define various schedules for the \\(T\\) timesteps (we'll choose one later on).

```python
def cosine_beta_schedule(timesteps, s=0.008):
    """"""
    cosine schedule as proposed in https://arxiv.org/abs/2102.09672
    """"""
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0.0001, 0.9999)

def linear_beta_schedule(timesteps):
    beta_start = 0.0001
    beta_end = 0.02
    return torch.linspace(beta_start, beta_end, timesteps)

def quadratic_beta_schedule(timesteps):
    beta_start = 0.0001
    beta_end = 0.02
    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2
Let's see how we can switch the AudioLDM 2 scheduler from DDIM to DPM Multistep. We'll use the [`ConfigMixin.from_config()`](https://huggingface.co/docs/diffusers/main/en/api/configuration#diffusers.ConfigMixin.from_config) 
method to load a [`DPMSolverMultistepScheduler`](https://huggingface.co/docs/diffusers/main/en/api/schedulers/multistep_dpm_solver#diffusers.DPMSolverMultistepScheduler) 
from the configuration of our original [`DDIMScheduler`](https://huggingface.co/docs/diffusers/api/schedulers/ddim):

```python
from diffusers import DPMSolverMultistepScheduler

pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
For finegrained detail like faces, we find that lower learning rates along with larger batch sizes work best.

For stage II, we find that lower learning rates are also needed.

We found experimentally that the DDPM scheduler with the default larger number of denoising steps to sometimes work better than the DPM Solver scheduler
used in the training scripts.

### Stage II additional validation images

The stage II validation requires images to upscale, we can download a downsized version of the training set:

```py
from huggingface_hub import snapshot_download

local_dir = ""./dog_downsized""
snapshot_download(
    ""diffusers/dog-example-downsized"",
    local_dir=local_dir,
    repo_type=""dataset"",
    ignore_patterns="".gitattributes"",
)
 
Here is the question: Why is the cosine schedule used in DDPMs?","According to the context, the cosine schedule is used in DDPMs because it was shown in [Nichol et al., 2021] that it achieves better results compared to the linear schedule used by the original DDPM authors."
How does the `Trainer` API in JAX/Flax simplify training workflows?,"[(17934, 'a346e6f1-8dc6-4546-85c3-dac1062bf6ff'), (1552, '7e1ef89e-8e2d-4b43-99cd-4e18f3216c29'), (1561, '2e093792-5611-47fa-9b0f-dccd79bce526'), (9754, '380dcb90-6ffe-42be-b20f-4f82a170b72a'), (12845, 'e33c4e68-81c4-4331-94e6-4b285dfcb094'), (18996, '6a19e0bf-7a11-49c8-9186-3e6ff36b3640'), (21558, 'efcd6a87-bc74-47c5-9ca8-20b911fcc57d'), (23607, '20d9cc82-5737-4053-943d-f16253935f72'), (23610, '518b5322-f18f-40d8-83b1-05a9605ad48c'), (30267, 'b5e7c466-a75d-4b41-ac19-d3cc183dfa7c'), (26684, '1efbd9e1-3025-4ea8-b6e6-ebe753a2f8a5'), (23616, '630bda0b-d880-4c34-9b9a-c90e67ba3ace'), (26688, '06c43668-d169-4991-9447-1d6ea35000d1'), (23617, 'c8a7fdd0-c469-442b-a16b-5be8c3025099'), (23619, 'e3503d8b-c3cf-42a9-a89b-fb4782e91edc'), (17475, '496f81a1-3559-4e5f-ae4f-25727935e2df'), (3654, 'e30cd7b7-ba8e-4ac6-9d9c-811f2e1be257'), (1609, '335231ef-3cdd-4154-bea0-92054d45edc3'), (10832, '075db094-d18b-4a58-bf74-8de8e487e77a'), (7258, '60ffe65d-54e6-4c62-a466-854687cd7645'), (1121, 'cf8c83be-c90e-47be-bca1-73be139d2220'), (24677, 'a3630e8d-f4f3-41a6-b62a-fc3337fd7713'), (4717, '3a237531-c4e8-4ab9-a982-09465c5ed278'), (25710, 'd79930fe-1968-4b54-987e-bc987274c7b2'), (5744, '8a324259-975e-4404-a067-26a272142939'), (5745, '77c39426-5363-43cb-9fee-bfc8d242613d'), (24180, '6fd43eed-f7b8-4151-808f-472f264f2aa2'), (18047, '88a9e52b-eec2-4a57-933d-f10b57098f70'), (21635, 'dc2413c5-b29d-4185-bd39-a90d0f73d0f1'), (3715, 'c4518fec-8276-4e52-964a-6148955da040'), (13957, '6452d1b9-cf35-4d81-aa93-772441ecdc1d'), (21641, '39b96774-1139-4c90-afc1-322f44ffaa96'), (13962, 'a7c31e26-ea04-4485-80fd-732f660daa25'), (18589, '1b6a9bc8-bc9d-4c7d-8c7d-a90f1b043552'), (21149, '93f58500-464d-41bb-8670-d75c3cba97ae'), (5795, '04034e54-e11b-4eac-a732-f90dde842501'), (29860, '73e8f57e-95b0-4751-958e-e49ebe330cac'), (29861, '937ac036-7437-4cef-b2ca-6e68352b7f7f'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (6852, '6314e627-a6f6-4b5b-81e6-111df0abd8a8'), (14538, '627bd8ba-f103-4cac-a069-e6a9b4e8c86d'), (14539, '9665d62e-053d-4587-99f5-9bb20435af05'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (8939, 'b814884d-260a-4040-b9e1-768b2bee9c41'), (19696, '43e599c7-2548-47c5-9039-cfa92c630506'), (26864, 'f883d67f-5980-4aa8-8e7d-82fff8a9aca8'), (19699, '10a481ad-92e0-4ff7-8bf5-1ec64a1e708f'), (18678, '024c9ef4-b4db-48b5-af25-45f6f55f2032'), (16122, '7950be94-ecd4-4eda-b3d6-1fc80aacafd4'), (26381, '0f4fa531-16f7-44b5-bdfa-96a15fcea391'), (26385, 'b3479e96-0b28-4112-8184-54b7abce069b'), (16147, '27008f92-4674-4cb1-b22a-6468630dc026'), (28949, '839a2dca-2d5f-49ff-938f-273a57b11a6b'), (16150, 'ea3c23db-9446-48cf-9cff-b124d174ccdf'), (28953, '6e1ad46f-6cdb-4457-8fc1-a17e2442ec7d'), (16157, 'e8e24e1f-95a9-4f56-bd14-0192a666c31d'), (3361, '82963c16-f4cb-4ef3-a57d-1025a4c09fe8'), (16168, '8720f9c5-bd5a-4961-b1b7-7dd1d69cb57d'), (16169, '28f13635-acb7-4098-9b99-064a295d026c'), (16172, 'b7a87971-1bc4-4771-bdaa-c67e3b637648'), (16176, 'a7578109-e271-40f8-aa02-f89b5dddab55'), (10040, '41bd4b8c-cb79-4a90-b723-46c86076ed1f'), (16185, '91f950e5-c9c1-40fb-a64e-ffaa632b3f4d'), (10041, '61256d30-f3fb-4a77-b783-c6a4a918dd0e'), (16184, '08e02bad-54da-4851-82ae-02218bac7497'), (10044, 'bb24d82d-8c5b-4e60-a20b-488cfff229bb'), (10045, '8f8befd3-1f42-449f-9b31-97e81ad871fc'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (10047, '6ad5d7a1-4fad-4e6c-a3f8-3ff5071a694b'), (16192, 'ead14a30-856c-4c68-b676-f32d311a4375'), (17727, '83dae02e-ed27-46ed-9d51-5ac0e180afef'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (16196, '70e6ae83-91ce-42c2-abe6-64b6f74358cc'), (16197, '9a286ec4-bd86-44d0-839f-bdbc41b5d244'), (19782, 'c22f8dcb-277f-4ba0-a48f-276448ff8b7a'), (16200, '6bd03780-adda-4fc1-9d3b-32057e28a091'), (16201, '74a34dcf-6a1b-4918-a45a-b5f73a7e5d91'), (845, '3c529c99-efdc-4766-b82a-fc0e95ad9cfd'), (4947, 'ce283e08-0d74-4343-a585-eb31e4533555'), (11617, '832a382e-e9c3-4108-9e67-a72962c4a5a6'), (354, '2a268797-e79e-4063-843a-7cbe133cb03e'), (17254, '28529e61-3225-47e0-8759-0e9a4884dd37'), (7527, 'fc516ac8-263b-47e2-981b-5c6954f368e1'), (21865, 'aed3a1e4-ca73-4239-a584-5c016709d32b'), (16239, '948fe2a6-ea63-4bdf-ba5e-f3291774774f'), (25470, '1b28100d-5003-49dd-8be3-1fe8b7596a96'), (24968, '6e58d9ed-7fd1-469a-bd90-305d0bc248f7'), (5006, '318b6e92-fdd0-4aac-b970-90072c7c4bfa'), (7569, '767086c3-8554-4dac-a175-2097c23fe2dd'), (31656, '65796113-22a8-4ff8-a251-ca0750bfb8fe'), (29097, '6ff1fe79-75da-49ba-bb26-0184494f69a1'), (29096, '71cbab35-3ce2-4f7f-af3f-132c47bf50ab'), (16815, 'eef3bcbc-cbb5-4733-aa46-b4a36551b90b'), (3506, '4e992d07-de67-463d-9af5-8cad259a8ab0'), (25523, '9153511f-0378-46e2-b0fc-33c95548e246'), (16306, '1b1de27b-61a6-438a-b0fe-099dd4b762e5'), (5558, '9ec5833c-607b-4bc4-a91a-de2935c9e48a'), (16843, '926901bf-b2bd-4869-8ac4-10833c3d0dd1'), (16844, '14804510-d68b-43ef-b630-fea72e07be9a'), (28626, '000dd359-4e6d-4100-81f5-36125aa0e5d3'), (22488, '848e97ad-839a-47dc-bb11-539523b1ab46'), (31192, '82e7acc7-5423-4531-84b3-3dde89b388e6'), (27098, '4dbebf8f-6846-4cf6-b5c4-39d614dd7a87'), (8667, '83f4e0e5-f70b-4a54-acef-2b200d5689bb'), (8669, '8f464916-06ad-48e4-85c4-3ae8a88c6ac2'), (30177, '244726ac-82fc-433d-91db-7a414866651f'), (30178, '9c3784c8-013e-454a-a71d-ca6d3cc6283f'), (2539, 'ce5f995f-2dfe-405b-ab3d-94f57ccd71ba'), (2540, '2eca103c-f25b-4c3c-b688-c58ef25404f2'), (27116, 'ee44caad-d368-4c27-b4ad-92f672f5e3da')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: *NOTE*: Currently, there is no ""Trainer"" abstraction for JAX/Flax -- all examples contain an explicit training loop.

The following table lists all of our examples on how to use 🤗 Transformers with the JAX/Flax backend:
- with information about the model and dataset used,
- whether or not they leverage the [🤗 Datasets](https://github.com/huggingface/datasets) library,
- links to **Colab notebooks** to walk through the scripts and run them easily.
```

We use `jax.jit` to compile the function to get maximum performance. Note that in the above example, we set `padding=max_length` to pad all examples to the same length. We do this because JAX's compiler has to recompile a function everytime its input shape changes - in a sense a compiled function is not only defined by its code but also by its input and output shape. It is usually much more effective to pad the input to be of a fixed static shape than having to recompile every the function multiple times.


#### **How to write a training loop**

Now let's see how we can write a simple training loop to train Flax models, we will use `FlaxGPT2ForCausalLM` as an example.
## Intro: JAX and Flax

[JAX](https://github.com/google/jax) is a numerical computation library that exposes a NumPy-like API with tracing capabilities. With JAX's `jit`, you can
trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. JAX
supports additional transformations such as `grad` (for arbitrary gradients), `pmap` (for parallelizing computation on multiple devices), `remat` (for gradient checkpointing), `vmap` (automatic
efficient vectorization), and `pjit` (for automatically sharded model parallelism). All JAX transformations compose arbitrarily with each other -- e.g., efficiently
computing per-example gradients is simply `vmap(grad(f))`.
Summarization (Seq2Seq model) training examples

The following example showcases how to finetune a sequence-to-sequence model for summarization
using the JAX/Flax backend.

JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.
Models written in JAX/Flax are **immutable** and updated in a purely functional
way which enables simple and efficient model parallelism.

`run_summarization_flax.py` is a lightweight example of how to download and preprocess a dataset from the 🤗 Datasets library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.

For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets#json-files and you also will find examples of these below.

### Train the model
Next we can run the example script to train the model:
Each example README contains more details on the specific model and training
procedure.


## Running on single or multiple GPUs

All of our JAX/Flax examples also run efficiently on single and multiple GPUs. You can use the same instructions in the README to launch training on GPU.
Distributed training is supported out-of-the box and scripts will use all the GPUs that are detected.

You should follow this [guide for installing JAX on GPUs](https://github.com/google/jax/#pip-installation-gpu-cuda) since the installation depends on
your CUDA and CuDNN version.

## Supported models

Porting models from PyTorch to JAX/Flax is an ongoing effort. 
Feel free to reach out if you are interested in contributing a model in JAX/Flax -- we'll 
be adding a guide for porting models from PyTorch in the upcoming few weeks.

For a complete overview of models that are supported in JAX/Flax, please have a look at [this](https://huggingface.co/transformers/main/index.html#supported-frameworks) table.
 
Here is the question: How does the `Trainer` API in JAX/Flax simplify training workflows?","There is no `Trainer` API in JAX/Flax, and all examples contain an explicit training loop. Therefore, it does not simplify training workflows."
What challenge does 8-bit quantization address in deploying large-scale models?,"[(12289, 'fe670b2c-6435-4e7f-8e1f-e1058894e410'), (17487, 'dae5694d-d613-4e40-85bc-34903ea8a2be'), (2645, 'c0529244-876b-4941-83d4-3c85320ab532'), (85, '09af5d1e-345c-4f7f-836d-b759982966b5'), (2650, 'cbef60ea-38ed-4e9a-9a58-5692ec67d873'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (21089, '9e8976c4-72d9-49eb-9b17-5078f33abec4'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (5731, '0d967011-f6bb-453c-94d9-0c8031acf313'), (14437, '6f4d674a-5ac8-464d-8dde-f98b8f555e87'), (30319, 'a04b6e09-b517-497d-bf98-2432285b7d43'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (30322, '7aab1247-fa08-4335-a051-0b4737639821'), (30325, '16c886b9-b19c-44ea-9e7a-e5bd22b82ced'), (30326, '9527ab2d-2568-40f2-8af9-d9d87d77583c'), (30327, 'a62abcc1-500d-4f5e-a758-64fcef6ee3e7'), (21109, '0626c7f9-dffb-4329-86ab-21a3d6000823'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (2695, '38e95994-7560-49d4-8884-6a69c4b9f459'), (18059, '0c61ecab-c341-4e5e-9b6b-11f05c7bf24e'), (22162, '020e1b99-33f3-4bed-9103-d758cf7f98a4'), (22163, '8d167b0c-14d6-4205-a383-f37c9f717a64'), (22166, '10bb5bda-4d63-4d38-a475-fbb3876f8082'), (22167, '27793adb-fb93-47e7-bbf6-90ccaa62cc27'), (3736, 'e3e83e2c-6c23-41d7-8f7e-c1919c0feea6'), (3737, '515efb7f-1ab1-41a2-89e5-5c66253070fa'), (3738, '95bed131-0d04-468e-b476-46221cb94b4b'), (22168, 'e6c58e6f-1ad9-42ec-beab-d54096522c9e'), (8348, '98b2c4b0-c732-4141-8e11-330c1cd787e8'), (22173, 'c6da122a-96dc-44f4-931e-80f5975b4687'), (22172, 'c6f094fb-9441-4569-90b2-0fa48d57a1a3'), (8881, 'b3952e5f-6672-4957-9666-15ca6d043573'), (28353, '841b803d-8a56-40f9-a606-deac23e25ac8'), (5839, 'd97b59e5-561b-4a61-bcff-51b7f943fa47'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (6358, 'c5b8b78f-d9a9-483d-8f87-70642fe0c191'), (25303, '9215e63c-2a98-4f8e-988b-31c2d71d12a2'), (25304, 'b80ca7fa-2902-4f9b-bfe3-492030f9ce0c'), (25305, '6126e14a-e501-46c0-8773-957993663f6d'), (25306, '9d30be4f-6f9f-4452-84ee-5219508fa7be'), (5851, 'f900f9e7-c44b-445a-901e-adcb0acc6add'), (5850, '434464e7-67bb-4e01-b697-e32aab332035'), (21208, 'b21de659-6d35-4f86-be6d-7f65a8150522'), (27362, 'fcf527e3-17fe-43ea-9f64-d5d198cf9cbc'), (25321, 'b7befe0e-86b5-4603-8086-b358c0606f7c'), (25841, '944d2878-310e-4406-a580-e27546617a5b'), (27378, '69fd6e93-802f-4dfc-a4ff-6caf775f803d'), (21266, '92adf75c-feec-4fca-a3d6-d7ec3ab3c977'), (21269, '78ae6612-508c-47ad-9a00-a5911c9e8ecd'), (21272, '76efa72a-7e7a-4994-8958-c6de170f0c1c'), (21273, '6e7d8ca1-c6a3-4f3d-b7f4-d961cc88716d'), (21274, '6dc1af30-a439-4f39-baba-fb652d70f065'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (6946, '9c56617c-a3b0-434c-aa47-5100679e7ee5'), (16683, '8816fbb3-bf82-4453-886f-29f192a503f6'), (1839, '2e1f7f32-7b5b-44e1-b786-cda596c27fee'), (21807, 'b3346506-6c4a-48ac-9d75-c1258f287641'), (21818, '73d48f15-6f37-4445-aa1f-d7942fd7ca06'), (19259, 'f519d5f3-91ef-4576-8c06-268c476e2894'), (19260, '5426cc73-0d90-44a0-b3b1-ee7e31d8605c'), (19261, '828a7548-bebe-48da-bd43-915ef22bbb9b'), (19262, 'cb4dea97-cfcf-4d5d-827a-c683ab6dcaf2'), (1858, 'ebbabbf7-d895-47f9-abba-d2685cb8f307'), (1859, '52a06e39-5439-41fa-b672-c8652dd2ef41'), (1861, '1fceace0-5b7f-450f-af60-1b82c2f8e316'), (14663, '4814da72-44c5-416d-84cf-7d56aa314256'), (1864, '9ba52445-c296-42f0-8c37-a67e6f6728e9'), (14664, '10a5d466-9698-40d2-aa8e-c005dc0608d1'), (1866, '1fac8bfb-b415-40ae-a7d3-380e95ecad4a'), (1867, 'ecfeb56f-f1a7-497d-a969-0db6183f60ed'), (1868, '2e91e1c2-545c-4276-b542-91f9bf500752'), (1863, 'dc10a6f2-1b16-4604-beaa-3c00b3fbb844'), (14670, '4934aa67-784a-4d9b-b8a9-e356c97d7c55'), (14671, 'f63072f2-da40-472c-a639-1b77176258b8'), (14672, 'e0d05374-3982-41cc-91ba-8f6d65c9832c'), (1871, 'af5812d9-acbc-4fcc-be8c-9bde8b337014'), (30545, 'cd0b21b5-7027-43c2-846c-b2ed65d53de5'), (14676, '888122bb-5af6-412f-b7ef-7cc898f025ea'), (14679, '941c9eda-09fb-46bf-a0c8-12c756f40a6a'), (14685, 'f6385cc5-2ea5-401f-bace-7cbe69ca1700'), (27485, 'c8923acb-7c31-4de9-8212-92c682de819e'), (14699, 'd175660e-257c-44d7-8313-c03464f55f38'), (14708, '395e14d3-5924-41c7-ad35-eaa5772f82d1'), (23939, '0c15a3e9-0bd1-48d4-ad86-562d0f7e7b19'), (20361, '38a40d70-31f1-4631-97f6-df1d876a9b8d'), (3997, '2662a32f-d93d-48c2-afe0-d243cab54e4c'), (3999, '09d1277f-d722-4457-b060-831350863007'), (27039, '36d3c0d7-f037-410f-bf80-edce8404e6bf'), (10658, 'd6ec2162-dac4-467f-8b2c-399544dc7154'), (4010, 'f7201c35-6a48-44a3-b8a2-3b51387783a3'), (10667, '99592116-e7df-436c-aefb-86283541f025'), (4012, 'b1e161a0-d490-4161-880d-9be316427ee3'), (10668, '7cf2d4f8-152f-450c-bdd9-5b4a138844ce'), (7599, 'c64e7824-8c85-482f-a5a9-75513d131cd6'), (7600, '035f5b2b-a0fa-409f-bca1-b121604c1b57'), (23475, '68a40f24-cf09-4158-a626-b4357e796ceb'), (7604, '76a18b88-7a2a-4930-8f66-1b41521667f6'), (4031, '2b10b56f-b719-4f10-b0a1-04970839e977'), (11714, '81248f40-6e7d-4225-8017-1c87a3d5c64a'), (11719, '1ed4be54-1156-4f04-a0e0-7df8819c9045'), (11722, '1c4509b6-9894-4c45-8e40-b0099ab2f9ef'), (11231, '3125e614-0163-4e1c-ac40-acb21059ede0'), (10729, 'de922e06-e6ad-46b7-a335-22673ac01fd5'), (31737, '1db4c46e-0450-4584-a576-b34a60bf4b35'), (31738, '7d8861cd-9646-4d02-9213-734c5cbe5597'), (31739, 'ae5209af-c84b-4c9d-8cad-45339d0bf030')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The crux of these challenges lies in augmenting the computational and memory capabilities of LLMs, especially when handling expansive input sequences.

In this guide, we will go over the effective techniques for efficient LLM deployment:

1.  **Lower Precision:** Research has shown that operating at reduced numerical precision, namely [8-bit and 4-bit](./main_classes/quantization.md) can achieve computational advantages without a considerable decline in model performance.

2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.
The deep learning community has created world class tools to run
resource intensive models on consumer hardware:

- [🤗 accelerate](https://github.com/huggingface/accelerate) provides
utilities for working with [large models](https://huggingface.co/docs/accelerate/usage_guides/big_modeling).
- [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) makes [8-bit quantization](https://github.com/TimDettmers/bitsandbytes#features) available to all PyTorch models.
- [🤗 safetensors](https://github.com/huggingface/safetensors) not only ensures that save code is executed but also significantly speeds up the loading time of large models.

Diffusers seamlessly integrates the above libraries to allow for a
simple API when optimizing large models.

The free-tier Google Colab is both CPU RAM constrained (13 GB RAM) as
well as GPU VRAM constrained (15 GB RAM for T4), which makes running the
whole >10B IF model challenging!
After completing the training of BLOOM-176B, we at HuggingFace and BigScience were looking for ways to make this big model easier to run on less GPUs. Through our BigScience community we were made aware of research on Int8 inference that does not degrade predictive performance of large models and reduces the memory footprint of large models by a factor or 2x. Soon we started collaboring on this research which ended with a full integration into Hugging Face `transformers`. With this blog post, we offer LLM.int8() integration for all Hugging Face models which we explain in more detail below. If you want to read more about our research, you can read our paper, [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339).

This article focuses on giving a high-level overview of this quantization technology, outlining the difficulties in incorporating it into the `transformers` library, and drawing up the long-term goals of this partnership.
Therefore, we asked ourselves the following question: how far can we go with just data parallelism? Can we use existing tools to fit super-large training processes (including active model, reference model and optimizer states) in a single device? The answer appears to be yes. The main ingredients are: adapters and 8bit matrix multiplication! Let us cover these topics in the following sections:

### 8-bit matrix multiplication

Efficient 8-bit matrix multiplication is a method that has been first introduced in the paper LLM.int8() and aims to solve the performance degradation issue when quantizing large-scale models. The proposed method  breaks down the matrix multiplications that are applied under the hood in Linear layers in two stages: the outlier hidden states part that is going to be performed in float16 & the “non-outlier” part that is performed in int8.
# Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA


LLMs are known to be large, and running or training them in consumer hardware is a huge challenge for users and accessibility. 
Our [LLM.int8 blogpost](https://huggingface.co/blog/hf-bitsandbytes-integration) showed how the techniques in the [LLM.int8 paper](https://arxiv.org/abs/2208.07339) were integrated in transformers using the `bitsandbytes` library.
As we strive to make models even more accessible to anyone, we decided to collaborate with bitsandbytes again to allow users to run models in 4-bit precision. This includes a large majority of HF models, in any modality (text, vision, multi-modal, etc.). Users can also train adapters on top of 4bit models leveraging tools from the Hugging Face ecosystem. This is a new method introduced today in the QLoRA paper by Dettmers et al. The abstract of the paper is as follows:
 
Here is the question: What challenge does 8-bit quantization address in deploying large-scale models?","The challenge that 8-bit quantization addresses in deploying large-scale models is the memory footprint of large models, reducing it by a factor of 2x without a considerable decline in model performance."
What distinguishes DDPMs from traditional VAEs?,"[(4610, '91bf9f33-527d-4a97-adb6-7cdcece4bf07'), (11267, 'c3f0d944-1538-4736-bc5b-f1e7ab372a64'), (23563, 'e7685caa-1084-4d53-bf74-95a103f346bb'), (19475, 'c390cb49-1370-4600-ab31-315185ca3061'), (23, 'b4a4cc6d-32ee-4dde-b079-7db3389dd5ee'), (26, '3b03cc49-afcd-4150-90d5-1381b35995f2'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (23596, '5d4cd8af-5359-4005-8a1a-8f899353667e'), (28717, '45cc6367-7b38-4f30-a36f-c96fb2293202'), (9263, '2e1affcb-5fd4-4b9b-a787-6ddb6248fbfb'), (9264, 'd7098fc1-c855-434f-99b6-87edff2baa20'), (8250, 'e76202f5-cd28-46d6-8985-23081190b092'), (9792, 'c23887e9-c847-42df-b582-76d15b62b7e1'), (10816, 'bd308ba6-1d27-413a-b87e-202b891b21fc'), (9794, 'a588cee3-37ac-472d-9b7f-80645fe6441e'), (9793, '86da3069-1a46-483f-ade8-1bff25b33a5e'), (26693, '74a1767c-cb28-44b5-9581-88def78019d3'), (18501, 'dc889574-ebe9-462c-8e67-4cf4a649d73e'), (26191, '364a33fb-16f8-44a9-9a00-f1ccb6431ee0'), (21080, '8a17b393-7880-420d-9b0d-415445f53d4a'), (14431, '0deb95ce-33eb-4b01-9c27-f4c096addec3'), (14432, 'af38595a-44d8-4995-a41e-5175299ae8a2'), (21088, 'fb49fb03-082d-499d-ae0d-08b8fa2132f0'), (30307, '5e90fb0f-59a7-4aba-9b15-7eb5bfd43dae'), (23652, 'cd12ea63-d36d-4a93-854c-2280becede75'), (21097, '56e065d6-6f6a-4e60-95f0-a7ebdb267c73'), (13420, 'b3b15a23-f570-442e-ac93-6a55de104978'), (11888, '498213c1-4575-47f9-8b4b-a7af4db9d535'), (6801, 'a8d073e8-4092-47e2-b7f0-c61300a489a7'), (6802, '816d52f3-ceec-436a-b89e-8e94e93f67cd'), (31380, '3c434488-f490-468e-82c9-102eae828e2d'), (23197, '654b3017-275c-4c8f-b6d9-4af107ef2e82'), (12446, '127582a4-61cc-41b0-8528-e5f0a328eb67'), (17058, 'cde6daf7-a56c-4f14-a8ed-763addee809e'), (28334, '5b42e27e-2adb-427f-8ae4-75d895ed8bb1'), (20143, 'd869b9f1-cd4d-4d20-955b-89d5173b1e6e'), (20144, '48868250-7cd6-416a-999d-65c68f28be99'), (10933, 'b2789e50-f10a-4c41-9023-3c64d5949b58'), (20155, '04e7e95a-9eb0-4302-a0ec-07716aa382f2'), (7875, 'a763d775-7294-4d44-8464-f9d0ed3d3740'), (14027, '34f160cc-0f0d-4ba1-8d11-6810d3aa7404'), (23757, 'd4eb4874-d76a-420c-b113-e69ec9305542'), (23760, '49c11fd5-4984-4710-bb5b-cda2a2da40af'), (23762, '8161842a-e5a7-4131-bb2d-89e612d5a0ec'), (23763, '0331ca31-194a-49b0-aa36-b9a61017c85b'), (23769, 'c48ce51e-eb78-4fbc-b7e8-bc6ece6613fc'), (18652, '2f14ded4-19ec-4e64-8d9c-194d33f2e588'), (23772, '85e8ff8e-fa16-48e1-873d-cceead77f9da'), (10974, 'bdd949bd-5d4e-41ac-a597-4cb8a5e35df4'), (23774, '5b9abd62-ab9f-417b-ad88-2d00d44b3aac'), (23773, '251879f0-7c7b-43f5-bc25-78b94fede4c0'), (23777, 'e8d6bb71-ce43-4ba7-bdcb-8d0d203337a7'), (19678, 'dd83df43-fcf2-44ff-9dc0-6fa37b7755f2'), (25322, '38f73fd4-1863-4e9d-b938-de882d1679e7'), (23788, '56c682f7-2a5e-419a-b067-a91384f00487'), (23789, '1d90878f-1c99-451b-b6b9-c6a02aa3ee99'), (15605, '9f241ce4-3d04-4868-8576-832003581357'), (8955, '11133dce-6d7c-45cf-b8e6-fa0180834552'), (25852, 'e68a74a5-1c65-4315-9cac-e0006e78744c'), (20733, 'e8fff71e-b36a-40c5-920d-7bc903c8cd18'), (20734, 'e475d05a-9d7b-4171-89f4-6d26a5e946ce'), (9987, '1b0502d4-7475-4478-a949-b6e132394dad'), (21254, 'b435763a-f888-4e05-8003-8a28ace9a2fa'), (9991, 'aa0b5922-9e54-4d5b-8d48-9a9b30cc5bca'), (10000, '11b6cc75-c6fd-4e28-86d3-d42657a01b52'), (30995, '2482d3c6-479e-411a-9276-d86969cedbe9'), (3861, 'b29474d6-97e9-4a9a-87ad-fa2fb569cd9f'), (16663, 'd5bcc95f-8f00-4c1e-b318-b1bc54a9556c'), (5919, 'bef2fd9d-5d6e-4d07-a8fe-1fc9829a8f50'), (10537, 'dba8ae3c-a95e-4638-aa41-2e2ea7884bd2'), (17705, 'fe0b5515-c6a2-4316-b6a4-97cb643fb598'), (17707, '7d843880-b155-47da-8ec3-8562f8ef5dd3'), (12093, 'dfbb7831-7079-43b1-9589-a3bd4cc7ec8d'), (26432, 'bad5a360-cdaa-4da7-a6ac-bdcbaacffd05'), (12100, '1bbf45ef-90da-4af0-9bc6-54ff09763654'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (29515, '18fb13c6-8761-4aea-ac01-1b771967088f'), (22348, '0bcda2c2-c1e5-4dab-8526-15fba410752c'), (24909, '82290a82-2504-4656-a125-e4ba230d33ea'), (24910, '93139bb7-0ac6-44de-b998-dccfe267409d'), (24911, '8d336559-acfd-4d67-a0fb-e707975cad03'), (22867, 'cf69de79-909e-41e0-bdb0-38f008054173'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (3936, '21ff933b-dbd5-4cf7-9f50-2463e0091ea6'), (866, '39bf3695-c19e-4820-8d4b-021d27842d1e'), (13155, 'feb24b04-1aed-4866-9b73-320a1d8566b1'), (5991, 'ac3ddeae-3274-4fe4-8d77-2b3c3b80f768'), (21863, 'aac9a314-37da-4a6a-aab1-6357236f5979'), (19817, 'af742c64-40e4-4642-bc2d-7e03a3de6aba'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (10613, '0835f4fb-6088-4ffa-9b78-b5b2e6fd58d4'), (13174, 'a62f183d-35f1-48bf-a9b1-178a591da0e3'), (22904, 'a678407d-0ca1-4128-a53e-8ab8dccdddf1'), (13182, 'f1036252-4b58-4aef-922f-760793d82b3e'), (22911, 'beaca8ce-2939-41e0-b621-52375e694804'), (3459, 'bc1a44b5-f64b-4598-85e8-e3280271e021'), (19855, '9800f1ae-0b10-4f47-8b9b-9a97f69d8a08'), (8080, 'd6a21ffd-e82d-40f0-873f-40124dc6c6eb'), (26514, '9fcaf21e-79bb-4839-864d-0d857fa15240'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (414, '2818f552-96f0-4e01-8d6e-401758ad6440'), (23456, 'd1baa005-a401-42d2-924c-aece54f9448a'), (31650, 'b7d49d66-3894-4743-b041-a97c182c96a7'), (31652, '70e0a2f0-75da-42e8-ba35-f26d73b35790'), (30629, '7cf3310b-91ee-4c5f-a7ea-66073b3a9197'), (4529, '795ebffd-b6ef-42a5-ba9b-9d869516b585'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (29634, 'd756cc17-3381-4c73-99de-56f9d281f772'), (15301, '1eb2e2b0-a68b-4fb8-84ca-85eb3633a2b1'), (22469, '5b0182c1-368d-4526-a777-d0b2826a76a5'), (26063, '4c2cca9d-6bfd-40a3-9719-9f3190fbb2c8'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (13274, '41eb6ff6-8266-4419-a9ef-542c1c699d07'), (19950, '61f98945-5fc0-4952-850e-216527180361'), (21999, '03e7888c-7a45-4f5e-b53c-3ab0fac74369'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (6136, '818cf127-78a6-43ae-aad7-568810131041'), (6138, '32092b65-62d5-4162-9056-8238a416fa04')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Key differences include:
1. DDP performs only a single communication per batch - sending gradients, while DP performs five different data exchanges per batch.
DDP copies data using [torch.distributed](https://pytorch.org/docs/master/distributed.html), while DP copies data within 
the process via Python threads (which introduces limitations associated with GIL). As a result, **`DistributedDataParallel` (DDP) is generally faster than `DataParallel` (DP)** unless you have slow GPU card inter-connectivity.
2. Under DP, GPU 0 performs significantly more work than other GPUs, resulting in GPU under-utilization. 
3. DDP supports distributed training across multiple machines, whereas DP does not.

This is not an exhaustive list of differences between DP and DDP, however, other nuances are out of scope of this guide.
You can get a deeper understanding of these methods by reading this [article](https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/).
```

### Attention module

Next, we define the attention module, which the DDPM authors added in between the convolutional blocks. Attention is the building block of the famous Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which has shown great success in various domains of AI, from NLP and vision to [protein folding](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology). Phil Wang employs 2 variants of attention: one is regular multi-head self-attention (as used in the Transformer), the other one is a [linear attention variant](https://github.com/lucidrains/linear-attention-transformer) ([Shen et al., 2018](https://arxiv.org/abs/1812.01243)), whose time- and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.
For finegrained detail like faces, we find that lower learning rates along with larger batch sizes work best.

For stage II, we find that lower learning rates are also needed.

We found experimentally that the DDPM scheduler with the default larger number of denoising steps to sometimes work better than the DPM Solver scheduler
used in the training scripts.

### Stage II additional validation images

The stage II validation requires images to upscale, we can download a downsized version of the training set:

```py
from huggingface_hub import snapshot_download

local_dir = ""./dog_downsized""
snapshot_download(
    ""diffusers/dog-example-downsized"",
    local_dir=local_dir,
    repo_type=""dataset"",
    ignore_patterns="".gitattributes"",
)
- `--pretrained_teacher_model`: the path to a pretrained latent diffusion model to use as the teacher model
- `--pretrained_vae_model_name_or_path`: path to a pretrained VAE; the SDXL VAE is known to suffer from numerical instability, so this parameter allows you to specify an alternative VAE (like this [VAE]((https://huggingface.co/madebyollin/sdxl-vae-fp16-fix)) by madebyollin which works in fp16)
- `--w_min` and `--w_max`: the minimum and maximum guidance scale values for guidance scale sampling
- `--num_ddim_timesteps`: the number of timesteps for DDIM sampling
- `--loss_type`: the type of loss (L2 or Huber) to calculate for latent consistency distillation; Huber loss is generally preferred because it's more robust to outliers
- `--huber_c`: the Huber loss parameter

## Training script
```

## Defining the forward diffusion process

The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps \\(T\\). This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:

> We set the forward process variances to constants
increasing linearly from \\(\beta_1 = 10^{−4}\\)
to \\(\beta_T = 0.02\\).

However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. 

Below, we define various schedules for the \\(T\\) timesteps (we'll choose one later on).
 
Here is the question: What distinguishes DDPMs from traditional VAEs?","The context does not provide a direct answer to this question. The provided text discusses the differences between `DataParallel` (DP) and `DistributedDataParallel` (DDP), attention modules, and the training process of DDPMs, but it does not compare DDPMs with traditional VAEs. Therefore, I cannot provide an answer based on the given context."
How do positional embeddings improve U-Net performance in DDPMs?,"[(5634, '59a4e6a6-39fc-415a-9e3b-3691933b4ace'), (3593, '1985987b-6c12-4a20-89af-f6e17405b17c'), (27167, '9d246d60-330f-44ca-83ab-b6504683772e'), (20511, 'e3f039c3-f4ff-41a6-8680-ee6405668c98'), (27169, 'ef659728-4a97-4d27-8573-90e994336628'), (27173, '983485c5-6ba7-45d7-b986-08f63ce1b7c9'), (27174, 'c8ddd9c5-a470-4e5e-a92f-cb193a1e9864'), (1065, '8ea1935b-01c5-43c5-bcaf-64cbdd549f04'), (16450, '1373652c-6ba1-4274-9e77-d49afc9643e5'), (5192, 'f0c1ae46-5e1a-477b-8412-ca030b12f9d7'), (5193, 'dc3be556-c42a-492f-a1fb-4f6b48ff1251'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (14418, 'f50616ce-ba15-4e58-9fc9-30dbc62fb3c6'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (27247, 'cd291d57-56e6-453a-bccc-2b0e4efc75ad'), (27248, '7de32b72-9533-4ed3-9432-f0472e04b630'), (27249, 'ef91d98f-12c7-4ded-b85e-50b19106c776'), (27254, '4b2bccdd-2f9a-47bd-ae29-e1173c47a752'), (27255, 'a125f862-3445-42d4-af55-48b04362ee3d'), (27256, '7c4b9f40-01ce-4f3f-a45a-b4135cbd07b1'), (27259, '3c90ac1d-393d-4c22-a641-7c67afbabcec'), (5251, '7c3ead60-fe80-47eb-914d-5a2a0e5f870b'), (21638, 'f8e9a69f-a8f3-4f46-b11b-4b8bbbc27046'), (30343, '6b30a0cf-b903-492c-aba3-abc1809c663c'), (30344, 'ae4f7e23-8e67-4c7f-b808-814a47d3fe3c'), (30345, '81d6924f-0377-4a2a-9407-b39fd99660f2'), (30346, '3ca14458-1809-410f-b237-5dc0bd2720a2'), (30347, 'dc4d1756-b572-4c18-b319-f6dd879c47c5'), (30348, 'f6668c3c-4667-4787-aeda-897bd6fae57b'), (30350, '1007dace-e5a5-4299-92d8-9267b5da0106'), (30351, '4e28a951-5f38-4baa-a7eb-ed581e6ebc55'), (30352, 'e34ec7cc-9c0c-4715-b4d4-576fda1c7384'), (30353, 'cc4924ec-cef1-442c-8130-ebd3cdbc2a73'), (6800, 'ee670cef-b763-4a5c-9337-23b201f5aa8c'), (6804, '826c3e69-3171-40c6-ac38-a8532bfed277'), (15000, '7287e381-09dd-4681-81f4-f97ba32a6962'), (6809, '7fbff821-b6d1-4244-a28c-69605cb78fe5'), (15524, '1cd78884-efe0-4e8c-b808-d863d5468d98'), (11953, 'e5b47149-4a3e-4549-a9ef-c12eccfe493f'), (3249, 'ec516057-e475-45fa-aecd-ef5b655717e6'), (25290, '16b47a24-aff2-46e9-8b5f-87234f7699d4'), (15071, 'fbdaf006-31a1-4636-8fee-7b1e5d22d62b'), (11498, '83b1e05b-1af1-4740-b7b6-e3913543f76d'), (11499, 'e00fbed1-8a0d-4b1e-9e12-b1cddb314ed6'), (5356, 'e8a64bb4-fd7e-400d-b1f7-e9e8086dc4f5'), (16620, 'a72f7a54-12cc-4ba4-8676-4d4b76e1c760'), (9454, '9de797fe-8ad7-43f0-86df-89345e1c4416'), (16626, '0f1fbca2-1b75-41e4-8f72-91511808c65a'), (6900, '0ae7974c-5457-40e4-a353-fde028bee870'), (9979, '7442e7f0-33dd-496d-be20-9411f3dc1952'), (9982, '6f7f2051-b7da-4c82-8622-fe78a2738ec0'), (9984, '6c3ad997-641d-4020-bcc6-5e8e6570b864'), (9991, 'aa0b5922-9e54-4d5b-8d48-9a9b30cc5bca'), (23815, 'aac364dc-25b6-43cd-ba00-4ca3fae9b528'), (12047, '129e9113-1c4c-482f-8100-332f2ee98acd'), (27428, '613f45c2-d949-4116-ae2b-fed0f576e4a6'), (5928, '46517678-0861-4d86-b92b-eb731ae44060'), (21290, 'e04fb004-041f-4f28-863d-a2968988f978'), (21291, 'd94831c8-62b9-435c-b23d-e06b0423c02c'), (21292, '60c69841-6056-4e49-8d8c-834a90d0c15d'), (21293, 'ebc28c0e-857d-4867-9601-cdfa4cfae480'), (21294, 'e6172056-b7c7-452c-ab01-538ad0fd7920'), (21295, '94d02d6d-1cb5-4ea0-9ac8-dc36118f209d'), (21297, '740159e7-7fe1-43cc-9f5b-bbadd645c40e'), (21298, '3097670a-0ae0-4b8f-8336-fb8dd2422462'), (21299, '5c646f09-89d4-4d89-ba4a-4e458dd441dd'), (21300, 'f0e21b7f-680e-4412-a2bd-22eb73f5663f'), (23869, '26cd5348-6284-4c3b-96a3-3840b630697b'), (4421, '95a1272e-1f19-494f-96d5-f36f9a919a0a'), (20815, 'ff1e9e29-e8eb-4d43-9d9b-3c8a293051c3'), (29010, '23fb57fd-95c6-4c84-b85b-a01a35375df9'), (13150, 'da5b6fd3-746b-4e1a-a44a-549e924acbc6'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (17249, '01606722-6969-4f23-82bd-af700f892a6a'), (17250, '79b1326c-df62-47fb-8e4f-45c1f9d1281f'), (5992, 'f9f87c8a-b463-4465-8bb2-190abe9a2c8d'), (5993, '6e16964e-c3be-43d1-b214-1db2ae5be924'), (19817, 'af742c64-40e4-4642-bc2d-7e03a3de6aba'), (5996, '95d44852-e6f1-4950-8ede-d7cf96cd61ec'), (25969, '624cc538-26a8-47b9-82b6-b472bbd600eb'), (2930, '1cdeeab8-de41-4c2c-99e9-580b9279f768'), (2931, 'b85421b6-58d5-49ce-8dbd-70bb715186aa'), (8052, '4294f244-5d07-4073-911b-5dbf073918f4'), (22901, 'cff5ab68-f6d2-4261-847c-f144213d34c2'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (24459, '80257add-a9c0-4393-94c5-24ca8d8a8aed'), (13710, '6fb979d2-f0c4-4378-b614-2ebc9633c051'), (29592, '18ed3dbc-ab73-4bfa-92a9-ed4c35d08d04'), (25502, '1e14a118-369d-45a5-a43c-c4f140b294f9'), (2981, '1de1ff9b-fd9d-49d6-a529-b3b495d6a646'), (2983, '3fd0211d-bce9-4d38-9579-1fce22759529'), (25512, 'cbae7083-ee95-445b-bb27-fe5359398387'), (2985, '6ad9cc82-7643-4e0a-8a93-d62ad4e98de4'), (30638, 'bb43e4ba-f6ab-478d-8c31-6535c42d1276'), (30639, '72a35cdd-880b-4981-b0a0-ad666c3d6a77'), (23479, '9a2f2e4d-3e0b-45a4-98c7-8c3f308d9692'), (17862, 'c38fdf0b-0cba-4cbb-89ac-b453d9cbae5e'), (5575, '3ec6e7c7-caec-4432-8b39-d2cff30a6c72'), (15308, '6ccab088-3827-44e4-bcc0-3d8aa4d278c0'), (8142, 'aa2a8de3-4853-49a2-b469-e2a9e51aa467'), (7120, 'a7653516-ad78-4c53-a897-57cab61017cf'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (483, '49f5b7f4-c37a-4a20-b448-385219fca936'), (6627, '65f5633b-0855-44a8-93ab-9901a2e2e66d'), (12267, '0ce7c7ae-e1e4-4869-9ffe-8a2ccd12a259'), (24052, '84c02b74-a2ee-4cd8-aa9f-a42fad6738b8'), (6648, '28f56d90-d4e8-4b81-8601-33b9ca1a78bd'), (12281, '3f0cae05-aecf-43a7-b229-5f381c5335c7'), (6654, '0b0b6cb8-92ce-48f8-af7b-d14e0f5f071d')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Once trained, the fundamental LLM architecture is difficult to change, so it is important to make considerations about the LLM's tasks beforehand and accordingly optimize the model's architecture.
There are two important components of the model architecture that quickly become memory and/or performance bottlenecks for large input sequences.

-   The positional embeddings
-   The key-value cache

Let's go over each component in more detail

### 3.1 Improving positional embeddings of LLMs

Self-attention puts each token in relation to each other's tokens.
As an example, the \\( \text{Softmax}(\mathbf{QK}^T) \\) matrix of the text input sequence *""Hello"", ""I"", ""love"", ""you""* could look as follows:

![](/blog/assets/163_optimize_llm/self_attn_tokens.png)
Once trained, the fundamental LLM architecture is difficult to change, so it is important to make considerations about the LLM's tasks beforehand and accordingly optimize the model's architecture.
There are two important components of the model architecture that quickly become memory and/or performance bottlenecks for large input sequences.

-   The positional embeddings
-   The key-value cache

Let's go over each component in more detail

### 3.1 Improving positional embeddings of LLMs

Self-attention puts each token in relation to each other's tokens.
As an example, the \\( \text{Softmax}(\mathbf{QK}^T) \\) matrix of the text input sequence *""Hello"", ""I"", ""love"", ""you""* could look as follows:

![](/blog/assets/163_optimize_llm/self_attn_tokens.png)
From a practical point of view, the question of positional embeddings is also a crucial methodological aspect with computational efficiency trade-offs. Relative positional embeddings (introduced in Transformer-XL and used in Compressive Transformers) are appealing because they can easily be extended to yet-unseen sequence lengths, but at the same time, relative positional embeddings are computationally expensive. On the other side, absolute positional embeddings (used in Longformer and Linformer) are less flexible for sequences longer than the ones seen during training, but are computationally more efficient. Interestingly, [Shortformer](https://arxiv.org/abs/2012.15832) introduces a simple alternative by adding the positional information to the queries and keys of the self-attention mechanism instead of adding it to the token embeddings. The method is called position-infused attention and is shown to be very efficient while producing strong results.
Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence order into LLMs, but a couple of problems related to these positional encodings were found:

  1. Sinusoidal and learned position embeddings are both absolute positional embeddings, *i.e.* encoding a unique embedding for each position id: \\( 0, \ldots, N \\) . As shown by [Huang et al.](https://arxiv.org/abs/2009.13658) and [Su et al.](https://arxiv.org/abs/2104.09864), absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position.
  2. When using learned position embeddings, the LLM has to be trained on a fixed input length \\( N \\), which makes it difficult to extrapolate to an input length longer than what it was trained on.
Sinusoidal and learned position embeddings used to be the predominant methods to encode sentence order into LLMs, but a couple of problems related to these positional encodings were found:

  1. Sinusoidal and learned position embeddings are both absolute positional embeddings, *i.e.* encoding a unique embedding for each position id: \\( 0, \ldots, N \\) . As shown by [Huang et al.](https://arxiv.org/abs/2009.13658) and [Su et al.](https://arxiv.org/abs/2104.09864), absolute positional embeddings lead to poor LLM performance for long text inputs. For long text inputs, it is advantageous if the model learns the relative positional distance input tokens have to each other instead of their absolute position.
  2. When using learned position embeddings, the LLM has to be trained on a fixed input length \\( N \\), which makes it difficult to extrapolate to an input length longer than what it was trained on.
 
Here is the question: How do positional embeddings improve U-Net performance in DDPMs?","There is no mention of U-Net or DDPMs in the provided context, so it is not possible to answer this question based on the given information. The context only discusses Large Language Models (LLMs) and their positional embeddings, but does not mention U-Net or DDPMs."
What role does the KL divergence play in training DDPMs?,"[(6144, '34a43938-8523-4217-9140-ca800a80ae2b'), (23555, '2b1732eb-f658-40e3-a13e-87727b2d4d5b'), (6147, '5a0a9bce-c74d-4d84-9cbd-c2b32a3e4e57'), (23568, 'de4a11ac-4b1c-4ea7-8477-c2c7da4c59ba'), (30238, '7190ce66-f22b-4ec6-bf22-2e3724396d6f'), (13868, '7774b25f-7e55-479f-bd68-041a933d6cbf'), (5166, '10ad61d9-296e-4ccc-abcc-d7b194fa0fb7'), (21560, '6de334f4-432b-4f38-86da-f421d030fb94'), (9792, 'c23887e9-c847-42df-b582-76d15b62b7e1'), (26693, '74a1767c-cb28-44b5-9581-88def78019d3'), (30794, '60649a7f-802b-4687-8b03-4c75a01eca78'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (24665, '3253f58a-358c-4041-8bbe-eb18a06c354f'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (14438, 'adc4d77e-0477-455a-8002-6bde28537b35'), (10348, '708c3d0f-e5e4-4f40-abdb-53d49deaac4e'), (25196, '8c21a32d-1385-48b4-a73d-a90a33b3d6be'), (1664, '0cd9f5ea-7e9f-435b-8ec7-4c4b767d2505'), (20100, '329bc885-925f-4506-b954-b2425b99d20d'), (20105, '36efca03-13e6-426b-bbdd-869c54d767e1'), (21136, '6940c909-8fdc-4dfe-bd8c-d617a9c1cb8a'), (6801, 'a8d073e8-4092-47e2-b7f0-c61300a489a7'), (6802, '816d52f3-ceec-436a-b89e-8e94e93f67cd'), (23197, '654b3017-275c-4c8f-b6d9-4af107ef2e82'), (23719, '7e6b7060-8886-49db-a2bb-0f3f594a5b1b'), (23208, '7f9c15bb-3cfe-42a9-ad6e-b38315afde8c'), (23209, '4492fb38-aa38-435a-b755-49ab49b50780'), (4777, '84fd7f43-2674-4dd9-ac0f-3306fa9042df'), (25771, '127762da-66a3-4c7a-a239-ef11e8d873e0'), (11951, 'e2e3c3c6-9b1e-41a4-9614-e5c2e001ff14'), (689, '1164c4bd-9a6e-4de1-8613-07bff5d14cbe'), (25779, 'be631b06-23cd-4d93-bba7-2b715f3ca9b4'), (20155, '04e7e95a-9eb0-4302-a0ec-07716aa382f2'), (25279, '6320a197-f20f-4cda-9081-8b10f4eab704'), (25280, '8c7846b3-bc36-403f-b031-66f0a9105705'), (6348, 'c6923392-33ba-471c-bcf1-6fd85617b807'), (6861, '8c87c1fe-759d-4335-96a4-996320f370e6'), (17615, 'ee480905-57e5-4a3b-a74c-ee333a05b447'), (9452, '59aaeec8-b95f-4c82-89b3-f8905ba521d8'), (26354, 'ceb68dc1-fb77-45ce-a2de-aaeb57908d35'), (6899, '5fc72f0c-9870-4f07-bc4f-c8e16dc0b4f2'), (4863, '573d2b26-26ef-44f0-8df6-058a342dce58'), (26880, '439720fa-abdc-4492-afe7-ad647d8bd1e5'), (21250, 'ede0d6bb-0046-48df-8165-615ee0a812b3'), (4869, '991872b9-4c4c-4b9d-965f-980c21d4fbce'), (9493, '8c059764-2d44-47c9-a65e-0c7442efc12e'), (25375, '4a82a223-150f-4b59-9144-ad72f1697f2f'), (8497, '38b39a0d-627a-4af4-9b25-705999b7224c'), (8500, '0890d178-5a71-4831-aab9-8d94cc712d2a'), (8505, 'fa1d4c6f-2910-4cba-b78a-191386e0bd13'), (1341, '4b19942d-93e4-4685-abda-9949882ebf29'), (1342, 'b4875f62-8418-4119-832f-2fa04ff8c912'), (18751, '634c0835-8f15-499b-b83c-f715cd53199c'), (26432, 'bad5a360-cdaa-4da7-a6ac-bdcbaacffd05'), (1345, '64645c76-df63-4566-9ea6-fe57d419e168'), (1347, 'f10b5670-7893-4e80-bb6c-130fc65485ba'), (1348, '0054aec1-a356-46ab-805c-ec46a96d4f15'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (24908, 'e5fe80b3-8b91-4599-8072-37edf5ea77fd'), (24909, '82290a82-2504-4656-a125-e4ba230d33ea'), (18766, 'b7440b70-1331-459e-bdd7-e57b5ccec2b8'), (24911, '8d336559-acfd-4d67-a0fb-e707975cad03'), (24910, '93139bb7-0ac6-44de-b998-dccfe267409d'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (13158, 'd5de80e9-25fb-4ef5-8868-7bb0688c1611'), (23911, 'ff7b1156-3fcd-4b97-8db4-cda20e6c637d'), (13160, '56ef2665-6503-42de-a4e7-5efc2b2bc08a'), (16747, '1894999f-8f08-4bd4-a634-e75c63f7640e'), (16751, '3578b005-a146-4c9d-a52b-1612e7e47677'), (16752, 'b9ea5b0b-e8ef-459b-9671-c9a1f6973e32'), (23921, '609c8963-6100-4e46-9705-69991ce228e6'), (28530, 'a99bae22-028a-4b14-8f4c-f385dd757ce8'), (28531, 'f3ff6d8a-4d00-45e1-bba1-8468c7b59936'), (23923, '75526455-56ee-4d7d-a182-4884aa9d9f8c'), (20340, 'e1a20c98-b6e6-40d8-a88c-9083bad24d3e'), (13169, '3cd0b600-024b-4841-89f2-c5d09b3b1789'), (20850, '7ee5eed4-a3e1-4f5c-b8be-6285f6072194'), (20342, '63ba5e06-4514-4380-a011-5c55dede8208'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (20346, 'dbcd9319-2dc8-42b0-a155-f51b61a7391c'), (28535, 'c4e1adc6-dc57-4146-96e1-ff983b95b866'), (20341, 'f361a086-1d5b-414a-b813-7de74930ea18'), (24452, '24f7284d-65f7-4323-8260-c5c056cc1ede'), (23432, '16d96e6e-b934-4fd6-a87b-5fdf8025aaf4'), (7053, '95001c37-9fc5-4ae4-b145-d1f3f2747be7'), (26514, '9fcaf21e-79bb-4839-864d-0d857fa15240'), (8600, '269c11e4-e332-4535-8735-0da74c456141'), (31652, '70e0a2f0-75da-42e8-ba35-f26d73b35790'), (30629, '7cf3310b-91ee-4c5f-a7ea-66073b3a9197'), (11684, '9cfcfbdb-6441-4284-8ef3-d14780da146f'), (7595, 'e9a0cc5c-98d6-4e8d-8b64-1c28f8f15d45'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (22455, '91b67790-bdeb-4ffa-801a-3386f0953f00'), (14776, '25ac1795-a4d1-45b8-837a-29dc4ccf5415'), (13770, 'e80ca770-2237-42bc-b97a-eae174a16fda'), (25034, '6df4e515-fab2-4473-8dc2-849bad23124e'), (13773, '0ed5c38b-9f9c-4e11-9880-74cc2c1688a9'), (27085, 'ee8bb765-44a6-4187-9e25-b70671fe43b7'), (8658, '8b7e7bf4-444b-4eb5-aba2-a4b162ee880d'), (31704, '6f42ace7-d3d5-4441-b505-93d89f0c383b'), (12250, '93403956-c604-496f-bf51-91d1d6680dd8'), (20447, '87310f99-c045-4f09-b15e-b8efc2843938'), (20448, 'ceafade8-c0b3-4046-a95b-5bdf4ba58c32'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (20450, '8eef5a06-94ef-451c-b6d0-b2e9f6337acd'), (17389, '2c316cc2-5124-4aa4-b0e3-56e598636944'), (19950, '61f98945-5fc0-4952-850e-216527180361'), (12271, '8ea12943-21b0-4511-9794-abb8809c07a0'), (23533, '354bcc39-ac3c-4a0f-ac68-3b5a873cc33b'), (23535, '0de47e64-f26d-4a3b-bb54-a6517f3e5e5b'), (12272, '0715b4f7-d799-44ae-bbea-0c3b8cffbbe6'), (26614, 'ef74dcbf-21f2-4413-b74f-69aee8ff4bc3'), (6139, '2429e224-b783-45a8-b015-103d63acc064'), (21501, 'bf322ac2-26ac-4e0e-9136-75f9f4ddcbf9'), (12286, '2a89892b-4a32-4801-9d33-14f95f02fce5')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: over tokens, \\( r_\text{KL} \\). The KL divergence term penalizes the RL policy from moving substantially away from the initial pretrained model with each training batch, which can be useful to make sure the model outputs reasonably coherent text snippets. Without this penalty the optimization can start to generate text that is gibberish but fools the reward model to give a high reward. In practice, the KL divergence is approximated via sampling from both distributions (explained by John Schulman [here](http://joschu.net/blog/kl-approx.html)). The final reward sent to the RL update rule is \\( r = r_\theta - \lambda r_\text{KL} \\).
```

The same template was used for SFT, RM and RLHF stages.

A common issue with training the language model with RL is that the model can learn to exploit the reward model by generating complete gibberish, which causes the reward model to assign high rewards. To balance this, we add a penalty to the reward: we keep a reference of the model that we don’t train and compare the new model’s generation to the reference one by computing the KL-divergence:


\\( \operatorname{R}(x, y)=\operatorname{r}(x, y)- \beta \operatorname{KL}(x, y) \\)

where \\( r \\) is the reward from the reward model and  \\( \operatorname{KL}(x,y) \\) is the KL-divergence between the current  policy and the reference model.
```

Essentially, we want the student model (a randomly initialized MobileNet) to mimic the teacher model (fine-tuned vision transformer). To achieve this, we first get the logits output from the teacher and the student. Then, we divide each of them by the parameter `temperature` which controls the importance of each soft target. A parameter called `lambda` weighs the importance of the distillation loss. In this example, we will use `temperature=5` and `lambda=0.5`. We will use the Kullback-Leibler Divergence loss to compute the divergence between the student and teacher. Given two data P and Q, KL Divergence explains how much extra information we need to represent P using Q. If two are identical, their KL divergence is zero, as there's no other information needed to explain P from Q. Thus, in the context of knowledge distillation, KL divergence is useful.


```python
from transformers import TrainingArguments, Trainer
import torch
import torch.nn as nn
import torch.nn.functional as F
11. **Adaptive KL**
    - The KL divergence penalty coefficient \\(\beta\\) is modified adaptively based on the KL divergence between the current policy and the previous policy. If the KL divergence is outside a predefined target range, the penalty coefficient is adjusted to bring it closer to the target range ([lm_human_preferences/train_policy.py#L115-L124](https://github.com/openai/lm-human-preferences/blob/cbfd210bb8b08f6bc5c26878c10984b90f516c66/lm_human_preferences/train_policy.py#L115-L124)). It’s implemented as follows:
        
        ```python
        class AdaptiveKLController:
            def __init__(self, init_kl_coef, hparams):
                self.value = init_kl_coef
                self.hparams = hparams
        
            def update(self, current, n_steps):
                target = self.hparams.target
                proportional_error = np.clip(current / target - 1, -0.2, 0.2)
                mult = 1 + proportional_error * n_steps / self.hparams.horizon
### KL is always a positive value, isn’t it?

As we previously mentioned, a KL penalty term is used in order to push the model’s outputs remain close to that of the base policy. In general, KL divergence measures the distances between two distributions and is always a positive quantity. However, in `trl` we use an estimate of the KL which in expectation is equal to the real KL divergence.


\\( KL_{pen}(x,y) = \log \left(\pi_\phi^{\mathrm{RL}}(y \mid x) / \pi^{\mathrm{SFT}}(y \mid x)\right) \\)
 
Here is the question: What role does the KL divergence play in training DDPMs?","The KL divergence plays a role in training DDPMs (Decision-Directed Policy Models) as a penalty term to prevent the model from moving substantially away from the initial pretrained model or reference model. It ensures that the model outputs reasonably coherent text snippets and prevents the generation of gibberish text that may fool the reward model. The KL divergence term is used to measure the distance between the current policy and the reference model, and a penalty is applied to the reward function to keep the model's outputs close"
What distinguishes absmax quantization from zero-point quantization?,"[(14856, 'ed7771cc-947b-47e6-a398-7c90b8fd9aab'), (8749, 'afbad431-6278-4361-bdeb-866c1474bfbf'), (8750, '269f08cc-32e9-4eea-9f7b-f529f1cc3c6a'), (28728, '63aced76-8035-4a75-a9e6-d768917c53a3'), (10311, '0023a34d-dfe3-4f6c-83d0-b471f3ebd68d'), (2645, 'c0529244-876b-4941-83d4-3c85320ab532'), (17499, '2341d406-75b9-48c6-9dfc-d86b8e87f7c7'), (18523, '3b0fe938-bf71-4cb5-b101-e4d9ec34d280'), (2652, '3215b488-330c-46e6-b658-ab09a2d47b01'), (14437, '6f4d674a-5ac8-464d-8dde-f98b8f555e87'), (14438, 'adc4d77e-0477-455a-8002-6bde28537b35'), (30319, 'a04b6e09-b517-497d-bf98-2432285b7d43'), (30320, '5bc6df16-f89d-45a4-b6d3-6a628ecbef7b'), (14449, '34181db5-1e12-4c24-874c-212d35eac6f7'), (21113, 'd9ab3f29-fae0-450d-abfc-76d9ce16916b'), (5253, '2ccb1c5d-ddb0-41cc-9b09-4543c15e414a'), (2695, '38e95994-7560-49d4-8884-6a69c4b9f459'), (18058, '35410e0e-6026-4918-9a77-6d58fcfc8de0'), (18059, '0c61ecab-c341-4e5e-9b6b-11f05c7bf24e'), (22163, '8d167b0c-14d6-4205-a383-f37c9f717a64'), (22164, '35fd7412-e5b4-4127-be15-e38ab14cbcbc'), (22165, '1c5bf21e-2824-4192-bcd8-376a2840a46f'), (22166, '10bb5bda-4d63-4d38-a475-fbb3876f8082'), (3735, '11ef49b2-65b5-405b-b50e-54ea094c8b6c'), (3736, 'e3e83e2c-6c23-41d7-8f7e-c1919c0feea6'), (3737, '515efb7f-1ab1-41a2-89e5-5c66253070fa'), (22169, '1eefa675-c3d2-4400-8957-842b99fce9c4'), (8347, '6aa8880a-8695-4c23-b28b-def12d9eea77'), (22173, 'c6da122a-96dc-44f4-931e-80f5975b4687'), (7340, '663f2b28-0863-4499-a111-386a729d4bc1'), (29356, '102a05c8-dd14-4ade-9c61-94ca9a44c5c9'), (8893, '65d8236c-8241-48f8-8847-bd972e114aeb'), (5839, 'd97b59e5-561b-4a61-bcff-51b7f943fa47'), (8399, '42bde4ac-d1dc-426d-bfdc-31cb6224fd57'), (6356, 'fd9611a0-cae2-4941-8609-f6440c2b0285'), (25301, 'e2f2ef56-1929-4847-aba6-79781c358ba5'), (6358, 'c5b8b78f-d9a9-483d-8f87-70642fe0c191'), (25304, 'b80ca7fa-2902-4f9b-bfe3-492030f9ce0c'), (25305, '6126e14a-e501-46c0-8773-957993663f6d'), (27362, 'fcf527e3-17fe-43ea-9f64-d5d198cf9cbc'), (27365, '5bf57a04-7a40-46ff-9de0-6de03c730a54'), (25318, '7f293204-2059-483b-9146-91aa485398d2'), (27367, '41667532-e3e5-4133-b761-9964ee688e40'), (27368, '6f7bddf6-b76f-424e-ae72-de7ccdcad846'), (25321, 'b7befe0e-86b5-4603-8086-b358c0606f7c'), (27369, 'c32bc31b-ab88-4533-9d25-25e9a81e65ce'), (27370, 'c09a7f74-bdaf-48bc-aa92-486e5fe9fc50'), (27371, '3dde5faf-ccc0-40fb-8de7-c0385762e12e'), (27372, '469930fb-70b4-4e4c-99da-c25e223cab49'), (27373, '1cf527a1-db9e-45b4-affe-fab3bfe9777e'), (27374, '533ba9a2-9014-42fd-aca2-57e02724d646'), (27375, '29e4f8ea-8a3c-4830-9480-771d9ab8cd10'), (27376, 'fcaf5646-af76-4706-a958-daa188568f6e'), (27377, '7bb68b9a-de49-48df-8773-d7687fdc42f7'), (27378, '69fd6e93-802f-4dfc-a4ff-6caf775f803d'), (27379, '88aea9d0-3b36-4f80-986f-5040b1529bab'), (27380, '5efa5370-b16f-4fd9-a3b0-12f7fa4ce693'), (27388, '197b1ec1-a653-4a7c-9c1f-fc93a0a25712'), (10500, 'ffb4cde1-54be-482a-81a6-8f9122db3c30'), (23822, '658e470a-87d9-4fe9-958f-16db281f71f3'), (21266, '92adf75c-feec-4fca-a3d6-d7ec3ab3c977'), (21267, 'f536660d-4299-4359-b2ed-cad93980355f'), (21274, '6dc1af30-a439-4f39-baba-fb652d70f065'), (17181, '1c894887-1529-446e-8d87-8adb5014de68'), (3374, '2c4a50c0-ac87-40bd-93f8-00c0ed888083'), (1839, '2e1f7f32-7b5b-44e1-b786-cda596c27fee'), (21807, 'b3346506-6c4a-48ac-9d75-c1258f287641'), (21809, '84d3f72b-d1a6-40f6-bd7e-c332c7457886'), (1838, '10439f35-19b7-48cf-abc1-20e3a0f00896'), (3377, '156b12d0-1aa0-4557-a4ca-1e911e59c896'), (1840, '8f23ce73-b430-4409-8428-dd0a4377aed9'), (1842, '1efbd857-b063-44fe-8411-6cb052a298ed'), (5941, '235981d5-8f2e-4358-a6f9-90d62149ac88'), (19260, '5426cc73-0d90-44a0-b3b1-ee7e31d8605c'), (19261, '828a7548-bebe-48da-bd43-915ef22bbb9b'), (19262, 'cb4dea97-cfcf-4d5d-827a-c683ab6dcaf2'), (21822, '2ebdb7ae-8f7e-426c-9f66-44f123bea65c'), (21820, 'abaff0ac-c074-4ac1-8b57-2a71527a3c61'), (19266, '0e059998-e258-4fbd-b3d1-8130a2c38a88'), (21826, 'df059f13-a4e9-4def-a53b-5057d32c6c75'), (14671, 'f63072f2-da40-472c-a639-1b77176258b8'), (14672, 'e0d05374-3982-41cc-91ba-8f6d65c9832c'), (14673, '399d2b1e-2ea3-4bf9-b70d-2367f3bd8636'), (14674, '79166e17-b498-4c2c-9335-0b49aa501ebe'), (14675, '94fa6ff0-1fea-4954-8fd1-f4b825366b76'), (1872, '4ab11bb3-d5e2-4c59-ac01-a9f3d8773989'), (1875, 'ad5fbc4b-fd48-4f82-8cd4-94ebe2e31e5a'), (14679, '941c9eda-09fb-46bf-a0c8-12c756f40a6a'), (14680, 'a634e70d-07de-4872-a973-e578321ac293'), (22371, '21051c79-d40f-41ae-ad3c-c20ce6cd2470'), (14206, '5bfbab3f-215b-442a-8bd0-1810e815b192'), (23939, '0c15a3e9-0bd1-48d4-ad86-562d0f7e7b19'), (20359, '9f8816ec-c6b9-4a49-a1ac-2b3e50f83f77'), (20361, '38a40d70-31f1-4631-97f6-df1d876a9b8d'), (3999, '09d1277f-d722-4457-b060-831350863007'), (25506, '5db76deb-0ea6-4d0d-b417-4744536559a4'), (4010, 'f7201c35-6a48-44a3-b8a2-3b51387783a3'), (23474, '1c2ce11e-2e05-4f30-a42b-ee63d619fc26'), (23475, '68a40f24-cf09-4158-a626-b4357e796ceb'), (4031, '2b10b56f-b719-4f10-b0a1-04970839e977'), (11714, '81248f40-6e7d-4225-8017-1c87a3d5c64a'), (11720, '3d9b811a-57b7-4c50-93b3-a8f4e6e503d9'), (11722, '1c4509b6-9894-4c45-8e40-b0099ab2f9ef'), (27085, 'ee8bb765-44a6-4187-9e25-b70671fe43b7'), (12251, 'fc1065da-6b17-4122-9fce-a60100ee943e'), (31736, '223b8f0a-730b-472d-b84b-1c268a05cad9'), (31737, '1db4c46e-0450-4584-a576-b34a60bf4b35'), (31739, 'ae5209af-c84b-4c9d-8cad-45339d0bf030'), (31740, '7f15437c-296d-44c3-89c4-94654b268fc9'), (31741, '3cb8440b-d4b0-4bd5-930d-1fc4596f3292')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: The two most common 8-bit quantization techniques are zero-point quantization and absolute maximum (absmax) quantization. Zero-point quantization and absmax quantization map the floating point values into more compact int8 (1 byte) values. First, these methods normalize the input by scaling it by a quantization constant.

For example, in zero-point quantization, if my range is -1.0…1.0 and I want to quantize into the range -127…127, I want to scale by the factor of 127 and then round it into the 8-bit precision. To retrieve the original value, you would need to divide the int8 value by that same quantization factor of 127. For example, the value 0.3 would be scaled to `0.3*127 = 38.1`. Through rounding, we get the value of 38. If we reverse this, we get `38/127=0.2992` – we have a quantization error of 0.008 in this example. These seemingly tiny errors tend to accumulate and grow as they get propagated through the model’s layers and result in performance degradation.
![quantization](assets/96_hf_bitsandbytes_integration/quantization.png)

(Image taken from: [this blogpost](https://intellabs.github.io/distiller/algo_quantization.html) )

Now let's look at the details of absmax quantization. To calculate the mapping between the fp16 number and its corresponding int8 number in absmax quantization, you have to first divide by the absolute maximum value of the tensor and then multiply by the total range of the data type.

For example, let's assume you want to apply absmax quantization in a vector that contains `[1.2, -0.5, -4.3, 1.2, -3.1, 0.8, 2.4, 5.4]`. You extract the absolute maximum of it, which is `5.4` in this case. Int8 has a range of `[-127, 127]`, so we divide 127 by `5.4` and obtain `23.5` for the scaling factor. Therefore multiplying the original vector by it gives the quantized vector `[28, -12, -101, 28, -73, 19, 56, 127]`.

![out-quant.gif](assets/96_hf_bitsandbytes_integration/out-quant.gif)
![out-quant.gif](assets/96_hf_bitsandbytes_integration/out-quant.gif)

To retrieve the latest, one can just divide in full precision the int8 number with the quantization factor, but since the result above is ""rounded"" some precision will be lost.

![quant-freeze](assets/96_hf_bitsandbytes_integration/quant-freeze.png)

For an unsigned int8, we would subtract the minimum and scale by the absolute maximum. This is close to what zero-point quantization does. It's is similar to a min-max scaling but the latter maintains the value scales in such a way that the value “0” is always represented by an integer without any quantization error.
```
x = S * (x_q - Z)
```

where:

- `x_q` is the quantized `int8` value associated to `x`
- `S` and `Z` are the quantization parameters
  - `S` is the scale, and is a positive `float32`
  - `Z` is called the zero-point, it is the `int8` value corresponding to the value `0` in the `float32` realm. This is
  important to be able to represent exactly the value `0` because it is used everywhere throughout machine learning
  models.


The quantized value `x_q` of `x` in `[a, b]` can be computed as follows:

```
x_q = round(x/S + Z)
```

And `float32` values outside of the `[a, b]` range are clipped to the closest representable value, so for any
floating-point number `x`:

```
x_q = clip(round(x/S + Z), round(a/S + Z), round(b/S + Z))
!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# int8 training for automatic speech recognition

Quantization reduces the precision of floating point data types, decreasing the memory required to store model weights. However, quantization degrades inference performance because you lose information when you reduce the precision. 8-bit or `int8` quantization uses only a quarter precision, but it does not degrade performance because it doesn't just drop the bits or data. Instead, `int8` quantization *rounds* from one data type to another.

<Tip>

💡 Read the [LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339) paper to learn more, or you can take a look at the corresponding [blog post](https://huggingface.co/blog/hf-bitsandbytes-integration) for a gentler introduction.

</Tip>
 
Here is the question: What distinguishes absmax quantization from zero-point quantization?","According to the context, what distinguishes absmax quantization from zero-point quantization is the way they calculate the scaling factor and the zero-point value.

In zero-point quantization, the scaling factor is a fixed value (e.g., 127 in the example), and the zero-point value is not explicitly mentioned.

In absmax quantization, the scaling factor is calculated by dividing the maximum range of the data type (e.g., 127 for int8) by the absolute maximum value of"
Why is Flax preferred for distributed training on TPUs?,"[(17931, '9ca66b23-26fc-451a-95bc-f56dca1857b2'), (3596, '3d36888e-0b96-4b80-8d90-4f19fc5eb1cd'), (17933, 'c40f9fed-2aa0-440a-b827-f14dab4cc652'), (17934, 'a346e6f1-8dc6-4546-85c3-dac1062bf6ff'), (17935, 'd571b704-c625-45a1-9982-4c9bf2159841'), (9754, '380dcb90-6ffe-42be-b20f-4f82a170b72a'), (3614, 'b9c4358d-0cc8-49cc-8a02-0c202e91f09f'), (38, '7f1448df-4b4e-4cb0-b824-2176150e3e5b'), (29744, '6954830f-0ba6-4c15-a60d-3cd7e635d535'), (29745, 'ed5bb6d8-e497-4707-8feb-1b30b4362980'), (18996, '6a19e0bf-7a11-49c8-9186-3e6ff36b3640'), (6197, '36c5cec3-a6ae-4270-8e58-75e974db78d1'), (19002, 'b4b151aa-98d0-40e4-847b-3faecb435a73'), (26684, '1efbd9e1-3025-4ea8-b6e6-ebe753a2f8a5'), (26688, '06c43668-d169-4991-9447-1d6ea35000d1'), (29761, '04f9fa53-d75c-400d-a4ee-a93d795d3db0'), (17475, '496f81a1-3559-4e5f-ae4f-25727935e2df'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (19535, '66073b8b-617e-48b6-8911-9bff81d7df7a'), (25172, '720af5bd-c739-4d9f-b565-fc0ab476b269'), (4693, 'a69e9047-4018-4d6e-9ab6-7734049c49fb'), (7258, '60ffe65d-54e6-4c62-a466-854687cd7645'), (7259, '3a45b1bb-76a2-4e36-8e07-2845da111f5a'), (1121, 'cf8c83be-c90e-47be-bca1-73be139d2220'), (13409, '9631c56a-59c7-4d33-9626-1852ac46423d'), (24163, '6d254de1-aa08-4a87-9094-64313c3a0c4b'), (25196, '8c21a32d-1385-48b4-a73d-a90a33b3d6be'), (13934, 'db89e881-3a64-4f86-b624-3f67721d2160'), (5745, '77c39426-5363-43cb-9fee-bfc8d242613d'), (24178, '3c4d4424-cb56-479f-a8b9-d87c9e045138'), (13938, '46f349ea-1419-4e09-bfd7-e42051ddde5d'), (24180, '6fd43eed-f7b8-4151-808f-472f264f2aa2'), (13940, '23c24b43-7679-47b2-a1fe-47b057bf71b5'), (24177, '35b3eea1-aabc-4ecb-a399-ec042cd227aa'), (9336, 'ab4b0949-fe14-4128-b48c-1399f01dcfaf'), (14457, '24a91ac0-d333-49c9-a9a6-d6908a59a520'), (24699, '311706b8-dff2-4d65-b870-b160104fdf54'), (18047, '88a9e52b-eec2-4a57-933d-f10b57098f70'), (24706, 'b8a44ff2-1624-4d7e-8af2-7d48b4e7a549'), (13957, '6452d1b9-cf35-4d81-aa93-772441ecdc1d'), (13962, 'a7c31e26-ea04-4485-80fd-732f660daa25'), (21131, '5b35e730-f291-4ba4-83c6-8677efdc7317'), (31379, 'eb3e51c4-a1b8-4e3c-bcd5-82f80737cf17'), (668, '249a03e0-c503-4df7-824c-7f0c50892b55'), (18589, '1b6a9bc8-bc9d-4c7d-8c7d-a90f1b043552'), (5791, 'cdb54bca-3a8e-4c5e-9312-43d69ba57929'), (18595, 'b470b234-6446-4163-a27b-2246013fbd76'), (24228, '6caeeafb-c19f-4ebd-b872-1290bf5cf9fa'), (13482, '5895816d-23d7-4515-9ab2-e4f72fc7f1b3'), (6852, '6314e627-a6f6-4b5b-81e6-111df0abd8a8'), (14537, '0240a54b-50a2-4444-8f43-41902b470646'), (14538, '627bd8ba-f103-4cac-a069-e6a9b4e8c86d'), (14539, '9665d62e-053d-4587-99f5-9bb20435af05'), (4324, 'b7c3b982-b802-42fc-a2b6-4211d990ac32'), (24297, '23e563a1-038a-46c4-9308-9752e26b3551'), (8938, '3a40b433-c39e-486c-9e67-8b91a1c06d9c'), (26862, 'edbd63a0-d666-4cdb-84c2-26609a8ae5d7'), (18672, '51d56682-3787-49c9-b694-c8ccc94be302'), (19698, '778b47fe-c721-4b84-bd17-18c49f7ade13'), (19699, '10a481ad-92e0-4ff7-8bf5-1ec64a1e708f'), (16122, '7950be94-ecd4-4eda-b3d6-1fc80aacafd4'), (26875, '09d6e559-df0c-401d-ae66-2f3358d9b39a'), (26876, '4dfe8cb4-fe81-4e63-855e-6028faa0d7b5'), (26874, '36567ae8-f644-4fd0-aefb-39aba3713194'), (16123, '4eda9b2e-e50b-4f79-8331-60f515f046b6'), (16129, '72311dd7-59d6-41ec-bcf6-a097c6005cad'), (16147, '27008f92-4674-4cb1-b22a-6468630dc026'), (16150, 'ea3c23db-9446-48cf-9cff-b124d174ccdf'), (16157, 'e8e24e1f-95a9-4f56-bd14-0192a666c31d'), (16162, '85d5b632-e607-41c6-b9dd-f35152ef7f21'), (16168, '8720f9c5-bd5a-4961-b1b7-7dd1d69cb57d'), (16169, '28f13635-acb7-4098-9b99-064a295d026c'), (16172, 'b7a87971-1bc4-4771-bdaa-c67e3b637648'), (16175, '593e60d6-0896-4f95-985c-fe42f835237b'), (16176, 'a7578109-e271-40f8-aa02-f89b5dddab55'), (10040, '41bd4b8c-cb79-4a90-b723-46c86076ed1f'), (16185, '91f950e5-c9c1-40fb-a64e-ffaa632b3f4d'), (10041, '61256d30-f3fb-4a77-b783-c6a4a918dd0e'), (16184, '08e02bad-54da-4851-82ae-02218bac7497'), (10045, '8f8befd3-1f42-449f-9b31-97e81ad871fc'), (10046, '0ae0fd7b-1243-4bb2-aaac-8fb71b7d583c'), (29501, '672f1de2-afa6-4605-b508-293f01e7d666'), (10047, '6ad5d7a1-4fad-4e6c-a3f8-3ff5071a694b'), (16193, 'dc1889b7-9c05-4074-ac23-4bc47fe1125c'), (16195, '35bfe656-416b-46d9-9f70-7393985cdc2f'), (16196, '70e6ae83-91ce-42c2-abe6-64b6f74358cc'), (16197, '9a286ec4-bd86-44d0-839f-bdbc41b5d244'), (16201, '74a34dcf-6a1b-4918-a45a-b5f73a7e5d91'), (16202, '131487f8-0976-4023-81a2-641b00be6843'), (16210, '9fe165b2-82c7-4cf0-9c38-d9d6b7d37fe6'), (21865, 'aed3a1e4-ca73-4239-a584-5c016709d32b'), (16239, '948fe2a6-ea63-4bdf-ba5e-f3291774774f'), (7536, '410d9c73-0c4c-4e91-8a2a-d65adb5ce72b'), (15768, 'f0a867f0-3313-47f8-a4d9-49cb78888982'), (30623, '87542353-b6e3-4257-a07f-19317964e33d'), (11684, '9cfcfbdb-6441-4284-8ef3-d14780da146f'), (31655, '9b8cc4f3-e6f1-4f68-80aa-d6a781ba452f'), (29096, '71cbab35-3ce2-4f7f-af3f-132c47bf50ab'), (29097, '6ff1fe79-75da-49ba-bb26-0184494f69a1'), (25522, '47fd3a7e-a338-46c9-9b8f-b0ee0ca8b1f9'), (3506, '4e992d07-de67-463d-9af5-8cad259a8ab0'), (16838, '9857e7b6-2956-4be2-a6f7-8fe5c165254c'), (16844, '14804510-d68b-43ef-b630-fea72e07be9a'), (3030, 'e3812643-6ba2-4e81-afe0-bcc63edd1257'), (3031, '87db4b4e-191b-4b1f-ab44-7e5caf12b17f'), (8667, '83f4e0e5-f70b-4a54-acef-2b200d5689bb'), (8668, '300a4cf8-b18f-446d-906b-357415bd88cf'), (8669, '8f464916-06ad-48e4-85c4-3ae8a88c6ac2'), (30177, '244726ac-82fc-433d-91db-7a414866651f'), (30178, '9c3784c8-013e-454a-a71d-ca6d3cc6283f'), (22499, '186e7c08-bc8c-42e3-92f8-c0884796b602'), (27116, 'ee44caad-d368-4c27-b4ad-92f672f5e3da'), (20463, '4cd629c7-7e89-48ab-aadc-dcbd3e778504'), (7153, '7cbd81e1-352d-46dc-ba5e-77cf65ff8f54'), (7154, 'a06b9d90-8d8a-4fb6-b53c-619bed721fbb'), (22517, '67ea3b27-8027-4a45-8051-d1f4af05f97f'), (19958, 'a5878f32-2a7f-4f07-b78b-35d9e33c6d22'), (19961, '2fa05b74-697b-450c-88da-5af73974f65e')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: </Tip>

Text-to-image models like Stable Diffusion are conditioned to generate images given a text prompt.

Training a model can be taxing on your hardware, but if you enable `gradient_checkpointing` and `mixed_precision`, it is possible to train a model on a single 24GB GPU. If you're training with larger batch sizes or want to train faster, it's better to use GPUs with more than 30GB of memory. You can reduce your memory footprint by enabling memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing, gradient accumulation or xFormers. A GPU with at least 30GB of memory or a TPU v3 is recommended for training with Flax.
```

</hfoption>
<hfoption id=""Flax"">

Training with Flax can be faster on TPUs and GPUs thanks to [@duongna211](https://github.com/duongna21). Flax is more efficient on a TPU, but GPU performance is also great.

Set the environment variables `MODEL_NAME` and `dataset_name` to the model and the dataset (either from the Hub or a local path).

<Tip>

To train on a local dataset, set the `TRAIN_DIR` and `OUTPUT_DIR` environment variables to the path of the dataset and where to save the model to.

</Tip>

```bash
export MODEL_NAME=""runwayml/stable-diffusion-v1-5""
export dataset_name=""lambdalabs/pokemon-blip-captions""

python train_text_to_image_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$dataset_name \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --output_dir=""sd-pokemon-model"" \
  --push_to_hub
```

## Training with Flax/JAX

For faster training on TPUs and GPUs you can leverage the flax training example. Follow the instructions above to get the model and dataset before running the script.

**___Note: The flax example doesn't yet support features like gradient checkpoint, gradient accumulation etc, so to use flax for faster training we will need >30GB cards or TPU v3.___**


Before running the scripts, make sure to install the library's training dependencies:

```bash
pip install -U -r requirements_flax.txt
```

```bash
export MODEL_NAME=""duongna/stable-diffusion-v1-4-flax""
export DATASET_NAME=""lambdalabs/pokemon-blip-captions""

python train_text_to_image_flax.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --dataset_name=$DATASET_NAME \
  --resolution=512 --center_crop --random_flip \
  --train_batch_size=1 \
  --mixed_precision=""fp16"" \
  --max_train_steps=15000 \
  --learning_rate=1e-05 \
  --max_grad_norm=1 \
  --output_dir=""sd-pokemon-model""
```

* LoRA parameters that have separate identifiers for the UNet and the text encoder such as: [`""sayakpaul/dreambooth""`](https://huggingface.co/sayakpaul/dreambooth).

## Training with Flax/JAX

For faster training on TPUs and GPUs you can leverage the flax training example. Follow the instructions above to get the model and dataset before running the script.

____Note: The flax example don't yet support features like gradient checkpoint, gradient accumulation etc, so to use flax for faster training we will need >30GB cards.___


Before running the scripts, make sure to install the library's training dependencies:

```bash
pip install -U -r requirements_flax.txt
If you're training on a GPU with limited vRAM, you should try enabling the `gradient_checkpointing` and `mixed_precision` parameters in the training command. You can also reduce your memory footprint by using memory-efficient attention with [xFormers](../optimization/xformers). JAX/Flax training is also supported for efficient training on TPUs and GPUs, but it doesn't support gradient checkpointing or xFormers. With the same configuration and setup as PyTorch, the Flax training script should be at least ~70% faster!

This guide will explore the [textual_inversion.py](https://github.com/huggingface/diffusers/blob/main/examples/textual_inversion/textual_inversion.py) script to help you become more familiar with it, and how you can adapt it for your own use-case.

Before running the script, make sure you install the library from source:

```bash
git clone https://github.com/huggingface/diffusers
cd diffusers
pip install .
 
Here is the question: Why is Flax preferred for distributed training on TPUs?","According to the context, Flax is preferred for distributed training on TPUs because it is more efficient on a TPU, and it can provide faster training on TPUs and GPUs."
How does the `AutoTokenizer` simplify text preprocessing for models?,"[(19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (23051, 'd0b37d24-40ce-4441-9f5c-56c750734c66'), (23053, '1768087f-f3b0-4646-ba5f-d6c189fb0c6d'), (12827, '3329f522-1671-470f-a648-7baf29512e57'), (8229, '8f21e539-a202-474e-81a6-443e2d0a7115'), (14895, '08d2ffd8-393b-4beb-a116-7f4c24d516a0'), (14896, '1fab5d1c-faea-4cfa-a260-ebf8151763e4'), (18998, '989e75dd-dc43-4eb5-91cb-215c4e4adc17'), (19004, '112963e4-e074-4f17-aa4e-d0a1114feec3'), (8767, '564ce1ac-796f-4ceb-ad5a-cbc045cad5ec'), (24641, '4e34563d-b7fc-47a6-818a-27c23fb158d9'), (19010, 'bff05a49-89fc-4207-a21f-c5be71f9ace4'), (10823, 'e21b6aee-30a8-41ca-a03b-c9cbbe3e780e'), (10824, '6d96b69f-c6af-4766-b64b-337c86dd7faf'), (10825, '5ab44007-ce9e-4e86-a754-a03638ace665'), (13897, 'b546f9b4-073d-4c66-a471-be9f943cd6d5'), (19016, '02c05bd4-cf3d-4ffc-a737-bb651f95a396'), (24655, 'da13a572-a576-4bd6-96a8-e0b76a56d6cd'), (5718, 'e5c1a399-0835-4635-841b-8d00c339e0ac'), (18525, 'e1e7bd40-4274-4984-a6cd-f8af6a0360e0'), (24670, '6e0cabc7-9f4f-45a9-8838-4eeb6827925e'), (3679, '8e31b4d7-de14-44b3-be8a-7236eb150661'), (18529, '40c5d8ae-d12e-423e-b185-76b26f5668d8'), (12903, '44dc538d-ecf5-47f4-9862-c6179d76af59'), (3175, '3e4f3166-8eeb-4360-b27b-0b54a84005fb'), (30314, '062790b0-8e33-4435-ac93-85790f7ffb85'), (7276, '0d98213c-cf33-4909-bcf6-74ccaf3923d7'), (620, '607969d7-62ff-48ba-9157-ee5fc510a46a'), (626, 'beb0b4ef-6aa8-4f19-90f6-06a202ff082e'), (631, 'd8c42629-5fb7-4e48-8de4-57a1a4d10570'), (632, '1ae5445c-2cac-45f5-8d2c-a2d7986b517e'), (634, '06c1ab81-2669-4e9d-b9da-0defc2a88a4f'), (8314, 'b04a31e0-43f4-41a4-89cd-7b20c2fee1a8'), (640, 'bbc1c2ee-dc0a-4715-9793-051f01ad3934'), (18568, '3e6b192d-ac77-405e-b509-7c84053c11a8'), (31379, 'eb3e51c4-a1b8-4e3c-bcd5-82f80737cf17'), (15001, '5e52bb45-1656-4310-8d72-06c79196c8f4'), (13979, 'de950627-bfa4-44a7-aded-c6a93c8ee195'), (26271, 'c8488857-7387-4fef-a11b-2bdfa4527089'), (7341, 'b601a33f-7e1f-42fd-90b5-1a99b5987075'), (17587, 'fd31d5fb-3745-4e63-a1ce-28974559d05e'), (17588, '7dcc86f2-7b8e-4a06-977a-4aa5b493847b'), (17592, '48ede2f2-438c-4cd3-b958-b20d71e0b9aa'), (17593, '41dc0679-7826-43e8-87e0-44cefe8860e7'), (24764, '645657c8-1e5b-415a-9175-a7b48fbed112'), (24766, '99f55b4a-fe60-48df-8919-86bc72c7c1cb'), (19664, '2872ed56-b851-405f-8601-3748f47a00e1'), (5843, '53e94498-6454-46a9-90ea-1955b56dfd8f'), (3801, 'c7871b78-177e-4145-9897-6274d6ccd22c'), (15580, 'af186607-77e5-4bcd-a6bd-4347c6bdf981'), (8418, '8f7ee68c-66f6-42cf-a640-d0768ab7fd81'), (21261, '9fc8bb55-a9e1-487a-9702-6f727d330ef6'), (27406, '954a0b75-0188-4266-84c0-4a2f946626bf'), (3343, 'a583dfae-ba2d-47d5-83f4-089b36deba7c'), (27407, 'cc6b271a-4d91-452a-b62b-5123e6bd61d7'), (3345, 'b9d4c808-74d4-4630-bb52-89bee5a7c4ea'), (3346, '032420bb-66aa-4f3e-b636-fe7b4d17188b'), (21776, 'bb03e7f2-3467-4406-9dbf-4f8914ecc211'), (3349, 'c7ce5f52-1b2f-402f-a9ae-e6b7e720f7b0'), (3351, '6a70bba3-b7d2-4b60-a6c5-5adf098dffcf'), (28453, '2acaecbf-0a07-4739-a5e6-f4a3a5a14829'), (28456, 'dfde3629-5505-4d2a-bf17-94086d7a8748'), (25907, '4c651379-1c81-48ed-9f9b-a9d7060ae2e3'), (31544, 'eb25ff06-0119-48a6-822a-15328549c28a'), (11065, 'f276616a-e9b5-4dcd-ae38-4256ec518cf8'), (12096, '4207f003-cc4e-43c3-a82b-95f956abc976'), (11075, 'd819b801-c995-4bc1-93d6-22280e370c4e'), (24900, '4889caeb-6d43-410d-b474-d87d0da7fe19'), (24902, '21707633-c150-463b-a123-71a45823b814'), (24903, 'a4cedab8-3717-4d08-b41b-f6159b58e741'), (9543, 'c3e327b1-1624-44cf-9505-af5e90000a71'), (24904, '0d263af5-23ad-4f96-a3b0-81bee9b6bd23'), (16719, '7f7a0365-377c-4bfb-a3c8-97ce76ba9618'), (16720, '6f64e88f-35dc-46dc-9289-15572ee0760a'), (7507, 'ccc6d78e-a524-4c60-afef-5b359b51febf'), (3418, 'b0ebfddc-4547-4540-a745-59aac8f1ec6f'), (18271, 'd8a0b21a-3bfa-4527-aea2-525c77aeba86'), (24937, '5ab51995-d45a-44ae-8339-d077a7afb102'), (22382, '2fda53bd-8785-4632-86ac-82bf42d9cf70'), (17265, '5f688553-30e0-446c-bb4a-360a9f411306'), (24946, 'eacd2ad5-632e-4cda-928b-76b6342cce2f'), (24949, '83efc35a-a86a-47fb-9f8c-04d42a95a7a8'), (24951, '518851bb-0a7b-4098-993b-556790a3803b'), (10619, 'b25a62ca-be53-44f0-afc8-b23cadae9057'), (31101, 'c943ed4f-1ab8-411a-a1f2-28e45430aea2'), (22399, '8c31bbea-69a2-4f83-afb3-796762dba928'), (3967, '9f78b68f-cbd4-4f66-9007-c1960446d965'), (18316, '477b02f9-3b34-4924-bd6b-dc1f478a1d5b'), (5005, 'd1827718-52c5-46b8-a465-02f3edc3a83a'), (12174, '11b1816b-c38a-44f5-ae65-7589d37c8c3f'), (22419, '7fcbfd95-1315-43c8-9c0d-b02000de3730'), (4505, '82ea4097-6dd6-4ae8-b0e9-416160ce9a4a'), (4506, '6381c1d4-8050-42bd-a4f9-137f3a69cf1b'), (11676, 'a9cfe46a-9e1d-4ad1-9d16-634aeac45374'), (19871, '0a993e9c-5a70-49a8-8b8a-ea79fff01d49'), (19873, 'a9261a86-3290-4280-b361-39c1dcf8bd15'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (29605, '189ece72-2f2c-429a-80ea-17c6c17cef72'), (29607, '53bb22bc-6ef8-44d9-b3e1-004db92d0e73'), (31147, '0ad2706b-a090-4466-a78c-5cfe17740d93'), (29619, '228cf10d-872b-4bd7-b2e8-6087e98e55fa'), (29621, 'eefc33cd-1766-424e-a0fd-e40657b872d7'), (25527, '6370c66c-4f80-4bb8-aa8e-1c55bf8f87da'), (25575, 'd5dda4e8-ea7f-463a-984e-f76ed535f6bf'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (4589, '66162754-f18c-450c-a30a-0de5d29ac671'), (23023, 'bce01692-64e8-4d39-8f05-514945aaf065'), (3056, 'd4d04eec-32a0-4471-9e0a-6a0802f35104'), (13812, '919ad165-ce20-4d22-bff9-99d75dc3975c'), (9205, '813fcccf-2923-465b-ab73-2cfe6cc521fd'), (16375, '8c772737-ffa5-455e-b995-a3218ac3f244'), (7672, 'e5b9aca6-3dea-4557-af6f-509bb2a395e5'), (17404, '35034f8e-d7fb-40a9-b538-e5b7820c0f21'), (1534, 'fb44d1f3-3c02-4d77-afaf-99392c4d8752')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Let's return to the example from the previous section and see how you can use the `AutoClass` to replicate the results of the [`pipeline`].

### AutoTokenizer

A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the [tokenizer summary](./tokenizer_summary)). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.

Load a tokenizer with [`AutoTokenizer`]:

```py
>>> from transformers import AutoTokenizer

>>> model_name = ""nlptown/bert-base-multilingual-uncased-sentiment""
>>> tokenizer = AutoTokenizer.from_pretrained(model_name)
```

## Natural Language Processing

<Youtube id=""Yffk5aydLzg""/>

The main tool for preprocessing textual data is a [tokenizer](main_classes/tokenizer). A tokenizer splits text into *tokens* according to a set of rules. The tokens are converted into numbers and then tensors, which become the model inputs. Any additional inputs required by the model are added by the tokenizer.

<Tip>

If you plan on using a pretrained model, it's important to use the associated pretrained tokenizer. This ensures the text is split the same way as the pretraining corpus, and uses the same corresponding tokens-to-index (usually referred to as the *vocab*) during pretraining.

</Tip>

Get started by loading a pretrained tokenizer with the [`AutoTokenizer.from_pretrained`] method. This downloads the *vocab* a model was pretrained with:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained(""bert-base-cased"")
</Tip>

Generally, we recommend using the `AutoTokenizer` class and the `AutoModelFor` class to load pretrained instances of models. This will ensure you load the correct architecture every time. In the next [tutorial](preprocessing), learn how to use your newly loaded tokenizer, image processor, feature extractor and processor to preprocess a dataset for fine-tuning.
</pt>
<tf>
Finally, the `TFAutoModelFor` classes let you load a pretrained model for a given task (see [here](model_doc/auto) for a complete list of available tasks). For example, load a model for sequence classification with [`TFAutoModelForSequenceClassification.from_pretrained`]:

```py
>>> from transformers import TFAutoModelForSequenceClassification

>>> model = TFAutoModelForSequenceClassification.from_pretrained(""distilbert-base-uncased"")
- Splitting the input into words, subwords, or symbols (like punctuation) that are called *tokens*
- Mapping each token to an integer
- Adding additional inputs that may be useful to the model

All this preprocessing needs to be done in exactly the same way as when the model was pretrained, so we first need to download that information from the [Model Hub](https://huggingface.co/models). To do this, we use the `AutoTokenizer` class and its `from_pretrained()` method. Using the checkpoint name of our model, it will automatically fetch the data associated with the model's tokenizer and cache it (so it's only downloaded the first time you run the code below).

Since the default checkpoint of the `sentiment-analysis` pipeline is `distilbert-base-uncased-finetuned-sst-2-english` (you can see its model card [here](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)), we run the following:

```python
from transformers import AutoTokenizer
```

## 3. Preprocess the dataset

Before we can get started with training our model, the last step is to pre-process/tokenize our dataset. We will use our trained tokenizer to tokenize our dataset and then push it to the hub to load it easily later in our training. The tokenization process is also kept pretty simple, if documents are longer than `512` tokens those are truncated and not split into several documents.


```python
from transformers import AutoTokenizer
import multiprocessing

# load tokenizer
# tokenizer = AutoTokenizer.from_pretrained(f""{user_id}/{tokenizer_id}"")
tokenizer = AutoTokenizer.from_pretrained(""tokenizer"")
num_proc = multiprocessing.cpu_count()
print(f""The max length for the tokenizer is: {tokenizer.model_max_length}"")

def group_texts(examples):
    tokenized_inputs = tokenizer(
       examples[""text""], return_special_tokens_mask=True, truncation=True, max_length=tokenizer.model_max_length
    )
    return tokenized_inputs
 
Here is the question: How does the `AutoTokenizer` simplify text preprocessing for models?","The `AutoTokenizer` simplifies text preprocessing for models by automatically downloading the tokenization rules and vocabulary associated with a pretrained model, ensuring that the text is split and tokenized in the same way as the pretraining corpus. This ensures that the model receives the same inputs as it was trained on, which is crucial for achieving good results. Additionally, the `AutoTokenizer` class takes care of tasks such as splitting the input into words, subwords, or symbols, mapping each token to an integer"
What is a key limitation of DDPMs compared to GANs?,"[(1537, 'dbdbf78c-1993-4781-ad81-675c34c2cc44'), (5131, '5eaf4700-cb10-46b6-8f9b-32ba3b7d7a44'), (25101, 'b5c8f3d2-e8e0-4784-a8c8-4617296394b7'), (27150, '5eda56ff-f783-4df7-81d5-8cc91c59199e'), (12818, '05c47ebf-d4b5-4bab-aa32-e8725684377b'), (27156, '381bf1fe-42bb-4456-bbc7-6c4526252cac'), (14871, '4e270a6d-3c02-4c1b-a3e9-ff96a7d8b99a'), (1049, 'f64ee3dc-e839-4f01-963c-a64cc42d80bf'), (26, '3b03cc49-afcd-4150-90d5-1381b35995f2'), (9259, '9bc2d1de-f64d-4991-b7fe-8cd6b28d89ca'), (26158, '630b0c8f-efd9-4513-9fec-588a683b117e'), (9263, '2e1affcb-5fd4-4b9b-a787-6ddb6248fbfb'), (9264, 'd7098fc1-c855-434f-99b6-87edff2baa20'), (9266, '411293ba-795d-4cf8-8b9d-7884fae1a15e'), (9267, '151a67d4-9b5d-4d4e-83f1-61c1df216f6d'), (17461, '3ac72887-1223-42b9-b927-d94a9735744c'), (26693, '74a1767c-cb28-44b5-9581-88def78019d3'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (28748, 'becc862e-4303-4ef5-9f4f-0ac95b1e2e3c'), (9295, 'd0bbcd03-4314-48ad-8300-9b784b342297'), (9296, 'b982a595-b413-47c2-a522-d71819d17b20'), (27221, '3a0769e6-1ce2-4b41-9dbd-f9d0c6e8b04a'), (3158, '1eb57504-ed08-48bb-8092-c53d5c64b0c1'), (3157, '4be85d3e-4ebb-4805-8f1d-f8525329d95a'), (21080, '8a17b393-7880-420d-9b0d-415445f53d4a'), (23645, '44d712e3-4eed-4171-bcad-7f3c6da58bc4'), (3166, 'd553118b-d57e-420c-9fd4-749a0efae797'), (3167, '3da1ff3e-1da3-42ef-97e6-51557bddcbd4'), (9823, '97c2ebe5-f0ba-44bb-b107-f852019b5ea5'), (30305, '32ed7c96-e33b-4285-932f-7514e6799d85'), (23648, '3b33fc73-d107-47f5-b569-17fb196f4213'), (30307, '5e90fb0f-59a7-4aba-9b15-7eb5bfd43dae'), (25695, 'dd3f5cf7-33ff-487b-aa1f-9665a3813528'), (4215, '582ef246-2b6d-48f2-b065-95abf6e2a5f4'), (17021, '407d9cb5-8755-47dd-805e-b2f5ea2ee0f7'), (30373, 'adf014fd-0478-49ea-b8ec-f99c21e98539'), (4787, 'e0ec6c60-dd04-4ade-96c4-c47997cde848'), (8887, '19c53f49-7fbe-48f5-ac08-243c234c0e8d'), (8892, '91b7caf7-7012-4878-b2e4-e5e45737faa9'), (14018, '6ae44570-1333-46c2-85f2-0d6747ff00d1'), (6341, 'af6cf594-40ea-4161-8f74-97dbfa967545'), (10971, '8510d0e7-1349-487d-841b-7735a78b827e'), (5347, '0d0d0def-80c8-4eda-a320-016e24463829'), (2793, '4b1427af-b5bf-42b4-8726-123e8b1630df'), (25852, 'e68a74a5-1c65-4315-9cac-e0006e78744c'), (17157, 'b9540860-ae17-4e26-b1f5-6ea2ad1ca46e'), (21254, 'b435763a-f888-4e05-8003-8a28ace9a2fa'), (9991, 'aa0b5922-9e54-4d5b-8d48-9a9b30cc5bca'), (20232, '668b86d9-f6af-41ad-8730-c6c574c4a338'), (2830, 'e04c8d52-373e-4dbe-856f-62e81061f8c1'), (24852, '0d760d57-e681-493c-aaa5-257e7cb5970a'), (2843, '2e20db58-f224-4700-9774-c63622aa8a4d'), (5915, 'ab355c6a-ef67-406f-9a8d-28ce78fee6d3'), (17185, 'cd74116d-a81b-465b-b5a2-873c2ef3042a'), (8997, '51faf3fe-7ffe-4528-ba67-80721d615192'), (17189, '2e998f4f-d2ef-4146-ae69-eec125a40cec'), (5928, '46517678-0861-4d86-b92b-eb731ae44060'), (10031, 'd311da63-e4f2-4661-9207-d5accca2e35e'), (20795, 'd680f0cc-ba5c-41ef-a9cf-37017747c34e'), (6972, 'b71bfa08-6fef-4342-8cd6-c4a02b953fba'), (6973, '1b3ca299-1a5b-4ef1-913a-2012d57029c8'), (6974, 'df32e733-6704-498b-b2b3-390af1411179'), (23364, '79926030-52bf-49dc-ae6c-1a1aea10df7e'), (21317, 'e3d09f9f-54a1-46d0-8f42-deee45efbca7'), (29001, '4f83f0ba-093d-4986-9c3b-88389bd936da'), (16713, '3e516130-a3ee-4fac-8f92-cc51e8832e1c'), (5963, 'f5e9bc50-0ded-4256-9ef1-ae89bdfb4271'), (16205, '8fe5626c-c0fd-4a5f-a3ea-9f9cb48f1234'), (8529, '83a5303d-d0b9-4f17-99f4-d6263db35d48'), (29010, '23fb57fd-95c6-4c84-b85b-a01a35375df9'), (16215, '8a91f917-8488-4891-8229-d6abcb84b0ad'), (9561, '2be9427d-f0ec-44f9-a328-68cec96657be'), (14684, '5ae632de-fdb4-46f2-8f90-67cc8c042104'), (15712, 'fa050af4-c26d-406e-a80d-86a9fb43f832'), (15713, 'b8e85e8e-e297-4a15-ad13-c32e7e1ff073'), (13152, '011cb282-35bf-4f96-8aa2-1f67b1ecdc49'), (3939, '95555612-6e93-4b20-b595-6e5b5c0b6190'), (19817, 'af742c64-40e4-4642-bc2d-7e03a3de6aba'), (15724, 'cf08e3ee-cc1f-469a-864c-615037aa2306'), (13168, '21522921-543e-4aa7-8107-15b5e94dd2f3'), (23921, '609c8963-6100-4e46-9705-69991ce228e6'), (25458, '7cf90f12-fba3-4a08-9d0c-c471b27f211c'), (23410, '713a4b46-948e-45f8-877f-efd9033678ec'), (25459, '7e9e55e8-8027-4677-b297-11f6cebd0a17'), (13173, '944071de-f871-424d-9d14-424c81a28966'), (20344, 'edac2d6a-a444-44cc-b455-7144e89a1f09'), (23929, 'a2c8a7f0-2595-4f71-b52f-f20db37027a1'), (23930, 'b54bef38-4510-49ab-87a1-dbc20ba90085'), (19837, '62b75ee3-d60b-440f-8733-c526b3a020cf'), (22917, '7b4ee49b-063a-4b81-a8ba-1854ce6c69b4'), (10124, '8473c16a-90b4-40a5-b618-0d7dfd7b32f2'), (17293, '5285f352-e61b-4dfe-84a0-0ce2cf28e988'), (5525, '6c422722-ce3f-4773-8eaf-53e1808cee85'), (10133, '7735b57a-5b30-4c4f-9ab3-c1e84672a8be'), (3991, '400ebba5-8027-418f-a29e-b3d00506b1d8'), (27033, '6fd8c075-fe58-4672-84f9-9ea3ca16fb7f'), (7592, '68ddeb8e-b285-47f9-8754-09b4fd273d73'), (7598, '948345f7-a89e-484c-ad16-6c15807036e1'), (14767, 'db0fc7e3-d997-4659-abf7-2fb7983ab41e'), (10160, '04802092-194a-4989-9eb2-ec4f32c6987c'), (23475, '68a40f24-cf09-4158-a626-b4357e796ceb'), (9140, '9988e4be-9bf4-4ebc-b854-c24b81f0c843'), (20408, '3d9e54bc-47af-48d8-8638-ba1845b95e3d'), (11709, '7a66a9fb-54f1-4511-bfcb-e44126c124a0'), (2510, '7d4dc0bf-199e-49ec-8219-cc474ae04cc7'), (12249, '21736568-9224-40cd-b5ea-5c55a7f152b6'), (19930, 'c06b2fb6-92d6-4f36-b299-0acd12175599'), (28637, '9d71defc-407a-49ac-ad0f-a76eb6a9c647'), (12267, '0ce7c7ae-e1e4-4869-9ffe-8a2ccd12a259'), (12271, '8ea12943-21b0-4511-9794-abb8809c07a0'), (31732, '831eee66-8d88-46a5-b116-4d1c0b19db53'), (31735, '31b3ae48-f7ec-45ee-adb9-0d7d5e3541ca'), (14331, '50cf2132-6037-4a00-a088-51aa58b6f2c5')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Key differences include:
1. DDP performs only a single communication per batch - sending gradients, while DP performs five different data exchanges per batch.
DDP copies data using [torch.distributed](https://pytorch.org/docs/master/distributed.html), while DP copies data within 
the process via Python threads (which introduces limitations associated with GIL). As a result, **`DistributedDataParallel` (DDP) is generally faster than `DataParallel` (DP)** unless you have slow GPU card inter-connectivity.
2. Under DP, GPU 0 performs significantly more work than other GPUs, resulting in GPU under-utilization. 
3. DDP supports distributed training across multiple machines, whereas DP does not.

This is not an exhaustive list of differences between DP and DDP, however, other nuances are out of scope of this guide.
You can get a deeper understanding of these methods by reading this [article](https://www.telesens.co/2019/04/04/distributed-data-parallel-training-using-pytorch-on-aws/).
Since this technique does not modify or condition the original DDPM network itself, the model produces high-quality and diverse output images for any inpainting form. We validate our method for both faces and general-purpose image inpainting using standard and extreme masks. RePaint outperforms state-of-the-art Autoregressive, and GAN approaches for at least five out of six mask distributions. GitHub Repository: [this http URL](http://git.io/RePaint).*
Note that this list only includes important works until the time of writing, which is June 7th, 2022.

For now, it seems that the main (perhaps only) disadvantage of diffusion models is that they require multiple forward passes to generate an image (which is not the case for generative models like GANs). However, there's [research going on](https://arxiv.org/abs/2204.13902) that enables high-fidelity generation in as few as 10 denoising steps.
## Limitations

1. Right now `trl`'s DDPOTrainer is limited to finetuning vanilla SD models;
2. In our experiments we primarily focused on LoRA which works very well. We did a few experiments with full training which can lead to better quality but finding the right hyperparameters is more challenging.

## Conclusion

Diffusion models like Stable Diffusion, when fine-tuned using DDPO, can offer significant improvements in the quality of generated images as perceived by humans or any other metric once properly conceptualized as an objective function

The computational efficiency of DDPO and its ability to optimize without relying on approximations, especially over earlier methods to achieve the same goal of fine-tuning diffusion models, make it a suitable candidate for fine-tuning diffusion models like Stable Diffusion

`trl` library's `DDPOTrainer` implements DDPO for finetuning SD models.
## Supported Diffusers Features

The Core ML models and inference code don't support many of the features, options, and flexibility of 🧨 Diffusers. These are some of the limitations to keep in mind:

- Core ML models are only suitable for inference. They can't be used for training or fine-tuning.
- Only two schedulers have been ported to Swift, the default one used by Stable Diffusion and `DPMSolverMultistepScheduler`, which we ported to Swift from our `diffusers` implementation. We recommend you use `DPMSolverMultistepScheduler`, since it produces the same quality in about half the steps.
- Negative prompts, classifier-free guidance scale, and image-to-image tasks are available in the inference code. Advanced features such as depth guidance, ControlNet, and latent upscalers are not available yet.
 
Here is the question: What is a key limitation of DDPMs compared to GANs?","According to the context, a key limitation of DDPMs (Diffusion-based Probabilistic Models) compared to GANs (Generative Adversarial Networks) is that DDPMs require multiple forward passes to generate an image, whereas GANs do not."
What is the default hardware configuration for Hugging Face Spaces?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (7692, '2785b812-48ad-476f-834c-8694f938e3dd'), (7695, 'f5e49d60-b8bb-4254-a782-ed38da828ba8'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (7696, '1bf7be69-ea0a-4f67-8bb0-047937a08b55'), (1046, '2efa8a05-4ce9-4c09-bfd5-107d39229dee'), (1047, '1ace34f7-d7bc-410a-be23-3bae8d4d2876'), (14872, '288584f1-35dc-4e08-b78c-4bbd92c996fe'), (14876, '45893d20-107c-4db9-a5c6-937c7bfea6c4'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (1058, '6cec5090-3461-44a3-a5b6-bdc99a04fd47'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (4653, '8d3b2b54-b599-4810-8878-8b6b47d07133'), (4656, '8a5eb5ae-f89d-40e1-a550-28aee57cd98f'), (24120, 'e1160ee3-8307-4e5d-8ec4-13c38625d7d4'), (7226, 'be5b613e-380c-4d32-979b-2f25ee21d7fe'), (3643, 'ee5ea5c1-c94c-4b39-90c8-92a1683eb5c6'), (28733, 'f836bdf5-abd9-4d78-878b-1eed6e0815c8'), (20542, '8adb84bf-218f-4821-ae67-8e8603665c9d'), (15936, 'ab08450b-3b5c-4bbc-8c0e-355d1b29997d'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8273, '5ebe427f-bfba-401b-bc93-31042f3611b1'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (10848, 'e24b0ae6-43f0-498e-bd66-a33c5139c50d'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (19574, 'cc66b1c9-e31b-4dbf-b777-a98a2c4143be'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (19578, 'c39dbfdb-a1e4-46cb-b486-736ca8e047bb'), (11389, '7cf525f3-0375-4371-b8ed-2e4e97568b45'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (21654, '0ac0c293-df2f-4bd7-ab9a-ef9c0cb31739'), (26264, 'caec1e8d-2ac9-463f-95cd-490112c00671'), (26263, '83b240db-acd6-4670-bc6f-1697ec7ed779'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (26794, '1813d2fe-4f25-4ed3-a8ed-ca0946433c02'), (19117, 'a570815f-38b6-47ca-a5c8-1bd2674040a4'), (5806, '5054cac8-6958-4df8-9986-f0bb2157877f'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (1718, '02534d35-0b3a-48b9-83f2-849c6c0852b9'), (8892, '91b7caf7-7012-4878-b2e4-e5e45737faa9'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (22211, '740dfc56-1f28-4364-853c-997c6873e87b'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (225, '3ad317d7-101b-4379-9a04-92d617ddfd14'), (14050, '43174305-dba9-45e8-b46d-9ae2ea56ad48'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (22784, 'f29ec51a-a349-4b84-9053-19ad868c1573'), (23300, '2c8ef940-63f1-41ad-8fe0-9b4815570529'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (8462, '2c893bf2-57c0-475a-93c5-62cd31957575'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (12064, 'ec49857b-c395-49bb-b24a-dba896fff9fc'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (23857, '0ad1a287-cc1d-49ce-8747-028220591a6c'), (31057, 'eee260cd-cb8d-4ea4-b824-020af026b061'), (1368, 'e916d251-892a-48f3-b1ee-a20883a609b6'), (10074, '2aed197d-f4fc-4c34-a307-1983a496c31f'), (9562, 'b75f1239-64b9-4c25-9106-2b9ff2961471'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12128, '6fb6bb94-37c3-4962-b310-4d645b4883d6'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (21373, '46f93944-1684-4015-b212-08fbd8125761'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (3984, '0b5c9103-9bd3-4af7-a1ec-16b33970a4bc'), (16279, '24565971-98de-4d1a-8c46-330e59ab1b4e'), (16280, 'c23e651c-4bbd-46de-9db5-db7d30be24fa'), (16282, '116380bc-82d3-4b0c-aa7e-4209916db42b'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29596, 'b5aab919-c4fb-4392-be4d-0f6bd4e6b5dd'), (16287, 'c79b58a7-88e3-489e-a164-01632304341b'), (31135, '957d568e-e103-45a2-9bae-02e2fbebbc4a'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29106, '5b0357e1-a033-477f-83fe-c132c4035bd4'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (12728, '4364e7f3-1794-496e-9a2f-a14bcae5a530'), (16317, '2b682e0c-ad28-4060-9c8d-4754859a1913'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (16322, 'ad245a5f-b243-4e34-b382-6e08fd6c4955'), (20418, 'abbfadfe-24d9-40ef-a2bc-f9d4401c92e4'), (24523, '886bdffb-b817-4d00-8a9d-f50d4d02ed16'), (29135, 'aa204127-7dec-4d3b-b82a-c7d7aeb22e77'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (27606, '803b87b4-9d8e-4ec5-98c7-7785c20c0700'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (28641, '671bd19b-5f73-42a3-9003-ad03acb69999'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (15846, 'ee66a65a-c35d-4ed7-acbc-15fce331450b'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (5616, 'f1974887-c0b6-4d39-8068-c53a5ce076ad'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (28657, 'c0d8252e-c6dd-4c26-87b0-c76f56194d4f')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ![duplicate-space-3.jpg](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/165_ai_comic_factory/duplicate-space-3.jpg)

You can find more information about alternative engines and vendors in the project's [README](https://huggingface.co/spaces/jbilcke-hf/ai-comic-factory/blob/main/README.md) and the [.env](https://huggingface.co/spaces/jbilcke-hf/ai-comic-factory/blob/main/README.md) config file.

## Configuring the models

The AI Comic Factory comes with the following models pre-configured:
- `LLM_HF_INFERENCE_API_MODEL`: default value is `meta-llama/Llama-2-70b-chat-hf`
- `RENDERING_HF_RENDERING_INFERENCE_API_MODEL`: default value is `stabilityai/stable-diffusion-xl-base-1.0`

Your PRO Hugging Face account already gives you access to those models, so you don't have anything to do or change.

## Going further
To run this matchmaking process continuously, we use **free Hugging Face Spaces hardware with a Scheduler** to keep running the matchmaking process as a background task.

The Spaces is also used to fetch the ELO ratings of each model that have already been played and, from it display [a leaderboard](https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos) **from which everyone can check the progress of the models**.

<div align=""center""> 
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_aivsai/leaderboard.png"" alt=""Leaderboard""> 
</div>

The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).
In the initial set up phase of SafeCoder, the Hugging Face team provides containers, scripts and examples to work hand in hand with the customer to select, extract, prepare, duplicate, deidentify internal codebase data into a training dataset to be used in a Hugging Face provided training container configured to the hardware infrastructure available to the customer.

In the deployment phase of SafeCoder, the customer deploys containers provided by Hugging Face on their own infrastructure to expose internal private endpoints within their VPC. These containers are configured to the exact hardware configuration available to the customer, including NVIDIA GPUs, AMD Instinct GPUs, Intel Xeon CPUs, AWS Inferentia2 or Habana Gaudi accelerators.

## Compliance as a Core Principle

As the regulation framework around machine learning models and datasets is still being written across the world, global companies need to make sure the solutions they use minimize legal risks.
## Hardware resources

Each Spaces environment is limited to 16GB RAM, 2 CPU cores and 50GB of (not persistent) disk space by default, which you can use free of charge. You can upgrade to better hardware, including a variety of GPU accelerators and persistent storage, for a [competitive price](https://huggingface.co/pricing#spaces). To request an upgrade, please click the _Settings_ button in your Space and select your preferred hardware environment.
Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

### Hugging Face Models

Hugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)

## How did Hugging Face help the ONNX Model Zoo?
 
Here is the question: What is the default hardware configuration for Hugging Face Spaces?","The default hardware configuration for Hugging Face Spaces is 16GB RAM, 2 CPU cores, and 50GB of (not persistent) disk space."
How can you manage secrets in Hugging Face Spaces?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (7695, 'f5e49d60-b8bb-4254-a782-ed38da828ba8'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (7721, 'fda7f690-e2ee-4f39-8439-3290db3ed411'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (5679, '485c4c0c-b2b8-4c1b-9506-e6bb20fd388c'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (4657, 'd33514ae-0a67-4200-9281-3ac8294c114b'), (4656, '8a5eb5ae-f89d-40e1-a550-28aee57cd98f'), (5685, 'aca4e725-bad8-48e8-b20b-25dc99d9a1dc'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (5687, '56f80e00-a026-4153-8220-a3fd825d93c4'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (11343, 'eebf97c3-1d21-4c2d-accb-b625fa79ce6b'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (31340, '1bffbde6-8c01-4f3d-914e-12799d8de47f'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (23172, 'def32509-da1d-4b97-ad0b-943987ec2382'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (23695, 'e28c37d5-8504-4c28-8cb1-3b5cdcaeb545'), (8851, '617a3d8e-1689-476d-87f6-80997d01fd6d'), (27284, '344b1139-9e69-4a8a-a5b7-593e7b089268'), (20118, 'eedb12d1-0857-4adf-835f-967a6ce84a3d'), (20119, '85bf9acc-d92a-4adf-9387-6a497f4123cd'), (5784, 'd6d19884-cb3b-422e-9824-8f6ce3a33276'), (26264, 'caec1e8d-2ac9-463f-95cd-490112c00671'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26791, '40f4195c-9f9f-403e-aa4d-69df440b7b16'), (26794, '1813d2fe-4f25-4ed3-a8ed-ca0946433c02'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (1718, '02534d35-0b3a-48b9-83f2-849c6c0852b9'), (9404, 'fd87f105-1775-467d-b190-1af7c157e949'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (7367, '861da66f-aeed-45b4-992a-36ca7b7a1f20'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (26836, '9d0493e6-8f23-4c53-b979-3cfab7d1e205'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (24308, '73f25b52-6f49-4329-98b7-a6360654690c'), (6389, 'ad649443-bb1f-4477-adc1-f07902816889'), (24309, '6e5b89de-67a1-4d75-99ea-8c06819baf89'), (22783, '898e6630-6e42-41f4-bf5e-7ad200e3339e'), (23296, '2d8ceabd-5b42-4675-9eba-06fb3e924703'), (20737, '729395a5-576c-4e8f-b852-764490f4a4f4'), (23299, 'b99e8719-f73c-4e02-8e7a-b6e85b9d672f'), (2821, 'c78a116d-26c3-4854-a37b-d4ca6438c07b'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (25891, '6b116321-1493-47ef-9bda-78f2921ccc9b'), (25892, '1a9985c7-2ef5-4374-87fb-01840fb0dc88'), (25893, '20f28049-e508-4657-986b-2d7923d2cfb8'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (1373, '06b4535d-c110-4754-87c4-22a2af123fd9'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12128, '6fb6bb94-37c3-4962-b310-4d645b4883d6'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (8549, '7ee9801f-9160-4b67-ab5e-ccacabf36b60'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (2926, '5ca73fa1-549a-4df5-8e71-240928eadca3'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (7039, '3ea087fd-2f57-4e59-92f6-f7410097e496'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (6026, '88095e53-c92a-411c-90d2-5ae5ed5197f2'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (25518, '1f847aca-c3cc-4179-894b-7deef51d2560'), (27568, '14a9a9da-5246-490f-b304-14943de48a9a'), (20913, '3c71d8de-fdbc-4627-b72b-8338857044ce'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (3510, '4e9735e1-0a03-42b4-a237-4ea82a96730d'), (12729, '93d429ec-9cb7-4d85-9f1e-3db488055574'), (16317, '2b682e0c-ad28-4060-9c8d-4754859a1913'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (450, '923594af-c190-4de4-b360-37df7b62be36'), (16324, '0cb7caef-995c-4aec-9c86-5e22705a71b6'), (12230, '37bed0b9-108e-4d1f-8832-16b1705a0186'), (14790, '9b894d75-a0f9-4ce1-aa43-7ed77bb8059c'), (969, 'b1c39f04-73a4-4d2f-8474-36257a135a78'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (9174, '3767ffbd-cac9-45c5-b684-68ec257368f6'), (21463, 'e97bfba4-a14f-477a-9f94-afd38c01d4c5'), (27607, '477e553f-670e-4d2b-bda2-3ae34b7a5434'), (9689, 'adbdd4a5-7fd0-42c8-954f-afa15f31071c'), (9690, 'c221a30d-3ba1-4731-af02-978855235bc7'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (5597, '36ce3f3b-0065-4a41-b32a-a7e3629b2ceb'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10232, '27e77ecf-9ddd-433f-bcda-08fa044e1cdc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: <Tip>
If you are using the space just for testing and experimentation, you don't need
to make any changes to the configuration. Everything will work out of the box.
</Tip>

You can also use an external secrets backend together with your HuggingFace
Spaces as described in [our
documentation](https://docs.zenml.io/getting-started/deploying-zenml/docker#zenml-server-configuration-options). You should be
sure to use HuggingFace's inbuilt 'Repository secrets' functionality to
configure any secrets you need to use in your`Dockerfile` configuration. [See the
documentation](https://huggingface.co/docs/hub/spaces-sdks-docker#secret-management)
for more details how to set this up.

<Tip warning={true}>
If you wish to use a cloud secrets backend together with ZenML for secrets
management, **you must take the following minimal security precautions** on your ZenML Server on the
Dashboard:
!--⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->

# Manage your Space

In this guide, we will see how to manage your Space runtime
([secrets](https://huggingface.co/docs/hub/spaces-overview#managing-secrets),
[hardware](https://huggingface.co/docs/hub/spaces-gpus), and [storage](https://huggingface.co/docs/hub/spaces-storage#persistent-storage)) using `huggingface_hub`.

## A simple example: configure secrets and hardware.

Here is an end-to-end example to create and setup a Space on the Hub.

**1. Create a Space on the Hub.**

```py
>>> from huggingface_hub import HfApi
>>> repo_id = ""Wauplin/my-cool-training-space""
>>> api = HfApi()

# For example with a Gradio SDK
>>> api.create_repo(repo_id=repo_id, repo_type=""space"", space_sdk=""gradio"")
```

Next, set up a GitHub Action to push your main branch to Spaces. In the example below:

* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. 
* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.

```yaml
name: Sync to Hugging Face hub
on:
  push:
    branches: [main]

  # to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  sync-to-hub:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
          lfs: true
      - name: Push to hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main
## Managing secrets and environment variables[[managing-secrets]]
<a id=""managing-secrets""></a>
If your app requires environment variables (for instance, secret keys or tokens), do not hard-code them inside your app! Instead, go to the **Settings** page of your Space repository and add a new variable or secret. Use variables if you need to store non-sensitive configuration values and secrets for storing access tokens, API keys, or any sensitive value or credentials.

<div class=""flex justify-center"">
	<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/secrets-and-variables.png""/>
	<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/secrets-and-variables-dark.png""/>
</div>
You should provide a MongoDB endpoint where your chats will be written. If you leave this section blank, your logs will be persisted to a database inside the Space. Note that Hugging Face does not have access to your chats. You can configure the name and the theme of the Space by providing the application name and application color parameters.
Below this, you can select the Hugging Face Hub ID of the model you wish to serve. You can also change the generation hyperparameters in the dictionary below in JSON format. 

_Note_: If you'd like to deploy a model with gated access or a model in a private repository, you can simply provide `HUGGING_FACE_HUB_TOKEN` in repository secrets. You need to set its value to an access token you can get from [here](https://huggingface.co/settings/tokens).

<a href=""Parameters"">
    <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/huggingface.co_spaces_docker_chatui_params.png"" />
</a>
 
Here is the question: How can you manage secrets in Hugging Face Spaces?","You can manage secrets in Hugging Face Spaces by using the ""Repository secrets"" functionality. This involves adding secrets to the **Settings** page of your Space repository. You can store sensitive values or credentials, such as access tokens or API keys, as secrets. Additionally, you can use an external secrets backend together with your Hugging Face Spaces, and configure any secrets you need to use in your `Dockerfile` configuration."
What is the purpose of the `language` field in Hugging Face datasets?,"[(10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (7687, '20608e74-1307-46cf-9341-09115b2bb0ac'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (23059, 'de332ba3-fd9d-44d5-89a5-5c18bdb57ded'), (23060, '09e43017-f9b1-44a7-918e-57bd10790d6c'), (23061, '1fa89fbd-1f12-41ad-a09d-613f63f32b6b'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (3603, '868a7661-c119-493e-86ac-54156f0efe17'), (23064, '80eb8bc8-3947-409c-bc93-d0bbe04d5069'), (21012, '4252b2f2-3c37-45fc-8476-73ec6c4c1ee7'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (16924, '38fe9939-2e02-47cc-805a-7ffc8340c582'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (13359, '6caf8e23-63aa-413c-b673-dc039aa9a2ec'), (29233, '74f4f91f-2db4-42a8-a508-33e42d02d65f'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (7269, '2203fa03-15b5-4a03-87c9-1ad09aa9d60a'), (23147, 'a58a5d52-847f-49bb-a8de-783fe583b3c6'), (20600, '0e8f178d-148b-48bf-be1c-95d3f20535be'), (24703, '24765f0c-b621-411d-824a-f9d36f2430d3'), (23170, '7bd8ea6d-3f6f-4be9-914c-08cac82a76d9'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (23172, 'def32509-da1d-4b97-ad0b-943987ec2382'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (21137, '39920149-03ff-4b3c-bb0a-dd5ee6f44561'), (676, 'afe87c20-9db7-4741-8b99-6b4b97e7c8ab'), (9380, 'df7c3a54-77d5-43cc-b187-be12886969b0'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (9392, 'bc313c9e-ffd4-42b3-a182-a8cb7ff59637'), (11452, '7e2fd91b-ca25-40e4-9aea-47ac22782115'), (4797, '8c2fca57-7c65-4554-b6bb-85101749efa3'), (3793, '20be3864-830d-4112-ad66-401e5e96e6ea'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (3800, '82c0086d-96e5-4b26-a430-a265a26fdb8b'), (8408, '2183f6dd-6b00-48c0-9ef5-29f2063157e5'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (26861, 'eafad891-9b94-4d26-a80c-4ff48501f2c7'), (3822, 'b9bc39ef-056a-47c1-9e4b-02df6b6d4287'), (18158, '3e75b4bb-2900-466e-97bb-0edb64a61f8a'), (18176, '8a8f4414-5efa-401a-908e-15f6d9d435eb'), (26881, '6a5ea7b7-fc90-49e6-b2be-bb315aabc5f2'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (1797, '9cf6eef7-0cf2-4e9a-9443-ab438980128c'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (274, '5406807d-3f0f-4423-b5d3-fcc07ddbc7b9'), (13593, '79e309a1-7373-4306-8dcc-5a0b96bbcdd8'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (292, '085f6a0c-e881-4e0d-a875-1020c3bfd069'), (296, 'da61bbf7-b3d9-4a4d-aeb4-69794ca76f5c'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (6456, 'e67362e4-1b0d-4015-b7db-89d7fe57c072'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (830, '4253d734-8fdc-48f0-ae58-86941ebeca66'), (6463, 'deb1abed-40b0-4c6f-ab05-30f0375d9840'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (833, '483d860d-236e-4772-b051-bfd68f59cd37'), (831, '19f2bada-6be4-4b8f-b3be-f76b3343c3ef'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (837, '03afca3e-9150-4658-a4a2-8f97d1d0efea'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (1359, '1ee49ddb-9952-4cfc-b4ed-2acfce41628f'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (3416, 'aaa85ac8-8c40-4d85-b9e9-f810b639472a'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (19808, '5213e06f-bf0a-45be-86be-92a7b345964b'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (16759, '1b62ceba-cef9-48f4-9d42-37f27b4c1c9c'), (22393, 'b740f9d0-5fa2-469f-b406-53cacf9fde50'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (15741, 'e52c1082-1985-46a1-afec-26024efe0f42'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (9094, '95117b28-a37b-42d9-81d4-c4adcc0503c9'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (9112, '06791d3f-d0a1-4363-bfe4-0f0be4fc08fd'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (9117, '74400b1c-c3d4-4221-bf3d-d615c6862e50'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (31647, '2e660078-e155-46a0-9ee9-a99bab76bb1c'), (25518, '1f847aca-c3cc-4179-894b-7deef51d2560'), (10161, '0f40297b-1b0c-4993-9ea8-f5d11cc8b385'), (949, 'be5a6f56-0fb8-49e8-a5a3-3ec839ae461e'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (5569, '352e8632-382b-4223-8823-188eb1525f10'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (18388, '1a9977e1-1c9c-4e32-becc-6fd62e8516a8'), (10198, '19eb7655-1d17-42bc-985e-b1a424722b72'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (30170, 'fd65dda2-bf03-4ea4-81a5-f79c9e951095'), (5085, 'aa82db4a-174d-4da8-b638-41e10da5f5b2'), (2527, 'b35cdd4c-87c8-40bf-98a7-98cf594734a7'), (2534, 'c18dd192-33c8-464e-a9d7-524374c7162d'), (18406, '09e73a09-80b2-4ec3-b425-16cbd059b289'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (18411, '4be73c5f-f784-4448-8d45-1061cddc185a'), (5103, '0606953a-4aeb-4ca3-b14c-885e315fb1d4'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (5108, '0e1c7ec4-b9e7-4964-bac6-22f4455d6568'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Datasets without language challenge

Related to https://github.com/huggingface/hub-docs/issues/986.

## Context

The Hugging Face Hub hosts hundreds of thousands of public models and datasets. These datasets and models cover a wide range of languages. One of the main ways in which it's possible to know what language a dataset is in is by looking at the `language` field in the dataset's [metadata](https://huggingface.co/docs/hub/datasets-cards#dataset-card-metadata)  section of the dataset card. 

```yaml
language: 
- ""List of ISO 639-1 code for your language""
- lang1
pretty_name: ""Pretty Name of the Dataset""
tags:
- tag1
- tag2
license: ""any valid license identifier""
task_categories:
- task1
#### Why is Language Metadata Important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. 

Many people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.
### Language Metadata for Datasets on the Hub

There are currently ~50K public datasets on the Hugging Face Hub. Metadata about the language used in a dataset can be specified using a [YAML](https://en.wikipedia.org/wiki/YAML) field at the top of the [dataset card](https://huggingface.co/docs/datasets/upload_dataset#create-a-dataset-card).

All public datasets specify 1,716 unique languages via a language tag in their metadata. Note that some of them will be the result of languages being specified in different ways i.e. `en` vs `eng` vs `english` vs `English`. 

For example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML metadata (indicating English):

<p align=""center""> 
 <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_metadata.png"" alt=""Screenshot of YAML metadata""><br> 
<em>Section of the YAML metadata for the IMDB dataset</em> 
 </p>
### Language metadata for datasets on the Hub

There are currently ~50K public datasets on the Hugging Face Hub. Metadata about the language used in a dataset can be specified using a [YAML](https://en.wikipedia.org/wiki/YAML) field at the top of the [dataset card](https://huggingface.co/docs/datasets/upload_dataset#create-a-dataset-card).

All public datasets specify 1,716 unique languages via a language tag in their metadata. Note that some of them will be the result of languages being specified in different ways i.e. `en` vs `eng` vs `english` vs `English`. 

For example, the [IMDB dataset](https://huggingface.co/datasets/imdb) specifies `en` in the YAML metadata (indicating English):

![Screenshot of YAML metadata](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/huggy_lingo/lang_metadata.png)

* Section of the YAML metadata for the IMDB dataset*
# Databricks ❤️ Hugging Face: up to 40% faster training and tuning of Large Language Models


Generative AI has been taking the world by storm. As the data and AI company, we have been on this journey with the release of the open source large language model [Dolly](https://huggingface.co/databricks/dolly-v2-12b), as well as the internally crowdsourced dataset licensed for research and commercial use that we used to fine-tune it, the [databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k). Both the model and dataset are available on Hugging Face. We’ve learned a lot throughout this process, and today we’re excited to announce our first of many official commits to the Hugging Face codebase that allows users to easily create a Hugging Face Dataset from an Apache Spark™ dataframe.
 
Here is the question: What is the purpose of the `language` field in Hugging Face datasets?","The purpose of the `language` field in Hugging Face datasets is to specify the language of the dataset, allowing users to filter datasets by language and find relevant datasets for a particular language."
What is the main advantage of duplicating a Space in Hugging Face?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (7692, '2785b812-48ad-476f-834c-8694f938e3dd'), (7695, 'f5e49d60-b8bb-4254-a782-ed38da828ba8'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (23573, '952579f3-7c93-46c5-ae22-ea5e5fd0b30a'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (2587, 'ddf86248-9e13-4c6a-b61c-953e9a627629'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (22564, '510df01a-14dd-43b4-ab24-0e71cb97c5b2'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (18990, '004334bc-df33-4770-8ef4-585c87cbca88'), (4656, '8a5eb5ae-f89d-40e1-a550-28aee57cd98f'), (4659, '8dce1a2b-9969-459b-9f01-27b5967452b7'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (24120, 'e1160ee3-8307-4e5d-8ec4-13c38625d7d4'), (5178, 'f1cd9671-f15a-427f-899a-dca559a48209'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8273, '5ebe427f-bfba-401b-bc93-31042f3611b1'), (5721, 'e17e37df-9c68-4c4f-82a0-c065bf625bea'), (6748, 'ca6e6fb4-0b0c-4116-9409-fdb6e4e1ca30'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (31340, '1bffbde6-8c01-4f3d-914e-12799d8de47f'), (27757, '6e5e6c01-c127-42b7-8a35-8df6b5bc34f2'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (19574, 'cc66b1c9-e31b-4dbf-b777-a98a2c4143be'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (25721, '0c43f23f-837a-4591-bf5f-e22a5a6c567e'), (19578, 'c39dbfdb-a1e4-46cb-b486-736ca8e047bb'), (19579, 'f55d236a-ece9-4083-8146-0ed03ecb7235'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25220, 'd9714992-eee7-4b14-bcef-6d8ebda275a0'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (21652, '53aba63a-7c97-4993-a081-5d0b4d76c72d'), (21654, '0ac0c293-df2f-4bd7-ab9a-ef9c0cb31739'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (18099, 'b4a101b0-15e0-4ebf-9686-5edca4169313'), (1718, '02534d35-0b3a-48b9-83f2-849c6c0852b9'), (9405, '0766f520-b302-4b98-9285-8e0e6af106c2'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (26836, '9d0493e6-8f23-4c53-b979-3cfab7d1e205'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (6878, '12422685-88d6-4ee6-800d-5067c7729e36'), (6879, 'b12345f2-cef9-4580-a7e0-773737d19b1f'), (14050, '43174305-dba9-45e8-b46d-9ae2ea56ad48'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (23813, 'e529e90a-7751-49cc-8418-262a566b79d9'), (31498, 'd259184d-d3d4-431e-a435-287d452d7127'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (4365, '0e049057-3bcb-4ecb-afc0-901cd049382c'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (12064, 'ec49857b-c395-49bb-b24a-dba896fff9fc'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (10074, '2aed197d-f4fc-4c34-a307-1983a496c31f'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (18783, '44d0b22a-e8ef-4cba-bda9-fd404cb063c2'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (29558, '2b898abd-2ecb-4f67-acb7-f2cab065145f'), (24440, 'ee70f701-6e4d-48e1-addf-d47d469770fa'), (16764, '8e676bbc-b3e9-4463-a23d-1131ef9602fb'), (9604, '02063fa4-9d75-47e7-9541-a9ae55da82f0'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (23943, 'cfd67c0b-d84d-47e0-a02b-dd4ad0fd54a0'), (29577, '34a6c2e9-489b-4e28-a03f-f42b3d55dc90'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (31133, 'e42f7993-643f-492e-a6db-29ed9c755b45'), (31135, '957d568e-e103-45a2-9bae-02e2fbebbc4a'), (16287, 'c79b58a7-88e3-489e-a164-01632304341b'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (16288, '93acb11b-2034-46a7-bd59-7e9638a345a4'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (16317, '2b682e0c-ad28-4060-9c8d-4754859a1913'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (13765, 'f3ffb997-104d-46cc-b963-61ec795f108b'), (29135, 'aa204127-7dec-4d3b-b82a-c7d7aeb22e77'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9174, '3767ffbd-cac9-45c5-b684-68ec257368f6'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (9180, '1f0cf2a5-54cf-4122-935e-e21526ce5297'), (9179, '62958637-5904-47c3-aaf4-32018c284d91'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (31711, '09414786-5a2b-4e41-9468-db489cb73477'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19438, '1b351e34-f0ef-408a-b6b2-209c3c330c69'), (15863, 'e17826a6-d5b3-4729-ad97-387ae765122d'), (10235, '6285ff51-c4b7-47f1-9c33-4cf592322ccc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Duplicating a Space for private use

While you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space,
and then use it to make as many requests as you'd like!

The `gradio_client` includes a class method: `Client.duplicate()` to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens) or be logged in using the Hugging Face CLI):

```python
import os
from gradio_client import Client

HF_TOKEN = os.environ.get(""HF_TOKEN"")

client = Client.duplicate(""abidlabs/whisper"", hf_token=HF_TOKEN)
client.predict(""audio_sample.wav"")

>> ""This is a test of the whisper speech recognition model.""
```

## Duplicating a Space for private use

While you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space, and then use it to make as many requests as you'd like!

The `@gradio/client` exports another function, `duplicate`, to make this process simple (you'll need to pass in your [Hugging Face token](https://huggingface.co/settings/tokens)).

`duplicate` is almost identical to `client`, the only difference is under the hood:

```js
import { client } from ""@gradio/client"";

const response = await fetch(
	""https://audio-samples.github.io/samples/mp3/blizzard_unconditional/sample-0.mp3""
);
const audio_file = await response.blob();

const app = await duplicate(""abidlabs/whisper"", { hf_token: ""hf_..."" });
const transcription = app.predict(""/predict"", [audio_file]);
You can optionally choose a dataset that contains model results and other configuration options such as splits, subsets or dataset revisions.

<figure class=""image text-center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/scalable-data-inspection/space_duplication.png"" alt=""Creating a new dataset visualization with Spotlight by duplicating a Hugging Face space."">
</figure> 


## What’s next?

With Spotlight you can create **interactive visualizations** and leverage data enrichments to **identify critical clusters** in your Hugging Face datasets. In this blog, we have seen both an audio ML and a computer vision example.

You can use Spotlight directly to explore and curate your NLP, audio, CV or multimodal dataset:
![You can now find interactive demos under ArXiv papers](/blog/assets/arxiv/recording.gif)

Since its launch in October 2021, Hugging Face Spaces has been used to build and share over 12,000 open-source machine learning demos crafted by the community. With Spaces, Hugging Face users can share, explore, discuss models, and build interactive applications that enable anyone with a browser to try them out without having to run any code. These demos are built using open-source tools such as the Gradio and Streamlit Python libraries, and leverage models and datasets available on the Hugging Face Hub.
More ways to create Spaces

## Duplicating a Space

You can duplicate a Space by clicking the three dots at the top right and selecting **Duplicate this Space**. Learn more about it [here]](./spaces-overview#duplicating-a-space).

## Creating a Space from a model

New! You can now create a Gradio demo directly from most model pages, using the ""Deploy -> Spaces"" button.

<video src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/model-page-deploy-to-spaces.mp4"" controls autoplay muted loop />

As another example of how to create a Space from a set of models, the [Model Comparator Space Builder](https://huggingface.co/spaces/farukozderim/Model-Comparator-Space-Builder) from [@farukozderim](https://huggingface.co/farukozderim) can be used to create a Space directly from any model hosted on the Hub.
 
Here is the question: What is the main advantage of duplicating a Space in Hugging Face?","The main advantage of duplicating a Space in Hugging Face is to have unlimited usage of the Space without getting rate-limited by Hugging Face, allowing you to make as many requests as you'd like."
What metadata should you add to list linked models in a Hugging Face Space?,"[(27137, '29e333a5-a4d1-4f3a-92da-6eca5878e973'), (7178, '086ecf90-fb2d-4fb0-9e37-23a45c3703ea'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (19998, '40f7e87f-9355-49b7-a1a4-42183c181da6'), (19999, '49eb2d92-84fe-4bcf-a1c8-1bfdb01bcb11'), (16928, '1dd8da94-9861-4d4b-ae5d-0182de0a33d2'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (28718, '20436458-7cd8-4a96-8331-99513560667b'), (19503, '8227f7ee-a80a-4c29-b11a-108dd1adaef9'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (29233, '74f4f91f-2db4-42a8-a508-33e42d02d65f'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (29235, '28f83b83-b564-4757-a1e4-3c0a127d21bd'), (13364, 'ff9bd8cb-ca24-47d9-9550-421a25bdca05'), (19505, '9144c324-5a19-484b-9c67-cabcc0fcd676'), (4665, 'fafcf21c-e376-460a-bb7c-dcca97430243'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (4192, 'fc859fbf-f289-4177-a3a6-21b11cb22ad3'), (22644, 'ed1a5ca4-ee08-4d87-8519-b3bc240fd9e8'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (13447, 'b6ea664c-f662-4b91-8338-b97b37b1fb7e'), (13448, 'c265cc67-92bb-48f5-bfd6-5569ba9863df'), (13449, '7e3bdf26-52e5-4da5-ad7f-689dd45b292f'), (13452, 'ca545bf0-ea1e-4d32-86dc-0edf038e192c'), (13454, 'e47afbdc-2c45-4282-90cf-dd7b46634edb'), (19603, '561460ee-7e32-4394-b0d3-d59f4b7790dc'), (23714, 'a2d69303-a006-44f0-8008-1accb10a3697'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (22206, '805dce32-2ac6-4a44-bed2-ad1994630bc2'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (25805, 'fcaebe7b-c2ec-4995-829f-39de8f874270'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (26846, 'c96e859b-559a-45e4-aafb-93a24d363941'), (26847, '65fa4d33-0ac5-4808-93a9-37c439364727'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (19683, '5d3bfb1d-f892-4db6-8596-bd364235244a'), (22757, '02c5f13b-34f8-4850-8e1d-8d5e0c9efb32'), (25831, '0c6c27f1-da92-4e17-8600-7b047d8abb35'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (18165, '85426819-3385-487c-ba81-acbcb88d5b3f'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (16638, '6214eb65-8a1c-4818-ba45-434c36461a1f'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (13582, 'dcc529f2-40dc-4436-b0cc-68eff7adec83'), (28440, 'f186228b-4ecd-4f33-a849-39b930f0800a'), (281, '85447511-882f-4125-ab3c-e16041f25aec'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (17699, 'f896ccff-a53b-4e44-ab27-f7e12cc98e4a'), (7989, '7bf24f34-1a48-44c6-b805-592b4c1b9e1d'), (16701, '0c48fb11-97b9-4772-81d5-7152331ac26b'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (4948, '395b39f4-4332-49f3-8d68-c9acecd21bc5'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (5473, 'a64fa658-3cee-48f1-9ae3-9adc2f584c26'), (20833, '31e0f961-45b8-46bb-91ae-e87065f65b67'), (3937, '7450b67c-b9da-4dd2-a9a5-2772aa7ae9bf'), (358, '1f015812-8d62-4b47-a953-b29505c9c0e8'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (19330, '06bc4b79-264e-4176-8ccd-68c6f6bdfe54'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (19333, '7ba0b0f8-a75c-480d-b712-9a9b33d8f2e6'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (19335, 'e7a25af1-ba2a-4e2e-af04-f55deda54356'), (8586, '90a0534c-f6eb-45e6-8c1b-b6c343f84cf8'), (20876, 'a784cd91-18de-4adf-a288-2724d2b961e1'), (9101, '2108cabf-f9a6-4b41-aced-03134c8e8955'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (10141, '09343cb8-0577-4400-a10d-173d8b90c43d'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (16319, '3f64c3db-fe7d-4c58-9506-1df4f37aa265'), (6079, '2234d568-f21c-4b71-b407-a24d00ca3c01'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16322, 'ad245a5f-b243-4e34-b382-6e08fd6c4955'), (16320, 'aceded5d-8291-4ec6-937b-4ffa5e060758'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (14790, '9b894d75-a0f9-4ce1-aa43-7ed77bb8059c'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (21963, '68553924-5e76-42e9-89f5-36f1e2aa3328'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (2516, '02dc7eba-815e-47f3-a8bc-731fc3d4a427'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (18395, '4d9fdde3-9440-4453-9964-4991c551810e'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25057, '85b645cd-7921-4981-9d5f-dcbd47710494'), (5092, '02ac121c-75cb-4166-9355-fda09a9dc0e0'), (15342, '6b488267-10cb-4899-8876-aeebd1b6f856'), (15345, 'd28cdc9c-d295-4ed8-8f73-d0977f629967'), (30197, '6ae5b483-bafb-4b38-846d-c5169cc81b22'), (29691, 'b7f07372-3cfe-4ff3-8551-9286f01015e5'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Model card metadata[[model-card-metadata]]

If you have done a little exploring of the Hugging Face Hub, you should have seen that some models belong to certain categories: you can filter them by tasks, languages, libraries, and more. The categories a model belongs to are identified according to the metadata you add in the model card header.

For example, if you take a look at the [`camembert-base` model card](https://huggingface.co/camembert-base/blob/main/README.md), you should see the following lines in the model card header:
```

    *Note*: Here's an [overview on building demos on Hugging Face Spaces](./spaces-overview) and here are more specific instructions for [Gradio](./spaces-sdks-gradio) and [Streamlit](./spaces-sdks-streamlit). 

4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A ""Linked Models"" button should appear in the top right corner of the Space, as shown here: 

    ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)
    
    *Note*:  You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference).


Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes 🤗
We will now create the model card. The card should match the expected Hugging Face Hub format: a markdown part and a metadata section, which is a `yaml` section at the top. The keys to the metadata section are defined [here](https://huggingface.co/docs/hub/models-cards#model-card-metadata) and are used for the discoverability of the models. 
The content of the model card is determined by a template that has a:
- `yaml` section on top for metadata (e.g. model license, library name, and more)
- markdown section with free text and sections to be filled (e.g. simple description of the model),
The following sections are extracted by `skops` to fill in the model card:
- Hyperparameters of the model,
- Interactive diagram of the model,
- For metadata, library name, task identifier (e.g. tabular-classification), and information required by the inference widget are filled.
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
The Hugging Face Hub hosts hundreds of thousands of public models and datasets. Public doesn't necessarily mean open-source without any limitations. Authors can define which license applies to the work they share (e.g. [MIT](https://opensource.org/license/mit/), [Apache2.0](https://www.apache.org/licenses/LICENSE-2.0), [OpenRAIL](https://huggingface.co/blog/open_rail), etc.). All users must be able to quickly know which license applies to which model and even to list models with a specific license (e.g. [Apache2.0](https://huggingface.co/models?license=license:apache-2.0&sort=trending)). The Hub relies on the [Model Card](https://huggingface.co/docs/hub/model-cards) to do so. A Model Card is a file attached to a model providing handy information. They are essential for discoverability, reproducibility and sharing. In our case, we will focus on the [metadata](https://huggingface.co/docs/hub/model-cards#model-card-metadata) section of the Model Card. This metadata contains valuable
 
Here is the question: What metadata should you add to list linked models in a Hugging Face Space?","According to the context, you can add linked models manually by explicitly updating them in the README metadata for the Space, as described here: https://huggingface.co/docs/hub/spaces-config-reference."
What is the typical use case for the `facebook/fasttext-language-identification` model?,"[(5123, '3c49348e-9d5c-46a4-aaf6-e5f3275adbc0'), (7685, '5747e091-b999-48a3-a7dc-3f5517777c0d'), (19463, '72f624a9-b037-428d-9837-de3484ab1799'), (19465, '030ea1ff-9d3b-4c2f-bea7-3c5df8e44621'), (22539, 'd5069ed2-b237-4d02-8d50-e26d6e63c1d7'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (2585, '1323dd83-ede5-4548-8a85-0723f7314b3f'), (23066, 'db5b29f6-c011-4064-83aa-0677c241c9dd'), (23067, 'c4124e63-351d-48b1-ad95-32c2212f2547'), (16924, '38fe9939-2e02-47cc-805a-7ffc8340c582'), (16923, 'e71391a5-9dd4-45f7-be7f-aeef9418a46c'), (27164, 'b8b04fc5-bbbf-4e41-8425-8b37061edc2c'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (16927, '4d6c6db7-e0a7-4ce7-a301-1470bd171cb2'), (23069, '3ccd961f-5be3-4ee6-b807-4580897911c9'), (22053, 'e4a8d350-c84c-4e46-b354-d2620d149413'), (551, 'a9ae3818-04e0-4d9c-821d-f21818cb2e3d'), (24638, 'e51d5c4d-d26c-4446-a519-54d0ae6188d9'), (6719, 'ebe73be1-81c8-4014-b2cc-fd3cfa0649f9'), (74, 'b688f1bf-6603-4b03-be54-9d74d53219a4'), (21075, 'e9d9a12a-4927-48b5-8529-9587d83ad1e4'), (6758, '3d29e927-9a4f-4916-9290-e53743ad7e85'), (6759, 'dddf2ab7-6d0b-497b-8af7-2a0381ccd27d'), (11886, '985c9ea9-0716-4280-9c11-61635d83ad1c'), (1138, '1e450391-f267-4bbe-bb5b-26cb743419c2'), (6267, 'ecdf2c58-4706-4b03-b22c-746c89d363b6'), (26749, '330872b8-a907-4163-9dd9-036e06f92839'), (6270, 'c18172bc-ee0e-4969-8110-192dd5331ee7'), (9343, '772ffc0a-e399-42b4-a652-6bdc7a6bc4a1'), (6271, 'db13ad85-6738-4e86-bea9-8e92682af368'), (3716, '7409694b-c0ab-407c-8988-a97aae91288c'), (23172, 'def32509-da1d-4b97-ad0b-943987ec2382'), (15495, '3a2d93c5-ae50-48ec-bed4-4cfbdee9a9e3'), (18567, '82b5f6ec-fc59-45e3-8d39-a7e17af46f95'), (22161, '5c8afd47-f041-4bfe-9667-5a59a1024d0d'), (19605, '8d648ddf-1925-42b3-beb0-8fddac5f8017'), (16030, 'ebff3b8e-d84e-42ab-8015-0e9145257735'), (8862, '4d484846-68e1-4cdd-a00e-12f8d831ede6'), (7342, '4e3f892e-b398-4de7-91ee-ce188a823d68'), (8367, 'd8f0d66f-9fee-4bc8-8d6d-9f63f02222d8'), (30897, 'c4d781f5-c154-4a61-bdd9-76776461be08'), (8884, 'e8b9068c-8575-4234-a533-7fb6559c7234'), (28342, '85ecdc8d-9de3-4edb-84da-5b6da8a47501'), (28345, '39d9ae5f-35bc-45a5-9363-3a09318a777a'), (8392, '2f6fcdd4-f780-4959-92b2-d153fc16cdb7'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (18134, 'edec057b-dc14-495b-893a-7a04a35f89ac'), (22236, '7f5de617-4a0c-4797-b576-11a06835ca5c'), (26333, '9272b200-2eb1-44c7-abe3-5a5a7bc26b9d'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (8415, '2c5bee01-bd26-4a91-8bd7-06aeeaad462e'), (15585, '31f7ebf4-9f31-4897-9be5-0f1afccf4531'), (8418, '8f7ee68c-66f6-42cf-a640-d0768ab7fd81'), (3810, 'da9921e9-a9a0-4ba5-9305-58eaf6c5958d'), (15591, '7fb27af5-3a32-42dd-9212-725d7fbc3dc7'), (7400, '1d07b405-1719-43b8-8e99-83a98564ff4e'), (24816, 'a03b774f-629a-4869-940b-b700baa9d0d2'), (13552, '74848e30-fb78-4cca-afe2-1d027939cfbf'), (250, 'efa79a0b-ef9a-44ee-8dbc-9370a0757031'), (11517, 'cdd01495-d1de-426f-8b0f-47e08d93d1b8'), (21772, '8c847233-2be4-468a-bc36-69092f8589b1'), (14093, 'c0788b7f-53aa-476b-8096-aa38772a8149'), (11022, '4b2b2fd8-7a24-4962-bb2a-dcf2bdb2139e'), (1806, '7e4b23b4-6f6d-4dfa-a292-cab055399716'), (18194, 'db26ab63-8606-4fce-adeb-9039344f080a'), (31005, 'dfbe26eb-d794-43e1-8e52-483e186cd71c'), (20774, 'ad1a0664-122f-404f-99e9-817b2bd2d8f0'), (21803, 'c614deb4-46b5-43f5-b6a5-07a9c59cbd01'), (13611, '338896a5-047e-40e7-b52a-7336ea945618'), (7474, 'c46756cf-62a1-4fd4-913c-051f6e89f2d5'), (310, 'ba32687f-1ed4-4efe-8c70-d47465e2733b'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (12095, 'c4744c46-a374-4937-a8a6-60a1818dbaae'), (3904, '81d2775b-68a0-4e4e-a8a4-398be64c86df'), (836, '9ee86ea3-f33f-4cf5-9e92-0ad9009e33ba'), (837, '03afca3e-9150-4658-a4a2-8f97d1d0efea'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (10059, '4e4abb18-bba8-4f2d-a50a-284552555744'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (6993, '4243bc5b-c7a6-4a08-8545-448007bdb844'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (10583, '28277525-2ccf-48f3-952c-3b61615c6e51'), (13657, '12c37295-423f-4088-86da-e28cc7be4d1e'), (13660, '5c355e34-08c0-4619-b727-5e939d4b8d9a'), (21342, '43c34d2b-8a1a-4a72-8117-9d57107098c8'), (8036, 'a35a9cea-0ee5-4f3b-9eb8-d93d79922e35'), (24421, '441ce5ec-0c89-4dea-9356-3ec1c2d4fb74'), (15208, 'ee05e977-f99f-4b73-a248-a9c10f227f38'), (9070, 'f6396021-41a6-42e7-b12c-de89e8c6a0ea'), (3440, '51f99fe7-7751-4e98-914f-d073dd0afcf5'), (15741, 'e52c1082-1985-46a1-afec-26024efe0f42'), (17282, '4c954188-6044-4551-853d-5d48f138a4b4'), (23427, '44ebafae-3114-46b5-bbfa-3f2d81df8eb8'), (17285, 'e6ec2579-9493-453c-8057-f53e16ef65f5'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (23435, 'bbe49eda-72e1-4d79-b448-5b561246b9cd'), (10647, '4007e9cc-8f05-4126-8d8b-7fd3e90b7b61'), (14232, '2ce49145-eb57-4070-aa42-d8b3677f78aa'), (29610, 'ac1535f9-21ba-44dd-a66f-e962faf6b40f'), (29611, '3c70029e-ab2c-422a-81de-13783746fec2'), (9132, 'acbb6bfd-6d43-4a24-a2a8-92f7765350e9'), (31152, '9a71ca9f-e893-4429-98f0-6ea18dcd2315'), (432, 'f53ed73d-0efd-4683-9b23-13fcca5c8b53'), (16828, 'fc8554fa-b729-48cd-867a-3b1bd7c20354'), (18364, '82091703-8ce2-4abe-bc4a-5a4beda4c923'), (5061, '5ebc4085-43ae-4488-9355-8e1b17dfd415'), (1487, 'cc47f460-9957-43b9-b960-7d3eb58288c0'), (17361, 'b679f05e-592c-437d-9213-00559f003912'), (13266, '2cc7f9e6-9c1e-4dd0-9a6c-a952ef43a49b'), (29654, 'e55bc1f1-11a7-40e2-b557-49baaf068160'), (3547, 'c4046d4c-c32f-4748-8540-956375bf44f9'), (1510, 'a7df3897-3c02-4187-b2c6-88844cd91c0a'), (13290, '498d2730-f4fc-47fc-85f7-2ff7bcdab1e7'), (7147, '5590d5ed-4fcb-49ac-8e0e-1a98b6be734e'), (8173, '5b402856-6a77-4566-9dfd-407dcc650048'), (18426, 'e2f413c4-5d19-4219-baa1-bc1e3f068ff1'), (22524, 'f48a5037-44fc-426e-97ce-da12aa662d06'), (9726, 'ef486ead-c143-4c25-a64a-bf67a128b117')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: #### Predicting the language of a dataset 

Once we have some examples of text from a dataset, we need to predict the language. There are various options here, but for this work, we used the [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model created by [Meta](https://huggingface.co/facebook) as part of the [No Language Left Behind](https://ai.facebook.com/research/no-language-left-behind/) work. This model can detect 217 languages which will likely represent the majority of languages for datasets hosted on the Hub. 

We pass 20 examples to the model representing rows from a dataset. This results in 20 individual language predictions (one per row) for each dataset.  

Once we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:
#### Predicting the Language of a Dataset 

Once we have some examples of text from a dataset, we need to predict the language. There are various options here, but for this work, we used the [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model created by [Meta](https://huggingface.co/facebook) as part of the [No Language Left Behind](https://ai.facebook.com/research/no-language-left-behind/) work. This model can detect 217 languages which will likely represent the majority of languages for datasets hosted on the Hub. 

We pass 20 examples to the model representing rows from a dataset. This results in 20 individual language predictions (one per row) for each dataset.  

Once we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:
```

Here is how to use this model to detect the language of a given text:

```python
>>> import fasttext
>>> from huggingface_hub import hf_hub_download

>>> model_path = hf_hub_download(repo_id=""facebook/fasttext-language-identification"", filename=""model.bin"")
>>> model = fasttext.load_model(model_path)
>>> model.predict(""Hello, world!"")

(('__label__eng_Latn',), array([0.81148803]))

>>> model.predict(""Hello, world!"", k=5)

(('__label__eng_Latn', '__label__vie_Latn', '__label__nld_Latn', '__label__pol_Latn', '__label__deu_Latn'), 
 array([0.61224753, 0.21323682, 0.09696738, 0.01359863, 0.01319415]))
Hugging Face is now hosting official mirrors of word vectors of all 157 languages and the latest model for language identification. This means that using Hugging Face, you can easily download and use the models with a few commands. 

### Finding models

Word vectors for 157 languages and the language identification model can be found in the [Meta AI](https://huggingface.co/facebook) org. For example, you can find the model page for English word vectors [here](https://huggingface.co/facebook/fasttext-en-vectors) and the language identification model [here](https://huggingface.co/facebook/fasttext-language-identification).


### Widgets
This integration includes support for text classification and feature extraction widgets. Try out the language identification widget [here](https://huggingface.co/facebook/fasttext-language-identification) and feature extraction widget [here](https://huggingface.co/facebook/fasttext-en-vectors)!
### Using Librarian-Bot to Update Metadata

To ensure this valuable language metadata is incorporated back into the Hub, we turn to Librarian-Bot! Librarian-Bot takes the language predictions generated by Meta's [facebook/fasttext-language-identification](https://huggingface.co/facebook/fasttext-language-identification) fastText model and opens pull requests to add this information to the metadata of each respective dataset. 

This automated system not only updates the datasets with language information, but also does it swiftly and efficiently, without requiring manual work from humans. Once these pull requests are approved and merged, the language metadata becomes available for all users, significantly enhancing the usability of the Hugging Face Hub. You can keep track of what the librarian-bot is doing [here](https://huggingface.co/librarian-bot/activity/community)! 


#### Next steps
 
Here is the question: What is the typical use case for the `facebook/fasttext-language-identification` model?","The typical use case for the `facebook/fasttext-language-identification` model is to predict the language of a dataset, specifically to detect the language of text data in a dataset hosted on the Hugging Face Hub."
Which hardware tier provides persistent storage for Hugging Face Spaces?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (1046, '2efa8a05-4ce9-4c09-bfd5-107d39229dee'), (1047, '1ace34f7-d7bc-410a-be23-3bae8d4d2876'), (14872, '288584f1-35dc-4e08-b78c-4bbd92c996fe'), (14876, '45893d20-107c-4db9-a5c6-937c7bfea6c4'), (2589, 'c25fa226-6269-432b-8c83-5e07a1e79976'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (2592, 'aa839149-8f65-4cb4-8a0f-5b8464aed2f5'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (1058, '6cec5090-3461-44a3-a5b6-bdc99a04fd47'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (4653, '8d3b2b54-b599-4810-8878-8b6b47d07133'), (4656, '8a5eb5ae-f89d-40e1-a550-28aee57cd98f'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (24120, 'e1160ee3-8307-4e5d-8ec4-13c38625d7d4'), (3643, 'ee5ea5c1-c94c-4b39-90c8-92a1683eb5c6'), (15936, 'ab08450b-3b5c-4bbc-8c0e-355d1b29997d'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8273, '5ebe427f-bfba-401b-bc93-31042f3611b1'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (10848, 'e24b0ae6-43f0-498e-bd66-a33c5139c50d'), (10861, 'abe88219-dc18-4329-a486-d991c03dc29d'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (11389, '7cf525f3-0375-4371-b8ed-2e4e97568b45'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (21652, '53aba63a-7c97-4993-a081-5d0b4d76c72d'), (21653, '15ea1eb2-83fd-4842-9c19-9aa3b776051e'), (21654, '0ac0c293-df2f-4bd7-ab9a-ef9c0cb31739'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (21655, '8b214d8f-e1d5-414d-8e14-431a64b22550'), (21142, 'c2f93e01-4014-4fea-8f48-8aa1697503c5'), (21657, 'caaddc26-338c-4b5f-89ab-bc044b94afe4'), (26263, '83b240db-acd6-4670-bc6f-1697ec7ed779'), (26264, 'caec1e8d-2ac9-463f-95cd-490112c00671'), (21656, '151f3b8f-ca80-423f-9292-0d7506ae54ee'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (26791, '40f4195c-9f9f-403e-aa4d-69df440b7b16'), (26793, 'cecd3556-f231-4367-889d-2933d7acafc8'), (26794, '1813d2fe-4f25-4ed3-a8ed-ca0946433c02'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (5806, '5054cac8-6958-4df8-9986-f0bb2157877f'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (22211, '740dfc56-1f28-4364-853c-997c6873e87b'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (24276, 'a2805eeb-3e85-4a5e-bb54-b28ce74c3185'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (14050, '43174305-dba9-45e8-b46d-9ae2ea56ad48'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (6393, 'f4416fbc-8a38-4b97-823b-eb555298d294'), (21755, 'e6693759-5af4-4449-9f6f-4c0e45498df9'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (12064, 'ec49857b-c395-49bb-b24a-dba896fff9fc'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (31057, 'eee260cd-cb8d-4ea4-b824-020af026b061'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (29525, '4a7299ab-39a4-4b0e-8944-bf676cb6c96e'), (1368, 'e916d251-892a-48f3-b1ee-a20883a609b6'), (9562, 'b75f1239-64b9-4c25-9106-2b9ff2961471'), (9565, 'db2fe167-21d5-41f8-9529-32f39fe8ca07'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (9567, '21f12568-4fc0-4680-b763-b7907ea0ea81'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (7026, 'a54a170b-414e-4326-bff0-925d0cf07fab'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (11643, '428a11f6-079f-4810-9107-b8d8deefc0de'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (15245, '2bbb0b6d-9b21-4016-a9fb-ed4151f201d8'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (3984, '0b5c9103-9bd3-4af7-a1ec-16b33970a4bc'), (16279, '24565971-98de-4d1a-8c46-330e59ab1b4e'), (16280, 'c23e651c-4bbd-46de-9db5-db7d30be24fa'), (16282, '116380bc-82d3-4b0c-aa7e-4209916db42b'), (31133, 'e42f7993-643f-492e-a6db-29ed9c755b45'), (16287, 'c79b58a7-88e3-489e-a164-01632304341b'), (16288, '93acb11b-2034-46a7-bd59-7e9638a345a4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (11171, '2f84f4bf-52df-4e09-8231-528e901f80f9'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29105, '727ef3c7-a9c8-4f37-b84d-af3a7c52ecdb'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (12728, '4364e7f3-1794-496e-9a2f-a14bcae5a530'), (12220, '1c9c506f-1bca-47c9-9605-33530ec3299d'), (16317, '2b682e0c-ad28-4060-9c8d-4754859a1913'), (29118, '553e8a5e-a197-4b4c-9e25-dce549489f39'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (29135, 'aa204127-7dec-4d3b-b82a-c7d7aeb22e77'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (27606, '803b87b4-9d8e-4ec5-98c7-7785c20c0700'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (25061, 'a686f600-cfe9-467e-9d3e-91dc3239e164'), (15846, 'ee66a65a-c35d-4ed7-acbc-15fce331450b'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (19437, 'ba259ff9-c7ee-4873-9c9a-0babf0b9aec7'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Hardware resources

Each Spaces environment is limited to 16GB RAM, 2 CPU cores and 50GB of (not persistent) disk space by default, which you can use free of charge. You can upgrade to better hardware, including a variety of GPU accelerators and persistent storage, for a [competitive price](https://huggingface.co/pricing#spaces). To request an upgrade, please click the _Settings_ button in your Space and select your preferred hardware environment.
Disk usage on Spaces

Every Space comes with a small amount of disk storage. This disk space is ephemeral, meaning its content will be lost if your Space restarts or is stopped.
If you need to persist data with a longer lifetime than the Space itself, you can:
- [Subscribe to a persistent storage upgrade](#persistent-storage)
- [Use a dataset as a data store](#dataset-storage)

## Persistent storage

You can upgrade your Space to have access to persistent disk space from the **Settings** tab.


<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-storage-settings.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-storage-settings-dark.png""/>
</div>

You can choose the storage tier of your choice to access disk space that persists across restarts of your Space.
To run this matchmaking process continuously, we use **free Hugging Face Spaces hardware with a Scheduler** to keep running the matchmaking process as a background task.

The Spaces is also used to fetch the ELO ratings of each model that have already been played and, from it display [a leaderboard](https://huggingface.co/spaces/huggingface-projects/AIvsAI-SoccerTwos) **from which everyone can check the progress of the models**.

<div align=""center""> 
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/128_aivsai/leaderboard.png"" alt=""Leaderboard""> 
</div>

The process generally uses several Hugging Face Datasets to provide data persistence (here, matches history and model ratings).
If you are using Hugging Face open source libraries, you can make your Space restart faster by setting the environment variable `HF_HOME` to `/data/.huggingface`. Libraries like `transformers`, `diffusers`, `datasets` and others use that environment variable to cache any assets downloaded from the Hugging Face Hub. Setting this variable to the persistent storage path will make sure that cached resources do not need to be re-downloaded when the Space is restarted. 

<Tip warning={true}>
	WARNING: all data stored in the storage is lost when you delete it.
</Tip>

### Persistent storage specs

Here are the specifications for each of the different upgrade options:
![Choose the ZenML Docker template](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/zenml/choose_space.png)

You have the option here to select a higher tier machine to use for your server.
The advantage of selecting a paid CPU instance is that it is not subject to
auto-shutdown policies and thus will stay up as long as you leave it up. In
order to make use of a persistent CPU, you'll likely want to create and set up a
MySQL database to connect to (see below).

To personalize your Space's appearance, such as the title, emojis, and colors,
navigate to ""Files and Versions"" and modify the metadata in your README.md file.
Full information on Spaces configuration parameters can be found on the
HuggingFace [documentation reference guide](https://huggingface.co/docs/hub/spaces-config-reference).
 
Here is the question: Which hardware tier provides persistent storage for Hugging Face Spaces?","According to the context, you can upgrade your Space to have access to persistent disk space from the **Settings** tab, and you can choose the storage tier of your choice to access disk space that persists across restarts of your Space. This implies that all upgraded storage tiers provide persistent storage for Hugging Face Spaces."
What happens to a Space running on free hardware when unused?,"[(3593, '1985987b-6c12-4a20-89af-f6e17405b17c'), (7691, 'fa39068d-a471-4887-a405-eb0139364fec'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (26645, '27f8fd8d-a5b3-4c3a-892d-c64d955368aa'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (4652, 'd139f5cf-a206-4933-bea0-3cb5840d8a2e'), (4653, '8d3b2b54-b599-4810-8878-8b6b47d07133'), (4654, 'fbbddf5c-d10a-4009-8b85-ad5a1a8b7fd5'), (24111, '67ba489b-b1d5-43ea-a8ca-c94dbf1c46ac'), (4656, '8a5eb5ae-f89d-40e1-a550-28aee57cd98f'), (4658, 'a045fae8-db00-4284-8062-d3e7d854f006'), (4659, '8dce1a2b-9969-459b-9f01-27b5967452b7'), (4660, '73eae357-d132-4d5a-b618-751f154c873a'), (4661, 'fa883482-6edc-4a89-97b0-b75abbfcaae6'), (4662, '9ca235e7-3c2b-47d1-8608-bfd5af431c69'), (24119, '57b5b4e8-d18c-4047-90a5-af35ece74c08'), (4664, '39143b8e-b1fe-4350-8660-fa0af84b37f2'), (17466, '31a76609-9b66-4aeb-8d09-68b1fa0b5c33'), (3643, 'ee5ea5c1-c94c-4b39-90c8-92a1683eb5c6'), (20542, '8adb84bf-218f-4821-ae67-8e8603665c9d'), (10306, '64b78289-509a-41d8-b8e5-68066b3d05ea'), (16468, '0b398bdc-5508-4213-88aa-a3e6096ca035'), (19546, '6ed34159-b4cc-49e9-8b78-1a2a3c905259'), (16994, 'b9f3cd9d-226c-48d0-bea5-812c334a52a7'), (28259, '84bcf476-0a5d-4077-83cb-14b35a16bf45'), (20086, 'f455a1e2-5154-4895-b3c6-78eea81732e4'), (21652, '53aba63a-7c97-4993-a081-5d0b4d76c72d'), (21653, '15ea1eb2-83fd-4842-9c19-9aa3b776051e'), (21654, '0ac0c293-df2f-4bd7-ab9a-ef9c0cb31739'), (21655, '8b214d8f-e1d5-414d-8e14-431a64b22550'), (21656, '151f3b8f-ca80-423f-9292-0d7506ae54ee'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (26263, '83b240db-acd6-4670-bc6f-1697ec7ed779'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (18612, 'a8261981-bcba-477a-b826-20a39b31cdb1'), (9405, '0766f520-b302-4b98-9285-8e0e6af106c2'), (22211, '740dfc56-1f28-4364-853c-997c6873e87b'), (6878, '12422685-88d6-4ee6-800d-5067c7729e36'), (6879, 'b12345f2-cef9-4580-a7e0-773737d19b1f'), (11504, '43269dda-c142-498d-8dd5-8ca2146953dc'), (11505, '1a528512-e853-454b-ba90-8c2b73dd3cf9'), (20210, '99bae52b-e039-4795-baa8-e9172b7c66cf'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (6389, 'ad649443-bb1f-4477-adc1-f07902816889'), (6390, '56be4332-f832-455f-bfdd-4d36f94bedaf'), (25335, '669c024b-3bf4-49b4-88cf-3097db3a7d0b'), (6391, '4d5c359c-a1f2-4975-82d9-17e1409c2055'), (6392, 'cc2eb7f9-17aa-44bd-a370-c516443e6750'), (6393, 'f4416fbc-8a38-4b97-823b-eb555298d294'), (6394, '809fdd38-770e-4bb8-ab36-5b2b05841431'), (6395, '5e8d3f79-8a6c-46cb-ab1a-392e0a7216c0'), (6396, '2b52bdc8-be46-45a9-8734-72b2f801bd97'), (6397, 'a42d6ffe-a12b-4bd0-b1ee-ef817d09a213'), (6398, '18753aa2-cd40-4ab8-8abf-b60a28d8c89a'), (6400, '66c3dd8a-fd03-43f8-8964-a386b2c94939'), (25345, 'd20980b8-3557-4edf-951f-fc64368119e1'), (6399, '52b023a9-8cdb-4c93-a5b6-aac05a09c058'), (31508, 'd38f4f16-46d8-4b02-aab8-a91ca6500ba7'), (31509, 'ce6de00d-9b9d-4ca9-b702-b073099a10d6'), (31512, '2e8c6299-5a86-4302-bef4-a0d2189226a1'), (31513, '65ff7785-6988-442d-bddb-55eb49b5b999'), (11034, 'd3c5b6b8-b8f9-42e5-91ac-7aef866ced0a'), (18725, 'b736e5cb-07a5-4b11-8a38-74c6a20f4178'), (31564, '1b00fad6-62cb-4ccd-8b65-11f5870569a6'), (31567, '9dc26dbb-ad6e-43ef-9712-0952181a21f1'), (2385, 'a92a0c2b-6e84-4011-9b54-c9f61b565b96'), (8533, 'ca664c77-d523-4575-afe8-2d553be9ecbc'), (31063, '091c9cf4-f12e-4913-948e-386dc8cf54af'), (1368, 'e916d251-892a-48f3-b1ee-a20883a609b6'), (1369, 'b7c53cf6-2237-417a-bae9-ae108bfe822b'), (4452, '0aadb3d5-551b-4a7a-bbb5-fdb162e65d74'), (3942, 'b94b5ec5-7ed0-4a6a-abb0-7f8bbc3cc4c3'), (19307, 'f9cd260e-349e-4526-9b72-c3bb7c2a7109'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (29554, '0db755f8-6e26-4a56-89d3-6195a1b5576a'), (9594, '74d577bc-ac5a-4176-aeda-6ca1f6b67bfc'), (11643, '428a11f6-079f-4810-9107-b8d8deefc0de'), (11645, '8cf0cb38-14ac-429b-8ecb-da8db5863789'), (7038, '0152a05f-3a1a-4ff4-ad49-30f2fe494f73'), (11646, '61dc5a8f-c0ed-44ac-906e-d23266690446'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (9607, '172cfcc4-2f07-490d-ba2b-640c24562728'), (29578, '3fd981a2-5a54-485f-8828-aae698a9e50c'), (19853, '40bb732f-f865-45e3-bf29-9f8451bcb193'), (3984, '0b5c9103-9bd3-4af7-a1ec-16b33970a4bc'), (16279, '24565971-98de-4d1a-8c46-330e59ab1b4e'), (16280, 'c23e651c-4bbd-46de-9db5-db7d30be24fa'), (24471, 'ac733b1b-1830-4988-b281-99b7c5eef725'), (16282, '116380bc-82d3-4b0c-aa7e-4209916db42b'), (16281, 'c974fec2-e2dd-4886-a76a-884c7bacd773'), (16285, 'c9021bf9-9767-4179-a28b-8287f8c9392c'), (16286, 'c2f48a5a-7592-4636-be21-3bbaea8309fa'), (16287, 'c79b58a7-88e3-489e-a164-01632304341b'), (31133, 'e42f7993-643f-492e-a6db-29ed9c755b45'), (10665, 'e14680b1-7738-4c0b-a87e-9a53b513187f'), (16313, '52146a5e-9e3d-4a0c-adda-54eaa8bdf401'), (12230, '37bed0b9-108e-4d1f-8832-16b1705a0186'), (30667, '5ab35ed2-c28e-4c60-be97-eb6e05d3cce4'), (29135, 'aa204127-7dec-4d3b-b82a-c7d7aeb22e77'), (9688, '0302dee3-bb5a-42fa-b75b-393be7b4e362'), (9179, '62958637-5904-47c3-aaf4-32018c284d91'), (9180, '1f0cf2a5-54cf-4122-935e-e21526ce5297'), (26076, '9f623527-33e1-4536-9815-7a51de9b0760'), (15845, 'ad1b20f3-a36d-4007-b065-81501dd109c6'), (15846, 'ee66a65a-c35d-4ed7-acbc-15fce331450b'), (15847, '5ec00348-4cb1-4058-99dc-6f4c0b0a7a3a'), (25086, 'b5bff744-22d3-44e6-9dea-61ee62df0189')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

## Billing

Billing on Spaces is based on hardware usage and is computed by the minute: you get charged for every minute the Space runs on the requested hardware, 
regardless of whether the Space is used.

During a Space's lifecycle, it is only billed when the Space is actually `Running`. This means that there is no cost during build or startup.

If a running Space starts to fail, it will be automatically suspended and the billing will stop.

Spaces running on free hardware are suspended automatically if they are not used for an extended period of time (e.g. two days). Upgraded Spaces run indefinitely by default, even if there is no usage. You can change this behavior by [setting a custom ""sleep time""](#sleep-time) in the Space's settings. To interrupt the billing on your Space, you can change the Hardware to CPU basic, or [pause](#pause) it.

Additional information about billing can be found in the [dedicated Hub-wide section](./billing).

### Community GPU Grants
Some Spaces might have environment variables that you may need to set up. In these cases, the duplicate workflow will auto-populate the public Variables from the source Space, and give you a warning about setting up the Secrets. The duplicated Space will use a free CPU hardware by default, but you can later upgrade if needed.

## Networking

If your Space needs to make any network requests, you can make requests through the standard HTTP and HTTPS ports (80 and 443) along with port 8080. Any requests going to other ports will be blocked.

## Lifecycle management

On free hardware, your Space will ""go to sleep"" and stop executing after a period of time if unused. If you wish for your Space to run indefinitely, consider [upgrading to a paid hardware](./spaces-gpus). You can also manually pause your Space from the **Settings** tab. A paused Space stops executing until manually restarted by its owner.
Paused time is not billed.

## Helper environment variables
<Tip warning={true}>
	WARNING: all data stored in the storage is lost when you delete it.
</Tip>

### Persistent storage specs

Here are the specifications for each of the different upgrade options:

| **Tier**        	| **Disk space** 	| **Persistent** 	| **Monthly Price** 	|
|------------------	|------------------	|------------------	|----------------------	|
| Free tier        	| 50GB            	| No (ephemeral)  	| Free!                	|
| Small           	| 20GB             	| Yes           	| $5                	|
| Medium         	| 150 GB           	| Yes           	| $25                	|
| Large         	| 1TB              	| Yes           	| $100                	|


### Billing

Billing of Spaces is based on hardware usage and is computed by the minute: you get charged for every minute the Space runs on the requested hardware, regardless of whether the Space is used.
Billing of Spaces is based on hardware usage and is computed by the minute: you get charged for every minute the Space runs on the requested hardware, regardless of whether the Space is used.

Persistent storage upgrades are billed until deleted, even when the Space is not running and regardless of Space status or running state.

Additional information about billing can be found in the [dedicated Hub-wide section](./billing).

## Dataset storage

If you need to persist data that lives longer than your Space, you could use a [dataset repo](./datasets).
## Hardware resources

Each Spaces environment is limited to 16GB RAM, 2 CPU cores and 50GB of (not persistent) disk space by default, which you can use free of charge. You can upgrade to better hardware, including a variety of GPU accelerators and persistent storage, for a [competitive price](https://huggingface.co/pricing#spaces). To request an upgrade, please click the _Settings_ button in your Space and select your preferred hardware environment.
 
Here is the question: What happens to a Space running on free hardware when unused?","According to the context, a Space running on free hardware will be automatically suspended if it is not used for an extended period of time (e.g. two days)."
What tool does Hugging Face recommend for identifying languages in multilingual datasets?,"[(21509, '2e90ee09-c728-44e9-aa15-984bb64553e5'), (7687, '20608e74-1307-46cf-9341-09115b2bb0ac'), (22539, 'd5069ed2-b237-4d02-8d50-e26d6e63c1d7'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (23059, 'de332ba3-fd9d-44d5-89a5-5c18bdb57ded'), (23060, '09e43017-f9b1-44a7-918e-57bd10790d6c'), (23061, '1fa89fbd-1f12-41ad-a09d-613f63f32b6b'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (18450, '54e4b974-9b5d-49fc-9a44-63e631c2dba4'), (23064, '80eb8bc8-3947-409c-bc93-d0bbe04d5069'), (18452, '83b3bd6c-6dd0-41b4-a2f2-4ae71a24bfa1'), (21012, '4252b2f2-3c37-45fc-8476-73ec6c4c1ee7'), (23067, 'c4124e63-351d-48b1-ad95-32c2212f2547'), (16924, '38fe9939-2e02-47cc-805a-7ffc8340c582'), (5148, '2668e64e-5a73-4ea0-abfb-9239185fc243'), (5150, '5072d9d8-57a0-43b0-900c-549b7ebeae48'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (13359, '6caf8e23-63aa-413c-b673-dc039aa9a2ec'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (7269, '2203fa03-15b5-4a03-87c9-1ad09aa9d60a'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (23165, 'e03ab7b7-1296-4899-a795-d440d6735a2a'), (24703, '24765f0c-b621-411d-824a-f9d36f2430d3'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (23178, 'b6918b55-df66-472e-8a30-704503efcfc6'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (11452, '7e2fd91b-ca25-40e4-9aea-47ac22782115'), (4797, '8c2fca57-7c65-4554-b6bb-85101749efa3'), (9929, 'e5cfe006-3715-4630-b46b-ac0255b0d611'), (3603, '868a7661-c119-493e-86ac-54156f0efe17'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (8408, '2183f6dd-6b00-48c0-9ef5-29f2063157e5'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (9953, 'fde58ef7-3ca9-4340-9482-22bd56d64b93'), (18158, '3e75b4bb-2900-466e-97bb-0edb64a61f8a'), (18176, '8a8f4414-5efa-401a-908e-15f6d9d435eb'), (1793, 'ee32d640-6d91-4eca-a8d5-149f0e93d946'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (1797, '9cf6eef7-0cf2-4e9a-9443-ab438980128c'), (29445, 'bbcef900-4efc-40e4-a995-d0d601c26cdf'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (274, '5406807d-3f0f-4423-b5d3-fcc07ddbc7b9'), (13593, '79e309a1-7373-4306-8dcc-5a0b96bbcdd8'), (1819, '842e6f2a-f115-402b-987e-3ad11f0f136b'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (292, '085f6a0c-e881-4e0d-a875-1020c3bfd069'), (1829, '98ed0df5-e51e-4766-9380-1458db6d3ec6'), (1831, '31af1e9b-4ad6-412d-b081-bc1c83c96ffc'), (296, 'da61bbf7-b3d9-4a4d-aeb4-69794ca76f5c'), (18218, 'bb7681f6-dd34-4074-a8b2-aa96788ca58b'), (18220, 'c6f5e5b7-14a2-44f1-a560-fa908e274843'), (6456, 'e67362e4-1b0d-4015-b7db-89d7fe57c072'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (830, '4253d734-8fdc-48f0-ae58-86941ebeca66'), (831, '19f2bada-6be4-4b8f-b3be-f76b3343c3ef'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (833, '483d860d-236e-4772-b051-bfd68f59cd37'), (13635, '484e68e6-6a3f-4d6b-a91b-764b80d2f63c'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (837, '03afca3e-9150-4658-a4a2-8f97d1d0efea'), (13637, 'eb3201a9-649d-417a-8de3-6772001cc57f'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (334, '96f956bd-a66f-4085-b33e-15c1748f2298'), (336, 'f29d4191-d215-492c-9fa5-d1e0070ef5dd'), (4952, 'f70c8428-b149-4507-b817-0c9ab6cc5f0a'), (13663, 'acd04aa2-38f6-4dab-a7fb-09290536800b'), (19808, '5213e06f-bf0a-45be-86be-92a7b345964b'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (25440, 'f344d73e-1edb-43a3-8bab-c235c533086c'), (16759, '1b62ceba-cef9-48f4-9d42-37f27b4c1c9c'), (15741, 'e52c1082-1985-46a1-afec-26024efe0f42'), (8064, '25e3565f-3e8e-448e-95f1-9d71a468a4d0'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (9094, '95117b28-a37b-42d9-81d4-c4adcc0503c9'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (9112, '06791d3f-d0a1-4363-bfe4-0f0be4fc08fd'), (9117, '74400b1c-c3d4-4221-bf3d-d615c6862e50'), (31647, '2e660078-e155-46a0-9ee9-a99bab76bb1c'), (25518, '1f847aca-c3cc-4179-894b-7deef51d2560'), (949, 'be5a6f56-0fb8-49e8-a5a3-3ec839ae461e'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (6079, '2234d568-f21c-4b71-b407-a24d00ca3c01'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (18884, '5a73a356-0f23-4023-b582-fd34ed13e4aa'), (9157, 'd9539a98-60f5-441f-83d8-18513da4092b'), (18886, 'a01aef5d-3c56-4f8d-8854-e68353f4d189'), (9159, 'd8cdd0b3-74fc-47a6-9469-39332c669655'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (18388, '1a9977e1-1c9c-4e32-becc-6fd62e8516a8'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (10198, '19eb7655-1d17-42bc-985e-b1a424722b72'), (30170, 'fd65dda2-bf03-4ea4-81a5-f79c9e951095'), (5085, 'aa82db4a-174d-4da8-b638-41e10da5f5b2'), (2527, 'b35cdd4c-87c8-40bf-98a7-98cf594734a7'), (2534, 'c18dd192-33c8-464e-a9d7-524374c7162d'), (18406, '09e73a09-80b2-4ec3-b425-16cbd059b289'), (18411, '4be73c5f-f784-4448-8d45-1061cddc185a'), (15851, '0bde2ce9-28b0-4ae0-9459-a572cca7f941'), (5103, '0606953a-4aeb-4ca3-b14c-885e315fb1d4'), (5108, '0e1c7ec4-b9e7-4964-bac6-22f4455d6568')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: #### Why is Language Metadata Important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. 

Many people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.
## Preparing a multilingual corpus[[preparing-a-multilingual-corpus]]

We'll use the [Multilingual Amazon Reviews Corpus](https://huggingface.co/datasets/amazon_reviews_multi) to create our bilingual summarizer. This corpus consists of Amazon product reviews in six languages and is typically used to benchmark multilingual classifiers. However, since each review is accompanied by a short title, we can use the titles as the target summaries for our model to learn from! To get started, let's download the English and Spanish subsets from the Hugging Face Hub:

```python
from datasets import load_dataset

spanish_dataset = load_dataset(""amazon_reviews_multi"", ""es"")
english_dataset = load_dataset(""amazon_reviews_multi"", ""en"")
english_dataset
This model was contributed by [stefan-it](https://huggingface.co/stefan-it). The original code can be found [here](https://github.com/pytorch/fairseq/tree/master/examples/xlmr).

## Usage tips

- XLM-RoBERTa is a multilingual model trained on 100 different languages. Unlike some XLM multilingual models, it does
  not require `lang` tensors to understand which language is used, and should be able to determine the correct
  language from the input ids.
- Uses RoBERTa tricks on the XLM approach, but does not use the translation language modeling objective. It only uses masked language modeling on sentences coming from one language.

## Resources

A list of official Hugging Face and community (indicated by 🌎) resources to help you get started with XLM-RoBERTa. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.
- Empower Diverse Researcher Perspectives via Accessible Tooling and Resources
    - Tooling and resources must be available and accessible to different disciplines as well as the many languages and perspectives needed to drive responsible innovation. This means at minimum providing resources in multiple languages, which can be based on the most spoken languages in the U.S. The [BigScience Research Workshop](https://bigscience.huggingface.co/), a community of over 1000 researchers from different disciplines hosted by Hugging Face and the French government, is a good example of empowering perspectives from over 60 countries to build one of the most powerful open-source multilingual language models. 

Our <a href=""/blog/assets/92_us_national_ai_research_resource/Hugging_Face_NAIRR_RFI_2022.pdf"">memo</a> goes into further detail for each recommendation. We are eager for more resources to make AI broadly accessible in a responsible manner.
## A Tour of Audio Datasets on The Hub
This Section serves as a reference guide for the most popular speech recognition, speech translation and audio 
classification datasets on the Hugging Face Hub. We can apply everything that we've covered for the GigaSpeech dataset 
to any of the datasets on the Hub. All we have to do is switch the dataset identifier in the `load_dataset` function.
It's that easy!

1. [English Speech Recognition](#english-speech-recognition)
2. [Multilingual Speech Recognition](#multilingual-speech-recognition)
3. [Speech Translation](#speech-translation)
4. [Audio Classification](#audio-classification)

### English Speech Recognition

Speech recognition, or speech-to-text, is the task of mapping from spoken speech to written text, where both the speech 
and text are in the same language. We provide a summary of the most popular English speech recognition datasets on the Hub:
 
Here is the question: What tool does Hugging Face recommend for identifying languages in multilingual datasets?","Based on the context, Hugging Face recommends using language metadata as a vital tool for finding relevant datasets, particularly for identifying languages in multilingual datasets."
How can you prevent hardcoded secrets in Hugging Face Spaces?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (7695, 'f5e49d60-b8bb-4254-a782-ed38da828ba8'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (22566, '22a392b2-98a9-43f6-8ad9-dbf4ce6462f2'), (7721, 'fda7f690-e2ee-4f39-8439-3290db3ed411'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (5680, '766b2c6f-858b-4ec9-8469-cfcdabc9f66f'), (4657, 'd33514ae-0a67-4200-9281-3ac8294c114b'), (4656, '8a5eb5ae-f89d-40e1-a550-28aee57cd98f'), (5686, 'ded150dc-656c-4b52-bcb1-ac0a07561a5c'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (2124, '4778fe98-f068-48ab-b6b9-bb88bdddc78b'), (14926, '2768546f-2713-4a17-a282-bd49470822db'), (8273, '5ebe427f-bfba-401b-bc93-31042f3611b1'), (4178, 'fae17654-76a4-48ba-8a62-9b7bc23a7d8a'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (10848, 'e24b0ae6-43f0-498e-bd66-a33c5139c50d'), (8812, '99a22463-c724-477a-8a22-0965d1ff8629'), (12908, '5649b343-a5b2-4950-97c9-6b7acfcef17f'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (26236, '4e6c3199-8ffc-4351-876a-5f83eb281c1d'), (11389, '7cf525f3-0375-4371-b8ed-2e4e97568b45'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (24703, '24765f0c-b621-411d-824a-f9d36f2430d3'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (23172, 'def32509-da1d-4b97-ad0b-943987ec2382'), (23173, '06c055b9-11bb-4f47-9a1d-7b1b6751cdca'), (22150, '7f10791f-7256-49a0-b84e-770a0c38d937'), (23695, 'e28c37d5-8504-4c28-8cb1-3b5cdcaeb545'), (27284, '344b1139-9e69-4a8a-a5b7-593e7b089268'), (21141, '7e5c6db6-0748-406c-855d-7b0ac7fbb430'), (20118, 'eedb12d1-0857-4adf-835f-967a6ce84a3d'), (20119, '85bf9acc-d92a-4adf-9387-6a497f4123cd'), (5784, 'd6d19884-cb3b-422e-9824-8f6ce3a33276'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (26264, 'caec1e8d-2ac9-463f-95cd-490112c00671'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26791, '40f4195c-9f9f-403e-aa4d-69df440b7b16'), (26794, '1813d2fe-4f25-4ed3-a8ed-ca0946433c02'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (2735, '36de43ec-b784-438d-a483-67540bcc0645'), (5809, 'dd519b0d-8ea8-4ca5-8bf0-c64d7057c2ed'), (1718, '02534d35-0b3a-48b9-83f2-849c6c0852b9'), (9404, 'fd87f105-1775-467d-b190-1af7c157e949'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (8400, '50837027-5d1c-46f6-8ff0-4063b7ddf0a4'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (6878, '12422685-88d6-4ee6-800d-5067c7729e36'), (14051, '3cf94a0a-0ba2-4357-aeb2-3e9107310f84'), (8932, '52291d73-89b1-4af9-8668-af2a015d9763'), (22247, '4c0c828d-cbfa-4f24-9d14-9eed657d9e71'), (24808, '6ca62949-7a64-4634-8bba-b84c7fc2521a'), (28909, 'c66aeb8e-cb51-4807-9a5d-5b273159c78c'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (6389, 'ad649443-bb1f-4477-adc1-f07902816889'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (25891, '6b116321-1493-47ef-9bda-78f2921ccc9b'), (25892, '1a9985c7-2ef5-4374-87fb-01840fb0dc88'), (25893, '20f28049-e508-4657-986b-2d7923d2cfb8'), (2870, '8630c5af-b3e7-4d83-b67d-8cca87440b11'), (15181, 'fb7c5366-41d1-41cb-9f21-6d3b57cf1e94'), (1359, '1ee49ddb-9952-4cfc-b4ed-2acfce41628f'), (23890, '5d51a959-3b99-4ea4-9614-65e8b5666909'), (10074, '2aed197d-f4fc-4c34-a307-1983a496c31f'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (1373, '06b4535d-c110-4754-87c4-22a2af123fd9'), (12128, '6fb6bb94-37c3-4962-b310-4d645b4883d6'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (18283, 'b5a8b1e8-4409-4476-98a1-2f9cf7b891f1'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (16760, 'd936a97e-a944-4271-bca8-ab614fbc7ca8'), (7039, '3ea087fd-2f57-4e59-92f6-f7410097e496'), (17284, '4cbab01e-7377-4c07-a00f-33e995add895'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (6026, '88095e53-c92a-411c-90d2-5ae5ed5197f2'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (29593, '6a3d74c2-ed43-42ed-8939-0c89a33f7be8'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (31647, '2e660078-e155-46a0-9ee9-a99bab76bb1c'), (13728, '91c1a2c4-7e1d-4ddf-8e7c-768b77a28bbc'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (27558, '0c97b61c-186c-466d-b67d-c82c39947a6c'), (25518, '1f847aca-c3cc-4179-894b-7deef51d2560'), (6575, 'cc6f1585-393d-4a15-9d26-539101cf0f8d'), (27568, '14a9a9da-5246-490f-b304-14943de48a9a'), (27570, '81c00a54-cf9e-44f3-bef4-fc88896b327b'), (14261, '83bfbdeb-6c03-49d4-903f-91fbc30265fd'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (16317, '2b682e0c-ad28-4060-9c8d-4754859a1913'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (15807, '7b6ed994-c083-4e36-a701-5b01979432e2'), (10690, '198ad954-036c-4bf9-bf24-6394ec1221d0'), (16324, '0cb7caef-995c-4aec-9c86-5e22705a71b6'), (12230, '37bed0b9-108e-4d1f-8832-16b1705a0186'), (24022, 'c22f8a72-e51c-4737-9bbe-aae8fbb635e5'), (21463, 'e97bfba4-a14f-477a-9f94-afd38c01d4c5'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (30170, 'fd65dda2-bf03-4ea4-81a5-f79c9e951095'), (9174, '3767ffbd-cac9-45c5-b684-68ec257368f6'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (27105, 'e6e9e578-4777-4b70-8807-6697930800fc'), (27107, 'bf23ed30-0073-4ad6-a8e9-33cb319e5288'), (27109, 'e980eafb-92e5-4615-b816-fc20edbbd85f'), (2534, 'c18dd192-33c8-464e-a9d7-524374c7162d'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (27114, 'bc032949-d8d4-4c0e-8898-fdcb416b7e1b'), (28653, 'a82ff3c6-b2e8-4d9b-8757-0e7173b16c33'), (28657, 'c0d8252e-c6dd-4c26-87b0-c76f56194d4f')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

To prevent this issue, we run an automated bot (Spaces Secrets Scanner) that scans for hard-coded secrets and opens a discussion (in case hard-coded secrets are found) about the exposed secrets & how to handle this problem.

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/token-scanner-light.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/token-scanner-dark.png""/>
</div>
```

Next, set up a GitHub Action to push your main branch to Spaces. In the example below:

* Replace `HF_USERNAME` with your username and `SPACE_NAME` with your Space name. 
* Create a [Github secret](https://docs.github.com/en/actions/security-guides/encrypted-secrets#creating-encrypted-secrets-for-an-environment) with your `HF_TOKEN`. You can find your Hugging Face API token under **API Tokens** on your Hugging Face profile.

```yaml
name: Sync to Hugging Face hub
on:
  push:
    branches: [main]

  # to run this workflow manually from the Actions tab
  workflow_dispatch:

jobs:
  sync-to-hub:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
        with:
          fetch-depth: 0
          lfs: true
      - name: Push to hub
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: git push https://HF_USERNAME:$HF_TOKEN@huggingface.co/spaces/HF_USERNAME/SPACE_NAME main
## Managing secrets and environment variables[[managing-secrets]]
<a id=""managing-secrets""></a>
If your app requires environment variables (for instance, secret keys or tokens), do not hard-code them inside your app! Instead, go to the **Settings** page of your Space repository and add a new variable or secret. Use variables if you need to store non-sensitive configuration values and secrets for storing access tokens, API keys, or any sensitive value or credentials.

<div class=""flex justify-center"">
	<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/secrets-and-variables.png""/>
	<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/secrets-and-variables-dark.png""/>
</div>
<Tip>
If you are using the space just for testing and experimentation, you don't need
to make any changes to the configuration. Everything will work out of the box.
</Tip>

You can also use an external secrets backend together with your HuggingFace
Spaces as described in [our
documentation](https://docs.zenml.io/getting-started/deploying-zenml/docker#zenml-server-configuration-options). You should be
sure to use HuggingFace's inbuilt 'Repository secrets' functionality to
configure any secrets you need to use in your`Dockerfile` configuration. [See the
documentation](https://huggingface.co/docs/hub/spaces-sdks-docker#secret-management)
for more details how to set this up.

<Tip warning={true}>
If you wish to use a cloud secrets backend together with ZenML for secrets
management, **you must take the following minimal security precautions** on your ZenML Server on the
Dashboard:
Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
 
Here is the question: How can you prevent hardcoded secrets in Hugging Face Spaces?","To prevent hardcoded secrets in Hugging Face Spaces, you can use the following methods:

1. Run an automated bot called Spaces Secrets Scanner that scans for hard-coded secrets and opens a discussion about the exposed secrets and how to handle this problem.
2. Store sensitive values or credentials as secrets in the **Settings** page of your Space repository instead of hard-coding them inside your app.
3. Use environment variables for non-sensitive configuration values and secrets for storing access tokens, API keys, or any sensitive"
Which SDK is supported for creating Spaces on Hugging Face?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (14343, '9d89b13c-340e-4d34-9520-72ec88af5e16'), (7692, '2785b812-48ad-476f-834c-8694f938e3dd'), (7695, 'f5e49d60-b8bb-4254-a782-ed38da828ba8'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (12304, 'ac1c139b-bf34-4bb5-9552-5072853d6a68'), (7696, '1bf7be69-ea0a-4f67-8bb0-047937a08b55'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (19998, '40f7e87f-9355-49b7-a1a4-42183c181da6'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (16932, 'd165fea3-524c-4a1f-8627-923c16bdcb4f'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (24120, 'e1160ee3-8307-4e5d-8ec4-13c38625d7d4'), (28729, '73a9bd92-46d7-4671-a907-91d2a76f3b21'), (3643, 'ee5ea5c1-c94c-4b39-90c8-92a1683eb5c6'), (20542, '8adb84bf-218f-4821-ae67-8e8603665c9d'), (15936, 'ab08450b-3b5c-4bbc-8c0e-355d1b29997d'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8273, '5ebe427f-bfba-401b-bc93-31042f3611b1'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (10848, 'e24b0ae6-43f0-498e-bd66-a33c5139c50d'), (8802, 'cde8f183-1137-46ab-b716-a4d86c1e6e63'), (16998, '60e139ff-8758-4146-915e-e9dba50bef1e'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (19574, 'cc66b1c9-e31b-4dbf-b777-a98a2c4143be'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (19578, 'c39dbfdb-a1e4-46cb-b486-736ca8e047bb'), (31354, '50289d67-f345-42d3-bb11-7b4a1e8c5530'), (23677, '34a99b84-b8b5-468b-ba33-a5a439f57c4e'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (4756, '41400293-c3b2-4f8e-829b-5618b0030cce'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (26263, '83b240db-acd6-4670-bc6f-1697ec7ed779'), (26264, 'caec1e8d-2ac9-463f-95cd-490112c00671'), (10914, '2eb8593b-a9e0-410f-b6ff-dfbe8c9aaf9c'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (26791, '40f4195c-9f9f-403e-aa4d-69df440b7b16'), (17063, 'c4da4eef-3f4d-4eee-88fb-74f2ade4e8d7'), (26794, '1813d2fe-4f25-4ed3-a8ed-ca0946433c02'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (19117, 'a570815f-38b6-47ca-a5c8-1bd2674040a4'), (1718, '02534d35-0b3a-48b9-83f2-849c6c0852b9'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (22211, '740dfc56-1f28-4364-853c-997c6873e87b'), (16070, 'c25e9600-d460-45ec-9d7b-54be9e3a1dc1'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (7373, '50cf41d1-b5bb-4568-b2ad-422ff55f8249'), (7374, 'f080a759-14b5-4d2f-a4f6-7d49cbf9c053'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (4315, 'e6424b7d-ab90-4e47-9d74-082d5e90ef7c'), (225, '3ad317d7-101b-4379-9a04-92d617ddfd14'), (14050, '43174305-dba9-45e8-b46d-9ae2ea56ad48'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (16644, '48292295-18de-4870-999a-cf16ec29e3c1'), (16647, 'd95ce684-2d42-4dda-b614-f41463bfd8d8'), (31498, 'd259184d-d3d4-431e-a435-287d452d7127'), (19211, 'a6ef5cb5-149a-49c9-82d2-041336f90a44'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (31509, 'ce6de00d-9b9d-4ca9-b702-b073099a10d6'), (31510, '7452e207-ead0-4e06-97a5-c12d51072f69'), (31511, '4fd68b82-1f48-4cd1-a522-9520c910e7d5'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (12064, 'ec49857b-c395-49bb-b24a-dba896fff9fc'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (31057, 'eee260cd-cb8d-4ea4-b824-020af026b061'), (10074, '2aed197d-f4fc-4c34-a307-1983a496c31f'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (12128, '6fb6bb94-37c3-4962-b310-4d645b4883d6'), (9582, '85a672bd-3466-49d7-8336-b1cdf83eef5a'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (16760, 'd936a97e-a944-4271-bca8-ab614fbc7ca8'), (16764, '8e676bbc-b3e9-4463-a23d-1131ef9602fb'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (3984, '0b5c9103-9bd3-4af7-a1ec-16b33970a4bc'), (10640, 'f8c59197-206d-4cfe-bf25-62fe5fbb209a'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (31133, 'e42f7993-643f-492e-a6db-29ed9c755b45'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (12215, '6104b507-3f50-45e2-8c07-00ea5df81149'), (12728, '4364e7f3-1794-496e-9a2f-a14bcae5a530'), (31671, 'f2e7ad24-7a18-465a-b2f0-f1b12c57b30b'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (12730, '32c97e36-35b2-4401-a77e-7ef72c23100c'), (16317, '2b682e0c-ad28-4060-9c8d-4754859a1913'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16322, 'ad245a5f-b243-4e34-b382-6e08fd6c4955'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (30156, '1a7ef2f4-4d24-42f0-adeb-a1377143d42a'), (29135, 'aa204127-7dec-4d3b-b82a-c7d7aeb22e77'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9174, '3767ffbd-cac9-45c5-b684-68ec257368f6'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (6105, '9efc65f9-7879-4b2c-8da5-721897d26d7d'), (3035, '5c512548-7f49-487d-b32a-a32e51be3d95'), (12764, '39a888d0-63e5-4237-87dc-2dbccf6e87e5'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (3038, 'b45ad508-30e4-4a2d-aa18-f8e0d3227343'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (6109, 'b4f3a44d-9960-4eca-9d8d-a44262da7a79'), (5597, '36ce3f3b-0065-4a41-b32a-a7e3629b2ceb'), (27102, '5e81146e-e4e7-42f1-9881-b44dfe74a71c'), (25061, 'a686f600-cfe9-467e-9d3e-91dc3239e164'), (15846, 'ee66a65a-c35d-4ed7-acbc-15fce331450b'), (15847, '5ec00348-4cb1-4058-99dc-6f4c0b0a7a3a'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

### Hugging Face Models

Hugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)

## How did Hugging Face help the ONNX Model Zoo?
Spaces

[Hugging Face Spaces](https://huggingface.co/spaces) offer a simple way to host ML demo apps directly on your profile or your organization's  profile. This allows you to create your ML portfolio, showcase your projects at conferences or to stakeholders, and work collaboratively with other people in the ML ecosystem.

We have built-in support for two awesome SDKs that let you build cool apps in Python in a matter of minutes: **[Streamlit](https://streamlit.io/)** and **[Gradio](https://gradio.app/)**, but you can also unlock the whole power of Docker and host an arbitrary Dockerfile. Finally, you can create static Spaces using JavaScript and HTML.

You'll also be able to upgrade your Space to run [on a GPU or other accelerated hardware](./spaces-gpus). ⚡️

## Contents
```

    *Note*: Here's an [overview on building demos on Hugging Face Spaces](./spaces-overview) and here are more specific instructions for [Gradio](./spaces-sdks-gradio) and [Streamlit](./spaces-sdks-streamlit). 

4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A ""Linked Models"" button should appear in the top right corner of the Space, as shown here: 

    ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)
    
    *Note*:  You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference).


Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes 🤗
## What are Hugging Face Spaces & Gradio?

### Gradio

Gradio lets users demo their machine learning models as a web app, all in a few lines of Python. Gradio wraps any Python function (such as a machine learning model's inference function) into a user interface and the demos can be launched inside jupyter notebooks, colab notebooks, as well as embedded in your own website and hosted on Hugging Face Spaces for free.

Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

## Setting up a Gradio Demo for JoJoGAN
You can find a completed version of this hosted at [NimaBoscarino/hotdog-streamlit](https://huggingface.co/spaces/NimaBoscarino/hotdog-streamlit).

## Create a new Streamlit Space

We'll start by [creating a brand new Space](https://huggingface.co/new-space) and choosing **Streamlit** as our SDK. Hugging Face Spaces are Git repositories, meaning that you can work on your Space incrementally (and collaboratively) by pushing commits. Take a look at the [Getting Started with Repositories](./repositories-getting-started) guide to learn about how you can create and edit files before continuing.

## Add the dependencies

For the **Hot Dog Classifier** we'll be using a [🤗 Transformers pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) to use the model, so we need to start by installing a few dependencies. This can be done by creating a **requirements.txt** file in our repository, and adding the following dependencies to it:
 
Here is the question: Which SDK is supported for creating Spaces on Hugging Face?","According to the context, the three SDK options supported for creating Spaces on Hugging Face are:

1. Gradio
2. Streamlit
3. Static HTML demos"
What action should be taken for datasets without language metadata?,"[(22539, 'd5069ed2-b237-4d02-8d50-e26d6e63c1d7'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (22541, '9624258f-4f8f-4a20-aa98-03eaa5057142'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (23059, 'de332ba3-fd9d-44d5-89a5-5c18bdb57ded'), (23060, '09e43017-f9b1-44a7-918e-57bd10790d6c'), (23061, '1fa89fbd-1f12-41ad-a09d-613f63f32b6b'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (23063, '21ce1de6-8b94-480c-b290-80086a22a5ce'), (23064, '80eb8bc8-3947-409c-bc93-d0bbe04d5069'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (23066, 'db5b29f6-c011-4064-83aa-0677c241c9dd'), (23067, 'c4124e63-351d-48b1-ad95-32c2212f2547'), (23068, '17b19121-a9f3-446f-b249-3b1e64bae14f'), (26651, 'f1b12a30-cfeb-4c01-a3a4-7f8d4640303f'), (23070, 'dbb25c0a-8cb6-4e62-9b53-c8a6dd06b4c4'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (25649, '21dba2b3-c22c-4308-be7b-3647fdfaf8b1'), (25654, '16c3fe76-474d-4aea-bc36-a7ecb11025ec'), (29251, '45082001-5874-4c41-8477-97c24d5d9523'), (29253, 'a4570957-b255-44da-8589-a79ffdd409f6'), (29256, 'e581785d-f30a-47db-99a4-eccdd13d7903'), (29261, '1715bea8-d32b-4d78-b7b9-eff4094c9c6d'), (29265, '47595a18-5b82-4980-91ef-b2cc34ee15b8'), (14430, 'df8628a2-0050-42f4-be97-ef5d21f207ed'), (18530, 'acfc3bf9-e146-487c-beff-f124efac85a3'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (7269, '2203fa03-15b5-4a03-87c9-1ad09aa9d60a'), (6758, '3d29e927-9a4f-4916-9290-e53743ad7e85'), (22123, 'ae9711c0-d281-4415-96bc-797b9d148cfe'), (14463, 'f86c6b7f-b65b-41f1-80b2-18f5c43c04a6'), (13448, 'c265cc67-92bb-48f5-bfd6-5569ba9863df'), (16030, 'ebff3b8e-d84e-42ab-8015-0e9145257735'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (16050, '9709888d-3f4a-4186-ac8d-73d4ac30da40'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16060, 'bce28893-8630-4a96-a690-53061f00d352'), (7356, '7384d51b-c1f5-4e52-aee5-7401f6562bf2'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (7360, '807315b1-5b45-4562-8520-9f2c38c17779'), (7362, '1a61fb4d-7bc2-4d57-912a-e4a6b36c38e2'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (5844, 'a9a3fde4-b1b7-4939-8706-9b538d6f2858'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (7398, '383d361a-826c-44cb-a553-5def932b4a1a'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (20717, 'ceb57f22-5344-4bc1-8c33-80214f9440a7'), (4338, '5ac0912f-7d51-4e4d-9d3a-9bfffe0799a1'), (20723, '5f9e0e0a-3540-40f1-a037-3ae49cc51c50'), (6460, 'dd57f5fb-c9db-400c-9dd7-7ad7abaf21cf'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (23303, 'c8340867-9c9b-4274-adb6-873aeaef3797'), (11020, '682085c4-3907-4d85-b712-b412e400487d'), (7958, '310794e6-f08f-43e1-8263-12a3868fa63c'), (15127, '58cfa5d1-aa1f-4671-909f-1abfdb9b7292'), (23329, '9afe4111-7b48-4f69-87cf-d1e360f0331a'), (26922, '4f04aac2-017d-4b16-b9a8-9790885b5c01'), (2864, 'bd4a8a28-05e5-486b-8aff-f74ae7a91a34'), (6456, 'e67362e4-1b0d-4015-b7db-89d7fe57c072'), (6457, '416d0180-84a0-47ca-816b-e4151270b222'), (6458, '3f3603c0-3608-4809-94c7-59e7df3fdbb2'), (12091, 'c4608c28-4eda-4334-a158-2a4c29503dd6'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (6462, 'c72654c1-47d0-48f8-8318-b515b1a6fb8b'), (6461, 'b1f177f3-e93d-412b-b346-92ad1e300db7'), (6463, 'deb1abed-40b0-4c6f-ab05-30f0375d9840'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (833, '483d860d-236e-4772-b051-bfd68f59cd37'), (831, '19f2bada-6be4-4b8f-b3be-f76b3343c3ef'), (830, '4253d734-8fdc-48f0-ae58-86941ebeca66'), (837, '03afca3e-9150-4658-a4a2-8f97d1d0efea'), (838, 'b88fee3d-341d-4685-8fc6-fd51e1adf587'), (836, '9ee86ea3-f33f-4cf5-9e92-0ad9009e33ba'), (841, '3ea3ba81-cfdc-4309-824d-56a17beadb1d'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (840, '41ff972f-57da-4a02-9c34-558a02677b72'), (5966, 'cd175952-fce5-45c4-97a3-5ea93dfe65e9'), (23381, '78085d8f-a121-423a-85da-2372070a7f63'), (13664, '0d35fe7f-3409-414c-9082-1c9c87a329cf'), (13665, '714cb96e-9788-466a-9fd7-695ae8ff2daa'), (13668, '86808aee-afa4-4657-9f26-d6a8a9ad0e15'), (5477, '0a7e9332-7b2a-4aa6-86a1-427dd6f19f03'), (6506, 'eac226b1-19d1-47c5-9b7e-e7cc3d34914b'), (23429, '4225cd93-c771-45ff-b9cb-f17095cd13ae'), (8590, 'd83fb1f9-3d9c-409e-a570-5a3a708f8463'), (24979, '604b588d-5044-4d9e-8b53-b3031cd2bd65'), (27566, '228f8e78-7d1b-4627-8417-c05cbd941d34'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (21951, '358a38e1-9865-45a7-bb11-f4d56f194e8e'), (7108, '7e8be3a4-3d9e-4572-b072-eaacb2c1a9a1'), (7110, 'ace1a408-647b-4149-a9c1-b6a9a032d552'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (16844, '14804510-d68b-43ef-b630-fea72e07be9a'), (18897, '313f3297-f3ec-453b-8026-a42752a46cb8'), (29143, 'bc13a857-206c-4b91-b966-9c4913882054'), (1499, '1faf2c53-b46d-4aec-a233-dd712314dc97'), (1500, '958de4d0-417a-4556-b8cb-9391406533c5'), (1513, '710901b9-382c-447e-b95f-bb910a1fdce4'), (19962, '04858203-03cf-45bb-9b5a-7f506048c00c')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: 1. Find a dataset that doesn't have the `language` field filled in. You can find a list of datasets without the `language` field filled in [here](#datasets-without-language-field-filled-in). We start with datasets that have the most downloads and likes.
2. **Check that the dataset doesn't already have a PR to add a language tag(s).** Someone else may have already started working on it. You can check this by looking in the discussion section of the dataset page. 
3. If there is no PR to add language metadata already open, your next step is to identify the language (if possible for the dataset). There are a few main ways you can often identify the language
   1. The dataset's name. Often, the name of the dataset will include the language. Sometimes as a full name, i.e. `imdb_german` or sometimes as a language code, i.e. `imdb_de`. You can use that as the language tag if the dataset name includes the language.
*The percent of datasets which have language metadata. True indicates language metadata is specified, False means no language data is listed. No card data means that there isn't any metadata or it couldn't be loaded by the `huggingface_hub` Python library.*


#### Why is language metadata important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows.
#### Next Steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
#### Next steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
## F.A.Q.

### Does it make sense to add language metadata to all datasets?

No! This is why we have focused on datasets with a `task_categories` field indicating that the dataset has a text-related task. 

### Can I use a script to automate the process?

While it is possible to use machine learning to help assist this process, see [this blog](https://huggingface.co/blog/huggy-lingo) as an example; checking the accuracy of the PRs you are making is still important. 

## What about datasets with multiple languages?

Some datasets may have more than one language. Do your best to add all the languages you can identify in the datasets. If there is a vast number, this may be tricky. In this case, do your best. 

## What about code? 

Currently, you can add a language tag for `code`. You will need to do this directly in the `YAML` rather than the visual editor since using the visual editor will lead to an auto-completion for the `co` language code (Corsican).
 
Here is the question: What action should be taken for datasets without language metadata?","For datasets without language metadata, the following actions should be taken:

1. Check if the dataset already has a PR to add a language tag(s) in the discussion section of the dataset page.
2. If no PR exists, identify the language of the dataset (if possible) by checking the dataset's name, which may include the language.
3. Add the language metadata to the dataset."
How can you clone a Hugging Face Space repository locally?,"[(16896, 'ba56f1b5-041e-4543-a698-1ff174eedc16'), (24578, 'fa7bcda5-9b05-4203-9448-59419b2cbd37'), (16903, '5771bb1b-fbc2-4f44-8020-db707536f963'), (16904, '9b6ff231-3436-46f7-8fdd-7aebaf4d3c86'), (7178, '086ecf90-fb2d-4fb0-9e37-23a45c3703ea'), (7692, '2785b812-48ad-476f-834c-8694f938e3dd'), (7693, '37441e45-22aa-42c9-a745-327c0bdfd5c7'), (7697, 'c4d24c56-06a1-4e80-af53-2eee4d456e7c'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (18961, '0e7cde28-659f-43ea-ac5b-9ff6b920e639'), (543, 'cff77a13-da8c-4196-8bcd-5f41c1eab186'), (29735, '0c5afdaf-7779-4f80-9afa-85b9e51ea59f'), (23593, 'c4cad64a-26a2-4318-b2bf-37b79d234f09'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (10805, '430ef93a-e03d-45a9-aa7e-cd9c1c9db9b6'), (4664, '39143b8e-b1fe-4350-8660-fa0af84b37f2'), (7224, '49503e5a-ba18-43c6-9722-173bbeee8dd9'), (4665, 'fafcf21c-e376-460a-bb7c-dcca97430243'), (3643, 'ee5ea5c1-c94c-4b39-90c8-92a1683eb5c6'), (16955, '01636acd-5bb1-4d39-920e-519a40c5e1b2'), (16956, '20ec0163-3fb1-4235-935b-2239bec88f47'), (16954, 'b3f25eff-c105-4e0f-a20c-da1be8daf35e'), (16960, '0007c7b4-521b-40be-aa96-c50f792093fd'), (15936, 'ab08450b-3b5c-4bbc-8c0e-355d1b29997d'), (27715, '4d483cab-97fa-42f9-8677-4bf9b3dfc233'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (3071, 'e71f3558-bb8e-4032-a796-38cbf84636f8'), (23157, '7e197857-a836-405d-b4bd-374d1255e730'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (25725, '8d1c0f14-676d-47ad-b731-33cfbfdcf1f3'), (3726, '70833dd7-3e2c-44d3-b04d-4c05a67f137b'), (5776, '8001f35a-9ae7-4b87-b42a-486643837817'), (26773, '36a72c91-d9e1-4a83-8b43-432e315d3dd5'), (21654, '0ac0c293-df2f-4bd7-ab9a-ef9c0cb31739'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (7323, 'd6724881-25a5-462c-9bd0-dbedb21f1eec'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (3235, '94da9cee-060b-4a97-8a30-f5fd150285aa'), (17063, 'c4da4eef-3f4d-4eee-88fb-74f2ade4e8d7'), (22704, 'd853fcd0-1387-4353-a361-a5295f36b273'), (22705, '4a769e7a-70f2-490c-8cb5-087dc912139a'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (22213, 'f3c73041-0dba-4b9c-ad02-d79e600db3ef'), (25810, 'd403064f-ddbd-4772-b240-ca08249a3c6d'), (4315, 'e6424b7d-ab90-4e47-9d74-082d5e90ef7c'), (2780, '46337ead-702e-43e0-94be-4a5769a02f80'), (14050, '43174305-dba9-45e8-b46d-9ae2ea56ad48'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (24805, '5f103d70-5254-4011-b9dc-3f5b60ba28ab'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (6389, 'ad649443-bb1f-4477-adc1-f07902816889'), (12028, 'da0f499e-0b89-4a96-8974-e7bbb9af3fff'), (7437, '75498d17-c095-434e-a91f-09d1fcc65137'), (15121, 'a1117574-c998-4959-8338-c3dc538bbc6c'), (1301, '5137aed7-d723-4ecf-8874-26a7c9ecb926'), (28440, 'f186228b-4ecd-4f33-a849-39b930f0800a'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd'), (26401, '4e38d7b7-74bd-4737-9456-df195ad90116'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (16163, 'd946b344-28ff-488c-814e-80396645dc7c'), (26402, 'fd36e742-8ef7-4b94-820e-8f3facabc02b'), (26405, '083107e3-5c6f-4e2d-ac5e-50717bdd56d6'), (11581, '2601a0d2-947c-40f9-bc88-35febc4ce941'), (15181, 'fb7c5366-41d1-41cb-9f21-6d3b57cf1e94'), (31057, 'eee260cd-cb8d-4ea4-b824-020af026b061'), (29525, '4a7299ab-39a4-4b0e-8944-bf676cb6c96e'), (10582, 'fbd408d3-05ec-49b9-9ae6-3abd0333c09e'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (25947, 'fd0d1693-837a-4aa1-93be-7c02c6b77dc0'), (348, 'a2e55383-c0a3-42c9-bc84-8a5018baf440'), (350, '8edfb721-840b-4b1d-9036-65614d095d44'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (12128, '6fb6bb94-37c3-4962-b310-4d645b4883d6'), (19809, '297118a2-beb1-4806-9b58-00d95e4a203c'), (20834, 'af60653a-06c0-4816-856e-797b23d7e982'), (20835, '6f05f3ca-3fdf-40eb-8f47-7419c5df95c6'), (358, '1f015812-8d62-4b47-a953-b29505c9c0e8'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (7538, '0a189d45-3f70-4fee-92be-fa363d950e72'), (9588, '54e81e94-5a4a-4135-bcc9-b2fa4fbb5343'), (13685, 'c1714774-629f-4e58-82b7-7a2bd15060e9'), (16764, '8e676bbc-b3e9-4463-a23d-1131ef9602fb'), (7038, '0152a05f-3a1a-4ff4-ad49-30f2fe494f73'), (19329, '466543a2-ecbd-47e2-a220-a877dffa1c9f'), (19330, '06bc4b79-264e-4176-8ccd-68c6f6bdfe54'), (13815, 'd30ad370-efe6-4fa8-a47a-3ce2c00b19c2'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (19335, 'e7a25af1-ba2a-4e2e-af04-f55deda54356'), (6026, '88095e53-c92a-411c-90d2-5ae5ed5197f2'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (20891, '83937b90-0e3b-411d-aa8b-67b0ab298f46'), (31133, 'e42f7993-643f-492e-a6db-29ed9c755b45'), (31135, '957d568e-e103-45a2-9bae-02e2fbebbc4a'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (3006, 'df971df4-c8d6-4e5b-a79c-ea2f7c4575a4'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (25030, 'd840563e-295e-4b45-9f92-f02ce3de5769'), (14790, '9b894d75-a0f9-4ce1-aa43-7ed77bb8059c'), (19916, 'e7d06c45-10f0-4bd2-8d3f-e2478dd9da51'), (21967, '4fddfd37-8346-4ae9-9ebc-a7af205fbad7'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (24532, '60a7b56c-8699-44c3-845d-6e93ee024964'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (5597, '36ce3f3b-0065-4a41-b32a-a7e3629b2ceb'), (3038, 'b45ad508-30e4-4a2d-aa18-f8e0d3227343'), (31213, 'af30e609-e33b-4ca9-b712-3f14b697f4f0'), (16881, 'f5e426ab-7d73-4f19-a863-3b5b35f0e29a'), (16892, 'e4247f4b-6fe2-45c4-ac24-fdafca991487'), (20467, 'd0074f05-bf62-4e31-acfd-177faf27318a'), (3063, 'b7894e12-5042-4cb7-a753-a5b3c66c4dc1'), (3065, 'a7a0d4a9-49b2-4f19-8ef3-758e9152545c'), (16380, 'd2a9ae2e-839b-48d6-8cec-62615c3a4649'), (16893, '4d63bb0e-d773-4ecb-b3db-02de05b673cb'), (25087, 'aa1063f0-de9c-4fe6-8cec-47c5ff66c3be')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ## Clone the Repository

You can easily clone your Space repo locally. Start by clicking on the dropdown menu in the top right of your Space page: 

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/SpacesCloneRepo2.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/SpacesCloneRepo1.png""/>
</div>

Select ""Clone repository"", and then you'll be able to follow the instructions to clone the Space repo to your local machine using HTTPS or SSH. 

<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/HttpsClone2.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/HttpsClone1.png""/>
</div>
## Step 1: Create a Space using the Static HTML template

First, navigate to [Hugging Face Spaces](https://huggingface.co/new-space) to create a space.

<figure class=""image text-center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/1.png"">
</figure> 

Select the ""Static HTML"" template, give your Space a name, and create it.

<figure class=""image text-center"">
  <img src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/124_ml-for-games/games-in-spaces/2.png"">
</figure> 

## Step 2: Use Git to Clone the Space

Clone your newly created Space to your local machine using Git. You can do this by running the following command in your terminal or command prompt:
### Loading from Hugging Face Spaces[[loading-from-hugging-face-spaces]]
To load any Space from the Hugging Face Hub and recreate it locally, you can pass `spaces/` to the `Interface`, followed by the name of the Space.

Remember the demo from section 1 that removes the background of an image? Let's load it from Hugging Face Spaces:

```py
gr.Interface.load(""spaces/abidlabs/remove-bg"").launch()
```

### Clone

The `clone_from` parameter clones a repository from a Hugging Face repository ID to a local directory specified by the `local_dir` argument:

```py
>>> from huggingface_hub import Repository
>>> repo = Repository(local_dir=""w2v2"", clone_from=""facebook/wav2vec2-large-960h-lv60"")
```

`clone_from` can also clone a repository using a URL:

```py
>>> repo = Repository(local_dir=""huggingface-hub"", clone_from=""https://huggingface.co/facebook/wav2vec2-large-960h-lv60"")
```

You can combine the `clone_from` parameter with [`create_repo`] to create and clone a repository:

```py
>>> repo_url = create_repo(repo_id=""repo_name"")
>>> repo = Repository(local_dir=""repo_local_path"", clone_from=repo_url)
```

Lastly, to push our model to the Hub, we will need to create a `Repository` object in a working folder. First log in to the Hugging Face Hub, if you aren't logged in already. We'll determine the repository name from the model ID we want to give our model (feel free to replace the `repo_name` with your own choice; it just needs to contain your username, which is what the function `get_full_repo_name()` does):

```py
from huggingface_hub import Repository, get_full_repo_name

model_name = ""codeparrot-ds-accelerate""
repo_name = get_full_repo_name(model_name)
repo_name
```

```python out
'sgugger/codeparrot-ds-accelerate'
```

Then we can clone that repository in a local folder. If it already exists, this local folder should be an existing clone of the repository we are working with:

```py
output_dir = ""codeparrot-ds-accelerate""
repo = Repository(output_dir, clone_from=repo_name)
 
Here is the question: How can you clone a Hugging Face Space repository locally?","You can clone a Hugging Face Space repository locally by clicking on the dropdown menu in the top right of your Space page, selecting ""Clone repository"", and then following the instructions to clone the Space repo to your local machine using HTTPS or SSH. Alternatively, you can use Git to clone the Space by running a command in your terminal or command prompt."
What is the recommended way to specify multiple languages in a dataset?,"[(22530, '1de22cdb-825f-48bc-b92b-c9a32575561a'), (19459, 'cb31efe8-a229-4e7b-9ab4-f850276a68c2'), (21509, '2e90ee09-c728-44e9-aa15-984bb64553e5'), (22539, 'd5069ed2-b237-4d02-8d50-e26d6e63c1d7'), (30220, '683831a6-5b21-4e34-8508-f7affdc57f5a'), (30221, '5154192e-9a6d-4f65-a4da-8815b4c17dc5'), (23059, 'de332ba3-fd9d-44d5-89a5-5c18bdb57ded'), (23060, '09e43017-f9b1-44a7-918e-57bd10790d6c'), (23061, '1fa89fbd-1f12-41ad-a09d-613f63f32b6b'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (23063, '21ce1de6-8b94-480c-b290-80086a22a5ce'), (23064, '80eb8bc8-3947-409c-bc93-d0bbe04d5069'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (23066, 'db5b29f6-c011-4064-83aa-0677c241c9dd'), (23067, 'c4124e63-351d-48b1-ad95-32c2212f2547'), (23068, '17b19121-a9f3-446f-b249-3b1e64bae14f'), (21525, '973f8503-e4bf-48f0-ba00-93427990b2e9'), (23070, 'dbb25c0a-8cb6-4e62-9b53-c8a6dd06b4c4'), (26651, 'f1b12a30-cfeb-4c01-a3a4-7f8d4640303f'), (25658, '0c65501e-4a3d-4a13-bb47-0a0b3b489782'), (18492, '16d82abe-ccda-4e30-8882-b21bbc9fc3b2'), (29256, 'e581785d-f30a-47db-99a4-eccdd13d7903'), (7268, '6f475e41-2ca1-49b1-bf98-e8ed196d40da'), (7269, '2203fa03-15b5-4a03-87c9-1ad09aa9d60a'), (7270, '3ebe2aed-3d22-4395-b375-ae0bdf09d3e4'), (7272, '90527d40-f42d-47ed-a5d3-255fa1425973'), (30825, '8934d6ff-828c-4bde-8179-5d5a150bf5e1'), (7276, '0d98213c-cf33-4909-bcf6-74ccaf3923d7'), (7277, 'fcdd1c1a-bc8a-4b1f-aa30-7718209282e6'), (7278, '33dadcf6-6e0d-4569-8117-d99212ef78ff'), (7280, 'f968b214-cb48-463f-882e-be4af04c57b3'), (660, '64257943-b5d9-4952-b7f5-d7244d06493b'), (24732, '87da702e-c06f-410c-b3aa-804e7d524919'), (16030, 'ebff3b8e-d84e-42ab-8015-0e9145257735'), (16031, 'aeac043d-c603-454d-af9a-5ddd34d267ff'), (16032, '25002d66-8e51-4416-9eb2-004e71920083'), (8862, '4d484846-68e1-4cdd-a00e-12f8d831ede6'), (691, '49245ea8-a50c-409d-b767-70947845114d'), (15035, '444de721-5003-4b34-b2ac-29d99446ed6d'), (28860, 'fc3861d9-1d07-4c58-8b32-b8d616e14ea8'), (16063, '8ad3264d-4aeb-4378-b88e-742db7f1f60e'), (16068, '64eb0235-54e3-46e8-8476-1e8f906dcb4a'), (16069, '7a7a5af8-2336-4669-8e94-96bd57756d1e'), (17617, 'd0201a2e-3af0-47a5-98e7-e13defa7db4f'), (5842, '3b64b1f4-3187-44e6-a1b6-2814799742b2'), (9944, 'f9c31b62-8597-4734-84b4-7271f301abf9'), (8409, '1d84b64e-b7fb-47f9-92da-01b3136aab9f'), (8410, 'e04c7e22-4ce3-453a-9e06-072b3f1e61a9'), (8412, 'c6e94ad4-25b0-40fc-b263-c3ef240eadaf'), (9950, '3570da80-429e-4059-b1b0-db0dddb1ae97'), (8414, 'c2dae037-83d7-44d2-93f3-04b963e26181'), (24288, 'c3902cfe-af0d-4a07-be3e-582a995a11d9'), (20707, '4137788c-30a9-4ed6-ae8a-a3aafdddeaa9'), (20710, 'e8fb3276-9241-44ba-823f-99429260845e'), (24294, 'e68eef33-0ed9-4128-9982-8c3bfa7b61a4'), (13031, '1e80914c-2c76-4899-8a68-b2cc81f46ab7'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (20717, 'ceb57f22-5344-4bc1-8c33-80214f9440a7'), (28412, 'e4a4eca5-75e1-47d3-939d-2d244aad4f31'), (11518, '4e23dd4d-152c-45d5-adac-2a4186f44eb6'), (11011, 'b27b277a-b81c-4200-a925-1d8fcbed65f1'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (6921, 'af36e482-b12e-4a16-9e35-707b46684e6f'), (23329, '9afe4111-7b48-4f69-87cf-d1e360f0331a'), (25898, '73ac5dcb-df39-4bca-8d7e-42637669ea82'), (7470, '0740883c-e4ff-4fe6-8a9f-e18323e0ffeb'), (6456, 'e67362e4-1b0d-4015-b7db-89d7fe57c072'), (23353, '209f08ab-4109-44e3-9822-fbb3609f60fd'), (6457, '416d0180-84a0-47ca-816b-e4151270b222'), (6458, '3f3603c0-3608-4809-94c7-59e7df3fdbb2'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (6460, 'dd57f5fb-c9db-400c-9dd7-7ad7abaf21cf'), (6461, 'b1f177f3-e93d-412b-b346-92ad1e300db7'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (6463, 'deb1abed-40b0-4c6f-ab05-30f0375d9840'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (833, '483d860d-236e-4772-b051-bfd68f59cd37'), (830, '4253d734-8fdc-48f0-ae58-86941ebeca66'), (831, '19f2bada-6be4-4b8f-b3be-f76b3343c3ef'), (836, '9ee86ea3-f33f-4cf5-9e92-0ad9009e33ba'), (837, '03afca3e-9150-4658-a4a2-8f97d1d0efea'), (838, 'b88fee3d-341d-4685-8fc6-fd51e1adf587'), (17219, '479d6420-b6e9-4e6a-8b01-6a5fcb584f14'), (841, '3ea3ba81-cfdc-4309-824d-56a17beadb1d'), (840, '41ff972f-57da-4a02-9c34-558a02677b72'), (29522, '2dbdf22e-6dcc-4d60-9147-de5777540b3b'), (21339, '1469ab23-b52f-4d01-bc97-7fe0192adb06'), (13664, '0d35fe7f-3409-414c-9082-1c9c87a329cf'), (13665, '714cb96e-9788-466a-9fd7-695ae8ff2daa'), (24937, '5ab51995-d45a-44ae-8339-d077a7afb102'), (24939, 'ff1da006-ddd2-47fd-a177-9f39ad77b0f3'), (3966, '917fd5a4-9386-49f9-aefc-d57c6d8086e1'), (23429, '4225cd93-c771-45ff-b9cb-f17095cd13ae'), (30600, '7a92eab2-a6c5-4eaa-855c-3274d0859388'), (31139, 'c427537c-949e-48fc-8920-05867afeb5bd'), (14266, '106a18ff-ab1c-4df8-834c-1e329ce83188'), (28603, '8bff17e1-a7a6-4040-a128-a5dc15c58900'), (14267, '1781aeaa-0203-4adb-996c-6d4d9c5ebc20'), (17854, '2d60ed6c-f861-4b5d-a717-b1586c86f4f8'), (18886, 'a01aef5d-3c56-4f8d-8854-e68353f4d189'), (7110, 'ace1a408-647b-4149-a9c1-b6a9a032d552'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (18894, '5e4d227e-fb3c-4f27-b506-bbb749f95b7c'), (18897, '313f3297-f3ec-453b-8026-a42752a46cb8'), (31186, '36d5c05d-8ab6-4eb3-b089-320dda12359a'), (18900, '25ff3ddc-5699-4d64-b402-594b1a76de22'), (29148, '9902217f-9c5b-4984-8f29-5aa3947e9f58'), (19962, '04858203-03cf-45bb-9b5a-7f506048c00c'), (19963, '631b24f7-6340-4e12-b47f-def9e1e4ba31')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: 4. Once you've identified the language(s) of the dataset, you can add the language tag(s) to the dataset card. You can do this by clicking the `Edit` button on the dataset card. This will open a PR to the dataset repo. You can add the language tag(s) to the `language` field in the dataset card. Some datasets may have multiple languages. Try and add all of the languages you have identified. 
5. Once done, open a PR on GitHub to update the table below. Once merged, this will count as a Hacktoberfest contribution! Add the `pr_url` (the one on the Hub) and a status (      , merged, closed) in the PR. 
6. Adding a language tag to some of the datasets below may not make sense. If so, add `not relevant` as the link in the `pr_url`. There may also be datasets where you need help with the language. In these cases, you can open a discussion to suggest a language tag(s) is added to the dataset.
```

If you want to work with a different pair of languages, you can specify them by their codes. A total of 92 languages are available for this dataset; you can see them all by expanding the language tags on its [dataset card](https://huggingface.co/datasets/kde4).

<img src=""https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter7/language_tags.png"" alt=""Language available for the KDE4 dataset."" width=""100%"">

Let's have a look at the dataset:

```py
raw_datasets
```

```python out
DatasetDict({
    train: Dataset({
        features: ['id', 'translation'],
        num_rows: 210173
    })
})
## F.A.Q.

### Does it make sense to add language metadata to all datasets?

No! This is why we have focused on datasets with a `task_categories` field indicating that the dataset has a text-related task. 

### Can I use a script to automate the process?

While it is possible to use machine learning to help assist this process, see [this blog](https://huggingface.co/blog/huggy-lingo) as an example; checking the accuracy of the PRs you are making is still important. 

## What about datasets with multiple languages?

Some datasets may have more than one language. Do your best to add all the languages you can identify in the datasets. If there is a vast number, this may be tricky. In this case, do your best. 

## What about code? 

Currently, you can add a language tag for `code`. You will need to do this directly in the `YAML` rather than the visual editor since using the visual editor will lead to an auto-completion for the `co` language code (Corsican).
Once we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:

- Grouping the predictions for each dataset by language: some datasets return predictions for multiple languages. We group these predictions by the language predicted i.e. if a dataset returns predictions for English and Dutch, we group the English and Dutch predictions together. 
- For datasets with multiple languages predicted, we count how many predictions we have for each language. If a language is predicted less than 20% of the time, we discard this prediction. i.e. if we have 18 predictions for English and only 2 for Dutch we discard the Dutch predictions. 
- We calculate the mean score for all predictions for a language. If the mean score associated with a languages prediction is below 80% we discard this prediction.
Once we have these predictions, we do some additional filtering to determine if we will accept the predictions as a metadata suggestion. This roughly consists of:

- Grouping the predictions for each dataset by language: some datasets return predictions for multiple languages. We group these predictions by the language predicted i.e. if a dataset returns predictions for English and Dutch, we group the English and Dutch predictions together. 
- For datasets with multiple languages predicted, we count how many predictions we have for each language. If a language is predicted less than 20% of the time, we discard this prediction. i.e. if we have 18 predictions for English and only 2 for Dutch we discard the Dutch predictions. 
- We calculate the mean score for all predictions for a language. If the mean score associated with a languages prediction is below 80% we discard this prediction.
 
Here is the question: What is the recommended way to specify multiple languages in a dataset?","According to the context, the recommended way to specify multiple languages in a dataset is to ""add all the languages you can identify in the datasets"" and do your best if there are a vast number of languages."
What is a key feature of the Hugging Face Spaces lifecycle management?,"[(7686, '2b16cbd7-ad89-4ea8-880c-cf28426cba7b'), (7692, '2785b812-48ad-476f-834c-8694f938e3dd'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (8214, '32b84ebe-7ad3-4f43-b0b4-10e91d4e73bf'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (4134, 'f04b64f1-9907-4796-9640-503b31bcf8bc'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (4651, '549c990e-829c-47dc-b812-56c33158d9e6'), (24120, 'e1160ee3-8307-4e5d-8ec4-13c38625d7d4'), (20542, '8adb84bf-218f-4821-ae67-8e8603665c9d'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8268, '5c473274-7a15-4d90-a38b-3d9ff3b2932d'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (8273, '5ebe427f-bfba-401b-bc93-31042f3611b1'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (31340, '1bffbde6-8c01-4f3d-914e-12799d8de47f'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (19574, 'cc66b1c9-e31b-4dbf-b777-a98a2c4143be'), (19578, 'c39dbfdb-a1e4-46cb-b486-736ca8e047bb'), (21629, '98fbe65d-63b8-42b8-b534-0cd39021b65d'), (21630, '0eb27be1-3764-4410-87fb-e87241b97118'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (25218, '5bb62dbf-06c4-4489-86c1-8079f5f045d0'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (25220, 'd9714992-eee7-4b14-bcef-6d8ebda275a0'), (25222, '7d646828-2a3a-4355-a7d1-93ae7129c81c'), (4237, 'aa571318-cb21-42dc-96d5-9247c29a9ac5'), (21652, '53aba63a-7c97-4993-a081-5d0b4d76c72d'), (21654, '0ac0c293-df2f-4bd7-ab9a-ef9c0cb31739'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (26263, '83b240db-acd6-4670-bc6f-1697ec7ed779'), (26264, 'caec1e8d-2ac9-463f-95cd-490112c00671'), (16026, 'eb0d3c1b-92d5-4525-9881-db94cc268624'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (26794, '1813d2fe-4f25-4ed3-a8ed-ca0946433c02'), (5820, 'd582f858-eb3c-4a7a-ae8b-bbeb029471fd'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (23747, '0f44acad-65aa-40fe-bf64-4c5dbe89d572'), (22726, '17648b7f-c5ad-4773-aed5-f6d926b5f8ef'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (16076, '86d0bad6-da7c-4137-84bc-4ab20f21d914'), (7374, 'f080a759-14b5-4d2f-a4f6-7d49cbf9c053'), (4304, '981e402a-1877-4823-aa48-d34b04dcb1e0'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (14050, '43174305-dba9-45e8-b46d-9ae2ea56ad48'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (23299, 'b99e8719-f73c-4e02-8e7a-b6e85b9d672f'), (16647, 'd95ce684-2d42-4dda-b614-f41463bfd8d8'), (19212, 'f81c878a-5077-441c-ac6a-73abd2c2f438'), (4365, '0e049057-3bcb-4ecb-afc0-901cd049382c'), (8462, '2c893bf2-57c0-475a-93c5-62cd31957575'), (31509, 'ce6de00d-9b9d-4ca9-b702-b073099a10d6'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (1314, 'c8ac2be7-0bee-4fd6-a3c6-83943745ed86'), (26405, '083107e3-5c6f-4e2d-ac5e-50717bdd56d6'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (31057, 'eee260cd-cb8d-4ea4-b824-020af026b061'), (10066, '9ad89ca7-ebff-4406-96ba-8fb0f9731e9f'), (29525, '4a7299ab-39a4-4b0e-8944-bf676cb6c96e'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (18779, 'c18e65b6-40a6-4f9c-92f5-cd5c1183e6d5'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (18781, '08b2126b-d583-4a12-a41c-9bde15e5a047'), (18783, '44d0b22a-e8ef-4cba-bda9-fd404cb063c2'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (18788, 'c8b88dcc-6e3b-4f12-9ccc-23fa571e9a77'), (18790, 'fa7f9b15-26f7-4210-9f64-6a48bf51d93f'), (2920, '7d5499ed-62d9-4e66-a95a-885d6d4a6801'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (1395, '849b10f5-640e-4db8-b849-b3c736e6c6b8'), (16764, '8e676bbc-b3e9-4463-a23d-1131ef9602fb'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (19335, 'e7a25af1-ba2a-4e2e-af04-f55deda54356'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8592, 'a70cadc2-d231-41c9-bda5-50c872176420'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (29595, '1bebe884-0ac0-484e-91b8-426d7d20769f'), (31133, 'e42f7993-643f-492e-a6db-29ed9c755b45'), (31135, '957d568e-e103-45a2-9bae-02e2fbebbc4a'), (16288, '93acb11b-2034-46a7-bd59-7e9638a345a4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (16287, 'c79b58a7-88e3-489e-a164-01632304341b'), (11172, 'dc0369d8-6a73-4ded-9638-59b1955d9efe'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (12215, '6104b507-3f50-45e2-8c07-00ea5df81149'), (31674, 'ec6fe210-04d1-48a0-adae-1f34c69c8c3b'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16322, 'ad245a5f-b243-4e34-b382-6e08fd6c4955'), (14790, '9b894d75-a0f9-4ce1-aa43-7ed77bb8059c'), (29135, 'aa204127-7dec-4d3b-b82a-c7d7aeb22e77'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9684, '6dd7dc4d-5495-42ee-9c11-238154ed3b6c'), (16345, 'b4ee1919-1113-4100-8961-3817f135ea6a'), (9689, 'adbdd4a5-7fd0-42c8-954f-afa15f31071c'), (25051, 'fbb30773-5b39-4fb4-97d1-0601de9e050e'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (25053, '827ffd19-fb84-486a-a2fd-cfcec1d13dbd'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (25054, '65144b6f-2178-4ab0-8fff-2cf2e8308593'), (25062, '11757b4b-973c-4c5b-965a-e745b9ab436c'), (15846, 'ee66a65a-c35d-4ed7-acbc-15fce331450b'), (15847, '5ec00348-4cb1-4058-99dc-6f4c0b0a7a3a'), (27113, '7fa375ce-c95f-40d1-b8d8-18b25b31ce3c'), (15863, 'e17826a6-d5b3-4729-ad97-387ae765122d'), (10232, '27e77ecf-9ddd-433f-bcda-08fa044e1cdc'), (10235, '6285ff51-c4b7-47f1-9c33-4cf592322ccc')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Security

The Hugging Face Hub offers several security features to ensure that your code and data are secure. Beyond offering [private repositories](./repositories-settings#private-repositories) for models, datasets, and Spaces, the Hub supports access tokens, commit signatures, and malware scanning.

Hugging Face is GDPR compliant. If a contract or specific data storage is something you'll need, we recommend taking a look at our [Expert Acceleration Program](https://huggingface.co/support). Hugging Face can also offer Business Associate Addendums or GDPR data processing agreements through an [Enterprise Plan](https://huggingface.co/pricing). 

Hugging Face is also [SOC2 Type 2 certified](https://us.aicpa.org/interestareas/frc/assuranceadvisoryservices/aicpasoc2report.html), meaning we provide security certification to our customers and actively monitor and patch any security weaknesses.
# Safety features and mechanisms

In addition, we provide a non-exhaustive - and hopefully continuously expanding! - list of safety features and mechanisms implemented by the Hugging Face team and the broader community.

* **[Community tab](https://huggingface.co/docs/hub/repositories-pull-requests-discussions)**: it enables the community to discuss and better collaborate on a project.

* **Tag feature**: authors of a repository can tag their content as being “Not For All Eyes”

* **Bias exploration and evaluation**: the Hugging Face team provides a [Space](https://huggingface.co/spaces/society-ethics/DiffusionBiasExplorer) to demonstrate the biases in Stable Diffusion and DALL-E interactively. In this sense, we support and encourage bias explorers and evaluations.
## 2. What is the Private Hub?

The [Private Hub](https://huggingface.co/platform) allows companies to use Hugging Face’s complete ecosystem in their own private and compliant environment to accelerate their machine learning development. It brings ML tools for every step of the ML lifecycle together in one place to make collaborating in ML simpler and more productive, while having a compliant environment that companies need for building ML securely:

<figure class=""image table text-center m-0 w-full"">
  <medium-zoom background=""rgba(0,0,0,.7)"" alt=""The Private Hub"" src=""assets/92_introducing_private_hub/private-hub.png""></medium-zoom>
  <figcaption>The Private Hub</figcaption>
</figure>
Using Hugging Face Integrations

Related spaces: https://huggingface.co/spaces/gradio/en2es
Tags: HUB, SPACES, EMBED

Contributed by <a href=""https://huggingface.co/osanseviero"">Omar Sanseviero</a> 🦙

## Introduction

The Hugging Face Hub is a central platform that has hundreds of thousands of [models](https://huggingface.co/models), [datasets](https://huggingface.co/datasets) and [demos](https://huggingface.co/spaces) (also known as Spaces). 

Gradio has multiple features that make it extremely easy to leverage existing models and Spaces on the Hub. This guide walks through these features.


## Demos with the Hugging Face Inference API
![You can now find interactive demos under ArXiv papers](/blog/assets/arxiv/recording.gif)

Since its launch in October 2021, Hugging Face Spaces has been used to build and share over 12,000 open-source machine learning demos crafted by the community. With Spaces, Hugging Face users can share, explore, discuss models, and build interactive applications that enable anyone with a browser to try them out without having to run any code. These demos are built using open-source tools such as the Gradio and Streamlit Python libraries, and leverage models and datasets available on the Hugging Face Hub.
 
Here is the question: What is a key feature of the Hugging Face Spaces lifecycle management?","A key feature of the Hugging Face Spaces lifecycle management is that it enables users to share, explore, discuss models, and build interactive applications that allow anyone with a browser to try them out without having to run any code."
Why is it important to add language metadata to Hugging Face datasets?,"[(10244, '4dad0a4e-90e6-40d3-95d9-0d9d0fabaf87'), (7687, '20608e74-1307-46cf-9341-09115b2bb0ac'), (10249, '3e5c5c76-fcb2-414c-af88-386161761d6c'), (7178, '086ecf90-fb2d-4fb0-9e37-23a45c3703ea'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (9229, '3fc24bea-73cc-4368-8756-dd115fb5bff3'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (23059, 'de332ba3-fd9d-44d5-89a5-5c18bdb57ded'), (23060, '09e43017-f9b1-44a7-918e-57bd10790d6c'), (23061, '1fa89fbd-1f12-41ad-a09d-613f63f32b6b'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (23064, '80eb8bc8-3947-409c-bc93-d0bbe04d5069'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (3603, '868a7661-c119-493e-86ac-54156f0efe17'), (23067, 'c4124e63-351d-48b1-ad95-32c2212f2547'), (16924, '38fe9939-2e02-47cc-805a-7ffc8340c582'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23593, 'c4cad64a-26a2-4318-b2bf-37b79d234f09'), (28718, '20436458-7cd8-4a96-8331-99513560667b'), (29233, '74f4f91f-2db4-42a8-a508-33e42d02d65f'), (7224, '49503e5a-ba18-43c6-9722-173bbeee8dd9'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (8270, '65aad682-e7bd-4469-bdb0-c24507f7429a'), (10834, 'b00c2c89-40fc-4f4d-be75-34c5d5ad8527'), (24157, 'befedd3e-173a-4c86-9a3f-4d163de16f99'), (7265, '1f8d7093-2dc9-42bf-b41c-da9adb3b5b15'), (22122, 'c2d9224d-5d18-46b0-aa9f-c19bccddbedb'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (13448, 'c265cc67-92bb-48f5-bfd6-5569ba9863df'), (5776, '8001f35a-9ae7-4b87-b42a-486643837817'), (8849, '2a477869-a976-4084-ac9a-617d85715013'), (26773, '36a72c91-d9e1-4a83-8b43-432e315d3dd5'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (676, 'afe87c20-9db7-4741-8b99-6b4b97e7c8ab'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (8914, '58a9ff1e-5e03-4ec8-b27c-7c29e647028c'), (29396, 'ae9e7b29-ed42-49c7-b73a-a36298272694'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (9953, 'fde58ef7-3ca9-4340-9482-22bd56d64b93'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (18176, '8a8f4414-5efa-401a-908e-15f6d9d435eb'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (13593, '79e309a1-7373-4306-8dcc-5a0b96bbcdd8'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (18208, 'ed351b9d-dca9-43b4-8cf0-18b5565f1c16'), (292, '085f6a0c-e881-4e0d-a875-1020c3bfd069'), (296, 'da61bbf7-b3d9-4a4d-aeb4-69794ca76f5c'), (7988, 'adcc6191-aa19-4a9a-8c82-79132357a034'), (7989, '7bf24f34-1a48-44c6-b805-592b4c1b9e1d'), (6456, 'e67362e4-1b0d-4015-b7db-89d7fe57c072'), (13625, '667770fb-10fe-4dec-9624-a7340f2ab8e9'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (830, '4253d734-8fdc-48f0-ae58-86941ebeca66'), (831, '19f2bada-6be4-4b8f-b3be-f76b3343c3ef'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (833, '483d860d-236e-4772-b051-bfd68f59cd37'), (6461, 'b1f177f3-e93d-412b-b346-92ad1e300db7'), (324, '51076545-2d78-470a-bf6f-3b09cd577406'), (837, '03afca3e-9150-4658-a4a2-8f97d1d0efea'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (9559, 'a8284fc7-f6b2-4800-baa1-e5b290b17b91'), (3423, '520fda98-9420-47d5-b84e-0ec5aa430f25'), (19808, '5213e06f-bf0a-45be-86be-92a7b345964b'), (13663, 'acd04aa2-38f6-4dab-a7fb-09290536800b'), (9068, '7bd47481-c524-4925-8bb2-43c9c5ea4f24'), (15217, '77662209-3d2c-443f-8e8c-3085e7ecc6bf'), (10107, '6591d938-30d7-44b7-b703-302422817c28'), (15741, 'e52c1082-1985-46a1-afec-26024efe0f42'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (23434, '39b45c30-9548-4a32-9f31-7e7d0dbfdc4f'), (8589, 'fd205c43-7584-4abc-8190-0b9f2c04c9b0'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8590, 'd83fb1f9-3d9c-409e-a570-5a3a708f8463'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (9112, '06791d3f-d0a1-4363-bfe4-0f0be4fc08fd'), (29594, '2ddce748-eeea-4063-8729-d2bba4c9edf4'), (9117, '74400b1c-c3d4-4221-bf3d-d615c6862e50'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (29603, 'e6eee1c6-18dd-4dbf-b041-ce7d0504ed72'), (18362, '1d5841b4-8f33-4b82-982a-3dea413e5ec4'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (6079, '2234d568-f21c-4b71-b407-a24d00ca3c01'), (5568, 'e66cf99b-05db-4fa3-a2e9-a6209b0db750'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (19916, 'e7d06c45-10f0-4bd2-8d3f-e2478dd9da51'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (10198, '19eb7655-1d17-42bc-985e-b1a424722b72'), (2527, 'b35cdd4c-87c8-40bf-98a7-98cf594734a7'), (18406, '09e73a09-80b2-4ec3-b425-16cbd059b289'), (18411, '4be73c5f-f784-4448-8d45-1061cddc185a'), (15851, '0bde2ce9-28b0-4ae0-9459-a572cca7f941'), (5103, '0606953a-4aeb-4ca3-b14c-885e315fb1d4'), (5108, '0e1c7ec4-b9e7-4964-bac6-22f4455d6568'), (10740, '984fc3af-ce77-41f0-92f6-4ca2280f65b7'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: #### Why is Language Metadata Important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. 

Many people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.
*The percent of datasets which have language metadata. True indicates language metadata is specified, False means no language data is listed. No card data means that there isn't any metadata or it couldn't be loaded by the `huggingface_hub` Python library.*


#### Why is language metadata important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows.
#### Next Steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
#### Next steps 

As the number of datasets on the Hub grows, metadata becomes increasingly important. Language metadata, in particular, can be incredibly valuable for identifying the correct dataset for your use case.

With the assistance of the Datasets Server and the [Librarian-Bots](https://huggingface.co/librarian-bots), we can update our dataset metadata at a scale that wouldn't be possible manually. As a result, we're enriching the Hub and making it an even more powerful tool for data scientists, linguists, and AI enthusiasts around the world. 

As the machine learning librarian at Hugging Face, I continue exploring opportunities for automatic metadata enrichment for machine learning artefacts hosted on the Hub. Feel free to reach out (daniel at thiswebsite dot co) if you have ideas or want to collaborate on this effort!
## F.A.Q.

### Does it make sense to add language metadata to all datasets?

No! This is why we have focused on datasets with a `task_categories` field indicating that the dataset has a text-related task. 

### Can I use a script to automate the process?

While it is possible to use machine learning to help assist this process, see [this blog](https://huggingface.co/blog/huggy-lingo) as an example; checking the accuracy of the PRs you are making is still important. 

## What about datasets with multiple languages?

Some datasets may have more than one language. Do your best to add all the languages you can identify in the datasets. If there is a vast number, this may be tricky. In this case, do your best. 

## What about code? 

Currently, you can add a language tag for `code`. You will need to do this directly in the `YAML` rather than the visual editor since using the visual editor will lead to an auto-completion for the `co` language code (Corsican).
 
Here is the question: Why is it important to add language metadata to Hugging Face datasets?","Language metadata is important to add to Hugging Face datasets because it allows users to filter and find relevant datasets by language, making it easier to identify the correct dataset for a specific use case, particularly as the number of datasets on the Hub grows."
Which storage tier includes 1TB persistent storage on Hugging Face Spaces?,"[(2564, '3a73dc87-8b43-4a47-8b88-b81353e11ee4'), (522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (7178, '086ecf90-fb2d-4fb0-9e37-23a45c3703ea'), (10252, 'becb7573-8a4c-4bbb-a76a-1e490d0c380a'), (14351, '95845780-ad50-4e5c-9930-8181afd6a90e'), (529, '499d1640-76a4-4875-8cda-382f9387d183'), (7697, 'c4d24c56-06a1-4e80-af53-2eee4d456e7c'), (26644, '8dcc5f64-c79e-4a29-a0cd-63e4d5733fec'), (31263, 'cc489e80-538c-40ce-97ac-46a1434bf730'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (4650, '9f8a8ae1-0210-4702-8855-2333e9d7bbc6'), (24107, '82cb282e-5cdc-4dfb-bec2-87783b659fff'), (4653, '8d3b2b54-b599-4810-8878-8b6b47d07133'), (4654, 'fbbddf5c-d10a-4009-8b85-ad5a1a8b7fd5'), (4655, 'e4aa3d7f-5924-49a6-a0ad-8d166dfd8d6b'), (4656, '8a5eb5ae-f89d-40e1-a550-28aee57cd98f'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (4660, '73eae357-d132-4d5a-b618-751f154c873a'), (24120, 'e1160ee3-8307-4e5d-8ec4-13c38625d7d4'), (20542, '8adb84bf-218f-4821-ae67-8e8603665c9d'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (16962, 'd0dd3ee1-c2dc-405e-bf10-7eeb166dfb48'), (21576, 'a1b358a4-02e1-45ef-b013-1d2c71ca6ef9'), (8273, '5ebe427f-bfba-401b-bc93-31042f3611b1'), (27737, 'dee318d8-f8a9-42d7-b9b9-c278fe8feb22'), (22625, 'e74be5ed-cda9-49e1-b334-8bf8fac7250a'), (22627, 'ba704efa-2360-43da-abbc-84431ca21063'), (22628, '68aa3ad9-1cbd-4912-889e-4cc2cf985a52'), (16998, '60e139ff-8758-4146-915e-e9dba50bef1e'), (6764, 'ef94109d-b9d2-451c-a1df-df044139048b'), (6766, 'e35d1260-f910-44b2-a82d-648bc59cb455'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (11389, '7cf525f3-0375-4371-b8ed-2e4e97568b45'), (23693, '0d5d3ec6-c5c1-44de-968a-8c64c66540aa'), (23699, 'c0fe8306-8d27-4615-89eb-a462dcb97b0b'), (21652, '53aba63a-7c97-4993-a081-5d0b4d76c72d'), (21653, '15ea1eb2-83fd-4842-9c19-9aa3b776051e'), (21654, '0ac0c293-df2f-4bd7-ab9a-ef9c0cb31739'), (21655, '8b214d8f-e1d5-414d-8e14-431a64b22550'), (21656, '151f3b8f-ca80-423f-9292-0d7506ae54ee'), (21657, 'caaddc26-338c-4b5f-89ab-bc044b94afe4'), (26262, 'f76066ba-3f64-4ff2-a4e4-5c073923d2ad'), (26264, 'caec1e8d-2ac9-463f-95cd-490112c00671'), (26263, '83b240db-acd6-4670-bc6f-1697ec7ed779'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17062, 'af2f3d2d-c7f5-4e58-badb-81b09e9828e6'), (26794, '1813d2fe-4f25-4ed3-a8ed-ca0946433c02'), (18612, 'a8261981-bcba-477a-b826-20a39b31cdb1'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (22211, '740dfc56-1f28-4364-853c-997c6873e87b'), (20677, '1adf2aac-a190-4ded-9f1f-238094bc7e6a'), (7375, '176b46b9-705a-4292-83b5-db9712ae6e7d'), (4305, '749aedbd-715c-412e-9e09-8522456a90cf'), (7380, '382d60ff-b0b0-4263-b63e-5c400149754f'), (24281, 'f039e51e-20c9-4ccc-ad49-d62358997882'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (24801, '3c1920b9-21c1-444c-9534-66369d767873'), (14050, '43174305-dba9-45e8-b46d-9ae2ea56ad48'), (6387, '621b0de6-a81d-4e75-97c1-881bb279e445'), (6393, 'f4416fbc-8a38-4b97-823b-eb555298d294'), (6395, '5e8d3f79-8a6c-46cb-ab1a-392e0a7216c0'), (12028, 'da0f499e-0b89-4a96-8974-e7bbb9af3fff'), (21755, 'e6693759-5af4-4449-9f6f-4c0e45498df9'), (6396, '2b52bdc8-be46-45a9-8734-72b2f801bd97'), (25344, 'f8add13c-024b-46b5-8ef9-5d969e517c5f'), (23307, '049fb1dc-f252-4e18-8434-cf336665efb8'), (8462, '2c893bf2-57c0-475a-93c5-62cd31957575'), (31509, 'ce6de00d-9b9d-4ca9-b702-b073099a10d6'), (10232, '27e77ecf-9ddd-433f-bcda-08fa044e1cdc'), (28444, '824ef0c8-0314-43e6-aa22-d7e9b03f5eab'), (12064, 'ec49857b-c395-49bb-b24a-dba896fff9fc'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (26405, '083107e3-5c6f-4e2d-ac5e-50717bdd56d6'), (19255, '1934d99f-3b19-461c-9e38-2755e5a04cca'), (11581, '2601a0d2-947c-40f9-bc88-35febc4ce941'), (31057, 'eee260cd-cb8d-4ea4-b824-020af026b061'), (1873, '2aa9b326-5f5f-476e-9fa0-16899261bd78'), (27476, '2b91c605-23b1-43c5-834b-d375e139c3c5'), (29525, '4a7299ab-39a4-4b0e-8944-bf676cb6c96e'), (1877, 'c135d929-2f75-4a59-abbd-0179906e1233'), (31574, '09044f40-6e31-4206-bb91-b0bbf29725e0'), (1368, 'e916d251-892a-48f3-b1ee-a20883a609b6'), (1880, '5dd1bbcf-5404-4609-8379-214ff6113566'), (2903, '02ac7208-015c-4205-a0c1-ca04f4301241'), (2904, '0279ac4f-7cc9-4138-b042-a83175946360'), (18782, '21f56662-a723-4c21-9c12-87ff3d2dd356'), (14688, '5734183c-d827-494b-89d6-5be3d5abbbef'), (18786, '232e1906-b22b-41cf-a5c2-261cdcca5bbd'), (29540, '19de6ff0-223a-4874-9667-a75a00e40ca9'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (16760, 'd936a97e-a944-4271-bca8-ab614fbc7ca8'), (11643, '428a11f6-079f-4810-9107-b8d8deefc0de'), (16764, '8e676bbc-b3e9-4463-a23d-1131ef9602fb'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8582, 'e47b3eaf-b020-419e-ba0f-71396d25c043'), (15244, '4a6f18e1-5969-4c32-9403-d17ab6783ead'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (3984, '0b5c9103-9bd3-4af7-a1ec-16b33970a4bc'), (31133, 'e42f7993-643f-492e-a6db-29ed9c755b45'), (16287, 'c79b58a7-88e3-489e-a164-01632304341b'), (16288, '93acb11b-2034-46a7-bd59-7e9638a345a4'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (22959, '5be42903-bf52-4f5b-850a-b74653728727'), (29109, '85f546cc-b577-4720-b28e-1eecf6685f4e'), (12728, '4364e7f3-1794-496e-9a2f-a14bcae5a530'), (12220, '1c9c506f-1bca-47c9-9605-33530ec3299d'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (29119, 'df5b0070-b0ec-48f3-8ba7-2e67665f8fe6'), (16322, 'ad245a5f-b243-4e34-b382-6e08fd6c4955'), (3017, '2f2befe2-450f-4c02-8e39-5cfbb153b29b'), (29135, 'aa204127-7dec-4d3b-b82a-c7d7aeb22e77'), (9687, '1c746dff-5942-4382-8985-92b632b8b421'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (25061, 'a686f600-cfe9-467e-9d3e-91dc3239e164'), (26104, '4a06a5c0-dbc6-4c0f-b0d2-6662a7586296'), (3068, '7a88dfe2-08c8-4d9c-b5cc-6ea5a2775482')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: Disk usage on Spaces

Every Space comes with a small amount of disk storage. This disk space is ephemeral, meaning its content will be lost if your Space restarts or is stopped.
If you need to persist data with a longer lifetime than the Space itself, you can:
- [Subscribe to a persistent storage upgrade](#persistent-storage)
- [Use a dataset as a data store](#dataset-storage)

## Persistent storage

You can upgrade your Space to have access to persistent disk space from the **Settings** tab.


<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-storage-settings.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/spaces-storage-settings-dark.png""/>
</div>

You can choose the storage tier of your choice to access disk space that persists across restarts of your Space.
```

**Bonus: set a sleep time when creating or duplicating the Space!**

```py
>>> api.create_repo(
...     repo_id=repo_id,
...     repo_type=""space"",
...     space_sdk=""gradio""
...     space_hardware=""t4-medium"",
...     space_sleep_time=""3600"",
... )
```
```py
>>> api.duplicate_space(
...     from_id=repo_id,
...     hardware=""t4-medium"",
...     sleep_time=""3600"",
... )
```

**6. Add persistent storage to your Space**

You can choose the storage tier of your choice to access disk space that persists across restarts of your Space. This means you can read and write from disk like you would with a traditional hard drive. See [docs](https://huggingface.co/docs/hub/spaces-storage#persistent-storage) for more details.

```py
>>> from huggingface_hub import SpaceStorage
>>> api.request_space_storage(repo_id=repo_id, storage=SpaceStorage.LARGE)
```

You can also delete your storage, losing all the data permanently.
```py
>>> api.delete_space_storage(repo_id=repo_id)
<Tip warning={true}>
	WARNING: all data stored in the storage is lost when you delete it.
</Tip>

### Persistent storage specs

Here are the specifications for each of the different upgrade options:

| **Tier**        	| **Disk space** 	| **Persistent** 	| **Monthly Price** 	|
|------------------	|------------------	|------------------	|----------------------	|
| Free tier        	| 50GB            	| No (ephemeral)  	| Free!                	|
| Small           	| 20GB             	| Yes           	| $5                	|
| Medium         	| 150 GB           	| Yes           	| $25                	|
| Large         	| 1TB              	| Yes           	| $100                	|


### Billing

Billing of Spaces is based on hardware usage and is computed by the minute: you get charged for every minute the Space runs on the requested hardware, regardless of whether the Space is used.
If you are using Hugging Face open source libraries, you can make your Space restart faster by setting the environment variable `HF_HOME` to `/data/.huggingface`. Libraries like `transformers`, `diffusers`, `datasets` and others use that environment variable to cache any assets downloaded from the Hugging Face Hub. Setting this variable to the persistent storage path will make sure that cached resources do not need to be re-downloaded when the Space is restarted. 

<Tip warning={true}>
	WARNING: all data stored in the storage is lost when you delete it.
</Tip>

### Persistent storage specs

Here are the specifications for each of the different upgrade options:
## 🚀 Using the Default Configuration

By default, Label Studio is installed in Spaces with a configuration that uses
local storage for the application database to store configuration, account
credentials, and project information. Labeling tasks and data items are also held
in local storage. 

<Tip warning={true}>
Storage in Hugging Face Spaces is ephemeral, and the data you store in the default
configuration can be lost in a reboot or reset of the Space. Because of this,
we strongly encourage you to use the default configuration only for testing and
demonstration purposes.
</Tip>
 
Here is the question: Which storage tier includes 1TB persistent storage on Hugging Face Spaces?","According to the context, the ""Large"" storage tier includes 1TB of persistent storage on Hugging Face Spaces."
What type of metadata is essential for datasets on text-related tasks?,"[(29699, '02d9209a-70f1-48b5-ad6a-6d59dc638917'), (7, 'bb62d85a-4224-492b-9ffe-70d571ed09ab'), (9, 'ee938445-5e5d-4e33-b4a3-e17998c36b16'), (29198, '8ea8c261-753e-484c-a1ac-105ace4cab56'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (23059, 'de332ba3-fd9d-44d5-89a5-5c18bdb57ded'), (23060, '09e43017-f9b1-44a7-918e-57bd10790d6c'), (23061, '1fa89fbd-1f12-41ad-a09d-613f63f32b6b'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (26647, '307043a4-d01c-4f17-b903-84527c0970fc'), (26648, '8ebea2a2-316e-4a37-b27e-18b91e236b16'), (29208, '08e217db-995b-4a85-86dc-628e3c944409'), (29205, 'a20756dc-81e6-4b69-a9fe-3c8ed6eb6065'), (26651, 'f1b12a30-cfeb-4c01-a3a4-7f8d4640303f'), (23068, '17b19121-a9f3-446f-b249-3b1e64bae14f'), (23070, 'dbb25c0a-8cb6-4e62-9b53-c8a6dd06b4c4'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (29221, '107434a7-c27e-4a1a-84ee-3425afcdbae1'), (29222, 'f47e0b8c-3526-4297-b05b-4cb3e1e6c6f7'), (24103, '2016aa57-7123-47c2-83f4-4513165c66d5'), (29224, 'a1b49036-83a8-4545-96ac-718a57c76b4b'), (31272, 'e602f819-b5fe-4ff5-abe9-d1ffb9564448'), (28719, '348f9b74-e568-4574-9dab-2e6621e37239'), (25654, '16c3fe76-474d-4aea-bc36-a7ecb11025ec'), (29251, '45082001-5874-4c41-8477-97c24d5d9523'), (29252, '7191173d-f4d2-46ab-b63d-00b253aa0edc'), (29253, 'a4570957-b255-44da-8589-a79ffdd409f6'), (29254, 'fe2de998-7aa0-44df-9801-c50901c1341d'), (29255, '572553a1-920b-4c52-b0f4-705172927bd4'), (29256, 'e581785d-f30a-47db-99a4-eccdd13d7903'), (29258, '96d6327a-83bd-45cc-94fd-99b36c6daa76'), (29260, '8c369a9b-46f0-4598-9135-a83b597930c8'), (18008, '32028e68-0961-4234-a72b-6da78651215f'), (14430, 'df8628a2-0050-42f4-be97-ef5d21f207ed'), (22122, 'c2d9224d-5d18-46b0-aa9f-c19bccddbedb'), (22123, 'ae9711c0-d281-4415-96bc-797b9d148cfe'), (22124, 'e862ae05-e350-458c-893b-9b4a3d0b9f3f'), (22125, 'f1dfd275-a16a-43e0-b2b4-4ac58367492f'), (14463, 'f86c6b7f-b65b-41f1-80b2-18f5c43c04a6'), (13448, 'c265cc67-92bb-48f5-bfd6-5569ba9863df'), (14998, 'c6f5f084-481f-4240-8c31-b553669cf9a9'), (28842, '163b6ce7-27da-4720-b343-f8264ab1e5df'), (28846, 'a95944ca-cb3f-47e9-a1ca-670e4f9f9fea'), (29367, '8954b5d9-4dfb-4d14-868c-e726946b5055'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (7355, 'fadfe698-c03f-4512-b775-f152160d9589'), (7356, '7384d51b-c1f5-4e52-aee5-7401f6562bf2'), (7357, '499cf3de-6af7-46fa-8f4f-c2e3749c51b4'), (7360, '807315b1-5b45-4562-8520-9f2c38c17779'), (7362, '1a61fb4d-7bc2-4d57-912a-e4a6b36c38e2'), (7363, '6eab840d-d563-4d92-9258-b2d0caa8ebb4'), (7372, '05db145c-b971-4e2d-8142-5221e15155a6'), (5328, '7e28df2f-1656-43a9-b36b-adab211aeb45'), (7386, '1e75d6ef-eb2a-47b6-8bf4-a1960ff737f8'), (7389, 'b481fda8-b5de-4d04-8ec6-e36a9f1f4039'), (13024, '4f311bd3-b4c2-41d9-b7bb-86bed144f50d'), (7393, '69b3bd6a-2eb1-4f94-84eb-2ecd7239847c'), (20706, '42c858b3-1ec6-4b12-bc13-fa15f9ce7719'), (20707, '4137788c-30a9-4ed6-ae8a-a3aafdddeaa9'), (7906, '7999ef9b-7dbe-4ba5-80c0-751d5759e262'), (7401, 'acb2bd40-0e27-45a1-9c2b-e3ff1ab4509e'), (3817, '1398bd7b-be47-437b-a145-46035f0b8f6f'), (13036, 'f060113d-71e0-4537-be71-ea1b13869322'), (4338, '5ac0912f-7d51-4e4d-9d3a-9bfffe0799a1'), (20723, '5f9e0e0a-3540-40f1-a037-3ae49cc51c50'), (29429, 'f41b58ae-2ee2-4d7e-bd3c-b6cf97951859'), (28414, '0e69ac2d-e27f-4301-b639-f7070dc7a084'), (13060, '3e7d8f5c-48d0-4329-bfa0-0e62c2c0e607'), (19206, '6aa67b68-b3f9-498a-9628-f7c24a3f649b'), (6918, '7eab2d26-8d0f-488f-a208-919b2377f915'), (23303, 'c8340867-9c9b-4274-adb6-873aeaef3797'), (16141, 'aa084d5d-8025-4004-9e13-9e10abb5e17c'), (28433, '593c1267-44ab-4a34-8d86-ac17d65ea9d6'), (7957, 'e79d5025-a9c4-432e-b94c-57adc02cbeba'), (7958, '310794e6-f08f-43e1-8263-12a3868fa63c'), (21788, '76f89282-5e29-480a-9fb9-107b45af559d'), (8989, '28551de5-f06d-4b91-b2c9-f1fdcc62c353'), (22304, '3897446b-d474-4a3d-9db5-5b91924cce5e'), (2864, 'bd4a8a28-05e5-486b-8aff-f74ae7a91a34'), (2866, '1190142f-1663-4e32-8d95-5f3b34034875'), (7989, '7bf24f34-1a48-44c6-b805-592b4c1b9e1d'), (28981, 'bbc96ca5-bfa6-4390-86cd-17eaa483294a'), (28983, '18f9eb3d-e00b-4c5d-bfa9-baab63fa1da0'), (15672, '3c536d00-c1fc-410b-a634-96a25e8778ee'), (6457, '416d0180-84a0-47ca-816b-e4151270b222'), (6456, 'e67362e4-1b0d-4015-b7db-89d7fe57c072'), (5435, '968cfb3d-c68c-49d0-abe9-61d153b093cd'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (6461, 'b1f177f3-e93d-412b-b346-92ad1e300db7'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (831, '19f2bada-6be4-4b8f-b3be-f76b3343c3ef'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (17219, '479d6420-b6e9-4e6a-8b01-6a5fcb584f14'), (838, 'b88fee3d-341d-4685-8fc6-fd51e1adf587'), (841, '3ea3ba81-cfdc-4309-824d-56a17beadb1d'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (11600, 'a85867dd-5b33-46bb-b6fb-4b4dc34dc237'), (15217, '77662209-3d2c-443f-8e8c-3085e7ecc6bf'), (23429, '4225cd93-c771-45ff-b9cb-f17095cd13ae'), (8590, 'd83fb1f9-3d9c-409e-a570-5a3a708f8463'), (24979, '604b588d-5044-4d9e-8b53-b3031cd2bd65'), (22962, '38a34ff8-3c21-43cb-9ec6-82a26e80e07a'), (21945, 'bb2aad32-1962-44e9-8059-67c527f5ea2b'), (21950, 'fc8e6b4c-a3e5-48e0-ac05-10d685f842a8'), (21951, '358a38e1-9865-45a7-bb11-f4d56f194e8e'), (21952, '138ee00c-29cc-4a4a-b56c-0e17a9529580'), (7108, '7e8be3a4-3d9e-4572-b072-eaacb2c1a9a1'), (7109, '53a7c288-2301-40ab-9650-8970284b10e6'), (7110, 'ace1a408-647b-4149-a9c1-b6a9a032d552'), (16844, '14804510-d68b-43ef-b630-fea72e07be9a'), (1499, '1faf2c53-b46d-4aec-a233-dd712314dc97'), (1500, '958de4d0-417a-4556-b8cb-9391406533c5'), (1504, 'ec8d2cfe-1292-4728-ac5b-488dc0390e75'), (7136, '9f5a21ea-28b6-4869-9165-d6393b7e8814'), (1514, 'd61b2641-9e09-4c4e-8e1e-72599ec17422'), (15854, '517bf4ae-078f-4701-80c0-1091f4da6097'), (13817, 'f8a28592-da18-4073-9696-21473e18cb2c')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

This metadata contains essential information about your model or dataset for potential users. The license, for example, defines the terms under which a model or dataset can be used. Hub users can also use the fields defined in the `YAML` metadata as filters for identifying models or datasets that fit specific criteria.
## F.A.Q.

### Does it make sense to add language metadata to all datasets?

No! This is why we have focused on datasets with a `task_categories` field indicating that the dataset has a text-related task. 

### Can I use a script to automate the process?

While it is possible to use machine learning to help assist this process, see [this blog](https://huggingface.co/blog/huggy-lingo) as an example; checking the accuracy of the PRs you are making is still important. 

## What about datasets with multiple languages?

Some datasets may have more than one language. Do your best to add all the languages you can identify in the datasets. If there is a vast number, this may be tricky. In this case, do your best. 

## What about code? 

Currently, you can add a language tag for `code`. You will need to do this directly in the `YAML` rather than the visual editor since using the visual editor will lead to an auto-completion for the `co` language code (Corsican).
```

Some datasets have a metadata file (`metadata.csv`/`metadata.jsonl`) associated with it, containing other information about the data like bounding boxes, text captions, and labels. The metadata is automatically loaded when you call [`load_dataset`] and specify `imagefolder`. 

To ignore the information in the metadata file, set `drop_labels=False` in [`load_dataset`], and allow `ImageFolder` to automatically infer the label name from the directory name:

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset(""imagefolder"", data_dir=""/path/to/folder"", drop_labels=False)
```

Any additional information about your dataset, such as text captions or transcriptions, can be included with a `metadata.csv` file in the folder containing your dataset. The metadata file needs to have a `file_name` column that links the image or audio file to its corresponding metadata:

```
file_name, text
bulbasaur.png, There is a plant seed on its back right from the day this Pokémon is born.
charmander.png, It has a preference for hot things.
squirtle.png, When it retracts its long neck into its shell, it squirts out water with vigorous force.
#### Why is Language Metadata Important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. 

Many people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.
 
Here is the question: What type of metadata is essential for datasets on text-related tasks?","According to the context, language metadata is essential for datasets on text-related tasks."
What should you do if you encounter a dataset without a `language` field on the Hugging Face Hub?,"[(522, '6553b6f5-756d-45a8-b724-9f6a3d277ab5'), (9229, '3fc24bea-73cc-4368-8756-dd115fb5bff3'), (23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (23058, 'a4d8381f-b648-4cb3-8e2c-e018c2920d6d'), (23059, 'de332ba3-fd9d-44d5-89a5-5c18bdb57ded'), (23060, '09e43017-f9b1-44a7-918e-57bd10790d6c'), (23061, '1fa89fbd-1f12-41ad-a09d-613f63f32b6b'), (23062, '3d47aeaa-9435-4a71-9bd1-373b4c46d1d5'), (23063, '21ce1de6-8b94-480c-b290-80086a22a5ce'), (23064, '80eb8bc8-3947-409c-bc93-d0bbe04d5069'), (3604, 'bde882ae-0d0c-42ed-8b25-d32e08f40213'), (3603, '868a7661-c119-493e-86ac-54156f0efe17'), (23067, 'c4124e63-351d-48b1-ad95-32c2212f2547'), (23065, 'd98cc50f-6b46-43db-a019-c4f0aea3d979'), (23071, 'c0f8c0fb-b5a3-4eff-a409-35b15499e248'), (23072, '5bf8f7c3-1776-4645-b177-4a117f9f6546'), (16932, 'd165fea3-524c-4a1f-8627-923c16bdcb4f'), (19504, 'eccd285a-5b4d-4f9b-8210-3849515ea05b'), (19505, '9144c324-5a19-484b-9c67-cabcc0fcd676'), (54, '263c9d8a-6161-4418-a170-ac626dd34d16'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (29261, '1715bea8-d32b-4d78-b7b9-eff4094c9c6d'), (7269, '2203fa03-15b5-4a03-87c9-1ad09aa9d60a'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (4741, '210088c4-7fef-45fb-b3bd-ba5759a63329'), (11398, 'c896d600-5e2c-467c-87a5-e393c3572337'), (24727, '710d9250-ca94-45f1-a999-cbc01b7e8ff7'), (28841, '08c4156c-7ac1-46ba-8255-8313ee6f94ad'), (22704, 'd853fcd0-1387-4353-a361-a5295f36b273'), (9392, 'bc313c9e-ffd4-42b3-a182-a8cb7ff59637'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (9929, 'e5cfe006-3715-4630-b46b-ac0255b0d611'), (5327, '7bb56183-8dbd-4678-987c-44526c61ed91'), (11991, '9d582505-d546-4763-b777-e7b72e601a79'), (24794, '84b45eec-287f-49cc-9cce-55e1f9b2d94b'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (24796, 'a8c2fdef-702e-471f-91ab-2a79e1f5196a'), (24797, '7519c0d4-aaad-47b8-8c58-f05f34ecab2d'), (9952, '7b5b02be-c8e3-4b66-9ea2-d73cf414e65f'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (24805, '5f103d70-5254-4011-b9dc-3f5b60ba28ab'), (14585, 'edd5d46a-6a2f-488c-9d46-fb5e5ac1bdf0'), (27401, 'fc2765b4-8cef-4dc3-bd32-9f1e7797bf34'), (15125, 'a5ca0ff7-e4da-44e0-afee-1c64442af9b1'), (11542, 'edf77ab2-8806-45f5-9134-1be8b92bc90b'), (26397, '377c1ab8-915a-4001-b5cc-4e1d9104282b'), (26400, 'b15c6db2-240c-4305-b2c5-4a2b61e2266d'), (26405, '083107e3-5c6f-4e2d-ac5e-50717bdd56d6'), (26920, 'd8327b6f-caf6-4e2d-8b17-b3f0db98ab72'), (7988, 'adcc6191-aa19-4a9a-8c82-79132357a034'), (2870, '8630c5af-b3e7-4d83-b67d-8cca87440b11'), (6456, 'e67362e4-1b0d-4015-b7db-89d7fe57c072'), (6457, '416d0180-84a0-47ca-816b-e4151270b222'), (6458, '3f3603c0-3608-4809-94c7-59e7df3fdbb2'), (6459, '03e7da8b-b888-48b6-9c9a-344339298ec7'), (828, 'fdc20acd-6116-4b0d-9a5c-96cdd69956fc'), (829, 'c448c0d0-666e-4d26-935a-b8eca8bba6ec'), (830, '4253d734-8fdc-48f0-ae58-86941ebeca66'), (831, '19f2bada-6be4-4b8f-b3be-f76b3343c3ef'), (832, 'a073c323-eea9-452e-8e31-85dc933bc0a0'), (833, '483d860d-236e-4772-b051-bfd68f59cd37'), (6463, 'deb1abed-40b0-4c6f-ab05-30f0375d9840'), (6460, 'dd57f5fb-c9db-400c-9dd7-7ad7abaf21cf'), (835, '90be4731-3d87-4691-9bc4-3013cc2baba5'), (837, '03afca3e-9150-4658-a4a2-8f97d1d0efea'), (4936, '962fd286-918d-4a47-90ea-4f8faaac8dd3'), (4937, '24ec3f32-f79f-4d91-a838-361fdd56703f'), (842, 'f21b59d1-7d2f-4075-837d-da80b4a7fe9f'), (843, '1b47562c-2691-44fd-b242-ce6bbc2f26d0'), (15178, '98813955-fdd5-49a0-9a60-14d3ae8b1b65'), (15181, 'fb7c5366-41d1-41cb-9f21-6d3b57cf1e94'), (9550, '5e47a55e-951b-4891-8b5b-d94f557e4ff9'), (840, '41ff972f-57da-4a02-9c34-558a02677b72'), (10581, '7f1db1d7-f699-4dd2-911e-40beb0d3e5f3'), (10582, 'fbd408d3-05ec-49b9-9ae6-3abd0333c09e'), (13663, 'acd04aa2-38f6-4dab-a7fb-09290536800b'), (19808, '5213e06f-bf0a-45be-86be-92a7b345964b'), (25440, 'f344d73e-1edb-43a3-8bab-c235c533086c'), (13666, '8b9a35b3-46c1-4f42-8832-2fff472ad908'), (13665, '714cb96e-9788-466a-9fd7-695ae8ff2daa'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (15217, '77662209-3d2c-443f-8e8c-3085e7ecc6bf'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8580, '14576ef8-e3f0-4ac9-bcdc-c523b7589ded'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (8586, '90a0534c-f6eb-45e6-8c1b-b6c343f84cf8'), (8587, '5dfac67b-30fa-4e8d-b899-f2fb32fac46c'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (926, '51a45028-67c1-4cea-97db-c606cf4b19d4'), (25518, '1f847aca-c3cc-4179-894b-7deef51d2560'), (27568, '14a9a9da-5246-490f-b304-14943de48a9a'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16323, '28119abe-7500-4deb-8335-742d33c7a9eb'), (16324, '0cb7caef-995c-4aec-9c86-5e22705a71b6'), (21960, '0751be99-26b7-4515-b54b-8b6938941afe'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (12238, 'f8e22816-bacc-46f5-b195-df79d3ad9660'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (25060, 'ebc0d9b5-d75c-44b0-96ab-b8b885fc37f1'), (1520, '8bd88f51-ae69-41a7-9eb0-497d3d739088'), (15862, 'fc319cec-e0f0-4bac-8542-f9596dbfd54b'), (13814, 'd8be2885-e7f0-4d64-973b-c7c6e554c60d'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd'), (13819, 'b5bed38d-29b0-4d78-84a3-3a189d2995bc'), (13823, 'dcd89f7f-a8f9-4932-a255-d6c431d52cd9')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: #### Why is Language Metadata Important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows. 

Many people want to be able to find datasets for a particular language. One of the major barriers to training good open source LLMs for a particular language is a lack of high quality training data.
Datasets without language challenge

Related to https://github.com/huggingface/hub-docs/issues/986.

## Context

The Hugging Face Hub hosts hundreds of thousands of public models and datasets. These datasets and models cover a wide range of languages. One of the main ways in which it's possible to know what language a dataset is in is by looking at the `language` field in the dataset's [metadata](https://huggingface.co/docs/hub/datasets-cards#dataset-card-metadata)  section of the dataset card. 

```yaml
language: 
- ""List of ISO 639-1 code for your language""
- lang1
pretty_name: ""Pretty Name of the Dataset""
tags:
- tag1
- tag2
license: ""any valid license identifier""
task_categories:
- task1
Share a dataset using the CLI

At Hugging Face, we are on a mission to democratize good Machine Learning and we believe in the value of open source. That's why we designed 🤗 Datasets so that anyone can share a dataset with the greater ML community. There are currently thousands of datasets in over 100 languages in the Hugging Face Hub, and the Hugging Face team always welcomes new contributions!

Dataset repositories offer features such as:

- Free dataset hosting
- Dataset versioning
- Commit history and diffs
- Metadata for discoverability
- Dataset cards for documentation, licensing, limitations, etc.

This guide will show you how to share a dataset that can be easily accessed by anyone.

<a id='upload_dataset_repo'></a>

## Add a dataset

You can share your dataset with the community with a dataset repository on the Hugging Face Hub.
It can also be a private dataset if you want to control who has access to it.
*The percent of datasets which have language metadata. True indicates language metadata is specified, False means no language data is listed. No card data means that there isn't any metadata or it couldn't be loaded by the `huggingface_hub` Python library.*


#### Why is language metadata important?

Language metadata can be a vital tool for finding relevant datasets. The Hugging Face Hub allows you to filter datasets by language. For example, if we want to find datasets with Dutch language we can use [a filter](https://huggingface.co/datasets?language=language:nl&sort=trending) on the Hub to include only datasets with Dutch data. 

Currently this filter returns 184 datasets. However, there are datasets on the Hub which include Dutch but don't specify this in the metadata. These datasets become more difficult to find, particularly as the number of datasets on the Hub grows.
If you see that a dataset card is missing information that you are in a position to provide (as an author of the dataset or as an experienced user), the best thing you can do is to open a Pull Request on the Hugging Face Hub. To do, go to the ""Files and versions"" tab of the dataset page and edit the `README.md` file. We provide:

* a [template](https://github.com/huggingface/datasets/blob/main/templates/README.md)
* a [guide](https://github.com/huggingface/datasets/blob/main/templates/README_guide.md) describing what information should go into each of the paragraphs
* and if you need inspiration, we recommend looking through a [completed example](https://huggingface.co/datasets/eli5/blob/main/README.md)
 
Here is the question: What should you do if you encounter a dataset without a `language` field on the Hugging Face Hub?","If you encounter a dataset without a `language` field on the Hugging Face Hub, you should open a Pull Request on the Hugging Face Hub to add the missing information. To do this, go to the ""Files and versions"" tab of the dataset page and edit the `README.md` file."
What metadata key allows you to link models in a Hugging Face Space?,"[(23056, '8f4648ca-a4a4-4996-8479-8993102b475d'), (15890, 'cbf34ccf-5543-4ae0-b368-b288e3050fea'), (16928, '1dd8da94-9861-4d4b-ae5d-0182de0a33d2'), (23074, 'b90ebd68-7d33-45c0-964e-98b519bf2cad'), (13350, '9d7df366-6e30-4bbb-950e-a90c5315157d'), (28718, '20436458-7cd8-4a96-8331-99513560667b'), (29232, 'c484cde5-70df-4bc8-a3cf-e7d9c6ee1b05'), (13873, '4609e890-a953-4eeb-9807-70d1bf2102af'), (13874, '3aa09c5f-d782-4417-bf6d-9a16dc742ef1'), (29235, '28f83b83-b564-4757-a1e4-3c0a127d21bd'), (19506, 'a572266c-e157-486a-8c31-b3c8a9cc95aa'), (19505, '9144c324-5a19-484b-9c67-cabcc0fcd676'), (4665, 'fafcf21c-e376-460a-bb7c-dcca97430243'), (12349, '7e860e44-68e6-46fc-87c6-d2563f210b0f'), (15936, 'ab08450b-3b5c-4bbc-8c0e-355d1b29997d'), (15937, '41a1057c-4a53-4b62-9d5f-2844b1afc13f'), (10847, '1f618b96-d934-459a-8d73-f6712fdc6d88'), (4192, 'fc859fbf-f289-4177-a3a6-21b11cb22ad3'), (4194, '2be3b91a-ad57-44e7-9658-6cc5a9d5440f'), (8817, 'd035f52d-b604-4071-90ac-ba021dd6d527'), (23669, '2e5e89e9-a9c0-493d-abed-6fc2ed66162d'), (10871, 'b0710001-ecae-48d7-bda9-46f340ff6b69'), (19583, '3271f720-54f9-4355-97ef-9ef1c0ccbb9f'), (26752, '1a8162b3-1aae-4a02-bac6-14d43161c2a3'), (25217, '06764bf9-a3bf-4e19-8dcb-01b2fc9cca44'), (18562, 'a2ef1762-c095-4848-8ebf-8e757edde1ed'), (26755, '15ac8037-db85-4930-9b52-43c06b7d3944'), (13447, 'b6ea664c-f662-4b91-8338-b97b37b1fb7e'), (13448, 'c265cc67-92bb-48f5-bfd6-5569ba9863df'), (13449, '7e3bdf26-52e5-4da5-ad7f-689dd45b292f'), (13452, 'ca545bf0-ea1e-4d32-86dc-0edf038e192c'), (13454, 'e47afbdc-2c45-4282-90cf-dd7b46634edb'), (19603, '561460ee-7e32-4394-b0d3-d59f4b7790dc'), (23714, 'a2d69303-a006-44f0-8008-1accb10a3697'), (17059, '3adc41da-2910-463c-9c1e-da3d35e9c4de'), (17060, '5294839d-2278-426c-8b43-fdbcc40a2e8e'), (8365, 'a726a55f-fbbf-414c-a3a8-5a3c9cfc52fb'), (7353, '2c282087-a5a5-43ce-9cb9-6f7130ec2900'), (7355, 'fadfe698-c03f-4512-b775-f152160d9589'), (4284, 'f3e8ac0e-749a-41a1-998f-02f82594657b'), (24769, 'c670f053-0b3f-4df6-8feb-7d4a39872604'), (29892, 'f79b0a69-dca8-4c88-88f7-ceea7c21d7d5'), (22212, 'b8730661-3709-4c81-9282-f13b58437e96'), (16071, '93aceed9-5a7e-43e6-aec8-3c52e58768de'), (16074, 'cb0c2a53-3d93-46ef-9b2c-ae3839573cb1'), (16075, 'e006a9f1-56c6-4447-80be-9c170833baec'), (25805, 'fcaebe7b-c2ec-4995-829f-39de8f874270'), (24795, '9cda1edf-8807-4016-b1d6-f2c250b597d1'), (26846, 'c96e859b-559a-45e4-aafb-93a24d363941'), (24802, 'eadd1511-a222-47aa-bc42-d67dfec866db'), (19683, '5d3bfb1d-f892-4db6-8596-bd364235244a'), (22757, '02c5f13b-34f8-4850-8e1d-8d5e0c9efb32'), (7914, 'ac481bd3-263c-401d-be21-71719edf4142'), (24300, 'c928506c-c2de-4185-b9b1-b1614336090a'), (30970, 'b7c051b7-556a-4606-bcf5-1ebb45ec5918'), (21763, 'f7f8a47a-8cfc-471b-9303-d13bde775fdb'), (1301, '5137aed7-d723-4ecf-8874-26a7c9ecb926'), (1302, 'ed0a5235-53d9-41b4-8946-33b5eea88be0'), (28440, 'f186228b-4ecd-4f33-a849-39b930f0800a'), (1313, 'b11a4071-36fd-43a9-abab-d401c01108e1'), (17698, '9700c98e-489f-4726-b2a4-1bac1fdcc901'), (7989, '7bf24f34-1a48-44c6-b805-592b4c1b9e1d'), (16701, '0c48fb11-97b9-4772-81d5-7152331ac26b'), (29508, '64a851cb-4efd-41b1-810b-3523a623cc30'), (346, 'f122bb33-9f87-495b-8206-65b2fa827e0c'), (347, 'd614507c-0464-48b7-bad7-c25df4c56372'), (12125, '06567508-1f95-4e9d-baae-9365e7dfa392'), (350, '8edfb721-840b-4b1d-9036-65614d095d44'), (12126, 'aea223a7-0e70-47a2-afbf-19190ca43263'), (5473, 'a64fa658-3cee-48f1-9ae3-9adc2f584c26'), (358, '1f015812-8d62-4b47-a953-b29505c9c0e8'), (29039, '7c8dd49a-9985-440d-aedf-0be33d46eec0'), (22383, '5904e255-7dd4-42bc-be26-7f7e65fd7471'), (9585, '715cb55c-0da5-4173-9cda-be19fd80a62f'), (9586, 'ac497e83-e1fb-472c-8a03-d58b8335ce1f'), (1394, 'cc80a9fd-e2d0-47c4-a537-090984bcf982'), (1392, '443eb530-529b-4e24-a669-7b47cd529d41'), (9590, '38c79f24-cc78-4018-9bd6-856baf6207f4'), (19329, '466543a2-ecbd-47e2-a220-a877dffa1c9f'), (19330, '06bc4b79-264e-4176-8ccd-68c6f6bdfe54'), (8579, '7dbe22f4-7ee0-4107-8c9e-f0d4bdcfb4fd'), (8580, '14576ef8-e3f0-4ac9-bcdc-c523b7589ded'), (8581, '0d01d6f2-cc07-4333-ba78-71147c652f2f'), (19333, '7ba0b0f8-a75c-480d-b712-9a9b33d8f2e6'), (8583, '0e063cde-13e2-40cf-9097-04f15fe1aa86'), (19335, 'e7a25af1-ba2a-4e2e-af04-f55deda54356'), (8586, '90a0534c-f6eb-45e6-8c1b-b6c343f84cf8'), (6026, '88095e53-c92a-411c-90d2-5ae5ed5197f2'), (398, 'fde2258e-246e-422c-9d5c-18708c8b03d3'), (8594, 'eeebeeeb-3b21-429b-80dc-e88e6175436f'), (8597, 'bba784e8-4aab-462f-96d0-3ffdc8480c34'), (7574, '79aa1bc6-a469-409e-a817-48355035e840'), (7575, '6dd737a5-e6aa-4829-9727-ec0349230760'), (8598, '686821c3-dd80-4576-bf12-91cba3a1c79e'), (6042, '5b1abefc-94b9-4cfd-99ca-23843fd92410'), (31135, '957d568e-e103-45a2-9bae-02e2fbebbc4a'), (6560, 'a365c17f-f1f1-4912-a58d-76ed70672d37'), (13729, 'f12e8b9e-f989-4e46-94dc-eff321e58f88'), (14262, '58c49272-b785-477d-b7ee-8181d752747f'), (31675, '283a29eb-1bea-4a9e-9250-9d2375a74396'), (26044, '8f76898f-d973-4edd-85de-d497103fcbbd'), (9663, '9c0a242e-42d2-4ce2-813e-df97970c170a'), (16319, '3f64c3db-fe7d-4c58-9506-1df4f37aa265'), (6079, '2234d568-f21c-4b71-b407-a24d00ca3c01'), (16322, 'ad245a5f-b243-4e34-b382-6e08fd6c4955'), (16320, 'aceded5d-8291-4ec6-937b-4ffa5e060758'), (14790, '9b894d75-a0f9-4ce1-aa43-7ed77bb8059c'), (21961, '9452b199-0147-4e18-a00a-0d935ccc948f'), (15309, '933f27a6-ae03-469e-9282-e59c1e6731da'), (9683, '1a221a00-9976-4a2d-9b37-973760b5cfa3'), (9686, '0e12552c-c556-4f90-820f-f1774737c039'), (24024, '69ac2742-6a8f-4e63-a4cb-b469c11ea9a7'), (25052, 'ea504546-5e45-4a6f-9c71-cad824fb7ad3'), (6110, 'fc92e53f-45a3-4d39-8492-5c740b162fd6'), (25055, 'bde57117-c5e6-4564-81a4-4f13e9affc27'), (15342, '6b488267-10cb-4899-8876-aeebd1b6f856'), (15345, 'd28cdc9c-d295-4ed8-8f73-d0977f629967'), (10737, '7b724e54-21da-49d8-97e7-244d3ccd8246'), (10233, 'df7d81fa-1825-4778-9157-19e222ccefbd'), (29692, '483f8905-e181-45c3-a168-36850b8f4939'), (29693, '5c030cef-9902-408d-839b-2520cd0382a6')]","Using the information contained in the context, give a comprehensive answer to the question. Respond only to the question asked, response should be concise and relevant to the question. Provide answer based on all the source document when relevant.If the answer cannot be deduced from the context, do not give an answer. 
 Here is the context: ```

    *Note*: Here's an [overview on building demos on Hugging Face Spaces](./spaces-overview) and here are more specific instructions for [Gradio](./spaces-sdks-gradio) and [Streamlit](./spaces-sdks-streamlit). 

4. As soon as your Space is built, Hugging Face will detect that it is associated with the model. A ""Linked Models"" button should appear in the top right corner of the Space, as shown here: 

    ![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/linked-models.png)
    
    *Note*:  You can also add linked models manually by explicitly updating them in the [README metadata for the Space, as described here](https://huggingface.co/docs/hub/spaces-config-reference).


Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes 🤗
We will now create the model card. The card should match the expected Hugging Face Hub format: a markdown part and a metadata section, which is a `yaml` section at the top. The keys to the metadata section are defined [here](https://huggingface.co/docs/hub/models-cards#model-card-metadata) and are used for the discoverability of the models. 
The content of the model card is determined by a template that has a:
- `yaml` section on top for metadata (e.g. model license, library name, and more)
- markdown section with free text and sections to be filled (e.g. simple description of the model),
The following sections are extracted by `skops` to fill in the model card:
- Hyperparameters of the model,
- Interactive diagram of the model,
- For metadata, library name, task identifier (e.g. tabular-classification), and information required by the inference widget are filled.
<div class=""flex justify-center"">
<img class=""block dark:hidden"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/SSHClone2.png""/>
<img class=""hidden dark:block"" src=""https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/hub/SSHClone1.png""/>
</div>

## Linking Models and Datasets on the Hub

You can showcase all the models and datasets that your Space links to by adding their identifier in your Space's README metadata. To do so, you can define them under the `models` and `datasets` keys. In addition to listing the artefacts in the README file, you can also record them in any `.py`, `.ini` or `.html` file as well. We'll parse it auto-magically! 

Here's an example linking two models from a space:
And that's it! Your Space should appear in the Demo tab next to the paper on ArXiv in a few minutes 🤗

**Method 2: Linking a Related Model**

An alternative approach can be used to link Spaces to papers by linking an intermediate model to the Space. This requires that the paper is **associated with a model** that is on the Hugging Face Hub (or can be uploaded there)

1. First, upload the model associated with the ArXiv paper onto the Hugging Face Hub if it is not already there. ([Detailed instructions are here](./models-uploading))

2. When writing the model card (README.md) for the model, include a link to the ArXiv paper. It's good practice to include a full citation as well. You can see an example of a link and a citation on the [LayoutLM model card](https://huggingface.co/microsoft/layoutlm-base-uncased)
Get started [here](https://gradio.app/getting_started)

### Hugging Face Spaces

Hugging Face Spaces is a free hosting option for Gradio demos. Spaces comes with 3 SDK options: Gradio, Streamlit and Static HTML demos. Spaces can be public or private and the workflow is similar to github repos. There are over 2000+ spaces currently on Hugging Face. Learn more about spaces [here](https://huggingface.co/spaces/launch).

### Hugging Face Models

Hugging Face Model Hub also supports ONNX models and ONNX models can be filtered through the [ONNX tag](https://huggingface.co/models?library=onnx&sort=downloads)

## How did Hugging Face help the ONNX Model Zoo?
 
Here is the question: What metadata key allows you to link models in a Hugging Face Space?","According to the context, the metadata key that allows you to link models in a Hugging Face Space is the `models` key. You can define the models to link under this key in your Space's README metadata."
